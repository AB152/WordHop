A Graduate Course in Applied Cryptography
Dan Boneh and Victor Shoup
Version 0.6, Jan. 2023
Preface
Cryptography is an indispensable tool used to protect information in computing systems. It is
used everywhere and by billions of people worldwide on a daily basis. It is used to protect data at
rest and data in motion. Cryptographic systems are an integral part of standard protocols, most
notably the Transport Layer Security (TLS) protocol, making it relatively easy to incorporate
strong encryption into a wide range of applications.
While extremely useful, cryptography is also highly brittle. The most secure cryptographic
system can be rendered completely insecure by a single speciﬁcation or programming error. No
amount of unit testing will uncover a security vulnerability in a cryptosystem.
Instead, to argue that a cryptosystem is secure, we rely on mathematical modeling and proofs
to show that a particular system satisﬁes the security properties attributed to it. We often need to
introduce certain plausible assumptions to push our security arguments through.
This book is about exactly that: constructing practical cryptosystems for which we can argue
security under plausible assumptions. The book covers many constructions for diﬀerent tasks in
cryptography. For each task we deﬁne a precise security goal that we aim to achieve and then
present constructions that achieve the required goal. To analyze the constructions, we develop a
uniﬁed framework for doing cryptographic proofs. A reader who masters this framework will be
capable of applying it to new constructions that may not be covered in the book.
Throughout the book we present many case studies to survey how deployed systems operate.
We describe common mistakes to avoid as well as attacks on real-world systems that illustrate the
importance of rigor in cryptography. We end every chapter with a fun application that applies the
ideas in the chapter in some unexpected way.
Intended audience and how to use this book
The book is intended to be self contained. Some supplementary material covering basic facts from
probability theory and algebra is provided in the appendices. The book is divided into three parts.
• Part I develops symmetric encryption which explains how two parties, Alice and Bob, can
securely exchange information when they have a shared key unknown to the attacker. We
discuss data conﬁdentiality, data integrity, and the important concept of authenticated en-
cryption.
• Part II develops the concepts of public-key encryption and digital signatures , which allow
Alice and Bob to communicate securely, without having a pre-shared secret key.
• Part III is about cryptographic protocols, such as protocols for user identiﬁcation, key ex-
change, zero knowledge, and secure computation.
ii
A beginning reader can read though the book to learn how cryptographic systems work and
why they are secure. Every security theorem in the book is followed by a proof idea that explains
at a high level why the scheme is secure. On a ﬁrst read one can skip over the detailed proofs
without losing continuity. A beginning reader may also skip over the mathematical details sections
that explore nuances of certain deﬁnitions.
An advanced reader may enjoy reading the detailed proofs to learn how to do proofs in cryp-
tography. At the end of every chapter you will ﬁnd many exercises that explore additional aspects
of the material covered in the chapter. Some exercises rehearse what was learned, but many ex-
ercises expand on the material and present additional ideas that are not covered in the body. We
recommend that readers read through the exercises, even if they do not intend to solve them.
Status of the book
The current draft is mostly complete, although there are a few missing sections here and there.
Those sections, as well as the appendices, are forthcoming. We hope you enjoy this write-up. Please
send us comments and let us know if you ﬁnd typos or mistakes. We are very grateful to all the
readers who have already sent us comments.
Citations: While the current draft is mostly complete, we have not yet included citations and
references to the many works on which this book is based. Those will be coming soon and will be
presented in the Notes section at the end of every chapter.
Dan Boneh and Victor Shoup
Jan. 2023
iii
Contents
1 Introduction 1
1.1 Historic ciphers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2 Terminology used throughout the book . . . . . . . . . . . . . . . . . . . . . . . . . 1
I Secret key cryptography 3
2 Encryption 4
2.1 Shannon ciphers and perfect security . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2.1.1 Deﬁnition of a Shannon cipher . . . . . . . . . . . . . . . . . . . . . . . . . 4
2.1.2 Perfect security . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.1.3 The bad news . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
2.2 Computational ciphers and semantic security . . . . . . . . . . . . . . . . . . . . . . 13
2.2.1 Deﬁnition of a computational cipher . . . . . . . . . . . . . . . . . . . . . . 13
2.2.2 Deﬁnition of semantic security . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.2.3 Connections to weaker notions of security . . . . . . . . . . . . . . . . . . . 18
2.2.4 Consequences of semantic security . . . . . . . . . . . . . . . . . . . . . . . 22
2.2.5 Bit guessing: an alternative characterization of semantic security . . . . . . 25
2.3 Mathematical details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
2.3.1 Negligible, super-poly, and poly-bounded functions . . . . . . . . . . . . . . 28
2.3.2 Computational ciphers: the formalities . . . . . . . . . . . . . . . . . . . . . 29
2.3.3 Eﬃcient adversaries and attack games . . . . . . . . . . . . . . . . . . . . . 32
2.3.4 Semantic security: the formalities . . . . . . . . . . . . . . . . . . . . . . . . 34
2.4 A fun application: anonymous routing . . . . . . . . . . . . . . . . . . . . . . . . . . 35
2.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
2.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
3 Stream ciphers 45
3.1 Pseudo-random generators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
3.1.1 Deﬁnition of a pseudo-random generator . . . . . . . . . . . . . . . . . . . . 46
3.1.2 Mathematical details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
3.2 Stream ciphers: encryption with a PRG . . . . . . . . . . . . . . . . . . . . . . . . . 48
3.3 Stream cipher limitations: attacks on the one time pad . . . . . . . . . . . . . . . . 52
3.3.1 The two-time pad is insecure . . . . . . . . . . . . . . . . . . . . . . . . . . 53
3.3.2 The one-time pad is malleable . . . . . . . . . . . . . . . . . . . . . . . . . . 53
iv
3.4 Composing PRGs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
3.4.1 A parallel construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
3.4.2 A sequential construction: the Blum-Micali method . . . . . . . . . . . . . 59
3.4.3 Mathematical details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
3.5 The next bit test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
3.6 Case study: the Salsa and ChaCha PRGs . . . . . . . . . . . . . . . . . . . . . . . . 67
3.7 Case study: linear generators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
3.7.1 An example cryptanalysis: the linear congruential generator . . . . . . . . . 70
3.7.2 The subset sum generator . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
3.8 Case study: cryptanalysis of the DVD encryption system . . . . . . . . . . . . . . . 74
3.9 Case study: cryptanalysis of the RC4 stream cipher . . . . . . . . . . . . . . . . . . 77
3.9.1 Security of RC4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
3.10 Generating random bits in practice . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
3.11 A broader perspective: computational and statistical indistinguishability . . . . . . 82
3.11.1 Mathematical details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
3.12 A fun application: coin ﬂipping and bit commitment . . . . . . . . . . . . . . . . . . 88
3.13 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
3.14 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
4 Block ciphers 96
4.1 Block ciphers: basic deﬁnitions and properties . . . . . . . . . . . . . . . . . . . . . 96
4.1.1 Some implications of security . . . . . . . . . . . . . . . . . . . . . . . . . . 98
4.1.2 Eﬃcient implementation of random permutations . . . . . . . . . . . . . . . 101
4.1.3 Strongly secure block ciphers . . . . . . . . . . . . . . . . . . . . . . . . . . 101
4.1.4 Using a block cipher directly for encryption . . . . . . . . . . . . . . . . . . 102
4.1.5 Mathematical details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
4.2 Constructing block ciphers in practice . . . . . . . . . . . . . . . . . . . . . . . . . . 107
4.2.1 Case study: DES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
4.2.2 Exhaustive search on DES: the DES challenges . . . . . . . . . . . . . . . . 113
4.2.3 Strengthening ciphers against exhaustive search: the 3 Econstruction . . . . 115
4.2.4 Case study: AES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
4.3 Sophisticated attacks on block ciphers . . . . . . . . . . . . . . . . . . . . . . . . . . 122
4.3.1 Algorithmic attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
4.3.2 Side-channel attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
4.3.3 Fault injection attacks on AES . . . . . . . . . . . . . . . . . . . . . . . . . 130
4.3.4 Quantum exhaustive search attacks . . . . . . . . . . . . . . . . . . . . . . . 131
4.4 Pseudo-random functions: basic deﬁnitions and properties . . . . . . . . . . . . . . 132
4.4.1 Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
4.4.2 Eﬃcient implementation of random functions . . . . . . . . . . . . . . . . . 133
4.4.3 When is a secure block cipher a secure PRF? . . . . . . . . . . . . . . . . . 134
4.4.4 Constructing PRGs from PRFs . . . . . . . . . . . . . . . . . . . . . . . . . 138
4.4.5 Mathematical details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
4.5 Constructing block ciphers from PRFs . . . . . . . . . . . . . . . . . . . . . . . . . . 141
4.6 The tree construction: from PRGs to PRFs . . . . . . . . . . . . . . . . . . . . . . . 147
4.6.1 Variable length tree construction . . . . . . . . . . . . . . . . . . . . . . . . 151
4.7 The ideal cipher model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
v
4.7.1 Formal deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
4.7.2 Exhaustive search in the ideal cipher model . . . . . . . . . . . . . . . . . . 155
4.7.3 The Even-Mansour block cipher and the EX construction . . . . . . . . . . 158
4.7.4 Proof of the Even-Mansour and EX theorems . . . . . . . . . . . . . . . . . 159
4.8 A fun application: comparing information without revealing it . . . . . . . . . . . . 165
4.9 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
4.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
5 Chosen Plaintext Attack 177
5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
5.2 Security against multi-key attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
5.3 Semantic security against chosen plaintext attack . . . . . . . . . . . . . . . . . . . 181
5.4 Building CPA secure ciphers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
5.4.1 A generic hybrid construction . . . . . . . . . . . . . . . . . . . . . . . . . . 183
5.4.2 Randomized counter mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
5.4.3 CBC mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
5.4.4 Case study: CBC padding in TLS 1.0 . . . . . . . . . . . . . . . . . . . . . 199
5.4.5 Concrete parameters and a comparison of counter and CBC modes . . . . . 199
5.5 Nonce-based encryption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
5.5.1 Nonce-based generic hybrid encryption . . . . . . . . . . . . . . . . . . . . . 203
5.5.2 Nonce-based Counter mode . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
5.5.3 Nonce-based CBC mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
5.6 A fun application: revocable broadcast encryption . . . . . . . . . . . . . . . . . . . 205
5.7 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
5.8 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
6 Message integrity 215
6.1 Deﬁnition of a message authentication code . . . . . . . . . . . . . . . . . . . . . . . 217
6.1.1 Mathematical details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220
6.2 MAC veriﬁcation queries do not help the attacker . . . . . . . . . . . . . . . . . . . 220
6.3 Constructing MACs from PRFs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
6.4 Preﬁx-free PRFs for long messages . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
6.4.1 The CBC preﬁx-free secure PRF . . . . . . . . . . . . . . . . . . . . . . . . 226
6.4.2 The cascade preﬁx-free secure PRF . . . . . . . . . . . . . . . . . . . . . . . 229
6.4.3 Extension attacks: CBC and cascade are insecure MACs . . . . . . . . . . . 230
6.5 From preﬁx-free secure PRF to fully secure PRF (method 1): encrypted PRF . . . 231
6.5.1 ECBC and NMAC: MACs for variable length inputs . . . . . . . . . . . . . 232
6.6 From preﬁx-free secure PRF to fully secure PRF (method 2): preﬁx-free encodings . 235
6.6.1 Preﬁx free encodings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
6.7 From preﬁx-free secure PRF to fully secure PRF (method 3): CMAC . . . . . . . . 236
6.8 Converting a block-wise PRF to bit-wise PRF . . . . . . . . . . . . . . . . . . . . . 239
6.9 Case study: ANSI CBC-MAC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
6.10 Case study: CMAC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
6.11 PMAC: a parallel MAC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
6.12 A fun application: searching on encrypted data . . . . . . . . . . . . . . . . . . . . . 245
6.13 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
vi
6.14 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
7 Message integrity from universal hashing 252
7.1 Universal hash functions (UHFs) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252
7.1.1 Multi-query UHFs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
7.1.2 Mathematical details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
7.2 Constructing UHFs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
7.2.1 Construction 1: UHFs using polynomials . . . . . . . . . . . . . . . . . . . 255
7.2.2 Construction 2: CBC and cascade are computational UHFs . . . . . . . . . 258
7.2.3 Construction 3: a parallel UHF from a small PRF . . . . . . . . . . . . . . 260
7.3 PRF(UHF) composition: constructing MACs using UHFs . . . . . . . . . . . . . . . 262
7.3.1 Using PRF(UHF) composition: ECBC and NMAC security . . . . . . . . . 265
7.3.2 Using PRF(UHF) composition with polynomial UHFs . . . . . . . . . . . . 265
7.3.3 Using PRF(UHF) composition: PMAC 0 security . . . . . . . . . . . . . . . 266
7.4 The Carter-Wegman MAC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
7.4.1 Using Carter-Wegman with polynomial UHFs . . . . . . . . . . . . . . . . . 273
7.5 Nonce-based MACs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
7.5.1 Secure nonce-based MACs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274
7.6 Unconditionally secure one-time MACs . . . . . . . . . . . . . . . . . . . . . . . . . 275
7.6.1 Pairwise unpredictable functions . . . . . . . . . . . . . . . . . . . . . . . . 275
7.6.2 Building unpredictable functions . . . . . . . . . . . . . . . . . . . . . . . . 275
7.6.3 From PUFs to unconditionally secure one-time MACs . . . . . . . . . . . . 276
7.7 A fun application: timing attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276
7.8 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
7.9 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
8 Message integrity from collision resistant hashing 287
8.1 Deﬁnition of collision resistant hashing . . . . . . . . . . . . . . . . . . . . . . . . . 290
8.1.1 Mathematical details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
8.2 Building a MAC for large messages . . . . . . . . . . . . . . . . . . . . . . . . . . . 291
8.3 Birthday attacks on collision resistant hash functions . . . . . . . . . . . . . . . . . 293
8.4 The Merkle-Damg˚ ard paradigm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
8.4.1 Joux’s attack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298
8.5 Building Compression Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
8.5.1 A simple but ineﬃcient compression function . . . . . . . . . . . . . . . . . 299
8.5.2 Davies-Meyer compression functions . . . . . . . . . . . . . . . . . . . . . . 300
8.5.3 Collision resistance of Davies-Meyer . . . . . . . . . . . . . . . . . . . . . . 301
8.6 Case study: SHA256 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
8.6.1 Other Merkle-Damg˚ ard hash functions . . . . . . . . . . . . . . . . . . . . . 305
8.7 Case study: HMAC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
8.7.1 Security of two-key nest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308
8.7.2 The HMAC standard . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309
8.7.3 Davies-Meyer is a secure PRF in the ideal cipher model . . . . . . . . . . . 310
8.8 The Sponge Construction and SHA3 . . . . . . . . . . . . . . . . . . . . . . . . . . . 313
8.8.1 The sponge construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314
8.8.2 Case study: SHA3, SHAKE128, and SHAKE256 . . . . . . . . . . . . . . . 319
vii
8.9 Merkle trees: proving properties of a hashed list . . . . . . . . . . . . . . . . . . . . 320
8.9.1 Authenticated data structures . . . . . . . . . . . . . . . . . . . . . . . . . . 323
8.10 Key derivation and the random oracle model . . . . . . . . . . . . . . . . . . . . . . 324
8.10.1 The key derivation problem . . . . . . . . . . . . . . . . . . . . . . . . . . . 325
8.10.2 Random oracles: a useful heuristic . . . . . . . . . . . . . . . . . . . . . . . 327
8.10.3 Random oracles: safe modes of operation . . . . . . . . . . . . . . . . . . . 332
8.10.4 The leftover hash lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 334
8.10.5 Case study: HKDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335
8.11 Security without collision resistance . . . . . . . . . . . . . . . . . . . . . . . . . . . 336
8.11.1 Second preimage resistance . . . . . . . . . . . . . . . . . . . . . . . . . . . 336
8.11.2 Randomized hash functions: target collision resistance . . . . . . . . . . . . 337
8.11.3 TCR from 2nd-preimage resistance . . . . . . . . . . . . . . . . . . . . . . . 338
8.11.4 Using target collision resistance . . . . . . . . . . . . . . . . . . . . . . . . . 341
8.12 A fun application: commitments and auctions . . . . . . . . . . . . . . . . . . . . . 343
8.13 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347
8.14 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348
9 Authenticated Encryption 357
9.1 Authenticated encryption: deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . 358
9.1.1 One-time authenticated encryption . . . . . . . . . . . . . . . . . . . . . . . 359
9.2 Implications of authenticated encryption . . . . . . . . . . . . . . . . . . . . . . . . 360
9.2.1 Chosen ciphertext attacks: a motivating example . . . . . . . . . . . . . . . 360
9.2.2 Chosen ciphertext attacks: deﬁnition . . . . . . . . . . . . . . . . . . . . . . 362
9.2.3 Authenticated encryption implies chosen ciphertext security . . . . . . . . . 363
9.3 Encryption as an abstract interface . . . . . . . . . . . . . . . . . . . . . . . . . . . 365
9.4 Authenticated encryption ciphers from generic composition . . . . . . . . . . . . . . 367
9.4.1 Encrypt-then-MAC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
9.4.2 MAC-then-encrypt is not generally secure: padding oracle attacks on SSL . 369
9.4.3 More padding oracle attacks. . . . . . . . . . . . . . . . . . . . . . . . . . . 372
9.4.4 Secure instances of MAC-then-encrypt . . . . . . . . . . . . . . . . . . . . . 373
9.4.5 Encrypt-then-MAC or MAC-then-encrypt? . . . . . . . . . . . . . . . . . . 377
9.5 Nonce-based authenticated encryption with associated data . . . . . . . . . . . . . . 377
9.6 One more variation: CCA-secure ciphers with associated data . . . . . . . . . . . . 380
9.7 Case study: Galois counter mode (GCM) . . . . . . . . . . . . . . . . . . . . . . . . 381
9.8 Case study: the TLS 1.3 record protocol . . . . . . . . . . . . . . . . . . . . . . . . 383
9.9 Case study: an attack on non-atomic decryption in SSH . . . . . . . . . . . . . . . . 386
9.10 Case study: 802.11b WEP, a badly broken system . . . . . . . . . . . . . . . . . . . 389
9.11 A fun application: private information retrieval . . . . . . . . . . . . . . . . . . . . . 391
9.12 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 391
9.13 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 391
II Public key cryptography 398
10 Public key tools 399
10.1 A toy problem: anonymous key exchange . . . . . . . . . . . . . . . . . . . . . . . . 399
viii
10.2 One-way trapdoor functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 400
10.2.1 Key exchange using a one-way trapdoor function scheme . . . . . . . . . . . 401
10.2.2 Mathematical details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402
10.3 A trapdoor permutation scheme based on RSA . . . . . . . . . . . . . . . . . . . . . 403
10.3.1 Key exchange based on the RSA assumption . . . . . . . . . . . . . . . . . 405
10.3.2 Mathematical details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 405
10.4 Diﬃe-Hellman key exchange . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 406
10.4.1 The key exchange protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . 407
10.4.2 Security of Diﬃe-Hellman key exchange . . . . . . . . . . . . . . . . . . . . 407
10.5 Discrete logarithm and related assumptions . . . . . . . . . . . . . . . . . . . . . . . 408
10.5.1 Random self-reducibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411
10.5.2 Mathematical details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413
10.6 Collision resistant hash functions from number-theoretic primitives . . . . . . . . . . 414
10.6.1 Collision resistance based on DL . . . . . . . . . . . . . . . . . . . . . . . . 414
10.6.2 Collision resistance based on RSA . . . . . . . . . . . . . . . . . . . . . . . 415
10.7 Attacks on the anonymous Diﬃe-Hellman protocol . . . . . . . . . . . . . . . . . . . 417
10.8 Merkle puzzles: a partial solution to key exchange using block ciphers . . . . . . . . 418
10.9 A fun application: accumulators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 420
10.10 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 423
10.11 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 424
11 Public key encryption 434
11.1 Two further example applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435
11.1.1 Sharing encrypted ﬁles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435
11.1.2 Key escrow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435
11.2 Basic deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 436
11.2.1 Mathematical details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 437
11.3 Implications of semantic security . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 438
11.3.1 The need for randomized encryption . . . . . . . . . . . . . . . . . . . . . . 438
11.3.2 Semantic security against chosen plaintext attack . . . . . . . . . . . . . . . 439
11.4 Encryption based on a trapdoor function scheme . . . . . . . . . . . . . . . . . . . . 441
11.4.1 Instantiating ETDF with RSA . . . . . . . . . . . . . . . . . . . . . . . . . . 444
11.5 ElGamal encryption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445
11.5.1 Semantic security of ElGamal in the random oracle model . . . . . . . . . . 446
11.5.2 Semantic security of ElGamal without random oracles . . . . . . . . . . . . 448
11.6 A fun application: oblivious transfer based on Diﬃe-Hellman . . . . . . . . . . . . . 451
11.6.1 A secure OT from ElGamal encryption . . . . . . . . . . . . . . . . . . . . . 452
11.6.2 Adaptive oblivious transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . 453
11.6.3 Oblivious PRFs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454
11.6.4 A simple adaptive OT from an oblivious PRF . . . . . . . . . . . . . . . . . 456
11.7 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457
11.8 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458
ix
12 Chosen ciphertext secure public key encryption 467
12.1 Basic deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 467
12.2 Understanding CCA security . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 469
12.2.1 CCA security and ciphertext malleability . . . . . . . . . . . . . . . . . . . 469
12.2.2 CCA security vs. authentication . . . . . . . . . . . . . . . . . . . . . . . . 470
12.2.3 CCA security and key escrow . . . . . . . . . . . . . . . . . . . . . . . . . . 471
12.2.4 Encryption as an abstract interface . . . . . . . . . . . . . . . . . . . . . . . 472
12.3 CCA-secure encryption from trapdoor function schemes . . . . . . . . . . . . . . . . 474
12.3.1 Instantiating E′
TDF with RSA . . . . . . . . . . . . . . . . . . . . . . . . . . 479
12.4 CCA-secure ElGamal encryption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 479
12.5 CCA security from DDH without random oracles . . . . . . . . . . . . . . . . . . . 484
12.5.1 Universal projective hash functions . . . . . . . . . . . . . . . . . . . . . . . 484
12.5.2 Universal 2 projective hash functions . . . . . . . . . . . . . . . . . . . . . . 487
12.5.3 The ECS scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 488
12.6 CCA security via a generic transformation . . . . . . . . . . . . . . . . . . . . . . . 494
12.6.1 A generic instantiation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 499
12.6.2 A concrete instantiation with ElGamal . . . . . . . . . . . . . . . . . . . . . 499
12.7 CCA-secure public-key encryption with associated data . . . . . . . . . . . . . . . . 501
12.7.1 AD-only CCA security . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 502
12.8 Case study: PKCS1, OAEP, OAEP+, and SAEP . . . . . . . . . . . . . . . . . . . 503
12.8.1 Padding schemes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 504
12.8.2 PKCS1 padding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 504
12.8.3 Bleichenbacher’s attack on the RSA-PKCS1 encryption scheme . . . . . . . 505
12.8.4 Optimal Asymmetric Encryption Padding (OAEP) . . . . . . . . . . . . . . 508
12.8.5 OAEP+ and SAEP+ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 510
12.9 A fun application: private set intersection . . . . . . . . . . . . . . . . . . . . . . . . 511
12.10 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 512
12.11 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 512
13 Digital signatures 526
13.1 Deﬁnition of a digital signature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 528
13.1.1 Secure signatures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 529
13.1.2 Mathematical details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 532
13.2 Extending the message space with collision resistant hashing . . . . . . . . . . . . . 532
13.2.1 Extending the message space using TCR functions . . . . . . . . . . . . . . 533
13.3 Signatures from trapdoor permutations: the full domain hash . . . . . . . . . . . . . 534
13.3.1 Signatures based on the RSA trapdoor permutation . . . . . . . . . . . . . 536
13.4 Security analysis of full domain hash . . . . . . . . . . . . . . . . . . . . . . . . . . . 538
13.4.1 Repeated one-way functions: a useful lemma . . . . . . . . . . . . . . . . . 538
13.4.2 Proofs of Theorems 13.3 and 13.4 . . . . . . . . . . . . . . . . . . . . . . . . 543
13.5 An RSA-based signature scheme with a tight security proof . . . . . . . . . . . . . . 544
13.6 Case study: PKCS1 signatures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 546
13.6.1 Bleichenbacher’s attack on PKCS1 signatures . . . . . . . . . . . . . . . . . 548
13.7 Signcryption: combining signatures and encryption . . . . . . . . . . . . . . . . . . 549
13.7.1 Secure signcryption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 551
13.7.2 Signcryption as an abstract interface . . . . . . . . . . . . . . . . . . . . . . 554
x
13.7.3 Constructions: encrypt-then-sign and sign-then-encrypt . . . . . . . . . . . 556
13.7.4 A construction based on Diﬃe-Hellman key exchange . . . . . . . . . . . . . 560
13.7.5 Additional desirable properties: forward secrecy and non-repudiation . . . . 562
13.8 Certiﬁcates and the public-key infrastructure . . . . . . . . . . . . . . . . . . . . . . 566
13.8.1 Coping with malicious or negligent certiﬁcate authorities . . . . . . . . . . . 568
13.8.2 Certiﬁcate revocation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 571
13.9 Case study: legal aspects of digital signatures . . . . . . . . . . . . . . . . . . . . . . 573
13.10 A fun application: forward secure signatures . . . . . . . . . . . . . . . . . . . . . . 574
13.11 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 574
13.12 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 575
14 Fast hash-based signatures 583
14.1 Basic Lamport signatures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 584
14.1.1 Shrinking the signature using an enhanced TCR . . . . . . . . . . . . . . . 585
14.2 A general Lamport framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 586
14.2.1 An explicit containment free function . . . . . . . . . . . . . . . . . . . . . 588
14.3 Winternitz one-time signatures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 589
14.3.1 A domination free function for Winternitz signatures . . . . . . . . . . . . . 592
14.4 HORS: short Lamport signatures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 593
14.4.1 Shrinking the public-key using a Merkle tree . . . . . . . . . . . . . . . . . 594
14.5 Applications of one-time signatures . . . . . . . . . . . . . . . . . . . . . . . . . . . 595
14.5.1 Online/oﬄine signatures from one-time signatures . . . . . . . . . . . . . . 595
14.5.2 Authenticating streamed data with one-time signatures . . . . . . . . . . . 596
14.6 From one-time signatures to many-time signatures . . . . . . . . . . . . . . . . . . . 596
14.6.1 Indexed signatures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 597
14.6.2 A many-time signature scheme from an indexed signature . . . . . . . . . . 598
14.6.3 The complete Merkle stateless signature system . . . . . . . . . . . . . . . . 600
14.6.4 Nonce-based Merkle signatures . . . . . . . . . . . . . . . . . . . . . . . . . 602
14.7 A fun application: the TESLA broadcast MAC . . . . . . . . . . . . . . . . . . . . . 603
14.8 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 606
14.9 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 606
15 Elliptic curve cryptography and pairings 615
15.1 The group of points of an elliptic curve . . . . . . . . . . . . . . . . . . . . . . . . . 615
15.2 Elliptic curves over ﬁnite ﬁelds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 617
15.2.1 Montgomery and Edwards curves . . . . . . . . . . . . . . . . . . . . . . . . 618
15.3 Elliptic curve cryptography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 619
15.3.1 The curves secp256r1 and secp256k1 . . . . . . . . . . . . . . . . . . . . . . 620
15.3.2 A security twist . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 623
15.3.3 Curve25519 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624
15.4 Pairing based cryptography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624
15.5 Signature schemes from pairings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 627
15.5.1 The BLS signature scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . 627
15.5.2 Signature aggregation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 629
15.5.3 Secure BLS aggregation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 633
15.5.4 Signature schemes secure without random oracles . . . . . . . . . . . . . . . 639
xi
15.6 Advanced encryption schemes from pairings . . . . . . . . . . . . . . . . . . . . . . . 643
15.6.1 Identity based encryption . . . . . . . . . . . . . . . . . . . . . . . . . . . . 643
15.6.2 Related security notions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 646
15.6.3 Identity based encryption from pairings . . . . . . . . . . . . . . . . . . . . 648
15.6.4 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 654
15.7 The functional encryption paradigm . . . . . . . . . . . . . . . . . . . . . . . . . . . 658
15.7.1 Sample functional encryption schemes from pairings . . . . . . . . . . . . . 662
15.7.2 Variations on functional encryption . . . . . . . . . . . . . . . . . . . . . . . 665
15.8 Multilinear maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 666
15.9 A fun application: fair exchange of signatures . . . . . . . . . . . . . . . . . . . . . 668
15.10 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 671
15.11 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 671
16 Attacks on number theoretic assumptions 682
16.1 Analyzing the DL, CDH, and DDH assumptions . . . . . . . . . . . . . . . . . . . . 682
16.1.1 Square root time algorithms for discrete log . . . . . . . . . . . . . . . . . . 682
16.1.2 Discrete log in groups of composite order . . . . . . . . . . . . . . . . . . . 684
16.1.3 Information leakage in composite order groups . . . . . . . . . . . . . . . . 686
16.1.4 An attack on static Diﬃe-Hellman . . . . . . . . . . . . . . . . . . . . . . . 688
16.1.5 The relation between DL, CDH, and DDH . . . . . . . . . . . . . . . . . . . 690
16.2 Discrete log in Z∗
p: the general number ﬁeld sieve . . . . . . . . . . . . . . . . . . . . 690
16.2.1 Discrete log records in Z∗
p . . . . . . . . . . . . . . . . . . . . . . . . . . . . 691
16.2.2 A preprocessing attack on discrete log in Z∗
p . . . . . . . . . . . . . . . . . . 692
16.3 A lower bound on discrete log in generic prime order groups . . . . . . . . . . . . . 692
16.4 Analyzing the factoring and RSA assumptions . . . . . . . . . . . . . . . . . . . . . 696
16.4.1 Factoring algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 696
16.4.2 Attacks on RSA resulting from poor key generation . . . . . . . . . . . . . 698
16.4.3 A fault injection attack on optimized RSA . . . . . . . . . . . . . . . . . . . 701
16.4.4 An attack on low secret exponent RSA . . . . . . . . . . . . . . . . . . . . . 702
16.5 Quantum attacks on factoring and discrete log . . . . . . . . . . . . . . . . . . . . . 705
16.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 707
16.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 707
17 Post-quantum cryptography from lattices 712
17.1 Integer lattices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 713
17.2 Hard problems on lattices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 713
17.2.1 The SIS problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 713
17.2.2 The learning with errors (LWE) problem . . . . . . . . . . . . . . . . . . . . 713
17.2.3 The ring LWE problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 713
17.3 Trapdoor sampling from a lattice . . . . . . . . . . . . . . . . . . . . . . . . . . . . 713
17.4 Signatures from lattice problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 713
17.5 Public-key encryption from lattices . . . . . . . . . . . . . . . . . . . . . . . . . . . 713
17.6 Fully homomorphic encryption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 713
17.7 A fun application: factoring integers using lattices . . . . . . . . . . . . . . . . . . . 713
17.8 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 713
17.9 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 713
xii
III Protocols 714
18 Protocols for identiﬁcation and login 715
18.1 Interactive protocols: general notions . . . . . . . . . . . . . . . . . . . . . . . . . . 717
18.1.1 Mathematical details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 718
18.2 ID protocols: deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 718
18.3 Password protocols: security against direct attacks . . . . . . . . . . . . . . . . . . . 719
18.3.1 Password cracking using a dictionary attack . . . . . . . . . . . . . . . . . . 720
18.4 Making dictionary attacks harder . . . . . . . . . . . . . . . . . . . . . . . . . . . . 724
18.4.1 Public salts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 724
18.4.2 Secret salts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 726
18.4.3 Slow hash functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 726
18.4.4 Slow memory-hard hash functions . . . . . . . . . . . . . . . . . . . . . . . 728
18.4.5 More password management issues . . . . . . . . . . . . . . . . . . . . . . . 732
18.5 One time passwords: security against eavesdropping . . . . . . . . . . . . . . . . . . 733
18.5.1 PRF-based one-time passwords: HOTP and TOTP . . . . . . . . . . . . . . 735
18.5.2 The S/key system . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 737
18.6 Challenge-response: security against active attacks . . . . . . . . . . . . . . . . . . . 738
18.6.1 Challenge-response protocols . . . . . . . . . . . . . . . . . . . . . . . . . . 740
18.7 A fun application: rainbow tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . 742
18.8 Another fun application: hardening password storage . . . . . . . . . . . . . . . . . 746
18.9 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 746
18.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 747
19 Identiﬁcation and signatures from Sigma protocols 755
19.1 Schnorr’s identiﬁcation protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 755
19.1.1 Honest veriﬁer zero knowledge and security against eavesdropping . . . . . 760
19.2 From identiﬁcation protocols to signatures . . . . . . . . . . . . . . . . . . . . . . . 762
19.2.1 A useful abstraction: repeated impersonation attacks . . . . . . . . . . . . . 763
19.2.2 Security analysis of Schnorr signatures . . . . . . . . . . . . . . . . . . . . . 764
19.2.3 A concrete implementation and an optimization . . . . . . . . . . . . . . . . 769
19.3 Case study: ECDSA signatures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 770
19.4 Sigma protocols: basic deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 771
19.4.1 Special soundness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 773
19.4.2 Special honest veriﬁer zero knowledge . . . . . . . . . . . . . . . . . . . . . 774
19.5 Sigma protocols: examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 775
19.5.1 Okamoto’s protocol for representations . . . . . . . . . . . . . . . . . . . . . 775
19.5.2 The Chaum-Pedersen protocol for DH-triples . . . . . . . . . . . . . . . . . 777
19.5.3 A Sigma protocol for arbitrary linear relations . . . . . . . . . . . . . . . . 778
19.5.4 A Sigma protocol for the pre-image of a homomorphism . . . . . . . . . . . 780
19.5.5 A Sigma protocol for RSA . . . . . . . . . . . . . . . . . . . . . . . . . . . . 781
19.6 Identiﬁcation and signatures from Sigma protocols . . . . . . . . . . . . . . . . . . . 782
19.6.1 The Fiat-Shamir heuristic for signatures . . . . . . . . . . . . . . . . . . . . 784
19.7 Combining Sigma protocols: AND and OR proofs . . . . . . . . . . . . . . . . . . . 787
19.7.1 The AND-proof construction . . . . . . . . . . . . . . . . . . . . . . . . . . 787
19.7.2 The OR-proof construction . . . . . . . . . . . . . . . . . . . . . . . . . . . 788
xiii
19.8 Witness independence and applications . . . . . . . . . . . . . . . . . . . . . . . . . 789
19.8.1 Deﬁnition of witness independence . . . . . . . . . . . . . . . . . . . . . . . 790
19.8.2 Special HVZK implies witness independence . . . . . . . . . . . . . . . . . . 791
19.8.3 Actively secure identiﬁcation protocols . . . . . . . . . . . . . . . . . . . . . 792
19.8.4 Okamoto’s identiﬁcation protocol . . . . . . . . . . . . . . . . . . . . . . . . 794
19.9 Multi-extractability: another notion of “proof of knowledge” . . . . . . . . . . . . . 796
19.9.1 Multi-extractable Sigma protocols . . . . . . . . . . . . . . . . . . . . . . . 796
19.9.2 Applications and limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . 801
19.10 A fun application: a two round witness independent protocol . . . . . . . . . . . . . 808
19.11 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 808
19.12 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 808
20 Proving properties in zero-knowledge 823
20.1 Languages and soundness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 823
20.2 Proving properties on encrypted data . . . . . . . . . . . . . . . . . . . . . . . . . . 824
20.2.1 A generic protocol for non-linear relations . . . . . . . . . . . . . . . . . . . 829
20.3 Non-interactive proof systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 831
20.3.1 Example: a voting protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . 831
20.3.2 Non-interactive proofs: basic syntax . . . . . . . . . . . . . . . . . . . . . . 833
20.3.3 The Fiat-Shamir transform . . . . . . . . . . . . . . . . . . . . . . . . . . . 833
20.3.4 Non-interactive soundness . . . . . . . . . . . . . . . . . . . . . . . . . . . . 834
20.3.5 Non-interactive zero knowledge . . . . . . . . . . . . . . . . . . . . . . . . . 834
20.3.6 An example: applying the Fiat-Shamir transform to the Chaum-Pedersen
protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 837
20.4 Computational zero-knowledge and applications . . . . . . . . . . . . . . . . . . . . 838
20.4.1 Example: range proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 839
20.4.2 Special computational HVZK . . . . . . . . . . . . . . . . . . . . . . . . . . 840
20.4.3 An unconstrained generic protocol for non-linear relations . . . . . . . . . . 841
20.5 Bulletproofs: compressed Sigma protocols . . . . . . . . . . . . . . . . . . . . . . . . 842
20.6 Succinct non-interactive zero-knowledge proofs (SNARKs) . . . . . . . . . . . . . . 842
20.7 A fun application: everything that can be proved, can be proved in zero knowledge 842
20.8 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 842
20.9 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 843
21 Authenticated Key Exchange 855
21.1 Identiﬁcation and AKE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 857
21.2 An encryption-based protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 858
21.2.1 Insecure variations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 861
21.2.2 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 866
21.3 Perfect forward secrecy and a protocol based on ephemeral encryption . . . . . . . . 867
21.3.1 Assuming only semantically secure encryption . . . . . . . . . . . . . . . . . 869
21.4 HSM security . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 869
21.4.1 A technical requirement: strongly unpredictable ciphertexts . . . . . . . . . 872
21.4.2 Insecure variations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 872
21.5 Identity protection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 876
21.6 One-sided authenticated key exchange . . . . . . . . . . . . . . . . . . . . . . . . . . 878
xiv
21.6.1 A one-sided authenticated variant of AKE4 . . . . . . . . . . . . . . . . . . . 879
21.7 Deniability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 880
21.7.1 Deniability without identity protection . . . . . . . . . . . . . . . . . . . . . 881
21.7.2 Deniability with identity protection . . . . . . . . . . . . . . . . . . . . . . . 882
21.8 Channel bindings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 884
21.9 Formal deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 885
21.9.1 Understanding the deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . 889
21.9.2 Security of protocol AKE1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 890
21.9.3 Modeling perfect forward secrecy . . . . . . . . . . . . . . . . . . . . . . . . 891
21.9.4 Modeling HSM security . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 893
21.9.5 Modeling one-sided authentication . . . . . . . . . . . . . . . . . . . . . . . 896
21.9.6 Modeling channel bindings . . . . . . . . . . . . . . . . . . . . . . . . . . . . 897
21.10 Case study: TLS session setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 897
21.10.1 Authenticated key exchange with preshared keys . . . . . . . . . . . . . . . 900
21.11 Password authenticated key exchange . . . . . . . . . . . . . . . . . . . . . . . . . . 903
21.11.1 Phishing attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 903
21.11.2 PAKE: an introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 906
21.11.3 Protocol PAKE0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 906
21.11.4 Protocol PAKE1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 907
21.11.5 Protocol PAKE2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 909
21.11.6 Protocol PAKE+
2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 911
21.11.7 Explicit key conﬁrmation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 913
21.11.8 Phishing again . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 913
21.11.9 Case study: PAKE used in the WiFi WPA3 protocol . . . . . . . . . . . . . 914
21.12 Key exchange using an online trusted third party . . . . . . . . . . . . . . . . . . . 914
21.12.1 A key exchange protocol with an online TTP . . . . . . . . . . . . . . . . . 914
21.12.2 Insecure variations of protocol OnlineTTP . . . . . . . . . . . . . . . . . . . 916
21.12.3 Security for protocol OnlineTTP . . . . . . . . . . . . . . . . . . . . . . . . 921
21.13 A fun application: establishing Tor channels . . . . . . . . . . . . . . . . . . . . . . 921
21.14 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 921
21.15 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 921
22 Threshold cryptography 924
22.1 Shamir’s secret sharing scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 926
22.1.1 Shamir secret sharing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 927
22.1.2 Security of Shamir secret sharing . . . . . . . . . . . . . . . . . . . . . . . . 929
22.2 Threshold signatures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 930
22.2.1 A generic threshold signature scheme . . . . . . . . . . . . . . . . . . . . . . 932
22.2.2 BLS threshold signing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 932
22.2.3 Threshold signature security . . . . . . . . . . . . . . . . . . . . . . . . . . . 935
22.2.4 Security of threshold BLS . . . . . . . . . . . . . . . . . . . . . . . . . . . . 937
22.2.5 Accountability versus privacy . . . . . . . . . . . . . . . . . . . . . . . . . . 938
22.2.6 BLS accountable threshold signatures . . . . . . . . . . . . . . . . . . . . . 942
22.3 Threshold decryption schemes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 944
22.3.1 A generic threshold decryption scheme . . . . . . . . . . . . . . . . . . . . . 946
22.3.2 An insecure threshold decryption scheme . . . . . . . . . . . . . . . . . . . 947
xv
22.3.3 A secure threshold decryption scheme based on CDH . . . . . . . . . . . . . 949
22.3.4 Threshold decryption security . . . . . . . . . . . . . . . . . . . . . . . . . . 951
22.3.5 Security of threshold GS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 954
22.3.6 A pairing-free version of EthGS . . . . . . . . . . . . . . . . . . . . . . . . . . 956
22.4 Distributed key generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 959
22.4.1 Deﬁning the problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 959
22.4.2 A simple DKG protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 965
22.5 Beyond threshold: monotone access structures . . . . . . . . . . . . . . . . . . . . . 975
22.5.1 The generic secret sharing construction . . . . . . . . . . . . . . . . . . . . 976
22.5.2 Linear secret sharing schemes and monotone span programs . . . . . . . . . 977
22.5.3 A signature scheme from linear secret sharing . . . . . . . . . . . . . . . . . 984
22.6 Gap security for threshold cryptosystems . . . . . . . . . . . . . . . . . . . . . . . . 987
22.6.1 Gap security for threshold signature schemes . . . . . . . . . . . . . . . . . 988
22.6.2 Gap security for threshold decryption schemes . . . . . . . . . . . . . . . . 990
22.7 A fun application: a randomness beacon . . . . . . . . . . . . . . . . . . . . . . . . 992
22.8 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 992
22.9 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 992
23 Secure multi-party computation 997
23.1 The basic idea of MPC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 998
23.1.1 Informal notions of security . . . . . . . . . . . . . . . . . . . . . . . . . . . 998
23.1.2 Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 999
23.1.3 How to deﬁne security formally . . . . . . . . . . . . . . . . . . . . . . . . . 1001
23.1.4 Other applications of MPC . . . . . . . . . . . . . . . . . . . . . . . . . . . 1001
23.2 Securely evaluating arithmetic circuits . . . . . . . . . . . . . . . . . . . . . . . . . . 1003
23.2.1 Arithmetic circuit evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . 1003
23.2.2 Beaver’s protocol: an honest-but-curious 2.5-party protocol . . . . . . . . . 1005
23.2.3 Abstracting Beaver’s 2.5-party protocol . . . . . . . . . . . . . . . . . . . . 1009
23.2.4 A maliciously secure version of Beaver’s 2.5-party protocol . . . . . . . . . . 1011
23.3 Garbled circuits: another approach to MPC . . . . . . . . . . . . . . . . . . . . . . 1018
23.3.1 Boolean circuit evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1018
23.3.2 Yao’s 2-party garbled circuit technique: basic ideas . . . . . . . . . . . . . . 1020
23.3.3 Garbling schemes: an application to outsourcing computation . . . . . . . . 1021
23.3.4 Garble0: a simple but eﬃcient garbling scheme . . . . . . . . . . . . . . . . 1022
23.3.5 Garbling schemes: formally deﬁning security properties . . . . . . . . . . . 1027
23.3.6 A 2-party garbling-based protocol secure against honest-but-curious adver-
saries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1029
23.3.7 A 3-party garbling-based protocol secure against malicious adversaries . . . 1031
23.4 Multi-party computation based on a secure distributed core . . . . . . . . . . . . . . 1034
23.4.1 Processing inputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1035
23.4.2 Processing outputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1037
23.5 Formal models for multi-party computation: the universal composability framework 1037
23.5.1 The real protocol and its execution . . . . . . . . . . . . . . . . . . . . . . . 1038
23.5.2 The ideal protocol and its execution . . . . . . . . . . . . . . . . . . . . . . 1041
23.5.3 Example: the ideal functionality for secure function evaluation . . . . . . . 1043
23.5.4 Secure implementation: a strong security notion . . . . . . . . . . . . . . . 1044
xvi
23.5.5 Consequences of secure implementation . . . . . . . . . . . . . . . . . . . . 1046
23.5.6 Deﬁning honest-but-curious security . . . . . . . . . . . . . . . . . . . . . . 1054
23.5.7 A warmup honest-but-curious security proof: a simple OT protocol . . . . . 1056
23.5.8 A warmup malicious security proof: a simple OT protocol . . . . . . . . . . 1061
23.5.9 An example malicious security proof: Beaver’s 2.5-party protocol . . . . . . 1068
23.5.10 An example malicious security proof: multi-party computation based on a
secure distributed core . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1072
23.5.11 An example malicious security proof: the 3-party garbled circuit protocol . 1074
23.6 Distributed key generation: ideal functionalities and extension to threshold MPC . 1077
23.6.1 Formal models for secure DKG . . . . . . . . . . . . . . . . . . . . . . . . . 1077
23.6.2 A threshold MPC protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . 1079
23.7 OT extension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1087
23.8 A fun application: another stab at private set intersection . . . . . . . . . . . . . . . 1091
23.9 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1091
23.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1091
IV Appendices 1095
A Basic number theory 1096
A.1 Cyclic groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1096
A.2 Arithmetic modulo primes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1096
A.2.1 Basic concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1096
A.2.2 Structure of Z∗
p . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1097
A.2.3 Quadratic residues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1097
A.2.4 Computing in Zp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1098
A.2.5 Summary: arithmetic modulo primes . . . . . . . . . . . . . . . . . . . . . . 1099
A.3 Arithmetic modulo composites . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1099
B Basic probability theory 1101
B.1 The birthday Paradox . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1101
B.1.1 More collision bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1103
B.1.2 A simple distinguisher . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1103
C Basic complexity theory 1105
D Probabilistic algorithms 1106
xvii
xviii
Part I
Secret key cryptography
3
Chapter 2
Encryption
Suppose Alice and Bob share a secret key k. Alice wants to transmit a message m to Bob over a
network while maintaining the secrecy of m in the presence of an eavesdropping adversary. This
chapter begins the development of basic techniques to solve this problem. Besides transmitting a
message over a network, these same techniques allow Alice to store a ﬁle on a disk so that no one
else with access to the disk can read the ﬁle, but Alice herself can read the ﬁle at a later time.
We should stress that while the techniques we develop in this chapter to solve this fundamental
problem are important and interesting, they do not by themselves solve all problems related to
“secure communication.”
• The techniques only provide secrecy in the situation where Alice transmits a single message
per key. If Alice wants to secretly transmit several messages using the same key, then she
must use methods developed in Chapter 5.
• The techniques do not provide any assurances of message integrity: if the attacker has the
ability to modify the bits of the ciphertext while it travels from Alice to Bob, then Bob may
not realize that this happened, and accept a message other than the one that Alice sent. We
will discuss techniques for providing message integrity in Chapter 6.
• The techniques do not provide a mechanism that allow Alice and Bob to come to share a secret
key in the ﬁrst place. Maybe they are able to do this using some secure network (or a physical,
face-to-face meeting) at some point in time, while the message is sent at some later time when
Alice and Bob must communicate over an insecure network. However, with an appropriate
infrastructure in place, there are also protocols that allow Alice and Bob to exchange a secret
key even over an insecure network: such protocols are discussed in Chapter 21.
2.1 Shannon ciphers and perfect security
2.1.1 Deﬁnition of a Shannon cipher
The basic mechanism for encrypting a message using a shared secret key is called a cipher (or
encryption scheme). In this section, we introduce a slightly simpliﬁed notion of a cipher, which we
call a Shannon cipher.
A Shannon cipher is a pair E= (E,D) of functions.
4
• The function E (the encryption function) takes as input a key k and a message m (also
called a plaintext), and produces as output a ciphertext c. That is,
c= E(k,m),
and we say that c is the encryption of m under k.
• The function D (the decryption function) takes as input a key k and a ciphertext c, and
produces a message m. That is,
m= D(k,c),
and we say that m is the decryption of c under k.
• We require that decryption “undoes” encryption; that is, the cipher must satisfy the following
correctness property: for all keys k and all messages m, we have
D(k, E(k, m) ) = m.
To be slightly more formal, let us assume that Kis the set of all keys (the key space), Mis the
set of all messages (the message space), and that Cis the set of all ciphertexts (the ciphertext
space). With this notation, we can write:
E : K×M→C ,
D: K×C→M .
Also, we shall say that Eis deﬁned over (K,M,C).
Suppose Alice and Bob want to use such a cipher so that Alice can send a message to Bob.
The idea is that Alice and Bob must somehow agree in advance on a key k∈K. Assuming this is
done, then when Alice wants to send a message m∈M to Bob, she encrypts munder k, obtaining
the ciphertext c= E(k,m) ∈C, and then sends c to Bob via some communication network. Upon
receiving c, Bob decrypts c under k, and the correctness property ensures that D(k,c) is the same
as Alice’s original message m. For this to work, we have to assume that c is not tampered with in
transit from Alice to Bob. Of course, the goal, intuitively, is that an eavesdropper, who may obtain
cwhile it is in transit, does not learn too much about Alice’s message m— this intuitive notion is
what the formal deﬁnition of security, which we explore below, will capture.
In practice, keys, messages, and ciphertexts are often sequences of bytes. Keys are usually
of some ﬁxed length; for example, 16-byte (i.e., 128-bit) keys are very common. Messages and
ciphertexts may be sequences of bytes of some ﬁxed length, or of variable length. For example, a
message may be a 1GB video ﬁle, a 10MB music ﬁle, a 1KB email message, or even a single bit
encoding a “yes” or “no” vote in an electronic election.
Keys, messages, and ciphertexts may also be other types of mathematical objects, such as
integers, or tuples of integers (perhaps lying in some speciﬁed interval), or other, more sophisticated
types of mathematical objects (polynomials, matrices, or group elements). Regardless of how fancy
these mathematical objects are, in practice, they must at some point be represented as sequences
of bytes for purposes of storage in, and transmission between, computers.
For simplicity, in our mathematical treatment of ciphers, we shall assume that K, M, and C
are sets of ﬁnite size. While this simpliﬁes the theory, it means that if a real-world system allows
5
messages of unbounded length, we will (somewhat artiﬁcially) impose a (large) upper bound on
legal message lengths.
To exercise the above terminology, we take another look at some of the example ciphers discussed
in Chapter 1.
Example 2.1. A one-time pad is a Shannon cipher E= (E,D), where the keys, messages, and
ciphertexts are bit strings of the same length; that is, Eis deﬁned over (K,M,C), where
K:= M:= C:= {0,1}L,
for some ﬁxed parameter L. For a key k ∈{0,1}L and a message m ∈{0,1}L the encryption
function is deﬁned as follows:
E(k,m) := k⊕m,
and for a key k∈{0,1}L and ciphertext c∈{0,1}L, the decryption function is deﬁned as follows:
D(k,c) := k⊕c.
Here, “⊕” denotes bit-wise exclusive-OR, or in other words, component-wise addition modulo 2,
and satisﬁes the following algebraic laws: for all bit vectors x,y,z ∈{0,1}L, we have
x⊕y= y⊕x, x ⊕(y⊕z) = (x⊕y) ⊕z, x ⊕0L = x, and x⊕x= 0L.
These properties follow immediately from the corresponding properties for addition modulo 2.
Using these properties, it is easy to check that the correctness property holds for E: for all k,m ∈
{0,1}L, we have
D(k, E(k, m) ) = D(k, k⊕m) = k⊕(k⊕m) = (k⊕k) ⊕m= 0L ⊕m= m.
The encryption and decryption functions happen to be the same in this case, but of course, not all
ciphers have this property. 2
Example 2.2. A variable length one-time pad is a Shannon cipher E= (E,D), where the
keys are bit strings of some ﬁxed length L, while messages and ciphertexts are variable length bit
strings, of length at most L. Thus, Eis deﬁned over (K,M,C), where
K:= {0,1}L and M:= C:= {0,1}≤L.
for some parameter L. Here, {0,1}≤L denotes the set of all bit strings of length at mostL(including
the empty string). For a key k∈{0,1}L and a message m∈{0,1}≤L of length ℓ, the encryption
function is deﬁned as follows:
E(k,m) := k[0 . .ℓ−1] ⊕m,
and for a key k∈{0,1}L and ciphertext c∈{0,1}≤L of length ℓ, the decryption function is deﬁned
as follows:
D(k,c) := k[0 . .ℓ−1] ⊕c.
Here, k[0 . .ℓ−1] denotes the truncation of k to its ﬁrst ℓ bits. The reader may verify that the
correctness property holds for E. 2
6
Example 2.3. A substitution cipher is a Shannon cipher E= (E,D) of the following form. Let
Σ be a ﬁnite alphabet of symbols (e.g., the letters A–Z, plus a space symbol, ␣). The message space
Mand the ciphertext space Care both sequences of symbols from Σ of some ﬁxed length L:
M:= C:= ΣL.
The key space Kconsists of all permutations on Σ; that is, each k∈K is a one-to-one function from
Σ onto itself. Note that Kis a very large set; indeed, |K|= |Σ|! (for |Σ|= 27, |K|≈ 1.09 ·1028).
Encryption of a message m∈ΣL under a key k∈K (a permutation on Σ) is deﬁned as follows
E(k,m) :=
(
k(m[0]),k(m[1]),...,k (m[L−1])
)
,
where m[i] denotes the ith entry of m (counting from zero), and k(m[i]) denotes the application
of the permutation k to the symbol m[i]. Thus, to encrypt m under k, we simply apply the
permutation k component-wise to the sequence m. Decryption of a ciphertext c∈ΣL under a key
k∈K is deﬁned as follows:
D(k,c) :=
(
k−1(c[0]),k−1(c[1]),...,k −1(c[L−1])
)
.
Here, k−1 is the inverse permutation ofk, and to decryptcunder k, we simply applyk−1 component-
wise to the sequence c. The correctness property is easily veriﬁed: for a message m∈ΣL and key
k∈K, we have
D(k, E(k, m) ) = D(k, (k(m[0]),k(m[1]),...,k (m[L−1]) )
= (k−1(k(m[0])),k−1(k(m[1])),...,k −1(k(m[L−1])))
= (m[0],m[1],...,m [L−1]) = m. 2
Example 2.4 (additive one-time pad). We may also deﬁne a “addition mod n” variation of
the one-time pad. This is a cipher E= (E,D), deﬁned over ( K,M,C), where K:= M:= C:=
{0,...,n −1}, where n is a positive integer. Encryption and decryption are deﬁned as follows:
E(k,m) := m+ kmod n D (k,c) := c−kmod n.
The reader may easily verify that the correctness property holds for E. 2
2.1.2 Perfect security
So far, we have just deﬁned the basic syntax and correctness requirements of a Shannon cipher.
Next, we address the question: what is a “secure” cipher? Intuitively, the answer is that a secure
cipher is one for which an encrypted message remains “well hidden,” even after seeing its encryp-
tion. However, turning this intuitive answer into one that is both mathematically meaningful and
practically relevant is a real challenge. Indeed, although ciphers have been used for centuries, it
is only in the last few decades that mathematically acceptable deﬁnitions of security have been
developed.
In this section, we develop the mathematical notion of perfect security — this is the “gold
standard” for security (at least, when we are only worried about encrypting a single message and
do not care about integrity). We will also see that it is possible to achieve this level of security;
indeed, we will show that the one-time pad satisﬁes the deﬁnition. However, the one-time pad is
7
not very practical, in the sense that the keys must be as long as the messages: if Alice wants to
send a 1GB ﬁle to Bob, they must already share a 1GB key! Unfortunately, this cannot be avoided:
we will also prove that any perfectly secure cipher must have a key space at least as large as its
message space. This fact provides the motivation for developing a deﬁnition of security that is
weaker, but that is acceptable from a practical point of view, and which allows one to encrypt long
messages using short keys.
If Alice encrypts a message m under a key k, and an eavesdropping adversary obtains the
ciphertext c, Alice only has a hope of keeping m secret if the key k is hard to guess, and that
means, at the very least, that the key k should be chosen at random from a large key space. To
say that m is “well hidden” must at least mean that it is hard to completely determine m from
c, without knowledge of k; however, this is not really enough. Even though the adversary may
not know k, we assume that he does know the encryption algorithm and the distribution of k. In
fact, we will assume that when a message is encrypted, the key k is always chosen at random,
uniformly from among all keys in the key space. The adversary may also have some knowledge of
the message encrypted — because of circumstances, he may know that the set of possible messages
is quite small, and he may know something about how likely each possible message is. For example,
suppose he knows the message m is either m0 = "ATTACK␣AT␣DAWN" or m1 = "ATTACK␣AT␣DUSK",
and that based on the adversary’s available intelligence, Alice is equally likely to choose either one
of these two messages. Without seeing the ciphertext c, the adversary would only have a 50%
chance of guessing which message Alice sent. But we are assuming the adversary does know c.
Even with this knowledge, both messages may be possible; that is, there may exist keys k0 and
k1 such that E(k0,m0) = c and E(k1,m1) = c, so he cannot be sure if m = m0 or m = m1.
However, he can still guess. Perhaps it is a property of the cipher that there are 800 keys k0 such
that E(k0,m0) = c, and 600 keys k1 such that E(k1,m1) = c. If that is the case, the adversary’s
best guess would be that m = m0. Indeed, the probability that this guess is correct is equal to
800/(800 + 600) ≈57%, which is better than the 50% chance he would have without knowledge
of the ciphertext. Our formal deﬁnition of perfect security expressly rules out the possibility that
knowledge of the ciphertext increases the probability of guessing the encrypted message, or for that
matter, determining any property of the message whatsoever.
Without further ado, we formally deﬁne perfect security. In this deﬁnition, we will consider a
probabilistic experiment in which the key is drawn uniformly from the key space. We write k to
denote the random variable representing this random key. For a message m, E(k,m) is another
random variable, which represents the application of the encryption function to our random key
and the message m. Thus, every message m gives rise to a diﬀerent random variable E(k,m).
Deﬁnition 2.1 (perfect security). Let E= (E,D) be a Shannon cipher deﬁned over (K,M,C).
Consider a probabilistic experiment in which the random variable k is uniformly distributed over K.
If for all m0,m1 ∈M, and all c∈C, we have
Pr[E(k,m0) = c] = Pr[E(k,m1) = c],
then we say that Eis a perfectly secure Shannon cipher.
There are a number of equivalent formulations of perfect security that we shall explore. We
state a couple of these here.
Theorem 2.1. Let E= (E,D) be a Shannon cipher deﬁned over (K,M,C). The following are
equivalent:
8
(i) Eis perfectly secure.
(ii) For every c∈C, there exists an integer Nc (possibly depending on c) such that for all m∈M,
we have ⏐⏐{k∈K : E(k,m) = c}
⏐⏐= Nc.
(iii) If the random variable k is uniformly distributed over K, then each of the random variables
E(k,m), for m∈M, has the same distribution.
Proof. To begin with, let us restate (ii) as follows: for every c ∈ C, there exists a number Pc
(depending on c) such that for all m ∈M, we have Pr[ E(k,m) = c] = Pc. Here, k is a random
variable uniformly distributed over K. Note that Pc = Nc/|K|, where Nc is as in the original
statement of (ii).
This version of (ii) is clearly the same as (iii).
(i) =⇒ (ii). We prove (ii) assuming (i). To prove (ii), let c ∈C be some ﬁxed ciphertext.
Pick some arbitrary message m0 ∈M, and let Pc := Pr[E(k,m0) = c]. By (i), we know that for all
m∈M, we have Pr[E(k,m) = c] = Pr[E(k,m0) = c] = Pc. That proves (ii).
(ii) =⇒ (i). We prove (i) assuming (ii). Consider any ﬁxed m0,m1 ∈M and c∈C. (ii) says
that Pr[E(k,m0) = c] = Pc = Pr[E(k,m1) = c], which proves (i). 2
As promised, we give a proof that the one-time pad (see Example 2.1) is perfectly secure.
Theorem 2.2. The one-time pad is a perfectly secure Shannon cipher.
Proof. Suppose that the Shannon cipherE= (E,D) is a one-time pad, and is deﬁned over (K,M,C),
where K:= M:= C:= {0,1}L. For any ﬁxed message m ∈{0,1}L and ciphertext c ∈{0,1}L,
there is a unique key k∈{0,1}L satisfying the equation
k⊕m= c,
namely, k := m⊕c. Therefore, Esatisﬁes condition (ii) in Theorem 2.1 (with Nc = 1 for each c).
2
Example 2.5. Consider again the variable length one-time pad, deﬁned in Example 2.2. This
does not satisfy our deﬁnition of perfect security, since a ciphertext has the same length as the
corresponding plaintext. Indeed, let us choose an arbitrary string of length 1, call it m0, and an
arbitrary string of length 2, call it m1. In addition, suppose that c is an arbitrary length 1 string,
and that k is a random variable that is uniformly distributed over the key space. Then we have
Pr[E(k,m0) = c] = 1/2 and Pr[ E(k,m1) = c] = 0,
which provides a direct counter-example to Deﬁnition 2.1.
Intuitively, the variable length one-time pad cannot satisfy our deﬁnition of perfect security
simply because any ciphertext leaks the length of the corresponding plaintext. However, in some
sense (which we do not make precise right now), this is theonly information leaked. It is perhaps not
clear whether this should be viewed as a problem with the cipher or with our deﬁnition of perfect
security. On the one hand, one can imagine scenarios where the length of a message may vary
greatly, and while we could always “pad” short messages to eﬀectively make all messages equally
long, this may be unacceptable from a practical point of view, as it is a waste of bandwidth. On
9
the other hand, one must be aware of the fact that in certain applications, leaking just the length
of a message may be dangerous: if you are encrypting a “yes” or “no” answer to a question, just
the length of the obvious ASCII encoding of these strings leaks everything, so you better pad “no”
out to three characters. 2
Example 2.6. Consider again the substitution cipher deﬁned in Example 2.3. There are a couple
of diﬀerent ways to see that this cipher is not perfectly secure.
For example, choose a pair of messages m0,m1 ∈ΣL such that the ﬁrst two components of m0
are equal, yet the ﬁrst two components of m1 are not equal; that is,
m0[0] = m0[1] and m1[0] ̸= m1[1].
Then for each key k, which is a permutation on Σ, if c = E(k,m0), then c[0] = c[1], while if
c = E(k,m1), then c[0] ̸= c[1]. In particular, it follows that if k is uniformly distributed over the
key space, then the distributions of E(k,m0) and E(k,m1) will not be the same.
Even the weakness described in the previous paragraph may seem somewhat artiﬁcial. Another,
perhaps more realistic, type of attack on the substitution cipher works as follows. Suppose the
substitution cipher is used to encrypt email messages. As anyone knows, an email starts with a
“standard header,” such as"FROM". Suppose the ciphertext is c∈ΣL is intercepted by an adversary.
The secret key is actually a permutation k on Σ. The adversary knows that
c[0 ... 3] = (k(F),k(R),k(O),k(M)).
Thus, if the original message is m ∈ΣL, the adversary can now locate all positions in m where
an F occurs, where an R occurs, where an O occurs, and where an M occurs. Based just on this
information, along with speciﬁc, contextual information about the message, together with general
information about letter frequencies, the adversary may be able to deduce quite a bit about the
original message. 2
Example 2.7. Consider the additive one-time pad, deﬁned in Example 2.4. It is easy to verify
that this is perfectly secure. Indeed, it satisﬁes condition (ii) in Theorem 2.1 (with Nc = 1 for each
c). 2
The next two theorems develop two more alternative characterizations of perfect security. For
the ﬁrst, suppose an eavesdropping adversary applies some predicate φ to a ciphertext he has
obtained. The predicate φ (which is a boolean-valued function on the ciphertext space) may be
something very simple, like the parity function (i.e., whether the number of 1 bits in the ciphertext
is even or odd), or it might be some more elaborate type of statistical test. Regardless of how clever
or complicated the predicate φ is, perfect security guarantees that the value of this predicate on
the ciphertext reveals nothing about the message.
Theorem 2.3. Let E= (E,D) be a Shannon cipher deﬁned over (K,M,C). Consider a probabilistic
experiment in which k is a random variable uniformly distributed over K. Then Eis perfectly secure
if and only if for every predicate φ on C, for all m0,m1 ∈M, we have
Pr[φ(E(k,m0))] = Pr[φ(E(k,m1))].
Proof. This is really just a simple calculation. On the one hand, suppose Eis perfectly secure, and
let φ, m0, and m1 be given. Let S := {c∈C : φ(c)}. Then we have
Pr[φ(E(k,m0))] =
∑
c∈S
Pr[E(k,m0) = c] =
∑
c∈S
Pr[E(k,m1) = c] = Pr[φ(E(k,m1))].
10
Here, we use the assumption that Eis perfectly secure in establishing the second equality. On the
other hand, suppose Eis not perfectly secure, so there exist m0, m1, and c such that
Pr[E(k,m0) = c] ̸= Pr[E(k,m1) = c].
Deﬁning φ to be the predicate that is true for this particular c, and false for all other ciphertexts,
we see that
Pr[φ(E(k,m0))] = Pr[E(k,m0) = c] ̸= Pr[E(k,m1) = c] = Pr[φ(E(k,m1))]. 2
The next theorem states in yet another way that perfect security guarantees that the ciphertext
reveals nothing about the message. Suppose that m is a random variable distributed over the
message space M. We do not assume that m is uniformly distributed over M. Now suppose k
is a random variable uniformly distributed over the key space K, independently of m, and deﬁne
c := E(k,m), which is a random variable distributed over the ciphertext space C. The following
theorem says that perfect security guarantees that c and m are independent random variables.
One way of characterizing this independence is to say that for each ciphertext c∈C that occurs
with nonzero probability, and each message m∈M, we have
Pr[m = m|c = c] = Pr[m = m].
Intuitively, this means that after seeing a ciphertext, we have no more information about the
message than we did before seeing the ciphertext.
Another way of characterizing this independence is to say that for each message m∈M that
occurs with nonzero probability, and each ciphertext c∈C, we have
Pr[c = c|m = m] = Pr[c = c].
Intuitively, this means that the choice of message has no impact on the distribution of the ciphertext.
The restriction that m and k are independent random variables is sensible: in using any cipher,
it is a very bad idea to choose the key in a way that depends on the message, or vice versa (see
Exercise 2.16).
Theorem 2.4. Let E= (E,D) be a Shannon cipher deﬁned over (K,M,C). Consider a random
experiment in which k and m are random variables, such that
• k is uniformly distributed over K,
• m is distributed over M, and
• k and m are independent.
Deﬁne the random variable c := E(k,m). Then we have:
• if Eis perfectly secure, then c and m are independent;
• conversely, if c and m are independent, and each message in Moccurs with nonzero proba-
bility, then Eis perfectly secure.
11
Proof. For the ﬁrst implication, assume that Eis perfectly secure. Consider any ﬁxed m∈M and
c∈C. We want to show that
Pr[c = c∧m = m] = Pr[c = c] Pr[m = m].
We have
Pr[c = c∧m = m] = Pr[E(k,m) = c∧m = m]
= Pr[E(k,m) = c∧m = m]
= Pr[E(k,m) = c] Pr[m = m] (by independence of k and m).
So it will suﬃce to show that Pr[ E(k,m) = c] = Pr[c = c]. But we have
Pr[c = c] = Pr[E(k,m) = c]
=
∑
m′∈M
Pr[E(k,m) = c∧m = m′] (by total probability)
=
∑
m′∈M
Pr[E(k,m′) = c∧m = m′]
=
∑
m′∈M
Pr[E(k,m′) = c] Pr[m = m′] (by independence of k and m)
=
∑
m′∈M
Pr[E(k,m) = c] Pr[m = m′] (by deﬁnition of perfect security)
= Pr[E(k,m) = c]
∑
m′∈M
Pr[m = m′]
= Pr[E(k,m) = c] (probabilities sum to 1) .
For the second implication, assume thatc and m are independent, and each message inMoccurs
with nonzero probability. Let m∈M and c∈C. We will show that Pr[ E(k,m) = c] = Pr[c = c],
from which perfect security immediately follows. Since Pr[ m = m] ̸= 0, this is seen thusly:
Pr[E(k,m) = c] Pr[m = m] = Pr[E(k,m) = c∧m = m] (by independence of k and m)
= Pr[E(k,m) = c∧m = m]
= Pr[c = c∧m = m]
= Pr[c = c] Pr[m = m] (by independence of c and m). 2
2.1.3 The bad news
We have saved the bad news for last. The next theorem shows that perfect security is such a
powerful notion that one can really do no better than the one-time pad: keys must be at least as
long as messages. As a result, it is almost impossible to use perfectly secure ciphers in practice: if
Alice wants to send Bob a 1GB video ﬁle, then Alice and Bob have to agree on a 1GB secret key
in advance.
Theorem 2.5 (Shannon’s theorem). Let E = ( E,D) be a Shannon cipher deﬁned over
(K,M,C). If Eis perfectly secure, then |K|≥|M| .
12
Proof. Assume that |K|< |M|. We want to show that Eis not perfectly secure. To this end, we
show that there exist messages m0 and m1, and a ciphertext c, such that
Pr[E(k,m0) = c] >0, and (2.1)
Pr[E(k,m1) = c] = 0. (2.2)
Here, k is a random variable, uniformly distributed over K.
To do this, choose any message m0 ∈M, and any key k0 ∈K. Let c:= E(k0,m0). It is clear
that (2.1) holds.
Next, let
S := {D(k1,c) : k1 ∈K}.
Clearly,
|S|≤|K| <|M|,
and so we can choose a message m1 ∈M\ S.
To prove (2.2), we need to show that there is no key k1 such that E(k1,m1) = c. Assume to
the contrary that E(k1,m1) = c for some k1; then for this key k1, by the correctness property for
ciphers, we would have
D(k1,c) = D(k1, E(k1, m1) ) = m1,
which would imply that m1 belongs to S, which is not the case. That proves (2.2), and the theorem
follows. 2
2.2 Computational ciphers and semantic security
As we have seen in Shannon’s theorem (Theorem 2.5), the only way to achieve perfect security is
to have keys that are as long as messages. However, this is quite impractical: we would like to be
able to encrypt a long message (say, a document of several megabytes) using a short key (say, a few
hundred bits). The only way around Shannon’s theorem is to relax our security requirements. The
way we shall do this is to consider not all possible adversaries, but only computationally feasible
adversaries, that is, “real world” adversaries that must perform their calculations on real computers
using a reasonable amount of time and memory. This will lead to a weaker deﬁnition of security
called semantic security. Furthermore, our deﬁnition of security will be ﬂexible enough to allow
ciphers with variable length message spaces to be considered secure so long as they do not leak
any useful information about an encrypted message to an adversary other than the length of the
message. Also, since our focus is now on the “practical,” instead of the “mathematically possible,”
we shall also insist that the encryption and decryption functions are themselves eﬃcient algorithms,
and not just arbitrary functions.
2.2.1 Deﬁnition of a computational cipher
A computational cipher E= (E,D) is a pair of eﬃcient algorithms, E and D. The encryption
algorithm E takes as input a key k, along with a message m, and produces as output a ciphertext c.
The decryption algorithm Dtakes as input a key k, a ciphertext c, and outputs a message m. Keys
lie in some ﬁnite key space K, messages lie in a ﬁnite message space M, and ciphertexts lie in some
ﬁnite ciphertext space C. Just as for a Shannon cipher, we say that Eis deﬁned over (K,M,C).
13
Although it is not really necessary for our purposes in this chapter, we will allow the encryption
function E to be a probabilistic algorithm. This means that for ﬁxed inputs k and m, the output of
E(k,m) may be one of many values. Probabilistic algorithms are discussed further in Appendix D.
To emphasize the probabilistic nature of encryption we write
c←R E(k,m)
to denote the process of executing E(k,m) and assigning the output to the program variable c. We
shall use this notation throughout the book whenever we use probabilistic algorithms. Similarly,
we write
k←R K
to denote the process of assigning to the program variable k a random, uniformly distributed
element from the key space K. We shall use the analogous notation to sample uniformly from any
ﬁnite set.
We will not see any examples of probabilistic encryption algorithms in this chapter (we will see
our ﬁrst examples of this in Chapter 5). Although one could allow the decryption algorithm to
be probabilistic, we will have no need for this, and so will only discuss ciphers with deterministic
decryption algorithms. However, it will be occasionally be convenient to allow the decryption
algorithm to return a special reject value (distinct from all messages), indicating some kind of error
occurred during the decryption process.
Since the encryption algorithm is probabilistic, for a given key kand message m, the encryption
algorithm may output one of many possible ciphertexts; however, each of these possible ciphertexts
should decrypt to m. We can state this correctness requirement more formally as follows: for
all keys k∈K and messages m∈M, if we execute
c←R E(k,m), m′←D(k,c),
then m= m′with probability 1.
From now on, whenever we refer to a cipher, we shall mean a computational cipher,
as deﬁned above. Moreover, if the encryption algorithm happens to be deterministic, then
we may call the cipher a deterministic cipher.
Observe that any deterministic cipher is a Shannon cipher; however, a computational cipher
need not be a Shannon cipher (if it has a probabilistic encryption algorithm), and a Shannon
cipher need not be a computational cipher (if its encryption or decryption operations have no
eﬃcient implementations).
Example 2.8. The one-time pad (see Example 2.1) and the variable length one-time pad (see
Example 2.2) are both deterministic ciphers, since their encryption and decryption operations may
be trivially implemented as eﬃcient, deterministic algorithms. The same holds for the substitution
cipher (see Example 2.3), provided the alphabet Σ is not too large. Indeed, in the obvious imple-
mentation, a key — which is a permutation on Σ — will be represented by an array indexed by Σ,
and so we will require O(|Σ|) space just to store a key. This will only be practical for reasonably
sized Σ. The additive one-time pad discussed in Example 2.4 is also a deterministic cipher, since
both encryption and decryption operations may be eﬃciently implemented (if n is large, special
software to do arithmetic with large integers may be necessary). 2
14
2.2.2 Deﬁnition of semantic security
To motivate the deﬁnition of semantic security, consider a deterministic cipher E= (E,D), deﬁned
over (K,M,C). Consider again the formulation of perfect security in Theorem 2.3. This says that
for all predicates φ on the ciphertext space, and all messages m0,m1, we have
Pr[φ(E(k,m0))] = Pr[φ(E(k,m1))], (2.3)
where k is a random variable uniformly distributed over the key space K. Instead of insisting that
these probabilities are equal, we shall only require that they are very close; that is,
⏐⏐⏐Pr[φ(E(k,m0))] −Pr[φ(E(k,m1))]
⏐⏐⏐≤ϵ, (2.4)
for some very small, or negligible, value of ϵ. By itself, this relaxation does not help very much
(see Exercise 2.5). However, instead of requiring that (2.4) holds for every possible φ, m0, and
m1, we only require that (2.4) holds for all messages m0 and m1 that can be generated by some
eﬃcient algorithm, and all predicates φ that can be computed by some eﬃcient algorithm (these
algorithms could be probabilistic). For example, suppose it were the case that using the best
possible algorithms for generating m0 and m1, and for testing some predicate φ, and using (say)
10,000 computers in parallel for 10 years to perform these calculations, (2.4) holds for ϵ = 2−100.
While not perfectly secure, we might be willing to say that the cipher is secure for all practical
purposes.
Also, in deﬁning semantic security, we address an issue raised in Example 2.5. In that example,
we saw that the variable length one-time pad did not satisfy the deﬁnition of perfect security.
However, we want our deﬁnition to be ﬂexible enough so that ciphers like the variable length one-
time pad, which eﬀectively leak no information about an encrypted message other than its length,
may be considered secure as well.
Now the details. To precisely formulate the deﬁnition of semantic security, we shall describe
an attack game played between two parties, a challenger and an adversary. Throughout this
book, we shall formulate many attack games that capture various notions of security for diﬀerent
types of cryptographic primitives. In general, the challenger follows a very simple, ﬁxed protocol.
However, an adversary may follow an arbitrary (but still eﬃcient) protocol. The challenger and the
adversary send messages back and forth to each other, as speciﬁed by their protocols, and at the
end of the game, the adversary may output some value. The attack game also deﬁnes a probability
space, and this in turn deﬁnes the adversary’s advantage, which is determined by the probability
of one or more events.
Some attack games, such as the one deﬁning the semantic security of a cipher, comprise two
alternative “sub-games,” or “experiments” — in both experiments, the adversary follows the same
protocol, while the challenger behaves diﬀerently in each experiment. Speciﬁcally, in our attack
game deﬁning the semantic security of a cipher, the adversary generates two messages m0 and m1
(of the same length), and sends both messages to the challenger. In one experiment, the challenger
encrypts m0 under a random key, while in the other experiment, the challenger encrypts m1, again,
under a random key. In both experiments, the challenger sends the resulting ciphertext c back to
the adversary. After examining c, the adversary outputs a bit ˆb ∈{0,1}. The advantage of the
adversary is deﬁned to be the absolute diﬀerence between the probability the adversary outputs
ˆb= 1 in one experiment and the probability that it outputs ˆb= 1 the other experiment. We state
this more formally as follows:
15
Challenger A
m0, m1 2 M
k R
 K
ˆb 2 {0, 1}
(Experiment b)
cc R
 E(k, mb )
Figure 2.1: Experiment b of Attack Game 2.1
Attack Game 2.1 (semantic security). For a given cipher E= (E,D), deﬁned over (K,M,C),
and for a given adversary A, we deﬁne two experiments, Experiment 0 and Experiment 1. For
b= 0,1, we deﬁne
Experiment b:
• The adversary computes m0,m1 ∈M, of the same length, and sends them to the challenger.
• The challenger computes k←R K, c←R E(k,mb), and sends c to the adversary.
• The adversary outputs a bit ˆb∈{0,1}.
For b= 0,1, let Wb be the event that Aoutputs 1 in Experiment b. We deﬁne A’s semantic
security advantage with respect to Eas
SSadv[A,E] :=
⏐⏐⏐Pr[W0] −Pr[W1]
⏐⏐⏐. 2
Note that in the above game, the events W0 and W1 are deﬁned with respect to the probability
space determined by the random choice of k, the random choices made (if any) by the encryption
algorithm, and the random choices made (if any) by the adversary. The value SS adv[A,E] is a
number between 0 and 1.
See Fig. 2.1 for a schematic diagram of Attack Game 2.1. As indicated in the diagram, A’s
“output” is really just a ﬁnal message to the challenger.
Deﬁnition 2.2 (semantic security). A cipher E is semantically secure if for all eﬃcient
adversaries A, the value SSadv[A,E] is negligible.
As a formal deﬁnition, this is not quite complete, as we have yet to deﬁne what we mean by
“messages of the same length”, “eﬃcient adversaries”, and “negligible”. We will come back to this
shortly.
Let us relate this formal deﬁnition to the discussion preceding it. Suppose that the adversary
Ain Attack Game 2.1 is deterministic. First, the adversary computes in a deterministic fashion
16
messages m0,m1, and then evaluates a predicate φ on the ciphertext c, outputting 1 if true and
0 if false. Semantic security says that the value ϵ in (2.4) is negligible. In the case where Ais
probabilistic, we can view Aas being structured as follows: it generates a random value r from
some appropriate set, and deterministically computes messages m(r)
0 , m(r)
1 , which depend on r, and
evaluates a predicate φ(r) on c, which also depends on r. Here, semantic security says that the value
ϵin (2.4), with m0,m1,φ replaced by m(r)
0 ,m(r)
1 ,φ(r), is negligible — but where now the probability
is with respect to a randomly chosen key and a randomly chosen value of r.
Remark 2.1. Let us now say a few words about the requirement that the messages m0 and m1
computed by the adversary Attack Game 2.1 be of the same length.
• First, the notion of the “length” of a message is speciﬁc to the particular message space M;
in other words, in specifying a message space, one must specify a rule that associates a length
(which is a non-negative integer) with any given message. For most concrete message spaces,
this will be clear: for example, for the message space {0,1}≤L (as in Example 2.2), the length
of a message m ∈{0,1}≤L is simply its length, |m|, as a bit string. However, to make our
deﬁnition somewhat general, we leave the notion of length as an abstraction. Indeed, some
message spaces may have no particular notion of length, in which case all messages may be
viewed as having length 0.
• Second, the requirement thatm0 and m1 be of the same length means that the adversary is not
deemed to have broken the system just because he can eﬀectively distinguish an encryption
of a message of one length from an encryption of a message of a diﬀerent length. This is how
our formal deﬁnition captures the notion that an encryption of a message is allowed to leak
the length of the message (but nothing else).
We already discussed in Example 2.5 how in certain applications, leaking the length of the
message can be catastrophic. However, since there is no general solution to this problem, most
real-world encryption schemes (for example, TLS) do not make any attempt at all to hide the
length of the message. This can lead to real attacks. For example, Chen et al. [41] show that
the lengths of encrypted messages can reveal considerable information about private data that
a user supplies to a cloud application. They use an online tax ﬁling system as their example,
but other works show attacks of this type on many other systems. 2
Example 2.9. Let Ebe a deterministic cipher that is perfectly secure. Then it is easy to see that
for every adversary A(eﬃcient or not), we have SSadv[A,E] = 0. This follows almost immediately
from Theorem 2.3 (the only slight complication is that our adversary Ain Attack Game 2.1 may
be probabilistic, but this is easily dealt with). In particular, Eis semantically secure. Thus, if Eis
the one-time pad (see Example 2.1), we have SS adv[A,E] = 0 for all adversaries A; in particular,
the one-time pad is semantically secure. Because the deﬁnition of semantic security is a bit more
forgiving with regard to variable length message spaces, it is also easy to see that if Eis the variable
length one-time pad (see Example 2.2), then SS adv[A,E] = 0 for all adversaries A; in particular,
the variable length one-time pad is also semantically secure. 2
We need to say a few words about the terms “eﬃcient” and “negligible”. Below in Section 2.3
we will ﬁll in the remaining details (they are somewhat tedious, and not really very enlightening).
Intuitively, negligible means so small as to be “zero for all practical purposes”: think of a number
like 2−100 — if the probability that you spontaneously combust in the next year is 2 −100, then you
17
would not worry about such an event occurring any more than you would an event that occurred
with probability 0. We also use the following terms:
• An eﬃcient adversary is one that runs in a “reasonable” amount time.
• A value N is called super-poly if 1/N is negligible.
• A poly-bounded value is a “reasonably” sized number. In particular, we can say that the
running time of an eﬃcient adversary is poly-bounded.
Fact 2.6. If ϵ and ϵ′ are negligible values, and Q and Q′ are poly-bounded values, then:
(i) ϵ+ ϵ′ is a negligible value,
(ii) Q+ Q′ and Q·Q′ are poly-bounded values, and
(iii) Q·ϵ is a negligible value.
For now, the reader can just take these facts as axioms. Instead of dwelling on these technical
issues, we discuss an example that illustrates how one typically uses this deﬁnition in analyzing the
security of a larger system that uses a semantically secure cipher.
2.2.3 Connections to weaker notions of security
2.2.3.1 Message recovery attacks
Intuitively, in a message recovery attack, an adversary is given an encryption of a random message,
and is able to recover the message from the ciphertext with probability signiﬁcantly better than
random guessing, that is, probability 1 /|M|. Of course, any reasonable notion of security should
rule out such an attack, and indeed, semantic security does.
While this may seem intuitively obvious, we give a formal proof of this. One of our motivations
for doing this is to illustrate in detail the notion of a security reduction, which is the main technique
used to reason about the security of systems. Basically, the proof will argue that any eﬃcient
adversary Athat can eﬀectively mount a message recovery attack on Ecan be used to build an
eﬃcient adversary Bthat breaks the semantic security of E; since semantic security implies that no
such Bexists, we may conclude that no such Aexists.
To formulate this proof in more detail, we need a formal deﬁnition of a message recovery
attack. As before, this is done by giving attack game, which is a protocol between a challenger and
an adversary.
Attack Game 2.2 (message recovery). For a given cipher E= (E,D), deﬁned over (K,M,C),
and for a given adversary A, the attack game proceeds as follows:
• The challenger computes m←R M, k←R K, c←R E(k,m), and sends c to the adversary.
• The adversary outputs a message ˆm∈M.
Let W be the event that ˆm= m. We say that Awins the game in this case, and we deﬁne A’s
message recovery advantage with respect to Eas
MRadv[A,E] :=
⏐⏐Pr[W] −1/|M|
⏐⏐. 2
18
Deﬁnition 2.3 (security against message recovery). A cipher Eis secure against message
recovery if for all eﬃcient adversaries A, the value MRadv[A,E] is negligible.
Theorem 2.7. Let E= (E,D) be a cipher deﬁned over (K,M,C). If Eis semantically secure then
Eis secure against message recovery.
Proof. Assume that Eis semantically secure. Our goal is to show that Eis secure against message
recovery.
To prove that Eis secure against message recovery, we have to show that every eﬃcient ad-
versary Ahas negligible advantage in Attack Game 2.2. To show this, we let an arbitrary but
eﬃcient adversary Abe given, and our goal now is to show that A’s message recovery advantage,
MRadv[A,E], is negligible. Let p denote the probability that Awins the message recovery game,
so that
MRadv[A,E] =
⏐⏐p−1/|M|
⏐⏐.
We shall show how to construct an eﬃcient adversary Bwhose semantic security advantage in
Attack Game 2.1 is related to A’s message recovery advantage as follows:
MRadv[A,E] ≤SSadv[B,E]. (2.5)
Since Bis eﬃcient, and since we are assuming that Eis semantically secure, the right-hand side of
(2.5) is negligible, and so we conclude that MR adv[A,E] is negligible.
So all that remains to complete the proof is to show how to construct an eﬃcientBthat satisﬁes
(2.5). The idea is to use Aas a “black box” — we do not have to understand the inner workings
of Aat all.
Here is how Bworks. Adversary Bgenerates two random messages, m0 and m1 in M, and
sends these to its own SS challenger. This challenger sends Ba ciphertext c, which Bforwards
to A, as if it were coming from A’s MR challenger . When Aoutputs a message ˆm, our adversary B
outputs ˆb= 1 if ˆm= m1, and outputs ˆb= 0 otherwise.
That completes the description of B. Note that the running time of Bis essentially the same
as that of A. We now analyze the B’s SS advantage, and relate this to A’s MR advantage.
For b= 0,1, let pb be the probability that Boutputs 1 if B’s SS challenger encrypts mb. So by
deﬁnition
SSadv[B,E] = |p1 −p0|.
On the one hand, when c is an encryption of m1, the probability p1 is precisely equal to A’s
probability of winning the message recovery game, so p1 = p. On the other hand, when c is an
encryption of m0, the adversary A’s output is independent of m1, and so p0 = 1/|M|. It follows
that
SSadv[B,E] = |p1 −p0|=
⏐⏐p−1/|M|
⏐⏐= MRadv[A,E].
This proves (2.5). In fact, equality holds in (2.5), but that is not essential to the proof. 2
The reader should make sure that he or she understands the logic of this proof, as this type of
proof will be used over and over again throughout the book. We shall review the important parts
of the proof here, and give another way of thinking about it.
The core of the proof was establishing the following fact: for every eﬃcient MR adversary A
that attacks Eas in Attack Game 2.2, there exists an eﬃcient SS adversary Bthat attacks Eas in
Attack Game 2.1 such that
MRadv[A,E] ≤SSadv[B,E]. (2.6)
19
We are trying to prove that ifEis semantically secure, then Eis secure against message recovery.
In the above proof, we argued that if Eis semantically secure, then the right-hand side of (2.6)
must be negligible, and hence so must the left-hand side; since this holds for all eﬃcient A, we
conclude that Eis secure against message recovery.
Another way to approach the proof of the theorem is to prove the contrapositive: if Eis not
secure against message recovery, then Eis not semantically secure. So, let us assume that Eis not
secure against message recovery. This means there exists an eﬃcient adversary Awhose message
recovery advantage is non-negligible. Using Awe build an eﬃcient adversary Bthat satisﬁes (2.6).
By assumption, MRadv[A,E] is non-negligible, and (2.6) implies that SS adv[B,E] is non-negligible.
From this, we conclude that Eis not semantically secure.
Said even more brieﬂy: to prove that semantic security implies security against message recovery,
we show how to turn an eﬃcient adversary that breaks message recovery into an eﬃcient adversary
that breaks semantic security.
We also stress that the adversary Bconstructed in the proof just uses Aas a “black box.” In
fact, almost all of the constructions we shall see are of this type: Bis essentially just a wrapper
around A, consisting of some simple and eﬃcient “interface layer” between B’s challenger and a
single running instance of A. Ideally, we want the computational complexity of the interface layer
to not depend on the computational complexity of A; however, some dependence is unavoidable:
if an attack game allows Ato make multiple queries to its challenger, the more queries Amakes,
the more work must be performed by the interface layer, but this work should just depend on the
number of such queries and not on the running time of A.
Thus, we will say adversary Bis an elementary wrapper around adversary Awhen it can be
structured as above, as an eﬃcient interface interacting with A. The salient properties are:
• If Bis an elementary wrapper around A, and Ais eﬃcient, then Bis eﬃcient.
• If Cis an elementary wrapper around Band Bis an elementary wrapper around A, then Cis
an elementary wrapper around A.
These notions are formalized in Section 2.3 (but again, they are extremely tedious).
2.2.3.2 Computing individual bits of a message
If an encryption scheme is secure, not only should it be hard to recover the whole message, but it
should be hard to compute any partial information about the message.
We will not prove a completely general theorem here, but rather, consider a speciﬁc example.
Suppose E= (E,D) is a cipher deﬁned over ( K,M,C), where M= {0,1}L. For m ∈M, we
deﬁne parity(m) to be 1 if the number of 1’s in mis odd, and 0 otherwise. Equivalently, parity( m)
is the exclusive-OR of all the individual bits of m.
We will show that if Eis semantically secure, then given an encryption c of a random message
m, it is hard to predict parity( m). Now, since parity( m) is a single bit, any adversary can predict
this value correctly with probability 1 /2 just by random guessing. But what we want to show is
that no eﬃcient adversary can do signiﬁcantly better than random guessing.
As a warm up, suppose there were an eﬃcient adversary Athat could predict parity( m) with
probability 1. This means that for every message m, every key k, and every encryption c of m,
when we give Athe ciphertext c, it outputs the parity of m. So we could use Ato build an SS
adversary Bthat works as follows. Our adversary chooses two messages, m0 and m1, arbitrarily,
20
but with parity( m0) = 0 and parity( m1) = 1. Then it hands these two messages to its own SS
challenger, obtaining a ciphertext c, which it then forwards to A. After receiving c, adversary A
outputs a bit ˆb, and Boutputs this same bit ˆb as its own output. It is easy to see that B’s SS
advantage is precisely 1: when its SS challenger encrypts m0, it always outputs 0, and when its SS
challenger encrypts m1, it always outputs 1.
This shows that if E is semantically secure, there is no eﬃcient adversary that can predict
parity with probability 1. However, we can say even more: if Eis semantically secure, there is no
eﬃcient adversary that can predict parity with probability signiﬁcantly better than 1 /2. To make
this precise, we give an attack game:
Attack Game 2.3 (parity prediction). For a given cipher E= (E,D), deﬁned over (K,M,C),
and for a given adversary A, the attack game proceeds as follows:
• The challenger computes m←R M, k←R K, c←R E(k,m), and sends c to the adversary.
• The adversary outputs ˆb∈{0,1}.
Let W be the event that ˆb = parity(m). We deﬁne A’s parity prediction advantage with
respect to Eas
Parityadv[A,E] :=
⏐⏐⏐Pr[W] −1/2
⏐⏐⏐. 2
Deﬁnition 2.4 (parity prediction). A cipher Eis secure against parity prediction if for all
eﬃcient adversaries A, the value Parityadv[A,E] is negligible.
Theorem 2.8. Let E = ( E,D) be a cipher deﬁned over (K,M,C), and M= {0,1}L. If E is
semantically secure, then Eis secure against parity prediction.
Proof. As in the proof of Theorem 2.7, we give a proof by reduction. In particular, we will show
that for every parity prediction adversary Athat attacks Eas in Attack Game 2.3, there exists an
SS adversary Bthat attacks Eas in Attack Game 2.1, where Bis an elementary wrapper around
A, such that
Parityadv[A,E] = 1
2 ·SSadv[B,E].
Let A be a parity prediction adversary that predicts parity with probability 1 /2 + ϵ, so
Parityadv[A,E] = |ϵ|.
Here is how we construct our SS adversary B.
Our adversary Bgenerates a random message m0, and sets m1 ←m0 ⊕(0L−1 ∥1); that is,
m1 is the same as m0, except that the last bit is ﬂipped. In particular, m0 and m1 have opposite
parity.
Our adversary Bsends the pair m0,m1 to its own SS challenger, receives a ciphertext c from
that challenger, and forwards c to A. When Aoutputs a bit ˆb, our adversary Boutputs 1 if
ˆb= parity(m0), and outputs 0, otherwise.
For b= 0,1, let pb be the probability that Boutputs 1 if B’s SS challenger encrypts mb. So by
deﬁnition
SSadv[B,E] = |p1 −p0|.
We claim that p0 = 1 /2 + ϵ and p1 = 1 /2 −ϵ. This is because regardless of whether m0
or m1 is encrypted, the distribution of mb is uniform over M, and so in case b = 0, our parity
predictor Awill output parity(m0) with probability 1 /2 + ϵ, and when b= 1, our parity predictor
21
Awill output parity( m1) with probability 1 /2 + ϵ, and so outputs parity( m0) with probability
1 −(1/2 + ϵ) = 1/2 −ϵ.
Therefore,
SSadv[B,E] = |p1 −p0|= 2|ϵ|= 2 ·Parityadv[A,E],
which proves the theorem. 2
We have shown that if an adversary can eﬀectively predict the parity of a message, then it can
be used to break semantic security. Conversely, it turns out that if an adversary can break semantic
security, he can eﬀectively predict some predicate of the message (see Exercise 3.16).
2.2.4 Consequences of semantic security
In this section, we examine the consequences of semantic security in the context of a speciﬁc
example, namely, electronic gambling. The speciﬁc details of the example are not so important, but
the example illustrates how one typically uses the assumption of semantic security in applications.
Consider the following extremely simpliﬁed version of roulette, which is a game between the
house and a player. The player gives the house 1 dollar. He may place one of two kinds of bets:
• “high or low,” or
• “even or odd.”
After placing his bet, the house chooses a random number r ∈{0,1,..., 36}. The player wins if
r̸= 0, and if
• he bet “high” and r> 18,
• he bet “low” and r≤18,
• he bet “even” and r is even,
• he bet “odd” and r is odd.
If the player wins, the house pays him 2 dollars (for a net win of 1 dollar), and if the player loses, the
house pays nothing (for a net loss of 1 dollar). Clearly, the house has a small, but not insigniﬁcant
advantage in this game: the probability that the player wins is 18 /37 ≈48.65%.
Now suppose that this game is played over the Internet. Also, suppose that for various technical
reasons, the house publishes an encryption of r before the player places his bet (perhaps to be
decrypted by some regulatory agency that shares a key with the house). The player is free to analyze
this encryption before placing his bet, and of course, by doing so, the player could conceivably
increase his chances of winning. However, if the cipher is any good, the player’s chances should not
increase by much. Let us prove this, assuming r is encrypted using a semantically secure cipher
E= (E,D), deﬁned over ( K,M,C), where M= {0,1,..., 36}(we shall view all messages in M
as having the same length in this example). Also, from now on, let us call the player A, to stress
the adversarial nature of the player, and assume that A’s strategy can be modeled as an eﬃcient
algorithm. The game is illustrated in Fig. 2.2. Here, bet denotes one of “high,” “low,” “even,”
“odd.” Player Asends bet to the house, who evaluates the function W(r,bet), which is 1 if bet is a
winning bet with respect to r, and 0 otherwise. Let us deﬁne
IRadv[A] :=
⏐⏐Pr[W(r,bet) = 1] −18/37
⏐⏐.
Our goal is to prove the following theorem.
22
House
r
R
← {0, 1, . . . , 36}
k
R
← K
bet
A
outcome ← W (r, bet)
outcome
c
c R
← E (k , r)
Figure 2.2: Internet roulette
Theorem 2.9. If Eis semantically secure, then for every eﬃcient player A, the quantity IRadv[A]
is negligible.
As we did in Section 2.2.3, we prove this by reduction. More concretely, we shall show that for
every player A, there exists an SS adversary B, where Bis an elementary wrapper around A, such
that
IRadv[A] = SSadv[B,E]. (2.7)
Thus, if there were an eﬃcient player Awith a non-negligible advantage, we would obtain an
eﬃcient SS adversary Bthat breaks the semantic security ofE, which we are assuming is impossible.
Therefore, there is no such A.
To motivate and analyze our new adversary B, consider an “idealized” version of Internet
roulette, in which instead of publishing an encryption of the actual value r, the house instead
publishes an encryption of a “dummy”value, say 0. The logic of the ideal Internet roulette game is
illustrated in Fig. 2.3. Note, however, that in the ideal Internet roulette game, the house still uses
the actual value of r to determine the outcome of the game. Let p0 be the probability that Awins
at Internet roulette, and let p1 be the probability that Awins at ideal Internet roulette.
Our adversary Bis designed to play in Attack Game 2.1 so that if ˆbdenotes B’s output in that
game, then we have:
• if Bis placed in Experiment 0, then Pr[ ˆb= 1] = p0;
• if Bis placed in Experiment 1, then Pr[ ˆb= 1] = p1.
The logic of adversary Bis illustrated in Fig. 2.4. It is clear by construction that Bsatisﬁes the
properties claimed above, and so in particular,
SSadv[B,E] = |p1 −p0|. (2.8)
Now, consider the probability p1 that Awins at ideal Internet roulette. No matter how clever
A’s strategy is, he wins with probability 18/37, since in this ideal Internet roulette game, the value
23
House
r
R
← {0, 1, . . . , 36}
k
R
← K
bet
A
outcome ← W (r, bet)
outcome
c
c R
← E (k , 0)
Figure 2.3: ideal Internet roulette
r R
 {0, 1, . . . , 36}
k R
 K
bet
Challenger
m0  r
m1  0
(Experiment b)
m0, m1
ˆb  W (r, bet)ˆb
A
B
c R
 E(k, mb )
c
Figure 2.4: The SS adversary Bin Attack Game 2.1
24
of bet is computed from c, which is statistically independent of the value ofr. That is, ideal Internet
roulette is equivalent to physical roulette. Therefore,
IRadv[A] = |p1 −p0|. (2.9)
Combining (2.8) and (2.9), we obtain (2.7).
The approach we have used to analyze Internet roulette is one that we will see again and again.
The basic idea is to replace a system component by an idealized version of that component, and
then analyze the behavior of this new, idealized version of the system.
Another lesson to take away from the above example is that in reasoning about the security of
a system, what we view as “the adversary” depends on what we are trying to do. In the above
analysis, we cobbled together a new adversary Bout of several components: one component was
the original adversary A, while other components were scavenged from other parts of the system
(the algorithm of “the house,” in this example). This will be very typical in our security analyses
throughout this text. Intuitively, if we imagine a diagram of the system, at diﬀerent points in the
security analysis, we will draw a circle around diﬀerent components of the system to identify what
we consider to be “the adversary” at that point in the analysis.
2.2.5 Bit guessing: an alternative characterization of semantic security
The example in Section 2.2.4 was a typical example of how one could use the deﬁnition of semantic
security to analyze the security properties of a larger system that makes use of a semantically
secure cipher. However, there is another characterization of semantic security that is typically more
convenient to work with when one is trying to prove that a given cipher satisﬁes the deﬁnition. In
this alternative characterization, we deﬁne a new attack game. The role played by the adversary
is exactly the same as before. However, instead of having two diﬀerent experiments, there is just
a single experiment. In this bit-guessing version of the attack game, the challenger chooses
b∈{0,1}at random and runs Experiment bof Attack Game 2.1; it is the adversary’s goal to guess
the bit b with probability signiﬁcantly better than 1 /2. Here are the details:
Attack Game 2.4 (semantic security: bit-guessing version). For a given cipherE= (E,D),
deﬁned over (K,M,C), and for a given adversary A, the attack game runs as follows:
• The adversary computes m0,m1 ∈M, of the same length, and sends them to the challenger.
• The challenger computes b←R {0,1}, k←R K, c←R E(k,mb), and sends c to the adversary.
• The adversary outputs a bit ˆb∈{0,1}.
We say that Awins the game if ˆb= b. 2
Fig. 2.5 illustrates Attack Game 2.4. Note that in this game, the event that the Awins the
game is deﬁned with respect to the probability space determined by the random choice of band k,
the random choices made (if any) of the encryption algorithm, and the random choices made (if
any) by the adversary.
Of course, any adversary can win the game with probability 1/2, simply by ignoringccompletely
and choosing ˆb at random (or alternatively, always choosing ˆb to be 0, or always choosing it to be
1). What we are interested in is how much better than random guessing an adversary can do. If
W denotes the event that the adversary wins the bit-guessing version of the attack game, then we
are interested in the quantity |Pr[W] −1/2|, which we denote by SS adv∗[A,E]. Then we have:
25
Challenger A
m0, m1 2 M
k R
 K
ˆb 2 {0, 1}
b R
 {0, 1}
cc R
 E(k, mb )
Figure 2.5: Attack Game 2.4
Theorem 2.10. For every cipher Eand every adversary A, we have
SSadv[A,E] = 2 ·SSadv∗[A,E]. (2.10)
Proof. This is just a simple calculation. Let p0 be the probability that the adversary outputs 1 in
Experiment 0 of Attack Game 2.1, and let p1 be the probability that the adversary outputs 1 in
Experiment 1 of Attack Game 2.1.
Now consider Attack Game 2.4. From now on, all events and probabilities are with respect to
this game. If we condition on the event that b= 0, then in this conditional probability space, all
of the other random choices made by the challenger and the adversary are distributed in exactly
the same way as the corresponding values in Experiment 0 of Attack Game 2.1. Therefore, if ˆb is
the output of the adversary in Attack Game 2.4, we have
Pr[ˆb= 1 |b= 0] = p0.
By a similar argument, we see that
Pr[ˆb= 1 |b= 1] = p1.
So we have
Pr[ˆb= b] = Pr[ˆb= b|b= 0] Pr[b= 0] + Pr[ˆb= b|b= 1] Pr[b= 1]
= Pr[ˆb= 0 |b= 0] ·1
2 + Pr[ˆb= 1 |b= 1] ·1
2
= 1
2
(
1 −Pr[ˆb= 1 |b= 0] + Pr[ˆb= 1 |b= 1]
)
= 1
2 (1 −p0 + p1).
Therefore,
SSadv∗[A,E] =
⏐⏐⏐Pr[ˆb= b] −1
2
⏐⏐⏐= 1
2 |p1 −p0|= 1
2 ·SSadv[A,E].
That proves the theorem. 2
Just as it is convenient to refer to SS adv[A,E] as A’s “SS advantage,” we shall refer to
SSadv∗[A,E] as A’s “bit-guessing SS advantage.”
26
2.2.5.1 A generalization
As it turns out, the above situation is quite generic. Although we do not need it in this chapter, for
future reference we indicate here how the above situation generalizes. There will be a number of
situations we shall encounter where some particular security property, call it “X,” for some crypto-
graphic system, call it “ S,” can be deﬁned in terms of an attack game involving two experiments,
Experiment 0 and Experiment 1, where the adversary A’s protocol is the same in both experiments,
while that of the challenger is diﬀerent. For b= 0,1, we deﬁne Wb to be the event that Aoutputs
1 in Experiment b, and we deﬁne
Xadv[A,S] :=
⏐⏐⏐Pr[W0] −Pr[W1]
⏐⏐⏐
to be A’s “X advantage.” Just as above, we can always deﬁne a “bit-guessing” version of the attack
game, in which the challenger chooses b ∈{0,1}at random, and then runs Experiment b as its
protocol. If W is the event that the adversary’s output is equal to b, then we deﬁne
Xadv∗[A,S] :=
⏐⏐⏐Pr[W] −1/2
⏐⏐⏐
to be A’s “bit-guessing X advantage.”
Using exactly the same calculation as in the proof of Theorem 2.10, we have
Xadv[A,S] = 2 ·Xadv∗[A,S]. (2.11)
2.3 Mathematical details
Up until now, we have used the terms eﬃcient and negligible rather loosely, without a formal
mathematical deﬁnition:
• we required that a computational cipher have eﬃcient encryption and decryption algorithms;
• for a semantically secure cipher, we required that any eﬃcient adversary have a negligible
advantage in Attack Game 2.1.
The goal of this section is to provide precise mathematical deﬁnitions for these terms. While
these deﬁnitions lead to a satisfying theoretical framework for the study of cryptography as a
mathematical discipline, we should warn the reader:
• the deﬁnitions are rather complicated, requiring an unfortunate amount of notation; and
• the deﬁnitions model our intuitive understanding of these terms only very crudely.
We stress that the reader may safely skip this section without suﬀering a signiﬁcant loss in under-
standing. Before marching headlong into the formal deﬁnitions, let us remind the reader of what
we are trying to capture in these deﬁnitions.
• First, when we speak of an eﬃcient encryption or decryption algorithm, we usually mean one
that runs very quickly, encrypting data at a rate of, say, 10–100 computer cycles per byte of
data.
27
• Second, when we speak of an eﬃcient adversary, we usually mean an algorithm that runs in
some large, but still feasible amount of time (and other resources). Typically, one assumes
that an adversary that is trying to break a cryptosystem is willing to expend many more
resources than a user of the cryptosystem. Thus, 10,000 computers running in parallel for
10 years may be viewed as an upper limit on what is feasibly computable by a determined,
patient, and ﬁnancially well-oﬀ adversary. However, in some settings, like the Internet roulette
example in Section 2.2.4, the adversary may have a much more limited amount of time to
perform its computations before they become irrelevant.
• Third, when we speak of an adversary’s advantage as being negligible, we mean that it is so
small that it may as well be regarded as being equal to zero for all practical purposes. As
we saw in the Internet roulette example, if no eﬃcient adversary has an advantage better
than 2−100 in Attack Game 2.1, then no player can in practice improve his odds at winning
Internet roulette by more than 2 −100 relative to physical roulette.
Even though our intuitive understanding of the term eﬃcient depends on the context, our
formal deﬁnition will not make any such distinction. Indeed, we shall adopt the computational
complexity theorist’s habit of equating the notion of an eﬃcient algorithm with that of a (proba-
bilistic) polynomial-time algorithm. For better and for worse, this gives us a formal framework that
is independent of the speciﬁc details of any particular model of computation.
2.3.1 Negligible, super-poly, and poly-bounded functions
We begin by deﬁning the notions of negligible, super-poly, and poly-bounded functions.
Intuitively, a negligible function f : Z≥0 →R is one that not only tends to zero as n→∞, but
does so faster than the inverse of any polynomial.
Deﬁnition 2.5. A function f : Z≥1 → R is called negligible if for all c ∈ R>0 there exists
n0 ∈Z≥1 such that for all integers n≥n0, we have |f(n)|<1/nc.
An alternative characterization of a negligible function, which is perhaps easier to work with,
is the following:
Theorem 2.11. A function f : Z≥1 →R is negligible if and only if for all c> 0, we have
lim
n→∞
f(n)nc = 0.
Proof. Exercise. 2
Example 2.10. Some examples of negligible functions:
2−n, 2−√n, n−log n.
Some examples of non-negligible functions:
1
1000n4 + n2 log n, 1
n100 . 2
Once we have the term “negligible” formally deﬁned, deﬁning “super-poly” is easy:
Deﬁnition 2.6. A function f : Z≥1 →R is called super-poly if 1/f is negligible.
28
Essentially, a poly-bounded function f : Z≥1 →R is one that is bounded (in absolute value) by
some polynomial. Formally:
Deﬁnition 2.7. A function f : Z≥1 →R is called poly-bounded, if there exists c,d ∈R>0 such
that for all integers n≥0, we have |f(n)|≤ nc + d.
Note that if f is a poly-bounded function, then 1 /f is deﬁnitely not a negligible function.
However, as the following example illustrates, one must take care not to draw erroneous inferences.
Example 2.11. Deﬁne f : Z≥1 →R so that f(n) = 1/n for all even integers n and f(n) = 2−n
for all odd integers n. Then f is not negligible, and 1/f is neither poly-bounded nor super-poly. 2
2.3.2 Computational ciphers: the formalities
Now the formalities. We begin by admitting a lie: when we said a computational cipher E= (E,D)
is deﬁned over (K,M,C), where Kis the key space, Mis the message space, and Cis the ciphertext
space, and with each of these spaces being ﬁnite sets, we were not telling the whole truth. In the
mathematical model (though not always in real-world systems), we associate with Efamilies of key,
message, and ciphertext spaces, indexed by
• a security parameter, which is a positive integer, and is denoted by λ, and
• a system parameter, which is a bit string, and is denoted by Λ.
Thus, instead of just ﬁnite sets K, M, and C, we have families of ﬁnite sets
{Kλ,Λ}λ,Λ, {Mλ,Λ}λ,Λ, and {Cλ,Λ}λ,Λ,
which for the purposes of this deﬁnition, we view as sets of bit strings (which may represent
mathematical objects by way of some canonical encoding functions).
The idea is that when the cipher Eis deployed, the security parameter λis ﬁxed to some value.
Generally speaking, larger values of λimply higher levels of security (i.e., resistance against adver-
saries with more computational resources), but also larger key sizes, as well as slower encryption
and decryption speeds. Thus, the security parameter is like a “dial” we can turn, setting a trade-oﬀ
between security and eﬃciency.
Once λ is chosen, a system parameter Λ is generated using an algorithm speciﬁc to the cipher.
The idea is that the system parameter Λ (together with λ) gives a detailed description of a ﬁxed
instance of the cipher, with
(K,M,C) = (Kλ,Λ, Mλ,Λ, Cλ,Λ).
This one, ﬁxed instance may be deployed in a larger system and used by many parties — the values
of λ and Λ are public and known to everyone (including the adversary).
Example 2.12. Consider the additive one-time pad discussed in Example 2.4. This cipher was
described in terms of a modulus n. To deploy such a cipher, a suitable modulus n is generated,
and is made public (possibly just “hardwired” into the software that implements the cipher). The
modulus n is the system parameter for this cipher. Each speciﬁc value of the security parameter
determines the length, in bits, of n. The value nitself is generated by some algorithm that may be
probabilistic and whose output distribution may depend on the intended application. For example,
we may want to insist that n is a prime in some applications. 2
29
Before going further, we deﬁne the notion of an eﬃcient algorithm. For the purposes of this
deﬁnition, we shall only consider algorithms Athat take as input a security parameter λ, as well as
other parameters whose total length is bounded by some ﬁxed polynomial in λ. Basically, we want
to say that the running time of A is bounded by a polynomial in λ, but things are complicated if
A is probabilistic:
Deﬁnition 2.8 (eﬃcient algorithm). Let A be an algorithm (possibly probabilistic) that takes
as input a security parameter λ ∈Z≥1, as well as other parameters encoded as a bit string x ∈
{0,1}≤p(λ) for some ﬁxed polynomial p. We call A an eﬃcient algorithm if there exist a poly-
bounded function t and a negligible function ϵ such that for all λ ∈Z≥1, and all x ∈{0,1}≤p(λ),
the probability that the running time of A on input (λ,x) exceeds t(λ) is at most ϵ(λ).
We stress that the probability in the above deﬁnition is with respect to the coin tosses of A:
this bound on the probability must hold for all possible inputs x.1
Here is a formal deﬁnition that captures the basic requirements of systems that are parameter-
ized by a security and system parameter, and introduces some more terminology. In the following
deﬁnition we use the notation Supp( P(λ)) to refer to the support of the distribution P(λ), which
is the set of all possible outputs of algorithm P on input λ.
Deﬁnition 2.9. A system parameterization is an eﬃcient probabilistic algorithm P that given
a security parameter λ∈Z≥1 as input, outputs a bit string Λ, called a system parameter, whose
length is always bounded by a polynomial in λ. We also deﬁne the following terminology:
• A collection S = {Sλ,Λ}λ,Λ of ﬁnite sets of bits strings, where λruns over Z≥1 and Λ runs over
Supp(P(λ)), is called a family of spaces with system parameterization P, provided the
lengths of all the strings in each of the sets Sλ,Λ are bounded by some polynomial p in λ.
• We say that S is eﬃciently recognizable if there is an eﬃcient deterministic algorithm
that on input λ∈Z≥1, Λ ∈Supp(P(λ)), and s∈{0,1}≤p(λ), determines if s∈Sλ,Λ.
• We say that S is eﬃciently sampleable if there is an eﬃcient probabilistic algorithm that
on input λ∈Z≥1 and Λ ∈Supp(P(λ)), outputs an element uniformly distributed over Sλ,Λ.
• We say that S has an eﬀective length function if there is an eﬃcient deterministic
algorithm that on input λ ∈Z≥1, Λ ∈Supp(P(λ)), and s ∈Sλ,Λ, outputs a non-negative
integer, called the length of s.
We can now state the complete, formal deﬁnition of a computational cipher:
1By not insisting that a probabilistic algorithm halts in a speciﬁed time bound with probability 1, we give ourselves
a little “wiggle room,” which allows us to easily do certain types of random sampling procedure that have no a priori
running time bound, but are very unlikely to run for too long (e.g., think of ﬂipping a coin until it comes up “heads”).
An alternative approach would be to bound the expectedrunning time, but this turns out to be somewhat problematic
for technical reasons.
Note that this deﬁnition of an eﬃcient algorithm does not require that the algorithm halt with probability 1 on
all inputs. An algorithm that with probability 2 −λ entered an inﬁnite loop would satisfy the deﬁnition, even though
it does not halt with probability 1. These issues are rather orthogonal. In general, we shall only consider algorithms
that halt with probability 1 on all inputs: this can more naturally be seen as a requirement on the output distribution
of the algorithm, rather than on its running time.
30
Deﬁnition 2.10 (computational cipher). A computational cipher consists of a pair of algo-
rithms E and D, along with three families of spaces with system parameterization P:
K = {Kλ,Λ}λ,Λ, M = {Mλ,Λ}λ,Λ, and C = {Cλ,Λ}λ,Λ,
such that
1. K, M, and C are eﬃciently recognizable.
2. K is eﬃciently sampleable.
3. M has an eﬀective length function.
4. Algorithm E is an eﬃcient probabilistic algorithm that on input λ,Λ,k,m , where λ ∈Z≥1,
Λ ∈Supp(P(λ)), k∈Kλ,Λ, and m∈Mλ,Λ, always outputs an element of Cλ,Λ.
5. Algorithm D is an eﬃcient deterministic algorithm that on input λ,Λ,k,c , where λ ∈Z≥1,
Λ ∈Supp(P(λ)), k ∈Kλ,Λ, and c ∈Cλ,Λ, outputs either an element of Mλ,Λ, or a special
symbol reject /∈Mλ,Λ.
6. For all λ,Λ,k,m,c , where λ ∈ Z≥1, Λ ∈ Supp(P(λ)), k ∈ Kλ,Λ, m ∈ Mλ,Λ, and c ∈
Supp(E(λ,Λ; k,m)), we have D(λ,Λ; k,c) = m.
Note that in the above deﬁnition, the encryption and decryption algorithms take λ and Λ
as auxiliary inputs. So as to be somewhat consistent with the notation already introduced in
Section 2.2.1, we write this as E(λ,Λ; ···) and D(λ,Λ; ···).
Example 2.13. Consider the additive one-time pad (see Example 2.12). In our formal framework,
the security parameter λ determines the bit length L(λ) of the modulus n, which is the system
parameter. The system parameter generation algorithm takes as input λand generates a modulus
n of length L(λ). The function L(·) should be polynomially bounded. With this assumption, it is
clear that the system parameter generation algorithm satisﬁes its requirements. The requirements
on the key, message, and ciphertext spaces are also satisﬁed:
1. Elements of these spaces have polynomially bounded lengths: this again follows from our
assumption that L(·) is polynomially bounded.
2. The key space is eﬃciently sampleable: just choose k←R {0,...,n −1}.
3. The key, message, and ciphertext spaces are eﬃciently recognizable: just test if a bit string s
is the binary encoding of an integer between 0 and n−1.
4. The message space also has an eﬀective length function: just output (say) 0. 2
We note that some ciphers (for example the one-time pad) may not need a system parameter.
In this case, we can just pretend that the system parameter is, say, the empty string. We also note
that some ciphers do not really have a security parameter either; indeed, many industry-standard
ciphers simply come ready-made with a ﬁxed key size, with no security parameter that can be
tuned. This is simply mismatch between theory and practice — that is just the way it is.
31
That completes our formal mathematical description of a computational cipher, in all its glo-
rious detail. 2 The reader should hopefully appreciate that while these formalities may allow us
to make mathematically precise and meaningful statements, they are not very enlightening, and
mostly serve to obscure what is really going on. Therefore, in the main body of the text, we will
continue to discuss ciphers using the simpliﬁed terminology and notation of Section 2.2.1, with the
understanding that all statements made have a proper and natural interpretation in the formal
framework discussed in this section. This will be a pattern that is repeated in the sequel: we shall
mainly discuss various types of cryptographic schemes using a simpliﬁed terminology, without men-
tion of security parameters and system parameters — these mathematical details will be discussed
in a separate section, but will generally follow the same general pattern established here.
2.3.3 Eﬃcient adversaries and attack games
In deﬁning the notion of semantic security, we have to deﬁne what we mean by aneﬃcient adversary.
Since this concept will be used extensively throughout the text, we present a more general framework
here.
For any type of cryptographic scheme, security will be deﬁned using an attack game, played
between an adversary Aand a challenger: Afollows an arbitrary protocol, while the challenger
follows some simple, ﬁxed protocol determined by the cryptographic scheme and the notion of
security under discussion. Furthermore, both adversary and challenger take as input a common
security parameter λ, and the challenger starts the game by computing a corresponding system
parameter Λ, and sending this to the adversary.
To model these types of interactions, we introduce the notion of an interactive machine.
Before such a machine M starts, it always gets the security parameter λwritten in a special buﬀer,
and the rest of its internal state is initialized to some default value. Machine M has two other
special buﬀers: an incoming message buﬀer and an outgoing message buﬀer . Machine M may be
invoked many times: each invocation starts when M’s external environment writes a string to M’s
incoming message buﬀer; M reads the message, performs some computation, updates its internal
state, and writes a string on its outgoing message buﬀer, ending the invocation, and the outgoing
message is passed to the environment. Thus, M interacts with its environment via a simple message
passing system. We assume that M may indicate that it has halted by including some signal in its
last outgoing message, and M will essentially ignore any further attempts to invoke it.
We shall assume messages to and from the machine M are restricted to be of constant length.
This is not a real restriction: we can always simulate the transmission of one long message by
sending many shorter ones. However, making a restriction of this type simpliﬁes some of the
technicalities. We assume this restriction from now on, for adversaries as well as for any other type
of interactive machine.
For any given environment, we can measure the total running time of M by counting the
number of steps it performs across all invocations until it signals that it has halted. This running
time depends not only on M and its random choices, but also on the environment in which M
runs.3
2Note that the deﬁnition of a Shannon cipher in Section 2.1.1 remains unchanged. The claim made at the end of
Section 2.2.1 that any deterministic computational cipher is also a Shannon cipher needs to be properly interpreted:
for each λ and Λ, we get a Shannon cipher deﬁned over ( Kλ,Λ,Mλ,Λ,Cλ,Λ).
3Analogous to the discussion in footnote 1 on page 30, our deﬁnition of an eﬃcient interactive machine will not
require that it halts with probability 1 for all environments. This is an orthogonal issue, but it will be an implicit
32
Deﬁnition 2.11 (eﬃcient interactive machine). We say that M is an eﬃcient interactive
machine if there exist a poly-bounded function t and a negligible function ϵ, such that for all
environments (not even computationally unbounded ones), the probability that the total running
time of M exceeds t(λ) is at most ϵ(λ).
We naturally model an adversary as an interactive machine. An eﬃcient adversary is simply
an eﬃcient interactive machine.
We can connect two interactive machines together, say M′and M, to create a new interactive
machine M′′ = ⟨M′,M⟩. Messages from the environment to M′′ always get routed to M′. The
machine M′ may send a message to the environment, or to M; in the latter case, the output
message sent by M gets sent to M′. We assume that if M halts, then M′ does not send it any
more messages.
Thus, when M′′ is invoked, its incoming message is routed to M′, and then M′ and M may
interact some number of times, and then the invocation of M′′ ends when M′ sends a message to
the environment. We call M′the “open” machine (which interacts with the outside world), and M
the “closed” machine (which interacts only with M′).
Naturally, we can model the interaction of a challenger and an adversary by connecting two
such machines together as above: the challenger becomes the open machine, and the adversary
becomes the closed machine.
In our security reductions, we typically show how to use an adversary Athat breaks some
system to build an adversary Bthat breaks some other system. The essential property that we
want is that if Ais eﬃcient, then so is B. However, our reductions are almost always of a very
special form, where Bis a wrapper around A, consisting of some simple and eﬃcient “interface
layer” between B’s challenger and a single running instance of A.
Ideally, we want the computational complexity of the interface layer to not depend on the
computational complexity of A; however, some dependence is unavoidable: the more queries A
makes to its challenger, the more work must be performed by the interface layer, but this work
should just depend on the number of such queries and not on the running time of A.
To formalize this, we buildBas a composed machine ⟨M′,M⟩, where M′represents the interface
layer (the “open” machine), and M represents the instance of A(the “closed” machine). This leads
us to the following deﬁnition.
Deﬁnition 2.12 (elementary wrapper). An interactive machine M′ is called an eﬃcient
interface if there exists a poly-bounded function t and a negligible function ϵ, such that for all
M (not necessarily computationally bounded), when we execute the composed machine ⟨M′,M⟩in
an arbitrary environment (again, not necessarily computationally bounded), the following property
holds:
at every point in the execution of ⟨M′,M⟩, if I is the number of interactions between
M′ and M up to at that point, and T is the total running time of M′ up to that point,
then the probability that T >t(λ+ I) is at most ϵ(λ).
If M′ is an eﬃcient interface, and M is any machine, then we say ⟨M′,M⟩is an elementary
wrapper around M.
requirement of any machines we consider.
33
Thus, we will say adversary Bis an elementary wrapper around adversary Awhen it can be
structured as above, as an eﬃcient interface interacting with A. Our deﬁnitions were designed to
work well together. The salient properties are:
• If Bis an elementary wrapper around A, and Ais eﬃcient, then Bis eﬃcient.
• If Cis an elementary wrapper around Band Bis an elementary wrapper around A, then Cis
an elementary wrapper around A.
Also note that in our attack games, the challenger typically satisﬁes our deﬁnition of an eﬃcient
interface. For such a challenger and any eﬃcient adversary A, we can view their entire interaction
as a that of a single, eﬃcient machine.
Query bounded adversaries. In the attack games we have seen so far, the adversary makes
just a ﬁxed number of queries. Later in the text, we will see attack games in which the adversary
Ais allowed to make many queries — even though there is no a priori bound on the number of
queries it is allowed to make, if Ais eﬃcient, the number of queries will be bounded by some
poly-bounded value Q (at least with all but negligible probability). In proving security for such
attack games, in designing an elementary wrapper Bfrom A, it will usually be convenient to tell
Bin advance an upper bound Q on how many queries Awill ultimately make. To ﬁt this into our
formal framework, we can set things up so that Astarts out by sending a sequence of Q special
messages to “signal” this query bound to B. If we do this, then not only can Buse the value Qin its
logic, it is also allowed to run in time that depends on Q, without violating the time constraints in
Deﬁnition 2.12. This is convenient, as then Bis allowed to initialize data structures whose size may
depend on Q. Of course, all of this is just a legalistic “hack” to work around technical constraints
that would otherwise be too restrictive, and should not be taken too seriously. We will never make
this “signaling” explicit in any of our presentations.
2.3.4 Semantic security: the formalities
In deﬁning any type of security, we will deﬁne the adversary’s advantage in the attack game as
a function Adv( λ). This will be deﬁned in terms of probabilities of certain events in the attack
game: for each value of λwe get a diﬀerent probability space, determined by the random choices of
the challenger, and the random choices of the adversary. Security will mean that for every eﬃcient
adversary, the function Adv(·) is negligible.
Turning now to the speciﬁc situation of semantic security of a cipher, in Attack Game 2.1, we
deﬁned the value SS adv[A,E]. This value is actually a function of the security parameter λ. The
proper interpretation of Deﬁnition 2.2 is that Eis secure if for all eﬃcient adversariesA(modeled as
an interactive machine, as described above), the function SS adv[A,E](λ) in the security parameter
λ is negligible (as deﬁned in Deﬁnition 2.5). Recall that both challenger and adversary receive λ
as a common input. Control begins with the challenger, who sends the system parameter to the
adversary. The adversary then sends its query to the challenger, which consists of two plaintexts,
who responds with a ciphertext. Finally, the adversary outputs a bit (technically, in our formal
machine model, this “output” is a message sent to the challenger, and then the challenger halts).
The value of SS adv[A,E](λ) is determined by the random choices of the challenger (including the
choice of system parameter) and the random choices of the adversary. See Fig. 2.6 for a complete
picture of Attack Game 2.1.
34
Challenger A
ˆb 2 {0, 1}
(Experiment b)
 
⇤ R
 P( )
⇤
k R
 K  ,⇤
m0,m 1 2M ,⇤
c R
 E( , ⇤; k,m b) c
Figure 2.6: The fully detailed version of Attack Game 2.1
Also, in Attack Game 2.1, the requirement that the two messages presented by the adversary
have the same length means that the length function provided in part 3 of Deﬁnition 2.10 evaluates
to the same value on the two messages.
It is perhaps useful to see what it means for a cipher Eto be insecure according to this formal
deﬁnition. This means that there exists an adversary Asuch that SS adv[A,E] is a non-negligible
function in the security parameter. This means that SS adv[A,E](λ) ≥1/λc for some c> 0 and for
inﬁnitely many values of the security parameter λ. So this does not mean that Acan “break” E
for all values of the security parameter, but only inﬁnitely many values of the security parameter.
In the main body of the text, we shall mainly ignore security parameters, system parameters,
and the like, but it will always be understood that all of our “shorthand” has a precise mathematical
interpretation. In particular, we will often refer to certain values v as being negligible (resp., poly-
bounded), which really means that v is a negligible (resp., poly-bounded) function of the security
parameter.
2.4 A fun application: anonymous routing
Our friend Alice wants to send a message m to Bob, but she does not want Bob or anyone else to
know that the message m is from Alice. For example, Bob might be running a public discussion
forum and Alice wants to post a comment anonymously on the forum. Posting anonymously lets
Alice discuss health issues or other matters without identifying herself. In this section we will
assume Alice only wants to post a single message to the forum.
One option is for Alice to choose a proxy, Carol, send m to Carol, and ask Carol to forward
the message to Bob. This clearly does not provide anonymity for Alice since anyone watching the
35
network will see that m was sent from Alice to Carol and then from Carol to Bob. By tracing the
path of m through the network anyone can see that the post came from Alice.
A better approach is for Alice to establish a shared key k with Carol and send c:= E(k,m) to
Carol, where E= (E,D) is a semantically secure cipher. Carol decrypts cand forwards mto Bob.
Now, someone watching the network will see one message sent from Alice to Carol and a diﬀerent
message sent from Carol to Bob. Nevertheless, this method still does not ensure anonymity for
Alice: if on a particular day the only message that Carol receives is the one from Alice and the only
message she sends goes to Bob, then an observer can link the two and still learn that the posted
message came from Alice.
We solve this problem by having Carol provide a mixing service, that is, a service that mixes
incoming messages from many diﬀerent parties A1,...,A n. For i = 1 ,...,n , Carol establishes
a secret key ki with party Ai and each party Ai sends to Carol an encrypted message ci :=
E
(
ki, ⟨destinationi,mi⟩
)
. Carol collects all n incoming ciphertexts, decrypts each of them with
the correct key, and forwards the resulting plaintexts in some random order to their destinations.
Now an observer examining Carol’s traﬃc sees nmessages going in and nmessages going out, but
cannot tell which message was sent where. Alice’s message is one of the n messages sent out by
Carol, but the observer cannot tell which one. We say that Alice’s anonymity set is of size n.
The remaining problem is that Carol can still tell that Alice is the one who posted a speciﬁc
message on the discussion forum. To eliminate this ﬁnal risk Alice uses multiple mixing services,
say, Carol and David. She establishes a secret key kc with Carol and a secret key kd with David.
To send her message to Bob she constructs the following nested ciphertext c2:
c2 := E
(
kc, E(kd,m)
)
. (2.12)
For completeness Alice may want to embed routing information inside the ciphertext so that c2 is
actually constructed as:
c2 := E
(
kc, ⟨David,c1⟩
)
where c1 := E
(
kd, ⟨Bob,m⟩
)
.
Next, Alice sends c2 to Carol. Carol decrypts c2 and obtains the plaintext ⟨David,c1⟩which tells
her to send c1 to David. David decrypts c1 and obtains the plaintext ⟨Bob,m⟩which tells him to
send mto Bob. This process of decrypting a nested ciphertext, illustrated in Fig. 2.7, is similar to
peeling an onion one layer at a time. For this reason this routing procedure is often called onion
routing.
Now even if Carol observes all network traﬃc she cannot tell with certainty who posted a
particular message on Bob’s forum. The same holds for David. However, if Carol and David
collude they can ﬁgure it out. For this reason Alice may want to route her message through more
than two mixes. As long as one of the mixes does not collude with the others, Alice’s anonymity
will be preserved.
One complication is that when Alice establishes her shared secret key kd with David, she must
do so without revealing her identity to David. Otherwise, David will know that c1 came from
Alice, which we do not want. This is not diﬃcult to do, and we will see how later in the book
(Section 21.13).
Security of nested encryption. To preserve Alice’s anonymity it is necessary that Carol, who
knows kc, learn no information about m from the nested ciphertext c2 in (2.12). Otherwise, Carol
could potentially use the information she learns about mfrom c2 to link Alice to her post on Bob’s
36
Alice&
 Carol&
 David&
 Bob&
c2&
 c1&
 m&
mix& mix&
Figure 2.7: An example onion routing using two mixes
discussion forum. For example, suppose Carol could learn the ﬁrst few characters of mfrom c2 and
later ﬁnd that there is only one post on Bob’s forum starting with those characters. Carol could
then link the entire post to Alice because she knows that c2 came from Alice.
The same should hold for David. David has kd, and by observing network traﬃc, knows that
Alice sent c2. As in the previous paragraph, it is important that David learn nothing about m
from c2.
Let us argue that if Eis semantically secure, then no eﬃcient adversary can learn information
about m given c2 and one of kc or kd. More generally, for a cipher E = ( E,D) deﬁned over
(K,M,C), let us deﬁne the n-way nested cipher En = (En,Dn) as
En
(
(k1,...,k n), m
):= E
(
kn, E(kn−1, ···E(k1,m))
)
.
Decryption applies the keys in the reverse order:
Dn
(
(k1,...,k n), c
):= D
(
k1, D(k2, ···D(kn,c))
)
.
Our goal is to show that if E is semantically secure then En is semantically secure even if the
adversary is given all but one of the keysk1,...,k n. To make this precise, we deﬁne two experiments,
Experiment 0 and Experiment 1, where for b= 0,1, Experiment b is:
• The adversary gives the challenger (m0,m1,d) where m0,m1 ∈M are equal length messages
and 1 ≤d≤n.
• The challenger chooses n keys k1,...,k n ←R Kand computes c ←R En
(
(k1,...,k n), mb
)
. It
sends c to the adversary along with all keys k1,...,k n, but excluding the key kd.
• The adversary outputs a bit ˆb∈{0,1}.
This game captures the fact that the adversary sees all keys k1,...,k n except for kd and tries to
break semantic security.
We deﬁne the adversary’s advantage, NE(n)adv[A,E], as in the deﬁnition of semantic security:
NE(n)adv[A,E] :=
⏐⏐Pr[W0] −Pr[W1]
⏐⏐
where Wb is the event that Aoutputs 1 in Experiment b, for b= 0,1. We say that Eis semantically
secure for n-way nesting if NE(n)adv[A,E] is negligible.
Theorem 2.12. For every constant n> 0, if E= (E,D) is semantically secure then Eis seman-
tically secure for n-way nesting.
In particular, for every n-way nested adversary Aattacking En, there exists a semantic security
adversary Battacking E, where Bis an elementary wrapper around A, such that
NE(n)adv[A,E] = SSadv[B,E] .
The proof of this theorem is a good exercise in security reductions. We leave it for Exercise 2.15.
37
2.5 Notes
The one time pad is due to Gilbert Vernam in 1917, although there is evidence that it was discovered
earlier [15].
Citations to the literature to be added.
2.6 Exercises
2.1 (multiplicative one-time pad). We may also deﬁne a “multiplication mod p” variation of
the one-time pad. This is a cipher E= (E,D), deﬁned over ( K,M,C), where K:= M:= C:=
{1,...,p −1}, where p is a prime. Encryption and decryption are deﬁned as follows:
E(k,m) := k·mmod p D (k,c) := k−1 ·cmod p.
Here, k−1 denotes the multiplicative inverse of k modulo p. Verify the correctness property for this
cipher and prove that it is perfectly secure.
2.2 (A good substitution cipher). Consider a variant of the substitution cipher E = (E,D)
deﬁned in Example 2.3 where every symbol of the message is encrypted using an independent
permutation. That is, let M= C= ΣL for some a ﬁnite alphabet of symbols Σ and some L. Let
the key space be K= SL where S is the set of all permutations on Σ. The encryption algorithm
E(k,m) is deﬁned as
E(k,m) :=
(
k[0](m[0]), k[1](m[1]), ..., k[L−1](m[L−1])
)
Show that Eis perfectly secure.
2.3 (A broken one-time pad). Consider a variant of the one time pad with message space
{0,1}L where the key space Kis restricted to all L-bit strings with an even number of 1’s. Give an
eﬃcient adversary whose semantic security advantage is 1.
2.4 (Encryption chain). Let E= (E,D) be a cipher deﬁned over ( K,M,C) where K= M. Let
E′= (E′,D′) be a cipher where encryption is deﬁned as
E′(
(k1,k2),m
):=
(
E(k1,k2), E(k2,m)
)
∈C2.
Show that if E is perfectly secure then so is E′. Exercise 3.2 describes an application for this
encryption scheme.
2.5 (A stronger impossibility result). This exercise generalizes Shannon’s theorem (Theo-
rem 2.5). Let Ebe a cipher deﬁned over (K,M,C). Suppose that SS adv[A,E] ≤ϵfor all adversaries
A, even including computationally unbounded ones. Show that |K|≥ (1 −ϵ)|M|.
2.6 (A matching bound). This exercise develops a converse of sorts for the previous exercise.
For j = 1,...,L , let ϵ:= 1/2j. Consider the L-bit one-time pad variant Edeﬁned over (K,M,C)
where M= C= {0,1}L. The key space Kis restricted to all L-bit strings whose ﬁrst j bits are
not all zero, so that |K|= (1 −ϵ)|M|. Show that:
(a) there is an eﬃcient adversary Asuch that SSadv[A,E] = ϵ/(1 −ϵ);
38
(b) for all adversaries A, even including computationally unbounded ones, SSadv[A,E] ≤ϵ/(1−ϵ).
Note: Since the advantage of Ain part (a) is non-zero, the cipher Ecannot be perfectly secure.
2.7 (Deterministic ciphers). In this exercise, you are asked to prove in detail the claims made
in Example 2.9. Namely, show that if E is a deterministic cipher that is perfectly secure, then
SSadv[A,E] = 0 for every adversary A(bearing in mind that Amay be probabilistic); also show
that if Eis the variable length one-time pad, then SS adv[A,E] = 0 for all adversaries A.
2.8 (Roulette). In Section 2.2.4, we argued that if value r is encrypted using a semantically
secure cipher, then a player’s odds of winning at Internet roulette are very close to those of real
roulette. However, our “roulette” game was quite simple. Suppose that we have a more involved
game, where diﬀerent outcomes may result in diﬀerent winnings. The rules are not so important,
but assume that the rules are easy to evaluate (given a bet and the number r) and that every bet
results in a payout of 0 ,1,...,n dollars, where nis poly-bounded. Let µbe the expected winnings
in an optimal strategy for a real version of this game (with no encryption). Let µ′be the expected
winnings of some (eﬃcient) player in an Internet version of this game (with encryption). Show that
µ′≤µ+ ϵ, where ϵ is negligible, assuming the cipher is semantically secure.
Hint: You may want to use the fact that if X is a random variable taking values in the set
{0,1,...,n }, the expected value of X is equal to ∑n
i=1 Pr[X ≥i].
2.9. Prove Fact 2.6, using the formal deﬁnitions in Section 2.3.
2.10 (Exercising the deﬁnition of semantic security). Let E= (E,D) be a semantically
secure cipher deﬁned over ( K,M,C), where M= C= {0,1}L. Which of the following encryption
algorithms yields a semantically secure scheme? Either give an attack or provide a security proof
via an explicit reduction.
(a) E1(k,m) := 0 ∥E(k,m)
(b) E2(k,m) := E(k,m) ∥parity(m)
(c) E3(k,m) := reverse(E(k,m))
(d) E4(k,m) := E(k,reverse(m))
Here, for a bit string s, parity( s) is 1 if the number of 1’s in s is odd, and 0 otherwise; also,
reverse(s) is the string obtained by reversing the order of the bits in s, e.g., reverse(1011) = 1101.
2.11 (Key recovery attacks). Let E= (E,D) be a cipher deﬁned over (K,M,C). A key recovery
attack is modeled by the following game between a challenger and an adversary A: the challenger
chooses a random key k in K, a random message min M, computes c←R E(k,m), and sends (m,c)
to A. In response Aoutputs a guess ˆkin K. We say that Awins the game if D(ˆk,c) = mand deﬁne
KRadv[A,E] to be the probability that Awins the game. As usual, we say that Eis secure against
key recovery attacks if for all eﬃcient adversaries Athe advantage KRadv[A,E] is negligible.
(a) Show that the one-time pad is not secure against key recovery attacks.
(b) Show that if Eis semantically secure andϵ= |K|/|M|is negligible, then Eis secure against key
recovery attacks. In particular, show that for every eﬃcient key-recovery adversary Athere
39
is an eﬃcient semantic security adversary B, where Bis an elementary wrapper around A,
such that
KRadv[A,E] ≤SSadv[B,E] + ϵ
Hint: Your semantic security adversary Bwill output 1 with probability KRadv[A,E] in the
semantic security Experiment 1, and output 1 with probability at most ϵ in Experiment 0.
Deduce from this a lower bound on SS adv[B,E] in terms of ϵ and KRadv[A,E] from which
the result follows.
(c) Deduce from part (b) that if Eis semantically secure and |M|is super-poly, then |K|cannot
be poly-bounded.
Note: |K|can be poly-bounded when |M|is poly-bounded, as in the one-time pad.
2.12 (Security against message recovery). In Section 2.2.3.1 we developed the notion of
security against message recovery. Construct a cipher that is secure against message recovery, but
is not semantically secure.
2.13 (Advantage calculations in simple settings). Consider the following two experiments
Experiment 0 and Experiment 1:
• In Experiment 0 the challenger ﬂips a fair coin (probability 1 /2 for HEADS and 1 /2 for
TAILS) and sends the result to the adversary A.
• In Experiment 1 the challenger always sends TAILS to the adversary.
The adversary’s goal is to distinguish these two experiments: at the end of each experiment the
adversary outputs a bit 0 or 1 for its guess for which experiment it is in. For b = 0 ,1 let Wb
be the event that in experiment b the adversary output 1. The adversary tries to maximize its
distinguishing advantage, namely the quantity
⏐⏐Pr[W0] −Pr[W1]
⏐⏐ ∈[0,1] .
If the advantage is negligible for all eﬃcient adversaries then we say that the two experiments are
indistinguishable.
(a) Calculate the advantage of each of the following adversaries:
(i) A1: Always output 1.
(ii) A2: Ignore the result reported by the challenger, and randomly output 0 or 1 with even
probability.
(iii) A3: Output 1 if HEADS was received from the challenger, else output 0.
(iv) A4: Output 0 if HEADS was received from the challenger, else output 1.
(v) A5: If HEADS was received, output 1. If TAILS was received, randomly output 0 or 1
with even probability.
(b) What is the maximum advantage possible in distinguishing these two experiments? Explain
why.
40
2.14 (Permutation cipher). Consider the following cipher (E,D) deﬁned over (K,M,C) where
C= M= {0,1}ℓ and Kis the set of all ℓ! permutations of the set {0,...,ℓ −1}. For a key k ∈K
and message m∈M deﬁne E(k,m) to be result of permuting the bits of musing the permutation
k, namely E(k,m) = m[k(0)]...m[k(ℓ−1)]. Show that this cipher is not semantically secure by
showing an adversary that achieves advantage 1.
2.15 (Nested encryption). For a cipher E= (E,D) with key space Kdeﬁne the nested cipher
E′= (E′,D′) as
E′(
(k0,k1),m
):= E
(
k1,E(k0,m)
)
and D′(
(k0,k1),c
):= D(k0,D(k1,c)) .
Our goal is to show that if E is semantically secure then E′ is semantically secure even if the
adversary is given one of the keys k0 or k1.
(a) Consider the following two semantic security experiments, Experiments 0 and 1. For b= 0,1,
in Experiment b the adversary ﬁrst sends to the challenger two messages m0 and m1. The
challenger chooses keys k0,k1 ←R Kand sends back the pair (k1,c) where c←R E′(
(k0,k1), mb).
Finally, the adversary outputs ˆbin {0,1}. We deﬁne the adversary’s advantage, NEadv[A,E],
as in the usual the deﬁnition of semantic security. Show that for every nested encryption
adversary Aattacking E′, there is a semantic security adversary Battacking E, where Bis an
elementary wrapper around A, such that
NEadv[A,E] = SSadv[B,E] .
Draw a diagram with Aon the right, Bin the middle, and B’s challenger on the left. Show
the message ﬂow between these three parties that takes place in your proof of security.
(b) Repeat part (a), but now when the adversary gets back the pair ( k0,c) from the challenger
(i.e., k1 is replaced by k0), where c←R E′(
(k0,k1), mb) as before. Draw a diagram describing
the message ﬂow in your proof of security as you did in part (a).
This problem comes up in the context of anonymous routing on the Internet as discussed in Sec-
tion 2.4.
2.16 (Self referential encryption). Let us show that encrypting a key under itself can be
dangerous. Let Ebe a semantically secure cipher deﬁned over ( K,M,C), where K⊆M , and let
k ←R K. A ciphertext c∗ := E(k,k), namely encrypting k using k, is called a self referential
encryption.
(a) Construct a cipher ˜E= ( ˜E, ˜D) derived from Esuch that ˜Eis semantically secure, but becomes
insecure if the adversary is given ˜E(k,k). You have just shown that semantic security does
not imply security when one encrypts one’s key.
(b) Construct a cipher ˆE= ( ˆE, ˆD) derived from Esuch that ˆEis semantically secure and remains
semantically secure (provably) even if the adversary is given ˆE(k,k). To prove that ˆE is
semantically secure, you should show the following: for every adversary Athat attacks ˆE,
there exists and adversary Bthat attacks Esuch that (i) the running time Bis about the
same as that of A, and (ii) SS adv[A, ˆE] ≤SSadv[B,E].
2.17 (Compression and encryption). Two standards committees propose to save bandwidth
by combining compression (such as the Lempel-Ziv algorithm used in the zip and gzip programs)
with encryption. Both committees plan on using the variable length one time pad for encryption.
41
• One committee proposes to compress messages before encrypting them. Explain why this is
a bad idea.
Hint: Recall that compression can signiﬁcantly shrink the size of some messages while having
little impact on the length of other messages.
• The other committee proposes to compress ciphertexts after encryption. Explain why this is
a bad idea.
Over the years many problems have surfaced when combining encryption and compression. The
CRIME [136] and BREACH [131] attacks are good representative examples.
2.18 (Voting protocols). This exercise develops a simple voting protocol based on the additive
one-time pad (Example 2.4). Suppose we have t voters and a counting center. Each voter is going
to vote 0 or 1, and the counting center is going to tally the votes and broadcast the total sum S.
However, they will use a protocol that guarantees that no party (voter or counting center) learns
anything other than S (but we shall assume that each party faithfully follows the protocol).
The protocol works as follows. Let n>t be an integer. The counting center generates an encryption
of 0: c0 ←R {0,...,n −1}, and passes c0 to voter 1. Voter 1 adds his vote v1 to c0, computing
c1 ←c0 + v1 mod n, and passes c1 to voter 2. This continues, with each voter i adding vi to ci−1,
computing ci ←ci−1 + vi mod n, and passing ci to voter i+ 1, except that voter t passes ct to the
counting center. The counting center computes the total sum as S ←ct−c0 mod n, and broadcasts
S to all the voters.
(a) Show that the protocol correctly computes the total sum.
(b) Show that the protocol is perfectly secure in the following sense. For voter i= 1,...,t , deﬁne
Viewi := (S,ci−1), which represents the “view” of voter i. We also deﬁne View0 := (c0,ct),
which represents the “view” of the counting center. Show that for each i = 0 ,...,t and
S = 0,...,t , the following holds:
as the choice of votes v1,...,v t varies, subject to the restrictions that each vj ∈
{0,1}and ∑t
j=1 vj = S, the distribution of Viewi remains the same.
(c) Show that if two voters i,j collude, they can determine the vote of a third voter k. You are
free to choose the indices i,j,k .
2.19 (Two-way split keys). Let E = ( E,D) be a semantically secure cipher deﬁned over
(K,M,C) where K= {0,1}d. Suppose we wish to split the ability to decrypt ciphertexts across
two parties, Alice and Bob, so that both parties are needed to decrypt ciphertexts. For a random
key k in Kchoose a random r in Kand deﬁne ka := r and kb := k⊕r. Now if Alice and Bob get
together they can decrypt a ciphertext c by ﬁrst reconstructing the key k as k= ka ⊕kb and then
computing D(k,c). Our goal is to show that neither Alice nor Bob can decrypt ciphertexts on their
own.
(a) Formulate a security notion that captures the advantage that an adversary has in break-
ing semantic security given Bob’s key kb. Denote this 2-way key splitting advantage by
2KSadv[A,E].
(b) Show that for every 2-way key splitting adversary Athere is a semantic security adversary B
such that 2KSadv[A,E] = SSadv[B,E].
42
2.20 (Simple secret sharing). Let E= (E,D) be a semantically secure cipher with key space
K= {0,1}L. A bank wishes to split a decryption key k ∈{0,1}L into three shares p0,p1, and p2
so that two of the three shares are needed for decryption. Each share can be given to a diﬀerent
bank executive, and two of the three must contribute their shares for decryption to proceed. This
way, decryption can proceed even if one of the executives is out sick, but at least two executives
are needed for decryption.
(a) To do so the bank generates two random pairs (k0,k′
0) and (k1,k′
1) so thatk0⊕k′
0 = k1⊕k′
1 = k.
How should the bank assign shares so that any two shares enable decryption using k, but no
single share can decrypt?
Hint: The ﬁrst executive will be given the share p0 := (k0,k1).
(b) Generalize the scheme from part (a) so that 3-out-of-5 shares are needed for decryption.
Reconstituting the key only uses XOR of key shares. Two shares should reveal nothing about
the key k.
(c) More generally, we can design a t-out-of-w system this way for any t<w . How does the size
of each share scale with t? We will see a much better way to do this in Chapter 22
2.21 (Simple threshold decryption). Let E= (E,D) be a semantically secure cipher with key
space K. In this exercise we design a system that lets a bank split a key k into three shares p0,p1,
and p2 so that two of the three shares are needed for decryption, as in Exercise 2.20. However,
decryption is done without ever reconstituting the complete key at a single location.
We use nested encryption from Exercise 2.15. Choose a random key k:= (k0,k1,k2,k3) in K4 and
encrypt a message m as:
c←R
(
E
(
k1,E(k0,m)
)
, E
(
k3,E(k2,m)
))
.
(a) Construct the shares p0,p1,p2 so that any two shares enable decryption, but no single share
can decrypt. Hint: the ﬁrst share is p0 := (k0,k3).
Discussion: Suppose the entities holding shares p0 and p2 are available to decrypt. To
decrypt a ciphertext c, ﬁrst send c to the entity holding p2 to partially decrypt c. Then
forward the result to the entity holding p0 to complete the decryption. This way, decryption
is done without reconstituting the complete key k at a single location.
(b) Generalize the scheme from part (a) so that 3-out-of-5 shares are needed for decryption.
Explain how decryption can be done without reconstituting the key in a single location.
An encryption scheme where the key can be split into shares so that t-out-of-w shares are needed
for decryption, and decryption does not reconstitute the key at a single location, is said to provide
threshold decryption. We will see a much better way to do this in Chapter 22.
2.22 (Bias correction). Consider again the bit-guessing version of the semantic security attack
game (i.e., Attack Game 2.4). Suppose an eﬃcient adversary Awins the game (i.e., guesses the
hidden bit b) with probability 1 /2 + ϵ, where ϵ is non-negligible. Note that ϵ could be positive or
negative (the deﬁnition of negligible works on absolute values). Our goal is to show that there is
another eﬃcient adversary Bthat wins the game with probability 1/2+ ϵ′, where ϵ′is non-negligible
and positive.
43
(a) Consider the following adversary Bthat uses Aas a subroutine in Attack Game 2.4 in the
following two-stage attack. In the ﬁrst stage, Bplays challenger to A, but Bgenerates its
own hidden bit b0, its own key k0, and eventually Aoutputs its guess-bit ˆb0. Note that in
this stage, B’s challenger in Attack Game 2.4 is not involved at all. In the second stage, B
restarts A, and lets Ainteract with the “real” challenger in Attack Game 2.4, and eventually
Aoutputs a guess-bit ˆb. When this happens, Boutputs ˆb⊕ˆb0 ⊕b0. Note that this run of A
is completely independent of the ﬁrst — the coins of Aand also the system parameters are
generated independently in these two runs.
Show that Bwins Attack Game 2.4 with probability 1 /2 + 2ϵ2.
(b) One might be tempted to argue as follows. Just construct an adversary Bthat runs A, and
when Aoutputs ˆb, adversary Boutputs ˆb⊕1. Now, we do not know if ϵ is positive or
negative. If it is positive, then Asatisﬁes our requirements. If it is negative, then Bsatisﬁes
our requirements. Although we do not know which one of these two adversaries satisﬁes our
requirements, we know that one of them deﬁnitely does, and so existence is proved.
What is wrong with this argument? The explanation requires an understanding of the math-
ematical details regarding security parameters (see Section 2.3).
(c) Can you come up with another eﬃcient adversary B′ that wins the bit-guessing game with
probability at least 1/2 + |ϵ|/2? Your adversary B′will be less eﬃcient than B.
Hint: try running the ﬁrst stage of adversary Bmultiple times.
44
Chapter 3
Stream ciphers
In the previous chapter, we introduced the notions of perfectly secure encryption and semantically
secure encryption. The problem with perfect security is that to achieve it, one must use very long
keys. Semantic security was introduced as a weaker notion of security that would perhaps allow
us to build secure ciphers that use reasonably short keys; however, we have not yet produced any
such ciphers. This chapter studies one type of cipher that does this: the stream cipher.
3.1 Pseudo-random generators
Recall the one-time pad. Here, keys, messages, and ciphertexts are all L-bit strings. However, we
would like to use a key that is much shorter. So the idea is to instead use a short, ℓ-bit “seed” sas
the encryption key, where ℓis much smaller than L, and to “stretch” this seed into a longer, L-bit
string that is used to mask the message (and unmask the ciphertext). The string s is stretched
using some eﬃcient, deterministic algorithm G that maps ℓ-bit strings to L-bit strings. Thus, the
key space for this modiﬁed one-time pad is {0,1}ℓ, while the message and ciphertext spaces are
{0,1}L. For s∈{0,1}ℓ and m,c ∈{0,1}L, encryption and decryption are deﬁned as follows:
E(s,m) := G(s) ⊕m and D(s,c) := G(s) ⊕c.
This modiﬁed one-time pad is called a stream cipher, and the function G is called a pseudo-
random generator.
If ℓ < L, then by Shannon’s Theorem, this stream cipher cannot achieve perfect security;
however, if G satisﬁes an appropriate security property, then this cipher is semantically secure.
Suppose sis a random ℓ-bit string and ris a random L-bit string. Intuitively, if an adversary cannot
eﬀectively tell the diﬀerence between G(s) and r, then he should not be able to tell the diﬀerence
between this stream cipher and a one-time pad; moreover, since the latter cipher is semantically
secure, so should be the former. To make this reasoning rigorous, we need to formalize the notion
that an adversary cannot “eﬀectively tell the diﬀerence between G(s) and r.”
An algorithm that is used to distinguish a pseudo-random string G(s) from a truly random
string r is called a statistical test. It takes a string as input, and outputs 0 or 1. Such a test
is called eﬀective if the probability that it outputs 1 on a pseudo-random input is signiﬁcantly
diﬀerent than the probability that it outputs 1 on a truly random input. Even a relatively small
diﬀerence in probabilities, say 1%, is considered signiﬁcant; indeed, even with a 1% diﬀerence, if
we can obtain a few hundred independent samples, which are either all pseudo-random or all truly
45
random, then we will be able to infer with high conﬁdence whether we are looking at pseudo-random
strings or at truly random strings. However, a non-zero but negligible diﬀerence in probabilities,
say 2−100, is not helpful.
How might one go about designing an eﬀective statistical test? One basic approach is the
following: given an L-bit string, calculate some statistic, and then see if this statistic diﬀers greatly
from what one would expect if the string were truly random.
For example, a very simple statistic that is easy to compute is the number k of 1’s appearing
in the string. For a truly random string, we would expect k ≈L/2. If the PRG G had some
bias towards either 0-bits or 1-bits, we could eﬀectively detect this with a statistical test that,
say, outputs 1 if |k−0.5L|<0.01L, and otherwise outputs 0. This statistical test would be quite
eﬀective if the PRG G did indeed have some signiﬁcant bias towards either 0 or 1.
The test in the previous example can be strengthened by considering not just individual bits,
but pairs of bits. One could break the L-bit string up into ≈L/2 bit pairs, and count the number
k00 of pairs 00, the number k01 of pairs 01, the number k10 of pairs 10, and the number k11 of pairs
11. For a truly random string, one would expect each of these numbers to be ≈L/2 ·1/4 = L/8.
Thus, a natural statistical test would be one that tests if the distance from L/8 of each of these
numbers is less than some speciﬁed bound. Alternatively, one could sum up the squares of these
distances, and test whether this sum is less than some speciﬁed bound — this is the classical χ-
squared test from statistics. Obviously, this idea generalizes from pairs of bits to tuples of any
length.
There are many other simple statistics one might check. However, simple tests such as these do
not tend to exploit deeper mathematical properties of the algorithm G that a malicious adversary
may be able to exploit in designing a statistical test speciﬁcally geared towards G. For example,
there are PRG’s for which the simple tests in the previous two paragraphs are completely ineﬀective,
but yet are completely predictable, given suﬃciently many output bits; that is, given a preﬁx of
G(s) of suﬃcient length, the adversary can compute all the remaining bits of G(s), or perhaps even
compute the seed s itself.
Our deﬁnition of security for a PRG formalizes the notion that there should be no eﬀective (and
eﬃciently computable) statistical test.
3.1.1 Deﬁnition of a pseudo-random generator
A pseudo-random generator, or PRG for short, is an eﬃcient, deterministic algorithm Gthat,
given as input a seed s, computes an output r. The seed scomes from a ﬁnite seed space Sand
the output rbelongs to a ﬁnite output space R. Typically, Sand Rare sets of bit strings of some
prescribed length (for example, in the discussion above, we had S= {0,1}ℓ and R= {0,1}L). We
say that G is a PRG deﬁned over ( S,R).
Our deﬁnition of security for a PRG captures the intuitive notion that if sis chosen at random
from S and r is chosen at random from R, then no eﬃcient adversary can eﬀectively tell the
diﬀerence between G(s) and r: the two are computationally indistinguishable. The deﬁnition
is formulated as an attack game.
Attack Game 3.1 (PRG). For a given PRG G, deﬁned over ( S,R), and for a given adversary
A, we deﬁne two experiments, Experiment 0 and Experiment 1. For b= 0,1, we deﬁne:
Experiment b:
• The challenger computes r∈R as follows:
46
Challenger A
ˆb 2 {0, 1}
(Experiment 0)
s R
 S
r  G(s) r
Challenger A
ˆb 2 {0, 1}
r
(Exp eriment 1)
rR
 R
Figure 3.1: Experiments 0 and 1 of Attack Game 3.1
– if b= 0: s←R S, r←G(s);
– if b= 1: r←R R.
and sends r to the adversary.
• Given r, the adversary computes and outputs a bit ˆb∈{0,1}.
For b= 0,1, let Wb be the event that Aoutputs 1 in Experiment b. We deﬁne A’s advantage
with respect to G as
PRGadv[A,G] :=
⏐⏐⏐Pr[W0] −Pr[W1]
⏐⏐⏐. 2
The attack game is illustrated in Fig. 3.1.
Deﬁnition 3.1 (secure PRG). A PRG G is secure if the value PRGadv[A,G] is negligible for
all eﬃcient adversaries A.
As discussed in Section 2.2.5, Attack Game 3.1 can be recast as a “bit guessing” game, where
instead of having two separate experiments, the challenger chooses b∈{0,1}at random, and then
runs Experiment b against the adversary A. In this game, we measure A’s bit-guessing advantage
47
PRGadv∗[A,G] as |Pr[ˆb = b] −1/2|. The general result of Section 2.2.5 (namely, (2.11)) applies
here as well:
PRGadv[A,G] = 2 ·PRGadv∗[A,G]. (3.1)
We also note that a PRG can only be secure if the cardinality of the seed space is super-poly
(see Exercise 3.5).
3.1.2 Mathematical details
Just as in Section 2.3, we give here more of the mathematical details pertaining to PRGs. Just like
Section 2.3, this section may be safely skipped on ﬁrst reading with very little loss in understanding.
First, we state the precise deﬁnition of a PRG, using the terminology introduced in Deﬁni-
tion 2.9.
Deﬁnition 3.2 (pseudo-random generator). A pseudo-random generator consists of an
algorithm G, along with two families of spaces with system parameterization P:
S = {Sλ,Λ}λ,Λ and R = {Rλ,Λ}λ,Λ,
such that
1. S and R are eﬃciently recognizable and sampleable.
2. Algorithm G is an eﬃcient deterministic algorithm that on input λ,Λ,s, where λ ∈Z≥1,
Λ ∈Supp(P(λ)), and s∈Sλ,Λ, outputs an element of Rλ,Λ.
Next, Deﬁnition 3.1 needs to be properly interpreted. First, in Attack Game 3.1, it is to be
understood that for each value of the security parameter λ, we get a diﬀerent probability space,
determined by the random choices of the challenger and the random choices of the adversary.
Second, the challenger generates a system parameter Λ, and sends this to the adversary at the very
start of the game. Third, the advantage PRG adv[A,G] is a function of the security parameter λ,
and security means that this function is a negligible function.
3.2 Stream ciphers: encryption with a PRG
Let Gbe a PRG deﬁned over ({0,1}ℓ,{0,1}L); that is, Gstretches an ℓ-bit seed to an L-bit output.
The stream cipher E= (E,D) constructed from Gis deﬁned over ({0,1}ℓ,{0,1}≤L,{0,1}≤L);
for s ∈{0,1}ℓ and m,c ∈{0,1}≤L, encryption and decryption are deﬁned as follows: if |m|= v,
then
E(s,m) := G(s)[0 . .v−1] ⊕ m,
and if |c|= v, then
D(s,c) := G(s)[0 . .v−1] ⊕ c.
As the reader may easily verify, this satisﬁes our deﬁnition of a cipher (in particular, the correctness
property is satisﬁed).
Note that for the purposes of analyzing the semantic security of E, the length associated with a
message m in Attack Game 2.1 is the natural length |m|of m in bits. Also, note that if v is much
smaller than L, then for many practical PRGs, it is possible to compute the ﬁrst v bits of G(s)
much faster than actually computing all the bits of G(s) and then truncating.
The main result of this section is the following:
48
Theorem 3.1. If Gis a secure PRG, then the stream cipher Econstructed from Gis a semantically
secure cipher.
In particular, for every SS adversary Athat attacks E as in Attack Game 2.1, there exists a
PRG adversary Bthat attacks G as in Attack Game 3.1, where Bis an elementary wrapper
around A, such that
SSadv[A,E] = 2 ·PRGadv[B,G]. (3.2)
Proof idea. The basic idea is to argue that we can replace the output of the PRG by a truly random
string, without aﬀecting the adversary’s advantage by more than a negligible amount. However,
after making this replacement, the adversary’s advantage is zero. 2
Proof. Let Abe an eﬃcient adversary attacking Eas in Attack Game 2.1. We want to show that
SSadv[A,E] is negligible, assuming that Gis a secure PRG. It is more convenient to work with the
bit-guessing version of the SS attack game. We prove:
SSadv∗[A,E] = PRGadv[B,G] (3.3)
for some eﬃcient adversary B. Then (3.2) follows from Theorem 2.10. Moreover, by the assump-
tion that G is a secure PRG, the quantity PRG adv[B,G] must be negligible, and so the quantity
SSadv[A,E] is negligible as well.
So consider the adversary A’s attack of Ein the bit-guessing version of Attack Game 2.1. In
this game, Apresents the challenger with two messages m0,m1 of the same length; the challenger
then chooses a random key s and a random bit b, and encrypts mb under s, giving the resulting
ciphertext c to A; ﬁnally, Aoutputs a bit ˆb. The adversary Awins the game if ˆb= b.
Let us call this Game 0. The logic of the challenger in this game may be written as follows:
Upon receiving m0,m1 ∈{0,1}v from A, for some v≤L, do:
b←R {0,1}
s←R {0,1}ℓ, r←G(s)
c←r[0 . .v−1] ⊕mb
send c to A.
Game 0 is illustrated in Fig. 3.2.
Let W0 be the event that ˆb= b in Game 0. By deﬁnition, we have
SSadv∗[A,E] = |Pr[W0] −1/2|. (3.4)
Next, we modify the challenger of Game 0, obtaining a new game, called Game 1, which is
exactly the same as Game 0, except that the challenger uses a truly random string in place of a
pseudo-random string. The logic of the challenger in Game 1 is as follows:
Upon receiving m0,m1 ∈{0,1}v from A, for some v≤L, do:
b←R {0,1}
r←R {0,1}L
c←r[0 . .v−1] ⊕mb
send c to A.
49
A
ˆb 2 {0, 1}
Challenger
b R
 {0, 1}
s R
 {0, 1}`
r  G(s)
m0, m1 2 {0, 1}L
(|m0| = |m1| = v)
cc  r[0 ..v   1]   mb
Figure 3.2: Game 0 in the proof of Theorem 3.1
A
ˆb 2 {0, 1}
Challenger
b R
 {0, 1}
r R
 {0, 1}L
m 0,m 1 2 {0 ,1 }L
(|m0 | = |m1 | = v)
c  r [0..v   1]  m b
c
Figure 3.3: Game 1 in the proof of Theorem 3.1
50
A
r 2 {0, 1}L
b R
 {0, 1}
ˆb 2 {0, 1}
PRG Challenger
for G
B
 (ˆb, b)
m0, m1 2 {0, 1}L
(|m0| = |m1| = v)
c  r[0 ..v   1]   mb
c
Figure 3.4: The PRG adversary Bin the proof of Theorem 3.1
As usual, Aoutputs a bit ˆbat the end of this game. We have highlighted the changes from Game 0
in gray. Game 1 is illustrated in Fig. 3.3.
Let W1 be the event that ˆb= b in Game 1. We claim that
Pr[W1] = 1/2. (3.5)
This is because in Game 1, the adversary is attacking the variable length one-time pad. In particu-
lar, it is easy to see that the adversary’s output ˆband the challenger’s hidden bit bare independent.
Finally, we show how to construct an eﬃcient PRG adversary Bthat uses Aas a subroutine,
such that
|Pr[W0] −Pr[W1]|= PRGadv[B,G]. (3.6)
This is actually quite straightforward. The logic of our new adversary Bis illustrated in Fig. 3.4.
Here, δ is deﬁned as follows:
δ(x,y) :=
{
1 if x= y,
0 if x̸= y. (3.7)
Also, the box labeled “PRG Challenger” is playing the role of the challenger in Attack Game 3.1
with respect to G.
In words, adversary B, which is a PRG adversary designed to attack G(as in Attack Game 3.1),
receives r∈{0,1}L from its PRG challenger, and then plays the role of challenger to A, as follows:
Upon receiving m0,m1 ∈{0,1}v from A, for some v≤L, do:
b←R {0,1}
c←r[0 . .v−1] ⊕mb
send c to A.
51
Finally, when Aoutputs a bit ˆb, Boutputs the bit δ(ˆb,b).
Let p0 be the probability that Boutputs 1 when the PRG challenger is running Experiment 0
of Attack Game 3.1, and let p1 be the probability that Boutputs 1 when the PRG challenger is
running Experiment 1 of Attack Game 3.1. By deﬁnition, PRG adv[B,G] = |p1 −p0|. Moreover, if
the PRG challenger is running Experiment 0, then adversary Ais essentially playing our Game 0,
and so p0 = Pr[ W0], and if the PRG challenger is running Experiment 1, then Ais essentially
playing our Game 1, and so p1 = Pr[W1]. Equation (3.6) now follows immediately.
Combining (3.4), (3.5), and (3.6), yields (3.3). 2
In the above theorem, we reduced the security of Eto that of G by showing that if Ais an
eﬃcient SS adversary that attacks E, then there exists an eﬃcient PRG adversary Bthat attacks
G, such that
SSadv[A,E] ≤2 ·PRGadv[B,G].
(Actually, we showed that equality holds, but that is not so important.) In the proof, we argued
that if G is secure, then PRG adv[B,G] is negligible, hence by the above inequality, we conclude
that SSadv[A,E] is also negligible. Since this holds for all eﬃcient adversaries A, we conclude that
Eis semantically secure.
Analogous to the discussion after the proof of Theorem 2.7, another way to structure the proof
is by proving the contrapositive: indeed, if we assume that Eis insecure, then there must be an
eﬃcient adversary Asuch that SS adv[A,E] is non-negligible, and the reduction (and the above
inequality) gives us an eﬃcient adversary Bsuch that PRG adv[B,G] is also non-negligible. That
is, if we can break E, we can also break G. While logically equivalent, such a proof has a diﬀerent
“feeling”: one starts with an adversary Athat breaks E, and shows how to use Ato construct a
new adversary Bthat breaks G.
The reader should notice that the proof of the above theorem follows the same basic pattern
as our analysis of Internet roulette in Section 2.2.4. In both cases, we started with an attack game
(Fig. 2.2 or Fig. 3.2) which we modiﬁed to obtain a new attack game (Fig. 2.3 or Fig. 3.3); in
this new attack game, it was quite easy to compute the adversary’s advantage. Also, we used an
appropriate security assumption to show that the diﬀerence between the adversary’s advantages in
the original and the modiﬁed games was negligible. This was done by exhibiting a new adversary
(Fig. 2.4 or Fig. 3.4) that attacked the underlying cryptographic primitive (cipher or PRG) with an
advantage equal to this diﬀerence. Assuming the underlying primitive was secure, this diﬀerence
must be negligible; alternatively, one could argue the contrapositive: if this diﬀerence were not
negligible, the new adversary would “break” the underlying cryptographic primitive.
This is a pattern that will be repeated and elaborated upon throughout this text. The reader
is urged to study both of these analyses to make sure he or she completely understands what is
going on.
3.3 Stream cipher limitations: attacks on the one time pad
Although stream ciphers are semantically secure, they are quite brittle and become insecure if used
incorrectly.
52
3.3.1 The two-time pad is insecure
A stream cipher is well equipped to encrypt a single message from Alice to Bob. Alice, however,
may wish to send several messages to Bob. For simplicity suppose Alice wishes to encrypt two
messages m1 and m2. The naive solution is to encrypt both messages using the same stream cipher
key s:
c1 ←m1 ⊕G(s) and c2 ←m2 ⊕G(s) (3.8)
A moments reﬂection shows that this construction is insecure in a very strong sense. An adversary
who intercepts c1 and c2 can compute
∆ := c1 ⊕c2 =
(
m1 ⊕G(s)
)
⊕
(
m2 ⊕G(s)
)
= m1 ⊕m2
and obtain the xor of m1 and m2. Not surprisingly, English text contains enough redundancy that
given ∆ = m1 ⊕m2 the adversary can recover both m1 and m2 in the clear. Hence, the construction
in (3.8) leaks the plaintexts after seeing only two suﬃciently long ciphertexts.
The construction in (3.8) is jokingly called the two-time pad. We just argued that the two-
time pad is totally insecure. In particular, a stream cipher key should never be used to
encrypt more than one message . Throughout the book we will see many examples where a
one-time cipher is suﬃcient. For example, when choosing a new random key for every message as
in Section 5.4.1. However, in settings where a single key is used multiple times, one should never
use a stream cipher directly. We build multi-use ciphers in Chapter 5.
Incorrectly reusing a stream cipher key is a common error in deployed systems. For example,
a protocol called PPTP enables two parties A and B to send encrypted messages to one another.
Microsoft’s implementation of PPTP in Windows NT uses a stream cipher called RC4. The orig-
inal implementation encrypts messages from A to B using the same RC4 key as messages from
B to A [140]. Consequently, by eavesdropping on two encrypted messages headed in opposite
directions an attacker could recover the plaintext of both messages.
Another amusing story about the two-time pad is relayed by Klehr [85] who describes in great
detail how Russian spies in the US during World War II were sending messages back to Moscow,
encrypted with the one-time pad. The system had a critical ﬂaw, as explained by Klehr:
During WWII the Soviet Union could not produce enough one-time pads . . . to keep
up with the enormous demand . . . . So, they used a number of one-time pads twice,
thinking it would not compromise their system. American counter-intelligence during
WWII collected all incoming and outgoing international cables. Beginning in 1946, it
began an intensive eﬀort to break into the Soviet messages with the cooperation of the
British and by ... the Soviet error of using some one-time pads as two-time pads, was
able, over the next 25 years, to break some 2900 messages, containing 5000 pages of the
hundreds of thousands of messages that had been sent between 1941 and 1946 (when
the Soviets switched to a diﬀerent system).
The decryption eﬀort was codenamed project Venona. The Venona ﬁles are most famous for
exposing Julius and Ethel Rosenberg and helped give evidence of their involvement with the Soviet
spy ring. Starting in 1995 all 3000 Venona decrypted messages were made public.
3.3.2 The one-time pad is malleable
Although semantic security ensures that an adversary cannot read the plaintext, it provides no
guarantees for integrity. When using a stream cipher, an adversary can change a ciphertext and
53
the modiﬁcation will never be detected by the decryptor. Even worse, let us show that by changing
the ciphertext, the attacker can control how the decrypted plaintext will change.
Suppose an attacker intercepts a ciphertext c:= E(s,m) = m⊕G(s). The attacker changes cto
c′:= c⊕∆ for some ∆ of the attacker’s choice. Consequently, the decryptor receives the modiﬁed
message
D(s,c′) = c′⊕G(s) = (c⊕∆) ⊕G(s) = m⊕∆.
Hence, without knowledge of either m or s, the attacker was able to cause the decrypted message
to become m⊕∆ for ∆ of the attacker’s choosing. We say that stream-ciphers are malleable since
an attacker can cause predictable changes to the plaintext. We will construct ciphers that provide
both privacy and integrity in Chapter 9.
A simple example where malleability could help an attacker is an encrypted ﬁle system. To
make things concrete, suppose Bob is a professor and that Alice and Molly are students. Bob’s
students submit their homework by email, and then Bob stores these emails on a disk encrypted
using a stream cipher. An email always starts with a standard header. Simplifying things a bit, we
can assume that an email from, say, Alice, always starts with the characters From:Alice.
Now suppose Molly is able to gain access to Bob’s disk and locate the encryption of the email
from Alice containing her homework. Molly can eﬀectively steal Alice’s homework, as follows. She
simply XORs the appropriate ﬁve-character string into the ciphertext in positions 6 to 10, so as
to change the header From:Alice to the header From:Molly. Molly makes this change by only
operating on ciphertexts and without knowledge of Bob’s secret key. Bob will never know that the
header was changed, and he will grade Alice’s homework, thinking it is Molly’s, and Molly will get
the credit instead of Alice.
Of course, for this attack to be eﬀective, Molly must somehow be able to ﬁnd the email from Alice
on Bob’s encrypted disk. However, in some implementations of encrypted ﬁle systems, ﬁle metadata
(such as ﬁle names, modiﬁcation times, etc) are not encrypted. Armed with this metadata, it may
be straightforward for Molly to locate the encrypted email from Alice and carry out this attack.
3.4 Composing PRGs
In this section, we discuss two constructions that allow one to build new PRGs out of old PRGs.
These constructions allow one to increase the size of the output space of the original PRG while at
the same time preserving its security. Perhaps more important than the constructions themselves is
the proof technique, which is called a hybrid argument. This proof technique is used pervasively
throughout modern cryptography.
3.4.1 A parallel construction
Let G be a PRG deﬁned over ( S,R). Suppose that in some application, we want to use G many
times. We want all the outputs of Gto be computationally indistinguishable from random elements
of R. If G is a secure PRG, and if the seeds are independently generated, then this will indeed be
the case.
We can model the use of many applications of G as a new PRG G′. That is, we construct a
new PRG G′ that applies G to n seeds, and concatenates the outputs. Thus, G′ is deﬁned over
(Sn,Rn), and for s1,...,s n ∈S,
G′(s1,...,s n) := (G(s1),...,G (sn)).
54
We callG′the n-wise parallel composition ofG. The value nis called a repetition parameter,
and we require that it is a poly-bounded value.
Theorem 3.2. If G is a secure PRG, then the n-wise parallel composition G′of G is also a secure
PRG.
In particular, for every PRG adversary Athat attacks G′ as in Attack Game 3.1, there exists
a PRG adversary Bthat attacks G as in Attack Game 3.1, where Bis an elementary wrapper
around A, such that
PRGadv[A,G′] = n·PRGadv[B,G].
As a warm up, we ﬁrst prove this theorem in the special case n= 2. Let Abe an eﬃcient PRG
adversary that has advantage ϵ in attacking G′ in Attack Game 3.1. We want to show that ϵ is
negligible, under the assumption that G is a secure PRG. To do this, let us deﬁne Game 0 to be
Experiment 0 of Attack Game 3.1 with Aand G′. The challenger in this game works as follows:
s1 ←R S, r1 ←G(s1)
s2 ←R S, r2 ←G(s2)
send (r1,r2) to A.
Let p0 denote the probability with which Aoutputs 1 in this game.
Next, we deﬁne Game 1, which is played between Aand a challenger that works as follows:
r1 ←R R
s2 ←R S, r2 ←G(s2)
send (r1,r2) to A.
Note that Game 1 corresponds to neither Experiment 0 nor Experiment 1 of Attack Game 3.1;
rather, it is a “hybrid” experiment corresponding to something in between Experiments 0 and 1.
All we have done is replace the pseudo-random value r1 in Game 0 by a truly random value (as
highlighted). Intuitively, under the assumption that G is a secure PRG, the adversary Ashould
not notice the diﬀerence. To make this argument precise, let p1 be the probability that Aoutputs
1 in Game 1.
Let δ1 := |p1 −p0|. We claim that δ1 is negligible, assuming that G is a secure PRG. Indeed,
we can easily construct an eﬃcient PRG adversary B1 whose advantage in attacking G in Attack
Game 3.1 is precisely equal to δ1. The adversary B1 works as follows:
Upon receiving r∈R from its challenger, B1 plays the role of challenger toA, as follows:
r1 ←r
s2 ←R S, r2 ←G(s2)
send (r1,r2) to A.
Finally, B1 outputs whatever Aoutputs.
Observe that when B1 is in Experiment 0 of its attack game, it perfectly mimics the behavior of the
challenger in Game 0, while in Experiment 1, it perfectly mimics the behavior of the challenger in
Game 1. Thus, p0 is equal to the probability thatB1 outputs 1 in Experiment 0 of Attack Game 3.1,
while p1 is equal to the probability that B1 outputs 1 in Experiment 1 of Attack Game 3.1. Thus,
B1’s advantage in attacking G is precisely |p1 −p0|, as claimed.
Next, we deﬁne Game 2, which is played between Aand a challenger that works as follows:
55
r1 ←R R
r2 ←R R
send (r1,r2) to A.
All we have done is replace the pseudo-random value r2 in Game 1 by a truly random value (as
highlighted). Let p2 be the probability that Aoutputs 1 in Game 2. Note that Game 2 corresponds
to Experiment 1 of Attack Game 3.1 with Aand G′, and so p2 is equal to the probability that A
outputs 1 in Experiment 1 of Attack Game 3.1 with respect to G′.
Let δ2 := |p2 −p1|.By an argument similar to that above, it is easy to see that δ2 is negligible,
assuming that G is a secure PRG. Indeed, we can easily construct an eﬃcient PRG adversary B2
whose advantage in Attack Game 3.1 with respect to G is precisely equal to δ2. The adversary B2
works as follows:
Upon receiving r∈R from its challenger, B2 plays the role of challenger toA, as follows:
r1 ←R R
r2 ←r
send (r1,r2) to A.
Finally, B2 outputs whatever Aoutputs.
It should be clear that p1 is equal to the probability that B2 outputs 1 in Experiment 0 of Attack
Game 3.1, while p2 is equal to the probability thatB2 outputs 1 in Experiment 1 of Attack Game 3.1.
Recalling that ϵ= PRGadv[A,G′], then from the above discussion, we have
ϵ= |p2 −p0|= |p2 −p1 + p1 −p0|≤| p1 −p0|+ |p2 −p1|= δ1 + δ2.
Since both δ1 and δ2 are negligible, then so is ϵ (see Fact 2.6).
That completes the proof that G′ is secure in the case n = 2. Before giving the proof in the
general case, we give another proof in the casen= 2. While our ﬁrst proof involved the construction
of two adversaries B1 and B2, our second proof combines these two adversaries into a single PRG
adversary Bthat plays Attack Game 3.1 with respect to G, and which runs as follows:
upon receiving r ∈R from its challenger, adversary Bchooses ω ∈{1,2}at random,
and gives r to Bω; ﬁnally, Boutputs whatever Bω outputs.
Let W0 be the event that Boutputs 1 in Experiment 0 of Attack Game 3.1, and W1 be the
event that Boutputs 1 in Experiment 1 of Attack Game 3.1. Conditioning on the events ω = 1
and ω= 2, we have
Pr[W0] = Pr[W0 |ω= 1] Pr[ω= 1] + Pr[W0 |ω= 2] Pr[ω= 2]
= 1
2
(
Pr[W0 |ω= 1] + Pr[W0 |ω= 2]
)
= 1
2 (p0 + p1).
Similarly, we have
Pr[W1] = Pr[W1 |ω= 1] Pr[ω= 1] + Pr[W1 |ω= 2] Pr[ω= 2]
= 1
2
(
Pr[W1 |ω= 1] + Pr[W1 |ω= 2]
)
= 1
2 (p1 + p2).
56
Therefore, if δ is the advantage of Bin Attack Game 3.1 with respect to G, we have
δ=
⏐⏐Pr[W1] −Pr[W0]
⏐⏐=
⏐⏐1
2 (p1 + p2) −1
2 (p0 + p1)
⏐⏐= 1
2 |p2 −p0|= ϵ/2.
Thus, ϵ= 2δ, and since δ is negligible, so is ϵ (see Fact 2.6).
Now, ﬁnally, we present the proof of Theorem 3.2 for general, poly-bounded n.
Proof idea. We could try to extend the ﬁrst strategy outlined above from n = 2 to arbitrary n.
That is, we could construct a sequence of n+ 1 games, starting with a challenger that produces
a sequence ( G(s1),...,G (sn)), of pseudo-random elements replacing elements one at a time with
truly random elements of R, ending up with a sequence ( r1,...,r n) of truly random elements of
R. Intuitively, the adversary should not notice any of these replacements, since G is a secure
PRG; however, proving this formally would require the construction of n diﬀerent adversaries,
each of which attacks G in a slightly diﬀerent way. As it turns out, this leads to some annoying
technical diﬃculties when n is not an absolute constant, but is simply poly-bounded; it is much
more convenient to extend the second strategy outlined above, constructing a single adversary that
attacks G “in one blow.” 2
Proof. Let Abe an eﬃcient PRG adversary that plays Attack Game 3.1 with respect to G′. We
ﬁrst introduce a sequence of n+ 1 hybrid games, called Hybrid 0, Hybrid 1, . . . , Hybrid n. For
j = 0,1,...,n , Hybrid j is a game played between Aand a challenger that prepares a tuple of n
values, the ﬁrst j of which are truly random, and the remaining n−j of which are pseudo-random
outputs of G; that is, the challenger works as follows:
r1 ←R R
...
rj ←R R
sj+1 ←R S, rj+1 ←G(sj+1)
...
sn ←R S, rn ←G(sn)
send (r1,...,r n) to A.
As usual, Aoutputs 0 or 1 at the end of the game. Fig. 3.5 illustrates the values prepared by the
challenger in each of these n+1 games. Let pj denote the probability that Aoutputs 1 in Hybrid j.
Note that p0 is also equal to the probability that Aoutputs 1 in Experiment 0 of Attack Game 3.1,
while pn is equal to the probability that Aoutputs 1 in Experiment 1. Thus, we have
PRGadv[A,G′] = |pn −p0|. (3.9)
We next deﬁne a PRG adversary Bthat plays Attack Game 3.1 with respect to G, and which
works as follows:
Upon receiving r∈R from its challenger, Bplays the role of challenger to A, as follows:
57
Hybrid 0: G(s1) G(s2) G(s3) ··· G(sn)
Hybrid 1: r1 G(s2) G(s3) ··· G(sn)
Hybrid 2: r1 r2 G(s3) ··· G(sn)
...
Hybrid n−1: r1 r2 r3 ··· G(sn)
Hybrid n: r1 r2 r3 ··· rn
Figure 3.5: Values prepared by challenger in Hybrids 0 ,1,...,n . Each ri is a random element
of R, and each si is a random element of S.
ω←R {1,...,n }
r1 ←R R
...
rω−1 ←R R
rω ←r
sω+1 ←R S, rω+1 ←G(sω+1)
...
sn ←R S, rn ←G(sn)
send (r1,...,r n) to A.
Finally, Boutputs whatever Aoutputs.
Let W0 be the event that Boutputs 1 in Experiment 0 of Attack Game 3.1, and W1 be the
event that Boutputs 1 in Experiment 1 of Attack Game 3.1. The key observation is this:
conditioned on ω = j for every ﬁxed j = 1 ,...,n , Experiment 0 of B’s attack game
is equivalent to Hybrid j −1, while Experiment 1 of B’s attack game is equivalent to
Hybrid j.
Therefore,
Pr[W0 |ω= j] = pj−1 and Pr[ W1 |ω= j] = pj.
So we have
Pr[W0] =
n∑
j=1
Pr[W0 |ω= j] Pr[ω= j] = 1
n
n∑
j=1
Pr[W0 |ω= j] = 1
n
n∑
j=1
pj−1,
and similarly,
Pr[W1] =
n∑
j=1
Pr[W1 |ω= j] Pr[ω= j] = 1
n
n∑
j=1
Pr[W1 |ω= j] = 1
n
n∑
j=1
pj.
58
Finally, we have
PRGadv[B,G] = |Pr[W1] −Pr[W0]|
=
⏐⏐⏐⏐
1
n
n∑
j=1
pj −1
n
n∑
j=1
pj−1
⏐⏐⏐⏐
= 1
n|pn −p0|,
and combining this with (3.9), we have
PRGadv[A,G′] = n·PRGadv[B,G].
Since we are assuming Gis a secure PRG, it follows that PRGadv[B,G] is negligible, and since nis
poly-bounded, it follows that PRG adv[A,G′] is negligible (see Fact 2.6). That proves the theorem.
2
Theorem 3.2 says that the security of a PRG degrades at most linearly in the number of times
that we use it. One might ask if this bound is tight; that is, might security indeed degrade linearly
in the number of uses? The answer is in fact “yes” (see Exercise 3.15).
3.4.2 A sequential construction: the Blum-Micali method
We now present a sequential construction, invented by Blum and Micali, which uses a PRG that
stretches just a little, and builds a PRG that stretches an arbitrary amount.
Let Gbe a PRG deﬁned over (S,R×S), for some ﬁnite sets Sand R. For every poly-bounded
value n≥1, we can construct a new PRG G′, deﬁned over (S,Rn ×S). For s∈S, we let
G′(s) :=
s0 ←s
for i←1 to n do
(ri,si) ←G(si−1)
output (r1,...,r n,sn).
We call G′the n-wise sequential composition of G. See Fig. 3.6 for a schematic description of
G′for n= 3.
We shall prove below in Theorem 3.3 that if Gis a secure PRG, then so is G′. As a special case
of this construction, suppose Gis a PRG deﬁned over ({0,1}ℓ,{0,1}t+ℓ), for some positive integers
ℓ and t; that is, G stretches ℓ-bit strings to ( t+ ℓ)-bit strings. We can naturally view the output
space of G as {0,1}t ×{0,1}ℓ, and applying the above construction, and interpreting outputs as
bit strings, we get a PRG G′that stretches ℓ-bit strings to ( nt+ ℓ)-bit strings.
Theorem 3.3. If G is a secure PRG, then the n-wise sequential composition G′ of G is also a
secure PRG.
In particular, for every PRG adversary Athat plays Attack Game 3.1 with respect to G′, there
exists a PRG adversary Bthat plays Attack Game 3.1 with respect toG, where Bis an elementary
wrapper around A, such that
PRGadv[A,G′] = n·PRGadv[B,G].
59
G
G
G
s
s1
r1
s2
r2
r3
s3
Figure 3.6: The sequential construction for n= 3
Proof idea. The proof of this is a hybrid argument that is very similar in spirit to the proof of
Theorem 3.2. The intuition behind the proof is as follows: Consider a PRG adversary Awho
receives the (r1,...,r n,sn) in Experiment 0 of Attack Game 3.1. Since s= s0 is random and Gis a
secure PRG, we may replace (r1,s1) by a completely random element of R×S, and the probability
that Aoutputs 1 in this new, hybrid game should change by only a negligible amount. Now, since
s1 is random (and again, since Gis a secure PRG), we may replace (r2,s2) by a completely random
element of R×S , and the probability that Aoutputs 1 in this second hybrid game should again
change by only a negligible amount. Continuing in this way, we may incrementally replace ( r3,s3)
through (rn,sn) by random elements of R×S, and the probability that Aoutputs 1 should change
by only a negligible amount after making all these changes (assumingnis poly-bounded). However,
at this point, Aoutputs 1 with the same probability with which he would output 1 in Experiment 1
in Attack Game 3.1, and therefore, this probability is negligibly close to the probability that A
outputs 1 in Experiment 0 of Attack Game 3.1.
That is the idea; however, just as in the proof of Theorem 3.2, for technical reasons, we design
a single PRG adversary that attacks G. 2
Proof. Let Abe a PRG adversary that plays Attack Game 3.1 with respect toG′. We ﬁrst introduce
a sequence of n+ 1 hybrid games, called Hybrid 0, Hybrid 1, . . . , Hybrid n. For j = 0,1,...,n , we
deﬁne Hybrid j to be the game played between Aand the following challenger:
r1 ←R R
...
rj ←R R
sj ←R S
(rj+1,sj+1) ←G(sj)
...
(rn,sn) ←G(sn−1)
send (r1,...,r n,sn) to A.
As usual, Aoutputs 0 or 1 at the end of the game. See Fig. 3.7 for a schematic description of
how these challengers work in the case n = 3. Let pj denote the probability that Aoutputs 1
in Hybrid j. Note that p0 is also equal to the probability that Aoutputs 1 in Experiment 0 of
60
Attack Game 3.1, while pn is equal to the probability that Aoutputs 1 in Experiment 1 of Attack
Game 3.1. Thus, we have
PRGadv[A,G′] = |pn −p0|. (3.10)
We next deﬁne a PRG adversary Bthat plays Attack Game 3.1 with respect to G, and which
works as follows:
Upon receiving ( r,s) ∈R×S from its challenger, Bplays the role of challenger to A,
as follows:
ω←R {1,...,n }
r1 ←R R,...,r ω−1 ←R R
(rω,sω) ←(r,s)
(rω+1,sω+1) ←G(sω),..., (rn,sn) ←G(sn−1)
send (r1,...,r n,sn) to A.
Finally, Boutputs whatever Aoutputs.
Let W0 be the event that Boutputs 1 in Experiment 0 of Attack Game 3.1, and W1 be the
event that Boutputs 1 in Experiment 1 of Attack Game 3.1. The key observation is this:
conditioned on ω = j for every ﬁxed j = 1 ,...,n , Experiment 0 of B’s attack game
is equivalent to Hybrid j −1, while Experiment 1 of B’s attack game is equivalent to
Hybrid j.
Therefore,
Pr[W0 |ω= j] = pj−1 and Pr[ W1 |ω= j] = pj.
The remainder of the proof is a simple calculation that is identical to that in the last paragraph of
the proof of Theorem 3.2. 2
One criteria for evaluating a PRG is its expansion rate: a PRG that stretches an n-bit seed
to an m-bit output has expansion rate of m/n; more generally, if the seed space is S and the
output space is R, we would deﬁne the expansion rate as log |R|/log|S|. The sequential composi-
tion achieves a better expansion rate than the parallel composition. However, it suﬀers from the
drawback that it cannot be parallelized. In fact, we can obtain the best of both worlds: a large
expansion rate with a highly parallelizable construction (see Section 4.4.4).
3.4.3 Mathematical details
There are some subtle points in the proofs of Theorems 3.2 and 3.3 that merit discussion.
First, in both constructions, the underlying PRG G may have system parameters. That is,
there may be a probabilistic algorithm that takes as input the security parameter λ, and outputs
a system parameter Λ. Recall that a system parameter is public data that fully instantiates the
scheme (in this case, it might deﬁne the seed and output spaces). For both the parallel and
sequential constructions, one could use the same system parameter for all ninstances of G; in fact,
for the sequential construction, this is necessary to ensure that outputs from one round may be
used as inputs in the next round. The proofs of these security theorems are perfectly valid if the
same system parameter is used for all instances of G, or if diﬀerent system parameters are used.
61
G
G
G
r1
r2
r3
s3
S
G
G
r1
r2
r3
s3
S
R
G
r1
r2
r3
s3
R
S
R
r1
r2
r3
s3
R
R
S
R
Hyb rid 0
Hyb rid 1
Hyb rid 2
Hyb rid 3
Figure 3.7: The challenger’s computation in the hybrid games for n = 3. The circles indicate
randomly generated elements of Sor R, as indicated by the label.
62
Second, we brieﬂy discuss a rather esoteric point regarding hybrid arguments. To make things
concrete, we focus attention on the proof of Theorem 3.2 (although analogous remarks apply to the
proof of Theorem 3.3, or any other hybrid argument). In proving this theorem, we ultimately want
to show that if there is an eﬃcient adversary Athat breaks G′, then there is an eﬃcient adversary
that breaks G. Suppose that Ais an eﬃcient adversary that breaks G′, so that its advantage ϵ(λ)
(which we write here explicitly as a function of the security parameter λ) with respect to G′is not
negligible. This means that there exists a constant c such that ϵ(λ) ≥1/λc for inﬁnitely many λ.
Now, in the discussion preceding the proof of Theorem 3.2, we considered the special casen= 2,
and showed that there exist eﬃcient adversaries B1 and B2, such that ϵ(λ) ≤δ1(λ) +δ2(λ) for all λ,
where δj(λ) is the advantage of Bj with respect to G. It follows that either δ1(λ) ≥1/2λc inﬁnitely
often, or δ2(λ) ≥1/2λc inﬁnitely often. So we may conclude that either B1 breaks G or B2 breaks
G (or possibly both). Thus, there exists an eﬃcient adversary that breaks G: it is either B1 or
B2, which one we do not say (and we do not have to). However, whichever one it is, it is a ﬁxed
adversary that is deﬁned uniformly for all λ; that is, it is a ﬁxed machine that takes λ as input.
This argument is perfectly valid, and extends to every constant n: we would construct nadver-
saries B1,..., Bn, and argue that for some j = 1,...,n , adversary Bj must have advantage 1/nλc
inﬁnitely often, and thus break G. However, this argument does not extend to the case where n
is a function of λ, which we now write explicitly as n(λ). The problem is not that 1 /(n(λ)λc) is
perhaps too small (it is not). The problem is quite subtle, so before we discuss it, let us ﬁrst review
the (valid) proof that we did give. For each λ, we deﬁned a sequence of n(λ) + 1 hybrid games,
so that for each λ, we actually get a diﬀerent sequence of games. Indeed, we cannot speak of a
single, ﬁnite sequence of games that works for all λ, since n(λ) →∞. Nevertheless, we explicitly
constructed a ﬁxed adversary Bthat is deﬁned uniformly for all λ; that is, Bis a ﬁxed machine
that takes λ as input. The sequence of hybrid games that we deﬁne for each λ is a mathematical
object for which we make no claims as to its computability — it is simply a convenient device used
in the analysis of B.
Hopefully by now the reader has at least a hint of the problem that arises if we attempt to
generalize the argument for constant n to a function n(λ). First of all, it is not even clear what
it means to talk about n(λ) adversaries B1,..., Bn(λ): our adversaries are supposed to be ﬁxed
machines that takeλas input, and the machines themselves should not depend onλ. Such linguistic
confusion aside, our proof for the constant case only shows that there exists an “adversary” that for
inﬁnitely many values of λ somehow knows the “right” value of j = j(λ) to use in the ( n(λ) + 1)-
game hybrid argument — no single, constant value ofjnecessarily works for inﬁnitely many λ. One
can actually make sense of this type of argument if one uses a non-uniform model of computation,
but we shall not take this approach in this text.
All of these problems simply go away when we use a hybrid argument that constructs a single
adversary B, as we did in the proofs of Theorems 3.2 and 3.3. However, we reiterate that the original
analysis we did in the case where n= 2, or its natural extension to every constant n, is perfectly
valid. In that case, we construct a single, ﬁxed sequence of n+ 1 games, with each individual game
uniformly deﬁned for all λ (just as our attack games are in our security deﬁnitions), as well as a
ﬁnite collection of adversaries, each of which is a ﬁxed machine. We reiterate this because in the
sequel we shall often be constructing proofs that involve ﬁnite sequences of games like this (indeed,
the proof of Theorem 3.1 was of this type). In such cases, each game will be uniformly deﬁned for
all λ, and will be denoted Game 0, Game 1, etc. In contrast, when we make a hybrid argument
that uses non-uniform sequences of games, we shall denote these games Hybrid 0, Hybrid 1, etc.,
63
so as to avoid any possible confusion.
3.5 The next bit test
Let G be a PRG deﬁned over ( {0,1}ℓ,{0,1}L), so that it stretches ℓ-bit strings to L-bit strings.
There are a number of ways an adversary might be able to distinguish a pseudo-random output of
Gfrom a truly random bit string. Indeed, suppose that an eﬃcient adversary were able to compute,
say, the last bit of G’s output, given the ﬁrst L−1 bits of G’s output. Intuitively, the existence of
such an adversary would imply that Gis insecure, since given the ﬁrst L−1 bits of a truly random
L-bit string, one has at best a 50-50 chance of guessing the last bit. It turns out that an interesting
converse, of sorts, is also true.
We shall formally deﬁne the notion of unpredictability for a PRG, which essentially says
that given the ﬁrst i bits of G’s output, it is hard to predict the next bit (i.e., the ( i+ 1)-st
bit) with probability signiﬁcantly better that 1 /2 (here, i is an adversarially chosen index). We
shall then prove that unpredictability and security are equivalent. The fact that security implies
unpredictability is fairly obvious: the ability to eﬀectively predict the next bit in the pseudo-random
output string immediately gives an eﬀective statistical test. However, the fact that unpredictability
implies security is quite interesting (and requires more eﬀort to prove): it says that if there is any
eﬀective statistical test at all, then there is in fact an eﬀective method for predicting the next bit
in a pseudo-random output string.
Attack Game 3.2 (Unpredictable PRG). For a given PRG G, deﬁned over (S,{0,1}L), and a
given adversary A, the attack game proceeds as follows:
• The adversary sends an index i, with 0 ≤i≤L−1, to the challenger.
• The challenger computes
s←R S, r←G(s)
and sends r[0 . .i−1] to the adversary.
• The adversary outputs g∈{0,1}.
We say thatAwins if r[i] = g, and we deﬁneA’s advantagePredadv[A,G] to be |Pr[Awins]−1/2|.
2
Deﬁnition 3.3 (Unpredictable PRG). A PRG G is unpredictable if the value Predadv[A,G]
is negligible for all eﬃcient adversaries A.
We begin by showing that security implies unpredictability.
Theorem 3.4. Let G be a PRG, deﬁned over (S,{0,1}L). If G is secure, then G is unpredictable.
In particular, for every adversary Abreaking the unpredictability of G, as in Attack Game 3.2,
there exists an adversary Bbreaking the security of G as in Attack Game 3.1, where Bis an
elementary wrapper around A, such that
Predadv[A,G] = PRGadv[B,G].
64
Proof. Let Abe an adversary breaking the unpredictability of G, and let idenote the index chosen
by A. Also, suppose Awins Attack Game 3.2 with probability 1/2 +ϵ, so that Predadv[A,G] = |ϵ|.
We build an adversary Bbreaking the security of G, using Aas a subroutine, as follows:
Upon receiving r∈{0,1}L from its challenger, Bdoes the following:
• Bgives r[0 . .i−1] to A, obtaining A’s output g∈{0,1};
• if r[i] = g, then output 1, and otherwise, output 0.
For b = 0,1, let Wb be the event that Boutputs 1 in Experiment b of Attack Game 3.1. In
Experiment 0, r is a pseudo-random output of G, and W0 occurs if and only if r[i] = g, and so by
deﬁnition
Pr[W0] = 1/2 + ϵ.
In Experiment 1, r is a truly random bit string, but again, W1 occurs if and only if r[i] = g; in this
case, however, as random variables, the values of r[i] and g are independent, and so
Pr[W1] = 1/2.
It follows that
PRGadv[B,G] = |Pr[W1] −Pr[W0]|= |ϵ|= Predadv[A,G]. 2
The more interesting, and more challenging, task is to show that unpredictability implies secu-
rity. Before getting into all the details of the proof, we sketch the high level ideas.
First, we shall employ a hybrid argument, which will essentially allow us to argue that if Ais
an eﬃcient adversary that can eﬀectively distinguish a pseudo-random L-bit string from a random
L-bit string, then we can construct an eﬃcient adversary Bthat can eﬀectively distinguish
x1 ···xj xj+1
from
x1 ···xj r,
where j is a randomly chosen index, x1,...,x L is the pseudo-random output, and r is a random
bit. Thus, adversary Bcan distinguish the pseudo-random bit xj+1 from the random bit r, given
the “side information” x1,...,x j.
We want to turn B’s distinguishing advantage into a predicting advantage. The rough idea is
this: given x1,...,x j, we feed Bthe string x1,...,x j r for a randomly chosen bit r; if Boutputs 1,
our prediction for xj+1 is r; otherwise, our prediction for xj+1 is ¯r (the complement of r).
That this prediction strategy works is justiﬁed by the following general result, which we call
the distinguisher/predictor lemma. The general setup is as follows. We have:
• a random variable X, which corresponds to the “side information” x1,...,x j above, as well
as any random coins used by the adversary B;
• a 0/1-valued random variable B, which corresponds to xj+1 above, and which may be corre-
lated with X;
• a 0/1-valued random variable R, which corresponds to r above, and which is independent of
(X,B);
65
• a function d, which corresponds to B’s strategy, so that B’s distinguishing advantage is equal
to |ϵ|, where ϵ= Pr[d(X,B) = 1] −Pr[d(X,R) = 1].
The lemma says that if we deﬁne B′using the predicting strategy outlined above, namely B′= R if
d(X,R) = 1, and B′= R otherwise, then the probability that the prediction B′is equal to the actual
value B is precisely 1/2 + ϵ. Here is the precise statement of the lemma:
Lemma 3.5 (Distinguisher/predictor lemma). Let X be a random variable taking values in
some set S, and let B and R be a 0/1-valued random variables, where R is uniformly distributed
over {0,1}and is independent of (X,B). Let d: S×{0,1}→{ 0,1}be an arbitrary function, and
let
ϵ:= Pr[d(X,B) = 1] −Pr[d(X,R) = 1].
Deﬁne the random variable B′ as follows:
B′:=
{
R if d(X,R) = 1;
R otherwise.
Then
Pr[B′= B] = 1/2 + ϵ.
Proof. We calculate Pr[B′= B], conditioning on the events B = R and B = R:
Pr[B′= B] = Pr[B′= B |B = R] Pr[B = R] + Pr[B′= B |B = R] Pr[B = R]
= Pr
[
d(X,R) = 1
⏐⏐B = R
]1
2 + Pr
[
d(X,R) = 0
⏐⏐B = R
]1
2
= 1
2
(
Pr
[
d(X,R) = 1
⏐⏐B = R
]
+
(
1 −Pr
[
d(X,R) = 1
⏐⏐B = R
]))
= 1
2 + 1
2(α−β),
where
α:= Pr[d(X,R) = 1 |B = R] and β := Pr[d(X,R) = 1 |B = R].
By independence, we have
α= Pr[d(X,R) = 1 |B = R] = Pr[d(X,B) = 1 |B = R] = Pr[d(X,B) = 1].
To see the last equality, the result of Exercise 3.26 may be helpful.
We thus calculate that
ϵ= Pr[d(X,B) = 1] −Pr[d(X,R) = 1]
= α−
(
Pr[d(X,R) = 1 |B = R] Pr[B = R] + Pr[d(X,R) = 1 |B = R] Pr[B = R]
)
= α−1
2(α+ β)
= 1
2(α−β),
which proves the lemma. 2
66
Theorem 3.6. Let G be a PRG, deﬁned over (S,{0,1}L). If G is unpredictable, then G is secure.
In particular, for every adversary Abreaking the security of G as in Attack Game 3.1, there
exists an adversary B, breaking the unpredictability of G as in Attack Game 3.2, where Bis an
elementary wrapper around A, such that
PRGadv[A,G] = L·Predadv[B,G].
Proof. Let Aattack G as in Attack Game 3.1. Using A, we build a predictor B, which attacks G
as in Attack Game 3.2, and works as follows:
• Choose ω∈{1,...,L }at random.
• Send L−ω to the challenger, obtaining a string x∈{0,1}L−ω.
• Generate ω random bits r1,...,r ω, and give the L-bit string x∥r1 ···rω to A.
• If Aoutputs 1, then output r1; otherwise, output r1.
To analyze B, we consider L+ 1 hybrid games, called Hybrid 0, Hybrid 1, . . . , Hybrid L. For
j = 0,...,L , we deﬁne Hybrid j to be the game played between Aand a challenger that generates
a bit string r consisting of L−j pseudo-random bits, followed by j truly random bits; that is, the
challenger chooses s∈S and t∈{0,1}j at random, and sends Athe bit string
r:= G(s)[0 . .L−j−1] ∥t.
As usual, Aoutputs 0 or 1 at the end of the game, and we deﬁne pj to be the probability that A
outputs 1 in Hybrid j. Note that p0 is the probability that Aoutputs 1 in Experiment 0 of Attack
Game 3.1, while pL is the probability that Aoutputs 1 in Experiment 1 of Attack Game 3.1.
Let W be the event that Bwins in Attack Game 3.2 (that is, correctly predicts the next bit).
Then we have
Pr[W] =
L∑
j=1
Pr[W |ω= j] Pr[ω= j]
= 1
L
L∑
j=1
Pr[W |ω= j]
= 1
L
L∑
j=1
(1
2 + pj−1 −pj
)
(by Lemma 3.5)
= 1
2 + 1
L(p0 −pL),
and the theorem follows. 2
3.6 Case study: the Salsa and ChaCha PRGs
There are many ways to build PRGs and stream ciphers in practice. One approach builds PRGs
using the Blum-Micali paradigm discussed in Section 3.4.2. Another approach, discussed more
67
generally in Chapter 5, builds them from a more versatile primitive called a pseudorandom function
in counter mode. We start with a construction that uses this latter approach.
Salsa20/12 and Salsa20/20 are fast stream ciphers designed by Dan Bernstein in 2005.
Salsa20/12 is one of four Proﬁle 1 stream ciphers selected for the eStream portfolio of stream
ciphers. eStream is a project that identiﬁes fast and secure stream ciphers that are appropriate
for practical use. Variants of Salsa20/12 and Salsa20/20, called ChaCha12 and ChaCha20 respec-
tively, were proposed by Bernstein in 2008. These stream ciphers have been incorporated into
several widely deployed protocols such as TLS and SSH.
Let us brieﬂy describe the PRGs underlying the Salsa and ChaCha stream cipher families.
These PRGs take as input a 256-bit seed and a 64-bit nonce. For now we ignore the nonce and
simply set it to 0. We discuss the purpose of the nonce at the end of this section. The Salsa
and ChaCha PRGs follow the same high level structure shown in Fig. 3.8. They make use of two
components:
• A padding function denoted pad( s,j, 0) that combines a 256-bit seed swith a 64-bit counter
j to form a 512-bit block. The third input, a 64-bit nonce, is always set to 0 for now.
• A ﬁxed public permutation π: {0,1}512 →{0,1}512.
These components are used to output L< 264 pseudorandom blocks, each 512 bits long, using the
following algorithm (Fig. 3.8):
input: seed s∈{0,1}256
1. for j ←0 to L−1
2. hj ←pad(s,j, 0) ∈{0,1}512
3. rj ←π(hj) ⊕hj
4. output ( r0,...,r L−1).
The ﬁnal PRG output is 512 ·L bits long. We note that in Salsa and ChaCha the XOR on line 3
is a slightly more complicated operation: the 512-bit operands hj and π(hj) are split into 16 words
each 32-bits long and then added word-wise mod 2 32.
The design of Salsa and ChaCha is highly parallelizable and can take advantage of multiple
processor cores to speed-up encryption. Moreover, it enables random access to output blocks:
output block number j can be computed without having to ﬁrst compute all previous blocks.
Generators based on the Blum-Micali paradigm do not have these properties.
We analyze the security of the Salsa and ChaCha design in Exercise 4.23 in the next chapter,
after we develop a few more tools.
The details. We brieﬂy describe the padding function pad( s,j,n ) and the permutation π used
in ChaCha20. The padding function takes as input a 256-bit seed s0,...,s 7 ∈{0,1}32, a 64-bit
counter j0,j1 ∈{0,1}32, and 64-bit nonce n0,n1 ∈{0,1}32. It outputs a 512-bit block denoted
x0,...,x 15 ∈{0,1}32. The output is arranged in a 4 ×4 matrix of 32-bit words as follows:


x0 x1 x2 x3
x4 x5 x6 x7
x8 x9 x10 x11
x12 x13 x14 x15

←−


c0 c1 c2 c3
s0 s1 s2 s3
s4 s5 s6 s7
j0 j1 n0 n1

 (3.11)
68
π"
!
output"block"#0"
pad(","0","0)"
512"bits"
seed"
256"bits"
π"
!
output"block"#1"
pad(","1","0)"
512"bits"
π"
!
output"block"#2"
pad(","2","0)"
512"bits"
512"bits"
!
!
Figure 3.8: A schematic of the Salsa and ChaCha PRGs
where c0,c1,c2,c3 are ﬁxed 32-bit constants.
The permutation π : {0,1}512 →{0,1}512 is constructed by iterating a simple permutation a
ﬁxed number of times. The 512-bit input to π is treated as a 4 ×4 array of 32-bit words denoted
by x0,...,x 15. In ChaCha20 the function π is implemented by repeating the following sequence of
steps ten times:
(1) QuarterRound(x0,x4,x8,x12), (2) QuarterRound(x1,x5,x9,x13),
(3) QuarterRound(x2,x6,x10,x14), (4) QuarterRound(x3,x7,x11,x15),
(5) QuarterRound(x0,x5,x10,x15), (6) QuarterRound(x1,x6,x11,x12),
(7) QuarterRound(x2,x7,x8,x13), (8) QuarterRound(x3,x4,x9,x14).
Here QuarterRound(a,b,c,d) is deﬁned as the following sequence of steps written as C code using
a macro ROTL(a,b) that rotates left a 32-bit word a by b bits:
#define ROTL(a,b) (((a) << (b)) | ((a) >> (32 - (b))))
a += b; d ^= a; ROTL(d, 16);
c += d; b ^= c; ROTL(b, 12);
a += b; d ^= a; ROTL(d, 8);
c += d; b ^= c; ROTL(b, 7);
Observe that the ﬁrst four invocations of QuarterRound, steps (1-4), are applied to each of the four
columns of the 4 ×4 matrix, from left to right. The next four invocations, steps (5-8), are applied
to each of the four diagonals, with wrap around. This completes our description of ChaCha20,
except that we still need to discuss the use of nonces.
Using nonces. While the PRGs we discussed so far only take the seed as input, many PRGs used
in practice take an additional input called a nonce. That is, the PRG is a function G: S×N→R
where Sand Rare as before and N is called a nonce space. The nonce lets us generate multiple
pseudorandom outputs from a single seed s. That is, G(s,n0) is one pseudorandom output and
69
G(s,n1) for n1 ̸= n0 is another. The nonce turns the PRG into a more powerful primitive called
a pseudorandom function discussed in the next chapter. As we will see, secure pseudorandom
functions make it possible to use the same seed to encrypt multiple messages securely.
3.7 Case study: linear generators
In this section we look at two example PRGs built from linear functions. Both generators follow the
Blum-Micali paradigm presented in Section 3.4.2. Our ﬁrst example, called a linear congruential
generator, is completely insecure. We present it to show the beautiful mathematics that comes up
when attacking PRGs. Our second example, called a subset sum generator , is a provably secure
PRG assuming a certain version of the classic subset-sum problem is hard.
3.7.1 An example cryptanalysis: the linear congruential generator
Linear congruential generators (LCG) are used in statistical simulations to generate pseudorandom
values. They are fast, easy to implement, and widely deployed. Variants of LCG are used to generate
randomness in early versions of glibc, Microsoft Visual Basic, and the Java runtime. While
these generators may be suﬃcient for simulations, they should never be used for cryptographic
applications because they are insecure as PRGs. In particular, they are predictable: given a few
consecutive outputs of an LCG generator it is easy to compute all subsequent outputs. In this
section we describe an attack on LCG generators by showing a prediction algorithm.
The basic linear congruential generator is speciﬁed by four public system parameters: an inte-
ger q, two constants a,b ∈{0,...,q −1}, and a positive integer w≤q. The constant a is taken to
be relatively prime to q. We use Sq and Rto denote the sets:
Sq := {0,...,q −1}; R:=
{
0,..., ⌊(q−1)/w⌋
}
.
Here ⌊·⌋is the ﬂoor function: for a real number x, ⌊x⌋is the biggest integer less than or equal to x.
Now, the generator Glcg : Sq →R×S q with seed s∈Sq is deﬁned as follows:
Glcg(s) :=
(
⌊s/w⌋, as + bmod q
)
.
When w is a power of 2, say w= 2t, then the operation ⌊s/w⌋simply erases the t least signiﬁcant
bits of s. Hence, the left part of Glcg(s) is the result of dropping the t least signiﬁcant bits of s.
The generator Glcg is clearly insecure since given s′ := as+ bmod q it is straight-forward to
recover s and then distinguish ⌊s/w⌋from random. Nevertheless, consider a variant of the Blum-
Micali construction in which the ﬁnal Sq-value is not output:
G(n)
lcg (s) := s1 ←s
for i←1 to n do
ri ←⌊si/w⌋, s i+1 ←asi + bmod q
output (r1,...,r n).
We refer to each iteration of the loop as a single iteration of the LCG generator and call each one
of r1,...,r n the output of a single iteration.
Diﬀerent implementations use diﬀerent system parameters q,a,b,w . For example, the
Math.random function in the Java 8 Development Kit (JDKv8) uses q := 2 48, w := 2 22, and
70
the hexadecimal constants a:= 0x5DEECE66D and b:= 0x0B. Thus, every iteration of the LCG
generator outputs the top 48 −22 = 26 bits of the 48-bit state si.
The parameters used by this Java 8 generator are clearly too small for security applications since
the output r1 ∈R of the ﬁrst iteration of the generator reveals all but 22 bits of the seeds∈Sq. An
attacker can easily recover the unknown 22 bits ofsby exhaustive search. For every possible value of
the 22 bits, the attacker forms a candidate seed ˆs∈Sq. It tests if ˆsis the correct seed by checking if
the outputs ˆr1,ˆr2,ˆr3 ∈R computed from the seed ˆsare equal to the outputs r1,r2,r3 ∈R observed
from the actual generator. By trying all 2 22 candidates (about four million) the attacker eventually
ﬁnds the correct seed s and can then predict all subsequent outputs of the generator. This attack
runs in under a second on a modern processor.
Even when the LCG parameters are suﬃciently large to prevent exhaustive search, sayq= 2512,
the generator G(n)
lcg is insecure and should never be used for security applications despite its wide
availability in software libraries. Known attacks [65] on the LCG show that even if the generator
outputs only a few bits per iteration, it is still possible to predict the entire sequence from just a
few consecutive outputs. Let us see an elegant version of this attack.
Cryptanalysis. Suppose that q is large (e.g. q = 2 512) and the LCG generator G(n)
lcg outputs
about half the bits of the state s per iteration, as in the Java 8 Math.random generator. An
exhaustive search on the seed sis not possible given its size. Nevertheless, we show how to quickly
predict the generator from the output of only two consecutive iterations.
Suppose that w≤⌊√q/c⌋for some ﬁxed c> 0, say c= 32. This means that at every iteration
the generator outputs slightly more than half the bits of the current internal state si. For example,
when q= 2512 and c= 32 the generator would output at least 261 bits per iteration.
Suppose the attacker is given two consecutive outputs of the generator ri,ri+1 ∈R. We show
how it can predict the remaining sequence. The attacker knows that
ri = ⌊si/w⌋ and ri+1 = ⌊si+1/w⌋= ⌊(asi + bmod q)/w⌋.
for some unknown si ∈Sq. By multiplying both equation by w we obtain
ri ·w+ e0 = si and ri+1 ·w+ e1 = (asi + bmod q),
where e0 and e1 are the remainders after dividing si and si+1 by w. In particular,
0 ≤e0,e1 <w ≤⌊√q/c⌋.
The fact that e0,e1 are smaller than √q/c is an essential ingredient of the attack. Next, let us
write s in place of si, and eliminate the mod q by introducing an integer variable x, to obtain
ri ·w+ e0 = s and ri+1 ·w+ e1 = as+ b+ qx .
The integers x,s,e 0,e1 are unknown to the attacker, but it has the integers ri,ri+1,w,a,b . Re-
arranging terms to put the terms involving x and s on the left gives
s= ri ·w+ e0 and as+ qx= ri+1w−b+ e1 . (3.12)
Finally, we can write (3.12) in vector form as
s·
(1
a
)
+ x·
(0
q
)
= g+ e where g:=
( riw
ri+1w−b
)
and e:=
(e0
e1
)
. (3.13)
71
Figure 3.9: The two-dimensional lattice La associated with attacking the LCG. Here the lattice
is generated by the vectors (1 ,5)⊺ and (0,29)⊺. The attacker has a vector g = (9,7)⊺ ∈Z2 and
wants to ﬁnd the closest lattice vector u ∈La. In this picture there is indeed only one “close”
lattice vector to g, namely u= (7,6)⊺.
The attacker knows g ∈Z2, but it does not know s, x, or e ∈Z2. However, it knows that e is
short, namely ∥e∥∞ is less than ⌊√q/c⌋.
Let u∈Z2 denote the unknown vector u:= g+ e= s·(1,a)⊺+ x·(0,q)⊺. If the attacker could
ﬁnd uthen it could easily recover s and x from uby linear algebra. Using s it could predict the
rest of the PRG output. Thus, to break the generator it suﬃces to ﬁnd the vector u∈Z2. The
attacker has g∈Z2, and it knows that ∥g−u∥∞= ∥e∥∞ is short, meaning that uis “close” to g.
We show how to ﬁnd u from g. Consider the set of all integer linear combinations of the
vectors (1,a)⊺ and (0 ,q)⊺. This set, denoted by La, is a subset of Z2 and contains vectors like
(1,a)⊺, (2,2a)⊺, (3,3a−2q)⊺, and so on. The set La is illustrated in Fig. 3.9 where the solid dots
in the ﬁgure are the integer linear combinations of the vectors (1 ,a)⊺ and (0,q)⊺. The set La is
called the two-dimensional lattice generated by the vectors (1 ,a)⊺ and (0,q)⊺.
Now, the attacker has a vector g∈Z2 and knows that his target vector u∈La is close to g. If
it could ﬁnd the closest vector in La to gthen there is a good chance that this vector is the desired
vector u. The following lemma shows that indeed this is the case for most a∈Sq.
Lemma 3.7. For at least (1 −16/c2) ·q of the a in Sq, the lattice La ⊆Z2 has the following
property: for every g∈Z2 there is at most one vector u∈La such that ∥g−u∥∞<⌊√q/c⌋.
Taking c= 32 in Lemma 3.7 shows that for 98% of the a∈Sq, the closest vector to gin La is
precisely the desired vector u. Before proving the lemma, let us ﬁrst complete the description of
the attack.
It remains to eﬃciently ﬁnd the closest vector to g in La. This problem is a special case of
a general problem called the closest vector problem : given a lattice Land a vector g, ﬁnd
the vector in Lthat is closest to g. When the lattice Lis two dimensional there is an eﬃcient
polynomial time algorithm for this problem [153]. Armed with this algorithm the attacker can
72
recover the internal state si of the LCG generator from just two outputs ri,ri+1 of the generator
and predict the remaining sequence. This attack works for 98% of the a∈Sq.
For completeness we note that a= 1 and a= 2 are examples where Lemma 3.7 fails. For these
a there may be many lattice vectors in La close to a given g ∈Z2, and the attack will fail. We
leave it as a fun exercise to devise an attack that works for the a in Sq for which Lemma 3.7 does
not apply. We conclude this section with a proof of Lemma 3.7.
Proof of Lemma 3.7. Let g∈Z2 and suppose there are two vectors u0 and u1 in La that are close
to g, that is, ∥ui−g∥∞<⌊√q/c⌋for i= 0,1. Then u0 and u1 must be close to each other. Indeed,
by the triangle inequality, we have
∥u0 −u1∥∞≤∥u0 −g∥∞+ ∥g−u1∥∞<2⌊√q/c⌋.
Since any lattice is closed under addition, we know that u:= u0 −u1 is a vector in the lattice La,
and we conclude that La must contain a “short” vector, namely, a non-zero vector of norm less
than B := 2⌊√q/c⌋. So, let us bound the number of “bad” a ∈Sq for which La contains a short
vector of norm less than B.
First, consider the case when q is a prime. We show that every short vector in Z2 is contained
in at most one lattice La. Therefore, the number of bad a’s is at most the number of short vectors
in Z2. Let t= (s,y)⊺∈Z2 be some non-zero vector such that ∥t∥∞ <B . Suppose that t∈La for
some a∈Sq. Then there exist integers sa and xa such that sa ·(1,a)⊺+ xa ·(0,q)⊺= t= (s,y)⊺.
From this we obtain that s= sa and y = asmod q. Moreover, s̸= 0 since otherwise t= 0. Since
y = asmod q and s ̸= 0, the value of a is uniquely determined, namely, a = ys−1 mod q. Hence,
when q is prime, every non-zero short vector t is contained in at most one lattice La for some
a∈Sq. It follows that the number of bad ain Sq is at most the number of vectors in t∈Z2 where
∥t∥∞<B , which is at most (2 B)2 ≤16q/c2.
The same bound on the number of bad a’s holds when q is not a prime. To see why consider a
speciﬁc non-zero s∈Sq and let d= gcd(s,q). As above, a vector t= (s,y)⊺ is contained in some
lattice La only if there is an a ∈Sq satisfying as ≡y (mod q). This implies that y must be a
multiple of d so that we need only consider 2 B/d possible values of y. For each such y the vector
t= (s,y)⊺ is in at most d lattices La. Since ∥t∥∞<B , there are at most 2 B possible values for s.
Hence, the number of bad a’s is bounded by (d·2B/d) ·2B = (2B)2 as in the case when q is prime.
To conclude, there are at most 16q/c2 bad values of ain Sq. Therefore, for at least (1 −16/c2)·q
of the a values in Sq, the lattice La contains no non-zero short vectors and the lemma follows. 2
3.7.2 The subset sum generator
We next show how to construct a pseudorandom generator from simple linear operations. The
generator is secure assuming that a certain randomized version of the classic subset sum problem
is hard.
The modular subset problem. Let q be a positive integer and set Sq := {0,...,q −1}⊆ Z.
Choose n integers a:= (a1,...,a n) in Sq and deﬁne the subset sum function fa : {0,1}n →Sq as
for s= (s1,...,s n) ∈{0,1}n deﬁne fa(s) :=
n∑
i=1
ai ·si mod q .
73
For example, fa(101101) = a1 + a3 + a4 + a6 mod q. Now, for a target integer t∈Sq the modular
subset problem is deﬁned as follows:
given (q,a,t) as input, output a vector s∈{0,1}n such that fa(s) = t, if one exists.
In other words, the problem is to ﬁnd a pre-image of t for the function fa(·), if one exists. The
modular subset problem is known to be NP hard.
The subset sum PRG. The subset problem naturally suggests the following PRG: at setup
time ﬁx an integer q and choose n random integers a:= (a1,...,a n) in Sq. The PRG Gq,a takes a
seed s∈{0,1}n and outputs a pseudorandom value in Sq. It is deﬁned as
Gq,a(s) :=
n∑
i=1
ai ·si mod q .
The PRG expands an nbit seed to an element of Sq, which is about log2 qbits of output. Choosing
nand qso that log2 qis somewhat bigger than 2ngives a PRG whose output is about twice the size
of the input. We can then plug this PRG into the Blum-Micali construction to expand the output
further. Note that if the output of the PRG needs to be converted to a binary string, then q needs
to be close to a power of 2, otherwise the most signiﬁcant bit of the output will be biased.
While Gq,a is far slower than custom PRG constructions like ChaCha20 from Section 3.6, the
work on average per bit of output is a single modular addition in Sq, which may be appropriate for
some applications that are not time sensitive.
Impagliazzo and Naor [91] show that attacking Gq,a as a PRG is as hard as solving a certain
randomized variant of the modular subset sum problem. While there is considerable work on solving
the modular subset problem, the problem appears to be hard when log 2 q is approximately 2n, and
n is large, say n> 1000. This shows the security of Gq,a as a PRG.
Variants. Fischer and Stern [62] and others propose the following variation of the subset sum
generator:
Gq,A(s) := A·smod q
where q is a small prime, A is a random matrix in Sn×m
q for n < m, and the seed sis uniform in
{0,1}m. The generator maps an m-bit seed to nlog2 q bits of output. We discuss this generator
further in Chapter 17.
3.8 Case study: cryptanalysis of the DVD encryption system
The Content Scrambling System (CSS) is a system used for protecting movies on DVD disks. It
uses a stream cipher, called the CSS stream cipher, to encrypt movie contents. CSS was designed
in the 1980’s when exportable encryption was restricted to 40-bit keys. As a result, CSS encrypts
movies using a 40-bit secret key. While ciphers using 40-bit keys are woefully insecure, we show that
the CSS stream cipher is particularly weak and can be broken in far less time than an exhaustive
search over all 240 keys. It provides a fun opportunity for cryptanalysis.
74
01101001
0 1 2 3 4 5 6 7
0110100100010100 ...
⨁
Figure 3.10: The 8 bit linear feedback shift register {4,3,2,0}
Linear feedback shift registers (LFSR). The CSS stream cipher is built from two LFSRs.
An n-bit LFSR is deﬁned by a set of integers V := {v1,...,v d}where each vi is in the range
{0,...,n −1}. The elements of V are called tap positions . An LFSR gives a PRG as follows
(Fig. 3.10):
Input: s= (bn−1,...,b 0) ∈{0,1}n and s̸= 0n
Output: y∈{0,1}ℓ where ℓ>n
for i←1 ...ℓ do
output b0 / / output one bit
b←bv1 ⊕···⊕ bvd / / compute feedback bit
s←(b, bn−1,..., b 1) / / shift register bits to the right
The LFSR outputs one bit per clock cycle. Note that if an LFSR is started in state s = 0n then
its output is degenerate, namely all 0. For this reason one of the seed bits is always set to 1.
LFSR can be implemented in hardware with few transistors. As a result, stream ciphers built
from LFSR are attractive for low-cost consumer electronics such as DVD players, cell phones, and
Bluetooth devices.
Stream ciphers from LSFRs. A single LFSR is completely insecure as a PRG since given n
consecutive bits of its output it is trivial to compute all subsequent bits. Nevertheless, by combining
several LFSRs using a non-linear component it is possible to get some (weak) security as a PRG.
Trivium, one of the eStream portfolio stream ciphers, is built this way.
One approach to building stream ciphers from LFSRs is to run several LFSRs in parallel and
combine their output using a non-linear operation. The CSS stream cipher, described next, com-
bines two LFSRs using addition over the integers. The A5/1 stream cipher used to encrypt GSM
cell phone traﬃc combines the outputs of three LFSRs. The Bluetooth E0 stream cipher combines
four LFSRs using a 2-bit ﬁnite state machine. All these algorithms have been shown to be insecure
and should not be used: recovering the plaintext takes far less time than an exhaustive search on
the key space.
Another approach is to run a single LFSR and generate the output from a non-linear operation
on its internal state. The snow 3G cipher used to encrypt 3GPP cell phone traﬃc operates this
way.
The CSS stream cipher. The CSS stream cipher is built from the PRG shown in Fig. 3.11.
The PRG works as follows:
75
17-bit LFSR
25-bit LFSR
x+ y+ cmod 256
8 bits
x
8 bits
y
8
Figure 3.11: The CSS stream cipher
Input: seed s∈{0,1}40
Output: ℓ bytes
write s= s1∥s2 where s1 ∈{0,1}16 and s2 ∈{0,1}24
load 1∥s1 into a 17-bit LFSR
load 1∥s2 into a 25-bit LFSR
c←0 / / carry bit
for i= 1,...,ℓ :
run both LFSRs for eight cycles to obtain xi,yi ∈{0,1}8
treat xi and yi as integers in 0 ... 255
output zi := xi + yi + cmod 256
if xi + yi >255 then c←1 else c←0 / / carry bit
The PRG outputs one byte per iteration. Prepending 1 to both s1 and s2 ensures that the LFSRs
are never initialized to the all 0 state. The taps for both LFSRs are ﬁxed. The 17-bit LFSR uses
taps {14,0}. The 25-bit LFSR uses taps {12,4,3,0}.
The CSS PRG we presented is a minor variation of CSS that is a little easier to describe, but
has the same security. In the real CSS, instead of prepending a 1 to the initial seeds, one inserts
the 1 in bit position 9 for the 17-bit LFSR and in bit position 22 for the 25-bit LFSR. In addition,
the real CSS discards the ﬁrst byte output by the 17-bit LFSR and the ﬁrst two bytes output by
the 25-bit LFSR. Neither issue aﬀects the analysis presented next.
Insecurity of CSS. Given the PRG output, one can clearly recover the secret seed in time 2 40
by exhaustive search over the seed space. We show a much faster attack that takes only 216 guesses.
Suppose we are given the ﬁrst 100 bytes ¯z := (z1,z2,... ) output by the PRG. The attack is based
on the following observation:
Let (x1,x2,x3) and ( y1,y2,y3) be the ﬁrst three bytes output by the 17-bit and 25-bit
LFSR, respectively. Then
(216x3 + 28x2 + x1) + (216y3 + 28y2 + y1) ≡(216z3 + 28z2 + z1) (mod 2 24).
Therefore, once both ( z1,z2,z3) and ( x1,x2,x3) are known, one can easily compute
(y1,y2,y3), from which the initial state s2 of the 25-bit LFSR is easily obtained.
With this observation the attacker can recover the seed sby trying all possible 16-bit values for s1.
For each guess for s1 compute the corresponding ( x1,x2,x3) output from the 17-bits LFSR. Use
76
203 35 41 87 2 23 187 72
0 1 2 3 4 254 255
··· ···
i j
Figure 3.12: An example RC4 internal state
the observation above to obtain a candidate seed s2 for the 25-bit LFSR. Then to conﬁrm that
ˆs:= s1∥s2 is the correct secret seed, run the PRG using the seed ˆsfor 100 iterations, and compare
the resulting output to the given sequence ¯ z. If the sequences do not match, try another guess
for s1. Once the attacker hits the correct value for s1, the generated sequence will match the
given ¯z, in which case the attacker has the correct secret seed s:= s1∥s2.
We just showed that the entire seed s can be found after an expected 2 15 guesses for s1. This
is much faster than the naive 2 40-time exhaustive search attack.
3.9 Case study: cryptanalysis of the RC4 stream cipher
The RC4 stream cipher, designed by Ron Rivest in 1987, was historically used for securing Web
traﬃc (in the SSL/TLS protocol) and wireless traﬃc (in the 802.11b WEP protocol). It is designed
to operate on 8-bit processors with little internal memory. While RC4 is still in use, it has been
shown to be vulnerable to a number of signiﬁcant attacks and should not be used in new projects.
Our discussion of RC4 serves as an elegant example of stream cipher cryptanalysis.
At the heart of the RC4 cipher is a PRG, called the RC4 PRG. The PRG maintains an internal
state consisting of an array S of 256 bytes plus two additional bytes i,j used as pointers into S.
The array S contains all the numbers 0 ... 255 and each number appears exactly once. Fig. 3.12
gives an example of an RC4 state.
The RC4 stream cipher key s is a seed for the PRG and is used to initialize the array S to a
pseudo-random permutation of the numbers 0... 255. Initialization is performed using the following
setup algorithm:
input: string of bytes s
for i←0 to 255 do: S[i] ←i
j ←0
for i←0 to 255 do
k←s
[
imod |s|
]
/ / extract one byte from seed
j ←
(
j+ S[i] + k
)
mod 256
swap(S[i],S[j])
During the loop the index i runs linearly through the array while the index j jumps around. At
each iteration the entry at index i is swapped with the entry at index j.
Once the array S is initialized, the PRG generates pseudo-random output one byte at a time
using the following stream generator:
77
cipher speed1(MB/sec)
RC4 126
SEAL 375
Salsa20 408
Sosemanuk 727
Table 3.1: Software stream cipher speeds (higher speed is better)
i←0, j ←0
repeat
i←(i+ 1) mod 256
j ←(j+ S[i]) mod 256
swap(S[i],S[j])
output S
[
(S[i] + S[j]) mod 256
]
forever
The procedure runs for as long as necessary. Again, the index i runs linearly through the array
while the index j jumps around. Swapping S[i] and S[j] continuously shuﬄes the array S.
RC4 encryption speed. RC4 is well suited for software implementations. Other stream ciphers,
such as Grain and Trivium, are designed for hardware and perform poorly when implemented in
software. Table 3.1 provides running times for RC4 and a few other software stream ciphers.
Modern processors operate on 64-bit words, making the 8-bit design of RC4 relatively slow on
these architectures.
3.9.1 Security of RC4
At one point RC4 was believed to be a secure stream cipher and was widely deployed in applications.
The cipher fell from grace after a number of attacks showed that its output is somewhat biased.
We present two attacks that distinguish the output of RC4 from a random string. Throughout the
section we let n denote the size of the array S. n= 256 for RC4.
Bias in the initial RC4 output. The RC4 setup algorithm initializes the array S to a permuta-
tion of 0 ... 255 generated from the given random seed. For now, let us assume that the RC4 setup
algorithm is perfect and generates a uniform permutation from the set of all 256! permutations.
Mantin and Shamir [107] showed that, even assuming perfect initialization, the output of RC4 is
biased.
Lemma 3.8 (Mantin-Shamir). Suppose the array S is set to random permutation of 0 ...n −1
and that i,j are set to 0. Then the probability that the second byte of the output of RC4 is equal to
0 is 2/n.
1Performance numbers were obtained using the Crypto++ 5.6.0 benchmarks running on a 1.83 GhZ Intel Core 2
processor.
78
x 0 y
0 1 2 x
y 0 x S[x+ y]
y x 0 0
ij
i j
i j
Figure 3.13: Proof of Lemma 3.8
Proof idea. Let z2 be the second byte output by RC4. Let P be the event that S[2] = 0 and
S[1] ̸= 2. The key observation is that when event P happens then z2 = 0 with probability 1. See
Fig. 3.13. However, when P does not happen then z2 is uniformly distributed in 0 ...n −1 and
hence equal to 0 with probability 1 /n. Since Pr[ P] is about 1 /n we obtain (approximately) that
Pr[z2 = 0] = Pr
[
(z2 = 0) |P
]
·Pr[P] + Pr
[
(z2 = 0) |¬P
]
·Pr[¬P]
≈1 ·(1/n) + (1/n) ·(1 −1/n) ≈2/n 2
The lemma shows that the probability that the second byte in the output of RC4 is 0 is
twice what it should be. This leads to a simple distinguisher for the RC4 PRG. Given a string
x ∈{0 ... 255}ℓ, for ℓ ≥2, the distinguisher outputs 0 if the second byte of x is 0 and outputs 1
otherwise. By Lemma 3.8 this distinguisher has advantage approximately 1 /n, which is 0 .39% for
RC4.
The Mantin-Shamir distinguisher shows that the second byte of the RC4 output are biased.
This was generalized by AlFardan et al. [4] who showed, by measuring the bias over many random
keys, that there is bias in every one of the ﬁrst 256 bytes of the output: the distribution on each
byte is quite far from uniform. The bias is not as noticeable as in the second byte, but it is non-
negligible and suﬃcient to attack the cipher. They show, for example, that given the encryption of
a single plaintext encrypted under 2 30 random keys, it is possible to recover the ﬁrst 128 bytes of
the plaintext with probability close to 1. This attack is easily carried out on the Web where a secret
cookie is often embedded in the ﬁrst few bytes of a message. This cookie is re-encrypted over and
over with fresh keys every time the browser connects to a victim web server. Using Javascript the
attacker can make the user’s browser repeatedly re-connect to the target site giving the attacker
the 230 ciphertexts needed to mount the attack and expose the cookie.
In response, RSA Labs issued a recommendation suggesting that one discard the ﬁrst 1024 bytes
output by the RC4 stream generator and only use bytes 1025 and onwards. This defeats the initial
key stream bias distinguishers, but does not defeat other attacks, which we discuss next.
79
Bias in the RC4 stream generator. Suppose the RC4 setup algorithm is modiﬁed so that the
attack of the previous paragraph is ineﬀective. Fluhrer and McGrew [64] gave a direct attack on
the stream generator. They argue that the number of times that the pair of bytes (0 ,0) appears
in the RC4 output is larger than what it should be for a random sequence. This is suﬃcient to
distinguish the output of RC4 from a random string.
Let STRC4 be the set of all possible internal states of RC4. Since there are n! possible settings
for the array S and npossible settings for each of iand j, the size of STRC4 is n! ·n2. For n= 256,
as used in RC4, the size of ST RC4 is gigantic, namely about 10 511.
Lemma 3.9 (Fluhrer-McGrew). Suppose RC4 is initialized with a random state T in ST RC4.
Let (z1,z2) be the ﬁrst two bytes output by RC4 when started in state T. Then
i̸= n−1 = ⇒ Pr[(z1,z2) = (0,0)] ≥(1/n2) ·
(
1 + (1/n)
)
i̸= 0,1 = ⇒ Pr[(z1,z2) = (0,1)] ≥(1/n2) ·
(
1 + (1/n)
)
A pair of consecutive outputs ( z1,z2) is called a digraph. In a truly random string, the
probability of all digraphs ( x,y) is exactly 1 /n2. The lemma shows that for RC4 the probability
of (0,0) is greater by 1 /n3 from what it should be. The same holds for the digraph (0 ,1). In fact,
Fluhrer-McGrew identify several other anomalous digraphs, beyond those stated in Lemma 3.9.
The lemma suggests a simple distinguisher D between the output of RC4 and a random string.
If the distinguisher ﬁnds more (0 ,0) pairs in the given string than are likely to be in a random
string it outputs 1, otherwise it outputs 0. More precisely, the distinguisher D works as follows:
input: string x∈{0 ...n }ℓ
output: 0 or 1
let q be the number of times the pair (0 ,0) appears in x
if (q/ℓ) −(1/n2) >1/(2n3) output 0, else output 1
Using Theorem B.3 we can estimate D’s advantage as a function of the input length ℓ. In
particular, the distinguisher D achieves the following advantages:
ℓ= 214 bytes: PRG adv[D,RC4] ≥2−8
ℓ= 234 bytes: PRG adv[D,RC4] ≥0.5
Using all the anomalous digraphs provided by Fluhrer and McGrew one can build a distinguisher
that achieves advantage 0.8 using only 2 30.6 bytes of output.
Related key attacks on RC4. Fluhrer, Mantin, and Shamir [63] showed that RC4 is insecure
when used with related keys. We discuss this attack and its impact on the 802.11b WiFi protocol
in Section 9.10, attack 2.
3.10 Generating random bits in practice
Random bits are needed in cryptography for many tasks, such as generating keys and other
ephemeral values called nonces. Throughout the book we assume all parties have access to a
good source of randomness, otherwise many desirable cryptographic goals are impossible. So far
we used a PRG to stretch a short uniformly distributed secret seed to a long pseudorandom string.
80
internal
state
generate
function
entropy pseudorandom
output
Figure 3.14: A random number generator
While a PRG is an important tool in generating random (or pseudorandom) bits it is only part of
the story.
In practice, random bits are generated using a random number generator , or RNG. An
RNG, like a PRG, outputs a sequence of random or pseudorandom bits. RNGs, however, have an
additional interface that is used to continuously add entropy to the RNG’s internal state, as shown
in Fig. 3.14. The idea is that whenever the system has more random entropy to contribute to the
RNG, this entropy is added into the RNG internal state. Whenever someone reads bits from the
RNG, these bits are generated using the current internal state.
An example is the Linux RNG which is implemented as a device called /dev/random. Anyone
can read data from the device to obtain random bits. To play with the /dev/random try typing
cat /dev/random at a UNIX shell. You will see an endless sequence of random-looking characters.
The UNIX RNG obtains its entropy from a number of hardware sources:
• keyboard events: inter-keypress timings provide entropy;
• mouse events: both interrupt timing and reported mouse positions are used;
• hardware interrupts: time between hardware interrupts is a good source of entropy;
These sources generate a continuous stream of randomness that is periodically XORed into the
RNG internal state. Notice that keyboard input is not used as a source of entropy; only keypress
timings are used. This ensures that user input is not leaked to other users in the system via the
Linux RNG.
High entropy random generation. The entropy sources described above generate randomness
at a relatively slow rate. To generate true random bits at a faster rate, Intel added a hardware
random number generator starting with the Ivy Bridge processor family in 2012. Output from
the generator is read using the RdRand instruction that is intended to provide a fast uniform bit
generator.
To reduce biases in the generator output, the raw bits are ﬁrst passed through a function called
a “conditioner” designed to ensure that the output is a sequence of uniformly distributed bits,
assuming suﬃcient entropy is provided as input. We discuss this in more detail in Section 8.10
where we discuss the key derivation problem.
The RdRandgenerator should not replace other entropy sources such as the four sources described
above; it should only augment them as an additional entropy source for the RNG. This way, if the
generator is defective it will not completely compromise the cryptographic application.
One diﬃculty with Intel’s approach is that, over time, the hardware generator may stop produc-
ing high entropy random bits due to a hardware glitch. For example, the raw hardware generator
81
may always output ‘0’, resulting in highly non-random output. To prevent this from happening the
processor periodically tests the raw bits produced by the hardware using a ﬁxed set of statistical
tests. If any of the tests reports “non-random” the hardware generator is declared to be defective.
3.11 A broader perspective: computational and statistical indis-
tinguishability
Our deﬁnition of security for a pseudo-random generator G formalized the intuitive idea that an
adversary should not be able to eﬀectively distinguish between G(s) and r, where s is a randomly
chosen seed, and r is a random element of the output space.
This idea generalizes quite naturally and usefully to other settings. Suppose P0 and P1 are
probability distributions on some ﬁnite set R. Our goal is to formally deﬁne the intuitive notion
that an adversary cannot eﬀectively distinguish between P0 and P1. As usual, this is done via an
attack game. For b = 0,1, we write x ←R Pb to denote the assignment to x of a value chosen at
random from the set R, according to the probability distribution Pb.
Attack Game 3.3 (Distinguishing P0 from P1). For given probability distributions P0 and
P1 on a ﬁnite set R, and for a given adversary A, we deﬁne two experiments, Experiment 0 and
Experiment 1. For b= 0,1, we deﬁne:
Experiment b:
• The challenger computes x as follows:
x←R Pb
and sends x to the adversary.
• Given x, the adversary computes and outputs a bit ˆb∈{0,1}.
For b= 0,1, let Wb be the event that Aoutputs 1 in Experiment b. We deﬁne A’s advantage
with respect to P0 and P1 as
Distadv[A,P0,P1] :=
⏐⏐⏐Pr[W0] −Pr[W1]
⏐⏐⏐. 2
Deﬁnition 3.4 (Computational indistinguishability). Distributions P0 and P1 are called
computationally indistinguishable if the value Distadv[A,P0,P1] is negligible for all eﬃcient
adversaries A.
Using this deﬁnition we can restate the deﬁnition of a secure PRG more simply: a PRG G
deﬁned over (S,R) is secure if and only if P0 and P1 are computationally indistinguishable, where
P1 is the uniform distribution on R, and P0 is the distribution that assigns to each r ∈R the
weight
P0(r) := |{s∈S : G(s) = r}|
|S| .
Again, as discussed in Section 2.2.5, Attack Game 3.3 can be recast as a “bit guessing” game,
where instead of having two separate experiments, the challenger chooses b ∈{0,1}at random,
and then runs Experiment b against the adversary A. In this game, we measure A’s bit-guessing
82
advantage Distadv∗[A,P0,P1] as |Pr[ˆb = b] −1/2|. The general result of Section 2.2.5 (namely,
(2.11)) applies here as well:
Distadv[A,P0,P1] = 2 ·Distadv∗[A,P0,P1]. (3.14)
Typically, to prove that two distributions are computationally indistinguishable, we will have to
make certain other computational assumptions. However, sometimes two distributions are so similar
that no adversary can eﬀectively distinguish between them, regardless of how much computing
power the adversary may have. To make this notion of “similarity” precise, we introduce a useful
tool, called statistical distance:
Deﬁnition 3.5 (Statistical distance). Suppose P0 and P1 are probability distributions on a ﬁnite
set R. Then their statistical distance is deﬁned as
∆[P0,P1] := 1
2
∑
r∈R
⏐⏐P0(r) −P1(r)
⏐⏐.
Example 3.1. Suppose P0 is the uniform distribution on {1,...,m }, and P1 is the uniform dis-
tribution on {1,...,m −δ}, where δ∈{0,...,m −1}. Let us compute ∆[ P0,P1]. We could apply
the deﬁnition directly; however, consider the following graph of P0 and P1:
0
A
B
C
m − δ
m
1/m
1/(m − δ)
The statistical distance between P0 and P1 is just 1/2 times the area of regions Aand C in the
diagram. Moreover, because probability distributions sum to 1, we must have
area of B+ area of A= 1 = area of B+ area of C,
and hence, the areas of region A and region C are the same. Therefore,
∆[P0,P1] = area of A= area of C = δ/m. 2
The following theorem allows us to make a connection between the notions of computational
indistinguishability and statistical distance:
Theorem 3.10. Let P0 and P1 be probability distributions on a ﬁnite set R. Then we have
max
R′⊆R
⏐⏐P0[R′] −P1[R′]
⏐⏐= ∆[P0,P1],
where the maximum is taken over all subsets R′ of R.
Proof. Suppose we split the set Rinto two disjoint subsets: the set R0 consisting of those r ∈R
such that P0(r) < P1(r), and the set R1 consisting of those r ∈ Rsuch that P0(r) ≥P1(r).
Consider the following rough graph of the distributions of P0 and P1, where the elements of R0 are
placed to the left of the elements of R1:
83
A
B
C
R0
R1
P1
P0
Now, as in Example 3.1,
∆[P0,P1] = area of A= area of C.
Observe that for every subset R′of R, we have
P0[R′] −P1[R′] = area of C′−area of A′,
where C′is the subregion of C that lies above R′, and A′is the subregion of A that lies above R′.
It follows that |P0[R′] −P1[R′]|is maximized when R′= R0 or R′= R1, in which case it is equal
to ∆[P0,P1]. 2
The connection to computational indistinguishability is as follows:
Theorem 3.11. Let P0 and P1 be probability distributions on a ﬁnite set R. Then for every
adversary A, we have
Distadv[A,P0,P1] ≤∆[P0,P1].
Proof. Consider an adversary Athat tries to distinguish P0 from P1, as in Attack Game 3.3.
First, we consider the case where Ais deterministic. In this case, the output of Ais a function
f(r) of the value r∈R presented to it by the challenger. Let R′:= {r∈R : f(r) = 1}. If W0 and
W1 are the events deﬁned in Attack Game 3.3, then for b= 0,1, we have
Pr[Wb] = Pb[R′].
By the previous theorem, we have
Distadv[A,P0,P1] =
⏐⏐P0[R′] −P1[R′]
⏐⏐≤∆[P0,P1].
We now consider the case where Ais probabilistic. We can view Aas taking an auxiliary
input t, representing its random choices. We view t as being chosen uniformly at random from
some ﬁnite set T. Thus, the output of Ais a function g(r,t) of the value r ∈R presented to it
by the challenger, and the value t ∈T representing its random choices. For a given t ∈T , let
R′
t := {r∈R : g(r,t) = 1}. Then, averaging over the random choice of t, we have
Pr[Wb] = 1
|T|
∑
t∈T
Pb[R′
t].
84
It follows that
Distadv[A,P0,P1] = |Pr[W0] −Pr[W1]|
= 1
|T|
⏐⏐⏐
∑
t∈T
(P0[R′
t] −P1[R′
t])
⏐⏐⏐
≤ 1
|T|
∑
t∈T
|P0[R′
t] −P1[R′
t]|
≤ 1
|T|
∑
t∈T
∆[P0,P1]
= ∆[P0,P1]. 2
Analogous to the deﬁnition of computational indistinguishability, we have:
Deﬁnition 3.6 (Statistical indistinguishability). Let P0 and P1 be probability distributions
on a ﬁnite set R. We say that P0 and P1 are statistically indistinguishable if the statistical
distance ∆[P0,P1] is negligible.
An immediate consequence of Theorem 3.11 is that two distributions that are statistically
indistinguishable are also computationally indistinguishable:
Corollary 3.12. Let P0 and P1 be probability distributions on a ﬁnite set R. If P0 and P1 are
statistically indistinguishable, then they are also computationally indistinguishable.
Statistical distance between random variables. One also deﬁnes the statistical distance
between two random variables as the statistical distance between their corresponding distributions.
That is, if X and Y are random variables taking values in a ﬁnite set R, then their statistical
distance is
∆[X,Y] := 1
2
∑
r∈R
|Pr[X = r] −Pr[Y = r]|.
In this case, Theorem 3.10 says that
max
R′⊆R
⏐⏐⏐Pr[X ∈R′] −Pr[Y ∈R′]
⏐⏐⏐= ∆[X,Y],
where the maximum is taken over all subsets R′ of R. Deﬁnition 3.6, which deﬁnes statistical
inistinguishability, extends to random variables as well.
Analogously, one can deﬁne distinguishing advantage (as in Attack Game 3.3) and compu-
tational indistinguishability (as in Deﬁnition 3.4) with respect to random variables, rather than
distributions. The advantage of working with random variables is that we can more conveniently
work with distributions that are related to one another, as exempliﬁed in the following theorem.
Theorem 3.13. If Sand T are ﬁnite sets, X and Y are random variables taking values in S, and
f : S→T is a function, then ∆[f(X),f(Y)] ≤∆[X,Y].
85
Proof. We have
∆[f(X),f(Y)] = |Pr[f(X) ∈T ′] −Pr[f(Y) ∈T ′]| for some T′⊆T
(by Theorem 3.10)
= |Pr[X ∈f−1(T′)] −Pr[Y ∈f−1(T′)]|
≤∆[X,Y] (again by Theorem 3.10) . 2
Example 3.2. Let X be uniformly distributed over the set {0,...,m −1}, and let Y be uniformly
distributed over the set {0,...,N −1}, for N ≥m. Let f(t) := tmod m. We want to compute
an upper bound on the statistical distance between X and f(Y). We can do this as follows. Let
N = qm −r, where 0 ≤ r < m, so that q = ⌈N/m⌉. Also, let Z be uniformly distributed
over {0,...,qm −1}. Then f(Z) is uniformly distributed over {0,...,m −1}, since every element
of {0,...,m −1}has the same number (namely, q) of pre-images under f which lie in the set
{0,...,qm −1}. Since statistical distance depends only on the distributions of the random variables,
by the previous theorem, we have
∆[X,f(Y)] = ∆[f(Z),f(Y)] ≤∆[Z,Y],
and as we saw in Example 3.1,
∆[Z,Y] = r
qm < 1
q ≤m
N.
Therefore,
∆[X,f(Y)] < m
N. 2
Example 3.3. Suppose we want to generate a pseudo-random number in a given interval
{0,...,m −1}. However, suppose that we have at our disposal a PRG G that outputs L-bit
strings. Of course, an L-bit string can be naturally viewed as a number in the range {0,...,N −1},
where N := 2L. Let us assume that N ≥m.
To generate a pseudo-random number in the interval {0,...,m −1}, we can take the output
of G, view it as a number in the interval{0,...,N −1}, and reduce it modulo m. We will show that
this produces a number that is computationally indistinguishable from a truly random number in
the interval {0,...,m −1}, assuming G is secure and m/N is negligible (e.g., N ≥2100 ·m).
To this end, let P0 be the distribution representing the output of G, reduced modulo m, and
let P1 be the uniform distribution on {0,...,m −1}. Let Abe an adversary trying to distinguish
P0 from P1, as in Attack Game 3.3.
Let Game 0 be Experiment 0 of Attack Game 3.3, in which Ais presented with a random
sample distributed according to P0, and let W0 be the event that Aoutputs 1 in this game.
Now deﬁne Game 1 to be the same as Game 0, except that we replace the output of G by a
truly random value chosen from the interval {0,...,N −1}. This value is then reduced modulo m,
as in Game 0. Let W1 be the event that Aoutputs 1 in Game 1. One can easily construct an
eﬃcient adversary Bthat attacks G as in Attack Game 3.1, such that
PRGadv[B,G] =
⏐⏐Pr[W0] −Pr[W1]
⏐⏐.
The idea is that Btakes its challenge value, reduces it modulo m, gives this value to A, and outputs
whatever Aoutputs.
86
Finally, we deﬁne Game 2 to be Experiment 1 of Attack Game 3.3, in whichAis presented with
a random sample distributed according to P1, the uniform distribution on {0,...,m −1}. Let W2
be the event that Aoutputs 1 in Game 2. If P is the distribution of the value presented to Ain
Game 1, then by Theorem 3.11, we have |Pr[W1] −Pr[W2]|≤ ∆[P,P1]; moreover, by Example 3.2,
we have ∆[P,P1] ≤m/N.
Putting everything together, we see that
Distadv[A,P0,P1] =
⏐⏐Pr[W0] −Pr[W2]
⏐⏐≤
⏐⏐Pr[W0] −Pr[W1]
⏐⏐+
⏐⏐Pr[W1] −Pr[W2]
⏐⏐
≤PRGadv[B,G] + m
N,
which, by assumption, is negligible. 2
Remark 3.1. In statistics, a divergence function D(P0,P1) establishes a way to measure the
distance between two distributions P0 and P1. The statistical distance ∆[ P0,P1], developed in
this section, is one example of a divergence function. Many other divergence functions have been
developed, such as Kullback–Leibler (KL) divergence and R´ enyi Divergence. In some cases, these
alternate divergence functions lead to much tighter bounds than what can be proved using sta-
tistical distance. Exercise 3.12 introduces a divergence function with important applications to
cryptography (see Exercises 3.14 and 7.12). Other divergence functions and applications may be
found in [133, 1]. 2
3.11.1 Mathematical details
As usual, we ﬁll in the mathematical details needed to interpret the deﬁnitions and results of this
section from the point of view of asymptotic complexity theory.
In deﬁning computational and statistical indistinguishability (Deﬁnitions 3.4 and 3.6), one
should consider two families of probability distributions P0 = {P0,λ}λ and P1 = {P1,λ}λ, indexed
by a security parameter λ. For each λ, the distributions P0,λ and P1,λ should take values in a
ﬁnite set of bit strings Rλ, where the strings in Rλ are bounded in length by a polynomial in λ.
In Attack Game 3.3, the security parameter λ is an input to both the challenger and adversary,
and in Experiment b, the challenger produces a sample, distributed according to Pb,λ. The ad-
vantage should properly be written Dist adv[A,P0,P1](λ), which is a function of λ. Computational
indistinguishability means that this is a negligible function. Similarly, the deﬁnition of statistical
indistinguishability says that the value ∆[ P0,λ,P1,λ] grows negligibly, as a function of λ.
In some situations, it may be natural to introduce a probabilistically generated system parame-
ter; however, from a technical perspective, this is not necessary, as such a system parameter can be
incorporated in the distributions P0,λ and P1,λ. One could also impose the requirement that P0,λ
and P1,λ be eﬃciently sampleable; however, to keep the deﬁnition simple, we will not require this.
The deﬁnition of statistical distance (Deﬁnition 3.5) makes perfect sense from a non-asymptotic
point of view, and does not require any modiﬁcation or elaboration. Theorem 3.10 holds as stated,
for speciﬁc distributions P0 and P1. Theorem 3.11 may be viewed asymptotically as stating that for
all distribution families P0 = {P0,λ}λ and P1 = {P1,λ}λ, for all adversaries (even computationally
unbounded ones), and for all λ, we have
Distadv[A,P0,P1](λ) ≤∆[P0,λ,P1,λ].
87
3.12 A fun application: coin ﬂipping and bit commitment
Alice and Bob are going out on a date. Alice wants to see one movie and Bob wants to see another.
They decide to ﬂip a random coin to choose the movie. If the coin comes up “heads” they will go to
Alice’s choice; otherwise, they will go to Bob’s choice. When Alice and Bob are in close proximity
this is easy: one of them, say Bob, ﬂips a coin and they both verify the result. When they are far
apart and are speaking on the phone this is harder. Bob can ﬂip a coin on his side and tell Alice
the result, but Alice has no reason to believe the outcome. Bob could simply claim that the coin
came up “tails” and Alice would have no way to verify this. Not a good way to start a date.
A simple solution to their problem makes use of a cryptographic primitive called bit commit-
ment. It lets Bob commit to a bit b∈{0,1}of his choice. Later, Bob can open the commitment
and convince Alice that bwas the value he committed to. Committing to a bit bresults in a com-
mitment string c, that Bob sends to Alice, and an opening string s that Bob uses for opening
the commitment later. A commitment scheme is secure if it satisﬁes the following two properties:
• Hiding: The commitment string c reveals no information about the committed bit b. More
precisely, the distribution on c when committing to the bit 0 is indistinguishable from the
distribution on c when committing to the bit 1. In the bit commitment scheme we present,
the hiding property depends on the security of a certain PRG G.
• Binding: Let c be a commitment string output by Bob. If Bob can open the commitment
as some b ∈{0,1}then he cannot open it as ¯b. This ensures that once Bob commits to a
bit bhe can open it as band nothing else. In the commitment scheme we present the binding
property holds unconditionally.
Coin ﬂipping. Using a commitment scheme, Alice and Bob can generate a random bit b∈{0,1}
so that no side can bias the result towards their preferred outcome, assuming the protocol terminates
successfully. Such protocols are called coin ﬂipping protocols . The resulting bit b determines
what movie they go to.
Alice and Bob use the following simple coin ﬂipping protocol:
Step 1: Bob chooses a random bit b0 ←R {0,1}.
Alice and Bob execute the commitment protocol by which Alice obtains
a commitment c to b0 and Bob obtains an opening string s.
Step 2: Alice chooses a random bit b1 ←R {0,1}and sends b1 to Bob in the clear.
Step 3: Bob opens the commitment by revealing b0 and s to Alice.
Alice veriﬁes that c is indeed a commitment to b0 and aborts if veriﬁcation fails.
Output: the resulting bit is b:= b0 ⊕b1.
We argue that if the protocol terminates successfully and one side is honestly following the protocol
then the other side cannot bias the result towards their preferred outcome. By the hiding property,
Alice learns nothing about b0 at the end of Step 1 and therefore her choice of bitb1 is independent of
the value of b0. By the binding property, Bob can only open the commitment cin Step 3 to the bit
b0 he chose in Step 1. Because he chose b0 before Alice chose b1, Bob’s choice of b0 is independent
of b1. We conclude that the output bit b is the XOR of two independent bits. Therefore, if one
side is honestly following the protocol, the other side cannot bias the resulting bit.
One issue with this protocol is that Bob learns the generated bit at the end of Step 2, before
Alice learns the bit. In principle, if the outcome is not what Bob wants he could abort the protocol
88
at the end of Step 2 and try to re-initiate the protocol hoping that the next run will go his way.
More sophisticated coin ﬂipping protocols avoid this problem, but at the cost of many more rounds
of interaction (see, e.g., [116]).
Bit commitment from secure PRGs. It remains to construct a secure bit commitment scheme
that lets Bob commit to his bitb0 ∈{0,1}. We do so using an elegant construction due to Naor [122].
Let G: S→R be a secure PRG where |R|≥|S| 3 and R= {0,1}n for some n. To commit to
the bit b0, Alice and Bob engage in the following protocol:
Bob commits to bit b0 ∈{0,1}:
Step 1: Alice chooses a random r∈R and sends r to Bob.
Step 2: Bob chooses a random s∈S and computes c←com(s,r,b0)
where com(s,r,b0) is the following function:
c= com(s,r,b0) :=
{
G(s) if b0 = 0,
G(s) ⊕r if b0 = 1.
Bob outputs c as the commitment string and uses s as the opening string.
When it comes time to open the commitment Bob sends ( b0,s) to Alice. Alice accepts the opening
if c= com(s,r,b0) and rejects otherwise.
The hiding property follows directly from the security of the PRG: because the output G(s)
is computationally indistinguishable from a uniform random string in Rit follows that G(s) ⊕r
is also computationally indistinguishable from a uniform random string in R. Therefore, whether
b0 = 0 or b0 = 1, the commitment string c is computationally indistinguishable from a uniform
string in R, as required.
The binding property holds unconditionally as long as 1 /|S|is negligible. The only way Bob
can open a commitment c ∈R as both 0 and 1 is if there exist two seeds s0,s1 ∈S such that
c = G(s0) = G(s1) ⊕r which implies that G(s0) ⊕G(s1) = r. Let us say that r ∈R is “bad” if
there are seeds s0,s1 ∈S such that G(s0)⊕G(s1) = r. The number of pairs of seeds ( s0,s1) is |S|2,
and therefore the number of bad ris at most |S|2. It follows that the probability that Alice chooses
a bad r is at most |S|2/|R|<|S|2/|S|3 = 1/|S|which is negligible. Therefore, the probability that
Bob can open the commitment c as both 0 and 1 is negligible.
This completes the description of the bit commitment scheme. We will see a more eﬃcient
commitment scheme and more applications for commitments in Section 8.12, after we develop a
few more tools.
3.13 Notes
Citations to the literature to be added.
3.14 Exercises
3.1 (Semantic security for random messages). One can deﬁne a notion of semantic secu-
rity for random messages. Here, one modiﬁes Attack Game 2.1 so that instead of the adversary
89
choosing the messages m0,m1, the challenger generates m0,m1 at random from the message space.
Otherwise, the deﬁnition of advantage and security remains unchanged.
(a) Suppose that E = ( E,D) is deﬁned over ( K,M,C), where M= {0,1}L. Assuming that
Eis semantically secure for random messages, show how to construct a new cipher E′ that
is secure in the ordinary sense. Your new cipher should be deﬁned over ( K′,M′,C′), where
K′= Kand M′= M.
(b) Give an example of a cipher that is semantically secure for random messages but that is not
semantically secure in the ordinary sense.
3.2 (Encryption chain). Let E= (E,D) be a cipher deﬁned over ( K,M,C) where K= M. Let
E′ = (E′,D′) be a cipher where encryption is deﬁned as E′((k1,k2),m) :=
(
E(k1,k2),E(k2,m)
)
.
Show that if Eis semantically secure then so is E′.
Hint: Use a three move hybrid argument. In particular, deﬁne four games where in each game the
adversary outputs m0,m1 ∈M, and receives back an E′ciphertext c, as:
Game 1: c←R (
E(k1,k2),E(k2,m0)
)
, Game 3: c←R (
E(k1,0),E(k2,m1)
)
,
Game 2: c←R (
E(k1,0),E(k2,m0)
)
, Game 4: c←R (
E(k1,k2),E(k2,m1)
)
.
Argue that the adversary cannot distinguish each game from the one before it. Deduce that E′ is
semantically secure.
Discussion: This encryption scheme can be used to distribute large protected ﬁles. For example, a
movie rental service can place a large encrypted movie, E(k2,m), on a content distribution network
for anyone to download. When a customer, Bob, wants to watch the movie m, he pays the rental
service, and the service sends back a short ticket E(k1,k2), where Bob knows k1. Bob can now
stream the encrypted ﬁle from the content distribution network, and decrypt it locally. Exercise 5.4
gives another application of this construction.
3.3 (Indistinguishability from a random message). This exercise develops an alternative
characterization of semantic security. Let E= (E,D) be a cipher deﬁned over ( K,M,C). Assume
that one can eﬃciently generate messages from the message space Mat random. We deﬁne an
attack game between an adversary Aand a challenger as follows. The adversary selects a message
m∈M and sends m to the challenger. The challenger then computes:
b←R {0,1}, k←R K, m0 ←m, m1 ←R M, c←R E(k,mb),
and sends the ciphertext c to A, who then computes and outputs a bit ˆb. That is, the challenger
encrypts either m or a random message, depending on b. We deﬁne A’s advantage to be |Pr[ˆb =
b] −1/2|, and we say the Eis real/random semantically secure if this advantage is negligible for all
eﬃcient adversaries.
Show that E is real/random semantically secure if and only if it is semantically secure in the
ordinary sense.
3.4 (Pseudo-random ciphertexts). In this exercise, we develop a notion of security for a cipher,
called psuedo-random ciphertext security , which intuitively says that no eﬃcient adversary can
distinguish an encryption of a chosen message from a random ciphertext.
90
Let E= (E,D) be deﬁned over ( K,M,C). Assume that one can eﬃciently generate ciphertexts
from the ciphertext space Cat random. We deﬁne an attack game between an adversary Aand a
challenger as follows. The adversary selects a message m∈M and sends mto the challenger. The
challenger then computes:
b←R {0,1}, k←R K, c0 ←R E(k,m), c1 ←R C, c ←cb
and sends the ciphertext cto A, who then computes and outputs a bit ˆb. We deﬁne A’s advantage
to be |Pr[ˆb = b] −1/2|, and we say the Eis pseudo-random ciphertext secure if this advantage is
negligible for all eﬃcient adversaries.
(a) Show that if a cipher is pseudo-random ciphertext secure, then it is semantically secure.
(b) Show that the one-time pad is pseudo-random ciphertext secure.
(c) Give an example of a cipher that is semantically secure, but not pseudo-random ciphertext
secure.
3.5 (Small seed spaces are insecure). Suppose G is a PRG deﬁned over ( S,R) where |R|≥
2|S|. Let us show that |S|must be super-poly. To do so, show that there is an adversary that
achieves advantage at least 1/2 in attacking the PRG G whose running time is linear in |S|.
3.6 (Another malleability example). Let us give another example illustrating the malleability
of stream ciphers. Suppose you are told that the stream cipher encryption of the message “attack
at dawn” is 6c73d5240a948c86981bc294814d (the plaintext letters are encoded as 8-bit ASCII and
the given ciphertext is written in hex). What would be the stream cipher encryption of the message
“attack at dusk” under the same key?
3.7 (Exercising the deﬁnition of a secure PRG). Suppose G(s) is a secure PRG that outputs
bit-strings in {0,1}n. Which of the following derived generators are secure?
(a) G1(s1 ∥s2) := G(s1) ∧G(s2) where ∧denotes bit-wise AND.
(b) G2(s1 ∥s2) := G(s1) ⊕G(s2).
(c) G3(s) := G(s) ⊕1n.
(d) G4(s) := G(s)[0 . .n−1].
(e) G5(s) := (G(s),G(s)).
(f) G6(s1 ∥s2) := (s1,G(s2)).
3.8 (The converse of Theorem 3.1). In Section 3.2, we showed how to build a stream cipher
from a PRG. In Theorem 3.1, we proved that this encryption scheme is semantically secure if the
PRG is secure. Prove the converse: the PRG is secure if this encryption scheme is semantically
secure.
3.9 (Predicting the next character). In Section 3.5, we showed that if one could eﬀectively
distinguish a random bit string from a pseudo-random bit string, then one could succeed in pre-
dicting the next bit of a pseudo-random bit string with probability signiﬁcantly greater than 1 /2
91
(where the position of the “next bit” was chosen at random). Generalize this from bit strings to
strings over the alphabet {0,...,n −1}, for all n≥2, assuming that n is poly-bounded.
Hint: First generalize the distinguisher/predictor lemma (Lemma 3.5).
3.10 (Simple statistical distance calculations).
(a) Let X and Y be independent random variables, each uniformly distributed over Zp, where p
is prime. Calculate ∆[ ( X,Y), (X,XY) ].
(b) Let X and Y be random variables, each taking values in the interval [0 ,t]. Show that |E[X] −
E[Y]|≤ t∆[X,Y].
3.11 (A converse to Example 3.2). In Example 3.2, we saw that if Y be uniformly distributed
over the set{0,...,N −1}, then the statistical betweenR := (Y mod m) and the uniform distribution
on {0,...,m −1}is less than m/N. This exercise develops a converse of sorts.
Let R be uniformly distributed over {0,...,m −1} and Q be uniformly distributed over
{0,..., ⌊N/m⌋−1}, where R and Q are independent, and set Y := mQ+R. Show that (i) 0 ≤Y <N ,
(ii) R = (Y mod m), and (iii) the statistical distance between Y and the uniform distribution on
{0,...,N −1}is less than m/N.
The following three exercises should be done together; they will be used in exercises in
the following chapters.
3.12 (Distribution ratio). This exercise develops another way of comparing two probability
distributions, which considers ratios of probabilities, rather than diﬀerences. Let X and Y be two
random variables taking values on a ﬁnite set R, and assume that Pr[ X = r] > 0 for all r ∈R.
Deﬁne
ρ[X,Y] := max
{
Pr[Y = r]/Pr[X = r] : r∈R
}
We call this the max ratio distance between X and Y. Show that for every subset R′ of R, we
have Pr[Y ∈R′] ≤ρ[X,Y] ·Pr[X ∈R′].
3.13 (A variant of Bernoulli’s inequality). The following is a useful fact that will be used
in the following exercise. Prove the following statement by induction on n: for any real numbers
x1,...,x n in the interval [0,1], we have
n∏
i=1
(1 −xi) ≥1 −
n∑
i=1
xi.
3.14 (Sampling with and without replacement: distance and ratio). Let Xbe a ﬁnite set
of size N, and let Q≤N. Deﬁne random variables X and Y, where X is uniformly distributed over
all sequences of Q elements in X, and Y is uniformly distributed over all sequences of Q distinct
elements in X. Let ∆[ X,Y] be the statistical distance between X and Y, and let ρ[X,Y] be the max
ratio distance deﬁned in Exercise 3.12. Using the previous exercise, prove the following:
(a) ∆[ X,Y] = 1 −
Q−1∏
i=0
(1 −i/N) ≤Q2
2N,
92
(b) ρ[X,Y] = 1
∏Q−1
i=0 (1 −i/N)
≤ 1
1 −Q2
2N
(assuming Q2 <2N).
Discussion: The result of part (b) has applications to the security analysis of message authenti-
cation codes. See Exercise 7.12.
3.15 (Theorem 3.2 is tight). Let us show that the bounds in the parallel composition theorem,
Theorem 3.2, are tight. Consider the following, rather silly PRG G0, which “stretches” ℓ-bit strings
to ℓ-bit strings, with ℓ even: for s∈{0,1}ℓ, we deﬁne
G0(s) :=
if s[0 . .ℓ/2 −1] = 0ℓ/2
then output 0 ℓ
else output s.
That is, if the ﬁrst ℓ/2 bits of s are zero, then G0(s) outputs the all-zero string, and otherwise,
G0(s) outputs s.
Next, deﬁne the following PRG adversary B0 that attacks G0:
When the challenger presents B0 with r ∈{0,1}ℓ, if r is of the form 0 ℓ/2 ∥t, for some
t̸= 0ℓ/2, B0 outputs 1; otherwise, B0 outputs 0.
Now, let G′
0 be the n-wise parallel composition of G0. Using B0, we construct a PRG adversary A0
that attacks G′
0:
when the challenger presents A0 with the sequence of strings ( r1,...,r n), A0 presents
each ri to B0, and outputs 1 if B0 ever outputs 1; otherwise, A0 outputs 0.
(a) Show that PRG adv[B0,G0] = 2−ℓ/2 −2−ℓ.
(b) Show that PRG adv[A0,G′
0] ≥n2−ℓ/2 −n(n+ 1)2−ℓ.
(c) Show that no adversary attacking G0 has a better advantage thanB0 (hint: make an argument
based on statistical distance).
(d) Using parts (a)–(c), argue that Theorem 3.2 cannot be substantially improved; in particular,
show that the following cannot be true:
There exists a constant c <1 such that for every PRG G, poly-bounded n, and eﬃcient
adversary A, there exists an eﬃcient adversary Bsuch that
PRGadv[A,G′] ≤cn·PRGadv[B,G],
where G′ is the n-wise parallel composition of G.
3.16 (A converse (of sorts) to Theorem 2.8). Let E= (E,D) be a semantically secure cipher
deﬁned over (K,M,C), where M= {0,1}. Let Abe an eﬃcient adversary that takes as input an
encryption c←R E(k,b) of a random bit b←R {0,1}, where k ←R K, and outputs a guess b′∈{0,1}
for b. Show that the probability that b= b′is at most 1 /2 + ϵ, where ϵ is negligible.
Hint: Use Lemma 3.5.
93
3.17 (Previous-bit prediction). Suppose that Ais an eﬀective next-bit predictor. That is,
suppose that Ais an eﬃcient adversary whose advantage in Attack Game 3.2 is non-negligible.
Show how to use Ato build an explicit, eﬀective previous-bit predictor Bthat uses Aas a black
box. Here, one deﬁnes a previous-bit prediction game that is the same as Attack Game 3.2, except
that the challenger sends r[i+ 1 . .L−1] to the adversary. Also, express B’s previous-bit prediction
advantage in terms of A’s next-bit prediction advantage.
3.18 (An insecure PRG based on linear algebra). Let Abe a ﬁxed m×nmatrix with m>n
whose entries are all binary. Consider the following PRG G: {0,1}n →{0,1}m deﬁned by
G(s) := A·s (mod 2)
where A·smod 2 denotes a matrix-vector product where all elements of the resulting vector are
reduced modulo 2. Show that this PRG is insecure no matter what matrix A is used.
3.19 (Generating an encryption key using a PRG). Let G: S→R be a secure PRG. Let
E= (E,D) be a semantically secure cipher deﬁned over ( K,M,C). Assume K= R. Construct
a new cipher E′ = (E′,D′) deﬁned over ( S,M,C), where E′(s,m) := E(G(s),m) and D′(s,c) :=
D(G(s),c). Show that E′is semantically secure.
3.20 (Nested PRG construction). Let G0 : S→R 1 and G1 : R1 →R2 be two secure PRGs.
Show that G(s) := G1(G0(s)) mapping Sto R2 is a secure PRG.
3.21 (Self-nested PRG construction). Let G be a PRG that stretches n-bit strings to 2 n-bit
strings. For s ∈{0,1}n, write G(s) = G0(s) ∥G1(s), so that G0(s) represents the ﬁrst n bits of
G(s), and G1(s) represents the last nbits of G(s). Deﬁne a new PRG G′that stretches n-bit strings
to 4n-bit strings, as follows: G′(s) := G(G0(s)) ∥G(G1(s)). Show that if G is a secure PRG, then
so is G′.
Hint: You can give a direct proof; alternatively, you can use the previous exercise together with
Theorem 3.2.
Note: This construction is a special case of a more general construction discussed in Section 4.6.
3.22 (Bad seeds). Show that a secure PRG G: {0,1}n →R can become insecure if the seed is
not uniformly random in S.
(a) Consider the PRG G′ : {0,1}n+1 →R×{ 0,1}deﬁned as G′(s0 ∥s1) = ( G(s0),s1). Show
that G′is a secure PRG assuming G is secure.
(b) Show that G′ becomes insecure if its random seed s0 ∥s1 is chosen so that its last bit is
always 0.
(c) Construct a secure PRG G′′ : {0,1}n+1 →R×{ 0,1}that becomes insecure if its seed s is
chosen so that the parity of the bits in s is always 0.
3.23 (Good intentions, bad idea). Let us show that a natural approach to strengthening a
PRG is insecure. Let m > nand let G : {0,1}n →{0,1}m be a PRG. Deﬁne a new generator
G′(s) := G(s) ⊕(0m−n ∥s) derived from G. Show that there is a secure PRG G for which G′ is
insecure.
Hint: Use the construction from part (a) of Exercise 3.22.
94
3.24 (Seed recovery attacks). Let Gbe a PRG deﬁned over (S,R) where, |S|/|R|is negligible,
and suppose Ais an adversary that given G(s) outputs s with non-negligible probability. Show
how to use Ato construct a PRG adversary Bthat has non-negligible advantage in attacking Gas
a PRG. This shows that for a secure PRG it is intractable to recover the seed from the output.
3.25 (A PRG combiner). Suppose that G1 and G2 are PRG’s deﬁned over ( S,R), where
R= {0,1}L. Deﬁne a new PRG G′ deﬁned over (S×S ,R), where G′(s1,s2) = G1(s1) ⊕G2(s2).
Show that if either G1 or G2 is secure (we may not know which one is secure), then G′is secure.
3.26 (A technical step in the proof of Lemma 3.5). This exercise develops a simple fact from
probability that is helpful in understanding the proof of Lemma 3.5. Let X and Y be independent
random variables, taking values in S and T, respectively, where Y is uniformly distributed over T.
Let f : S →{0,1}and g : S →T be functions. Show that the events f(X) = 1 and g(X) = Y are
independent, and the probability of the latter is 1 /|T|.
95
Chapter 4
Block ciphers
This chapter continues the discussion begun in the previous chapter on achieving privacy against
eavesdroppers. Here, we study another kind of cipher, called a block cipher. We also study the
related concept of a pseudo-random function.
Block ciphers are the “work horse” of practical cryptography: not only can they can be used to
build a stream cipher, but they can be used to build ciphers with stronger security properties (as
we will explore in Chapter 5), as well as many other cryptographic primitives.
4.1 Block ciphers: basic deﬁnitions and properties
Functionally, a block cipher is a deterministic cipher E = ( E,D) whose message space and
ciphertext space are the same (ﬁnite) set X. If the key space of Eis K, we say that Eis a block
cipher deﬁned over (K,X). We call an element x∈X a data block, and refer to Xas the data
block space of E.
For every ﬁxed key k ∈K, we can deﬁne the function fk := E(k,·); that is, fk : X→X sends
x ∈X to E(k,x) ∈X . The usual correctness requirement for any cipher implies that for every
ﬁxed key k, the function fk is one-to-one, and as Xis ﬁnite, fk must be onto as well. Thus, fk is
a permutation on X, and D(k,·) is the inverse permutation f−1
k .
Although syntactically a block cipher is just a special kind of cipher, the security property we
shall expect for a block cipher is actually much stronger than semantic security: for a randomly
chosen key k, the permutation E(k,·) should — for all practical purposes — “look like” a random
permutation. This is a notion that we will soon make more precise.
One very important and popular block cipher is AES (the Advanced Encryption Standard).
We will study the internal design of AES in more detail below, but for now, we just give a very
high-level description. AES keys are 128-bit strings (although longer key sizes may be used, such
as 192-bits or 256-bits). AES data blocks are 128-bit strings. See Fig. 4.1. AES was designed to be
quite eﬃcient: one evaluation of the encryption (or decryption) function takes just a few hundred
cycles on a typical computer.
The deﬁnition of security for a block cipher is formulated as a kind of “black box test.” The intu-
ition is the following: an eﬃcient adversary is given a “black box.” Inside the box is a permutation
f on X, which is generated via one of two random processes:
• f := E(k,·), for a randomly chosen key k, or
96
m
c
k
128 bits
128 bits
128 bits
AES
Figure 4.1: The block cipher AES
• f is a truly random permutation, chosen uniformly from among all permutations on X.
The adversary cannot see inside the box, but he can “probe” it with questions: he can give the
box a value x ∈X , and obtain the value y := f(x) ∈X . We allow the adversary to ask many
such questions, and we quite liberally allow him to choose the questions in any way he likes; in
particular, each question may even depend in some clever way on the answers to previous questions.
Security means that the adversary should not be able to tell which type of function is inside the
box — a randomly keyed block cipher, or a truly random permutation. Put another way, a secure
block cipher should be computationally indistinguishable from a random permutation.
To make this deﬁnition more formal, let us introduce some notation:
Perms[X]
denotes the set of all permutations on X. Note that this is a very large set:
⏐⏐Perms[X]
⏐⏐= |X|!.
For AES, with |X|= 2128, the number of permutations is about
Perms[X] ≈22135
,
while the number of permutations deﬁned by 128-bit AES keys is at most 2 128.
As usual, to deﬁne security, we introduce an attack game. Just like the attack game used
to deﬁne a PRG, this attack game comprises two separate experiments. In both experiments,
the adversary follows the same protocol; namely, it submits a sequence of queries x1,x2,... to
the challenger; the challenger responds to query xi with f(xi), where in the ﬁrst experiment,
f := E(k,·) for a randomly chosen k ∈K, while in the second experiment, f is randomly selected
from Perms[X]; throughout each experiment, the same f is used to answer all queries. When the
adversary tires of querying the challenger, it outputs a bit.
Attack Game 4.1 (block cipher). For a given block cipher (E,D), deﬁned over (K,X), and for
a given adversary A, we deﬁne two experiments, Experiment 0 and Experiment 1. For b= 0,1, we
deﬁne:
Experiment b:
97
• The challenger selects f ∈Perms[X] as follows:
if b= 0: k←R K, f ←E(k,·);
if b= 1: f ←R Perms[X].
• The adversary submits a sequence of queries to the challenger.
For i= 1,2,..., the ith query is a data block xi ∈X.
The challenger computes yi ←f(xi) ∈X, and gives yi to the adversary.
• The adversary computes and outputs a bit ˆb∈{0,1}.
For b= 0,1, let Wb be the event that Aoutputs 1 in Experiment b. We deﬁne A’s advantage
with respect to Eas
BCadv[A,E] :=
⏐⏐Pr[W0] −Pr[W1]
⏐⏐.
Finally, we say that Ais a Q-query BC adversary if Aissues at most Q queries. 2
Fig. 4.2 illustrates Attack Game 4.1.
Deﬁnition 4.1 (secure block cipher). A block cipher Eis secure if for all eﬃcient adversaries
A, the value BCadv[A,E] is negligible.
We stress that the queries made by the adversary in Attack Game 4.1 are allowed to beadaptive;
that is, the adversary need not choose all its queries in advance; rather, it is allowed to concoct
each query in some clever way that depends on the previous responses from the challenger (see
Exercise 4.6).
As discussed in Section 2.2.5, Attack Game 4.1 can be recast as a “bit guessing” game, where
instead of having two separate experiments, the challenger chooses b∈{0,1}at random, and then
runs Experiment b against the adversary A. In this game, we measure A’s bit-guessing advantage
BCadv∗[A,E] as |Pr[ˆb= b] −1/2|. The general result of Section 2.2.5 (namely, (2.11)) applies here
as well:
BCadv[A,E] = 2 ·BCadv∗[A,E]. (4.1)
4.1.1 Some implications of security
Let E= (E,D) be a block cipher deﬁned over (K,X). To exercise the deﬁnition of security a bit, we
prove a couple of simple implications. For simplicity, we assume that |X|is large (i.e., super-poly).
4.1.1.1 A secure block cipher is unpredictable
We show that if E is secure in the sense of Deﬁnition 4.1, then it must be unpredictable, which
means that every eﬃcient adversary wins the following prediction game with negligible probability.
In this game, the challenger chooses a random key k, and the adversary submits a sequence of
queries x1,...,x Q; in response to the ith query xi, the challenger responds with E(k,xi). These
queries are adaptive, in the sense that each query may depend on the previous responses. Finally,
the adversary outputs a pair of values ( xQ+1,y), where xQ+1 /∈{x1,...,x Q}. The adversary wins
the game if y= E(k,xQ+1).
To prove this implication, suppose that Eis not unpredictable, which means there is an eﬃcient
adversary Athat wins the above prediction game with non-negligible probability p. Then we can
98
Challenger
Challenger
(Experiment 0)
(Experiment 1)
A
A
k R
 K
xi 2 X
xi 2 X
yi
yi
yi  f(xi)
ˆb 2 {0, 1}
ˆb 2 {0, 1}
f R
 Perms[X]
yi   E(k, xi)
Figure 4.2: Attack Game 4.1
99
use Ato break the security of Ein the sense of Deﬁnition 4.1. To this end, we design an adversary
Bthat plays Attack Game 4.1, and plays the role of challenger to Ain the above prediction game.
Whenever Amakes a query xi, adversary Bpasses xi through to its own challenger, obtaining a
response yi, which it passes back to A. Finally, when Aoutputs (xQ+1,y), adversary Bsubmits
xQ+1 to its own challenger, obtaining yQ+1, and outputs 1 if y= yQ+1, and 0, otherwise.
On the one hand, if B’s challenger is running Experiment 0, then Boutputs 1 with probability
p. On the other hand, if B’s challenger is running Experiment 1, then Boutputs 1 with negligible
probability ϵ (since we are assuming |X|is super-poly). This implies that B’s advantage in Attack
Game 4.1 is |p−ϵ|, which is non-negligible.
4.1.1.2 Unpredictability implies security against key recovery
Next, we show that if Eis unpredictable, then it is secure against key recovery, which means that
every eﬃcient adversary wins the following key-recovery game with negligible probability. In this
game, the adversary interacts with the challenger exactly as in the prediction game, except that at
the end, it outputs a candidate key k ∈K, and wins the game if k = k.
To prove this implication, suppose that Eis not secure against key recovery, which means that
there is an eﬃcient adversary Athat wins the key-recovery game with non-negligible probability p.
Then we can use Ato build an eﬃcient adversary Bthat wins the prediction game with probability
at least p. Adversary Bsimply runs A’s attack, and when Aoutputs k , adversary Bchooses an
arbitrary xQ+1 /∈{x1,...,x Q}, computes y←E(k ,xQ+1), and outputs ( xQ+1,y).
It is easy to see that if Awins the key-recovery game, then Bwins the prediction game.
4.1.1.3 Key space size and exhaustive-search attacks
Combining the above two implications, we conclude that if Eis a secure block cipher, then it must
be secure against key recovery. Moreover, if Eis secure against key recovery, it must be the case
that |K|is large.
One way to see this is as follows. An adversary can always win the key-recovery game with
probability 1/|K|by simply choosing k from Kat random. If |K|is not super-poly, then 1 /|K|
is non-negligible. Hence, when |K|is not super-poly this simple key guessing adversary wins the
key-recovery game with non-negligible probability.
We can trade success probability for running time using a diﬀerent attack, called an exhaustive-
search attack. In this attack, our adversary makes a few, arbitrary queries x1,...,x Q in the key-
recovery game, obtaining responses y1,...,y Q. One can argue — heuristically, at least, assuming
that |X|≥|K| and |X|is super-poly — that for fairly small values of Q (Q= 2, in fact), with all
but negligible probability, only one key k satisﬁes
yi = E(k,xi) for i= 1,...,Q. (4.2)
So our adversary simply tries all possible keys to ﬁnd one that satisﬁes (4.2). If there is only
one such key, then the key that our adversary ﬁnds will be the key chosen by the challenger, and
the adversary will win the game. Thus, our adversary wins the key-recovery game with all but
negligible probability; however, its running time is linear in |K|.
This time/advantage trade-oﬀ can be easily generalized. Indeed, consider an adversary that
chooses t keys at random, testing if each such key satisﬁes (4.2). The running time of such an
adversary is linear in t, and it wins the key-recovery game with probability ≈t/|K|.
100
We describe a few real-world exhaustive search attacks in Section 4.2.2. We present a de-
tailed treatment of exhaustive search in Section 4.7.2 where, in particular, we justify the heuristic
assumption used above that with high probability there is at most one key satisfying (4.2).
So it is clear that if a block cipher has any chance of being secure, it must have a large key
space, simply to avoid a key-recovery attack.
4.1.2 Eﬃcient implementation of random permutations
Note that the challenger’s protocol in Experiment 1 of Attack Game 4.1 is not very eﬃcient: he is
supposed to choose a very large random object. Indeed, just writing down an element of Perms[ X]
would require about |X|log2|X|bits. For AES, with |X|= 2128, this means about 10 40 bits!
While this is not a problem from a purely deﬁnitional point of view, for both aesthetic and
technical reasons, it would be nice to have a more eﬃcient implementation. We can do this by
using a “lazy” implementation of f. That is, the challenger represents the random permutation
f by keeping track of input/output pairs ( xi,yi). When the challenger receives the ith query xi,
he tests whether xi = xj for some j < i; if so, he sets yi ←yj (this ensures that the challenger
implements a function); otherwise, he chooses yi at random from the set X\{ y1,...,y i−1}(this
ensures that the function is a permutation); ﬁnally, he sends yi to the adversary. We can write the
logic of this implementation of the challenger as follows:
upon receiving the ith query xi ∈X from Ado:
if xi = xj for some j <i
then yi ←yj
else yi ←R X\{ y1,...,y i−1}
send yi to A.
To make this implementation as fast as possible, one would implement the test “if xi = xj for some
j < i” using an appropriate dictionary data structure (hash tables, digital search tries, balanced
trees, etc.). Assuming random elements of X can be generated eﬃciently, one way to implement
the step “yi ←R X\{ y1,...,y i−1}” is as follows:
repeat y←R X until y̸∈{y1,...,y i−1}
yi ←y,
again, using appropriate dictionary data structure for the tests “ y ̸∈{y1,...,y i−1}.” When i <
|X|/2 the loop will run for only two iterations in expectation.
One way to visualize this implementation is that the challenger in Experiment 1 is a “black box,”
but inside the box is a little faithful gnome whose job it is to maintain the table of input/output
pairs which represents a random permutation f. See Fig. 4.3.
4.1.3 Strongly secure block ciphers
Note that in Attack Game 4.1, the decryption algorithm D was never used. One can in fact deﬁne
a stronger notion of security by deﬁning an attack game in which the adversary is allowed to make
two types of queries to the challenger:
forward queries: the adversary sends a value xi ∈X to the challenger, who sends yi := f(xi) to
the adversary;
101
x f (x)
00101 10101
11111 01110
10111 01011
00011 10001
x
f (x)
Figure 4.3: A faithful gnome implementing random permutation f
inverse queries: the adversary sends a value yi ∈X to the challenger, who sends xi := f−1(yi)
to the adversary (in Experiment 0 in the attack game, this is done using algorithm D).
One then deﬁnes a corresponding advantage for this attack game. A block cipher is then called
strongly secure if for all eﬃcient adversaries, this advantage is negligible. We leave it to the
reader to work out the details of this deﬁnition (see Exercise 4.9). We will not make use of this
notion in this text, other than an example application in a later chapter (Exercise 9.12).
4.1.4 Using a block cipher directly for encryption
Since a block cipher is a special kind of cipher, we can of course consider using it directly for
encryption. The question is: is a secure block cipher also semantically secure?
The answer to this question is “yes,” provided the message space is equal to the data block
space. This will be implied by Theorem 4.1 below. However, data blocks for practical block ciphers
are very short: as we mentioned, data blocks for AES are just 128-bits long. If we want to encrypt
longer messages, a natural idea would be to break up a long message into a sequence of data blocks,
and encrypt each data block separately. This use of a block cipher to encrypt long messages is called
electronic codebook mode , or ECB mode for short.
More precisely, suppose E= (E,D) is a block cipher deﬁned over (K,X). For any poly-bounded
ℓ≥1, we can deﬁne a cipher E′= (E′,D′), deﬁned over (K,X≤ℓ,X≤ℓ), as follows.
• For k∈K and m∈X≤ℓ, with v:= |m|, we deﬁne
E′(k,m) :=
(
E(k,m[0]),...,E (k,m[v−1])
)
.
• For k∈K and c∈X≤ℓ, with v:= |c|, we deﬁne
D′(k,c) :=
(
D(k,c[0]),...,D (k,c[v−1])
)
.
102
E(k, ·)
E(k, ·)
E(k, ·)
· · ·
m[0]
m[1]
m[v − 1]
c[v − 1]
c[0]
c[1]
· · ·
m[0]
m[1]
m[v − 1]
c[v − 1]
c[0]
c[1]
D(k, ·)
D(k, ·)
D(k, ·)
(a) encryption
(b) decryption
Figure 4.4: Encryption and decryption for ECB mode
Fig. 4.4 illustrates encryption and decryption. We callE′the ℓ-wise ECB cipher derived fromE.
The ECB cipher is very closely related to the substitution cipher discussed in Examples 2.3
and 2.6. The main diﬀerence is that instead of choosing a permutation at random from among all
possible permutations on X, we choose one from the much smaller set of permutations{E(k,·) : k∈
K}. A less important diﬀerence is that in Example 2.3, we deﬁned our substitution cipher to have
a ﬁxed length, rather than a variable length message space (this was really just an arbitrary choice
— we could have deﬁned the substitution cipher to have a variable length message space). Another
diﬀerence is that in Example 2.3, we suggested an alphabet of size 27, while if we use a block cipher
like AES with a 128-bit block size, the “alphabet” is much larger — it has 2 128 elements. Despite
these diﬀerences, some of the vulnerabilities discussed in Example 2.6 apply here as well. For
example, an adversary can easily distinguish an encryption of two messages m0,m1 ∈X 2, where
m0 consists of two equal blocks (i.e., m0[0] = m0[1]) and m1 consists of two unequal blocks (i.e.,
103
usin g AE S
(b ) p lain te xt encryp ted in E C B m o de
(a) pl ain text
Figure 4.5: Encrypting in ECB mode
m1[0] ̸= m1[1]). For this reason alone, the ECB cipher does not satisfy our deﬁnition of semantic
security, and its use as an encryption scheme is strongly discouraged.
This ability to easily tell which plaintext blocks are the same is graphically illustrated in Fig. 4.5
(due to B. Preneel). Here, visual data is encrypted in ECB mode, with each data block encoding
some small patch of pixels in the original data. Since identical patches of pixels get mapped to
identical blocks of ciphertext, some patterns in the original picture are visible in the ciphertext.
Note, however, that some of the vulnerabilities discussed in Example 2.6 do not apply directly
here. Suppose we are encrypting ASCII text. If the block size of the cipher is 128-bits, then each
character of text will be typically encoded as a byte, with 16 characters packed into a data block.
Therefore, an adversary will not be able to trivially locate positions where individual characters
are repeated, as was the case in Example 2.6.
We close this section with a proof that ECB mode is in fact secure if the message space is
restricted to sequences on distinct data blocks. This includes as a special case the encryption of
single-block messages. It is also possible to encode longer messages as sequences of distinct data
blocks. For example, suppose we are using AES, which has 128-bit data blocks. Then we could
allocate, say, 32 bits out of each block as a counter, and use the remaining 96 bits for bits of the
message. With such a strategy, we can encode any message of up to 2 32 ·96 bits as a sequence of
distinct data blocks. Of course, this strategy has the disadvantage that ciphertexts are 33% longer
than plaintexts.
Theorem 4.1. Let E= (E,D) be a block cipher. Let ℓ ≥1 be any poly-bounded value, and let
E′= (E′,D′) be the ℓ-wise ECB cipher derived from E, but with the message space restricted to all
sequences of at most ℓ distinct data blocks. If Eis a secure block cipher, then E′ is a semantically
104
secure cipher.
In particular, for every SS adversary Athat plays Attack Game 2.1 with respect to E′, there
exists a BC adversary Bthat plays Attack Game 4.1 with respect to E, where Bis an elementary
wrapper around A, such that
SSadv[A,E′] = 2 ·BCadv[B,E]. (4.3)
Proof idea. The basic idea is that if an adversary is given an encryption of a message, which is a
sequence of distinct data blocks, then what he sees is eﬀectively just a sequence of random data
blocks (sampled without replacement). 2
Proof. If E is deﬁned over ( K,X), let X≤ℓ
∗ denote the set of all sequences of at most ℓ distinct
elements of X.
Let Abe an eﬃcient adversary that attacks E′as in Attack Game 2.1. Our goal is to show that
SSadv[A,E′] is negligible, assuming that Eis a secure block cipher. It is more convenient to work
with the bit-guessing version of the SS attack game. We prove:
SSadv∗[A,E′] = BCadv[B,E] (4.4)
for some eﬃcient adversary B. Then (4.3) follows from Theorem 2.10.
So consider the adversary A’s attack of E′ in the bit-guessing version of Attack Game 2.1. In
this game, Apresents the challenger with two messages m0,m1 of the same length; the challenger
then chooses a random key k and a random bit b, and encrypts mb under k, giving the resulting
ciphertext c to A; ﬁnally, Aoutputs a bit ˆb. The adversary Awins the game if ˆb= b.
The logic of the challenger in this game may be written as follows:
upon receiving m0,m1 ∈X≤ℓ
∗ , with v:= |m0|= |m1|, do:
b←R {0,1}
k←R K
c←(E(k,mb[0]),...,E (k,mb[v−1]))
send c to A.
Let us call this Game 0. We will deﬁne two more games: Game 1 and Game 2. For j = 0,1,2,
we deﬁne Wj to be the event that ˆb= b in Game j. By deﬁnition, we have
SSadv∗[A,E′] = |Pr[W0] −1/2|. (4.5)
Game 1. This is the same as Game 0, except the challenger uses a random f ∈Perms[X] in place
of E(k,·). Our challenger now looks like this:
upon receiving m0,m1 ∈X≤ℓ
∗ , with v:= |m0|= |m1|, do:
b←R {0,1}
f ←R Perms[X]
c←(f(mb[0]),...,f (mb[v−1]))
send c to A.
Intuitively, the fact that Eis a secure block cipher implies that the adversary should not notice
the switch. To prove this rigorously, we show how to build a BC adversary Bthat is an elementary
wrapper around A, such that
|Pr[W0] −Pr[W1]|= BCadv[B,E]. (4.6)
105
The design of Bfollows directly from the logic of Games 0 and 1. Adversary Bplays Attack
Game 4.1 with respect to E, and works as follows:
Let f be the function chosen by B’s BC challenger in Attack Game 4.1. We let Bplay
the role of challenger to A, as follows:
upon receiving m0,m1 ∈X≤ℓ
∗ from A, with v:= |m0|= |m1|, do:
b←R {0,1}
c←
(
f(mb[0]),...,f (mb[v−1])
)
send c to A.
Note that Bcomputes the values f(mb[0]),...,f (mb[v−1]) by querying its own BC
challenger. Finally, when Aoutputs a bit ˆb, Boutputs the bit δ(ˆb,b), as deﬁned in
(3.7).
It should be clear that when Bis in Experiment 0 of its attack game, it outputs 1 with probability
Pr[W0], while when Bis in Experiment 1 of its attack game, it outputs 1 with probability Pr[ W1].
The equation (4.6) now follows.
Game 2. We now rewrite the challenger in Game 1 so that it uses the “faithful gnome” imple-
mentation of a random permutation, discussed in Section 4.1.2. Each of the messages m0 and m1
is required to consist of distinct data blocks (our challenger does not have to verify this), and so
our gnome’s job is quite easy: it does not even have to look at the input data blocks, as these are
guaranteed to be distinct; however, it still has to ensure that the output blocks it generates are
distinct.
We can express the logic of our challenger as follows:
y0 ←R X, y1 ←R X\{ y0}, . . . ,yℓ−1 ←R X\{ y0,...,y ℓ−2}
upon receiving m0,m1 ∈X≤ℓ
∗ , with v:= |m0|= |m1|, do:
b←R {0,1}
c←(y0,...,y v−1)
send c to A.
Since our gnome is faithful, we have
Pr[W1] = Pr[W2]. (4.7)
Moreover, we claim that
Pr[W2] = 1/2. (4.8)
This follows from the fact that in Game 2, the adversary’s output ˆbis a function of its own random
choices, together with y0,...,y ℓ−1; since these values are (by deﬁnition) independent of b, it follows
that ˆb and b are independent. The equation (4.8) now follows.
Combining (4.5), (4.6), (4.7), and (4.8), yields (4.4), which completes the proof. 2
4.1.5 Mathematical details
As usual, we address a few mathematical details that were glossed over above.
Since a block cipher is just a special kind of cipher, there is really nothing to say about the
deﬁnition of a block cipher that was not already said in Section 2.3. As usual, Deﬁnition 4.1 needs
106
k
k1 k2 k3 ··· kn
key expansion
x yˆE ˆE ˆE ˆE
round 1 round 2 round 3 round n
Figure 4.6: Encryption in a real-world block cipher
to be properly interpreted. First, in Attack Game 4.1, it is to be understood that for each value of
the security parameter λ, we get a diﬀerent probability space, determined by the random choices of
the challenger and the random choices of the adversary. Second, the challenger generates a system
parameter Λ, and sends this to the adversary at the very start of the game. Third, the advantage
BCadv[A,E] is a function of the security parameter λ, and security means that this function is a
negligible function.
4.2 Constructing block ciphers in practice
Block ciphers are a basic primitive in cryptography from which many other systems are built.
Virtually all block ciphers used in practice use the same basic framework called the iterated
cipher paradigm. To construct an iterated block cipher the designer makes two choices:
• First, he picks a simple block cipher ˆE:= ( ˆE, ˆD) that is clearly insecure on its own. We call ˆE
the round cipher.
• Second, he picks a simple (not necessarily secure) PRG G that is used to expand the key k
into d keys k1,...,k d for ˆE. We call G the key expansion function.
Once these two choices are made, the iterated block cipherEis completely speciﬁed. The encryption
algorithm E(k,x) works as follows (see Fig. 4.6):
107
key size block size number of performance 1
(bits) (bits) rounds (MB/sec)
DES 56 64 16 80
3DES 168 64 48 30
AES-128 128 128 10 163
AES-256 256 128 14 115
Table 4.1: Sample block ciphers
Algorithm E(k,x):
• step 1. key expansion: use the key expansion function G to
stretch the key k of Eto d keys of ˆE:
(k1,...,k d) ←G(k)
• step 2. iteration: for i= 1,...,d apply ˆE(ki,·), namely:
y← ˆE(kd, ˆE(kd−1,..., ˆE(k2, ˆE(k1, x)) ... ))
Each application of ˆE is called a round and the total number of rounds is d. The keys k1,...,k d
are called round keys. The decryption algorithm D(k,y) is identical except that the round keys
are applied in reverse order. D(k,y) is deﬁned as:
x← ˆD(k1, ˆD(k2,..., ˆD(kd−1, ˆD(kd, y)) ... ))
Table 4.1 lists a few common block ciphers and their parameters. We describe DES and AES in
the next section.
Does iteration give a secure block cipher? Nobody knows. However, heuristic evidence
suggests that security of a block cipher comes from iterating a simple cipher many times. Not all
round ciphers will work. For example, iterating a linear function
ˆE(k,x) := k·xmod q
will never result in a secure block cipher since the iterate of ˆE is just another linear function. There
is currently no way to classify which round ciphers will eventually result in a secure block cipher.
Moreover, for a candidate round cipher ˆE there is no rigorous methodology to gauge how many
times it needs to be iterated before it becomes a secure block cipher. All we know is that certain
functions, like linear functions, never lead to secure block ciphers, while simple non-linear functions
appear to give a secure block cipher after a few iterations.
The challenge for the cryptographer is to come up with a fast round cipher that converges to a
secure block cipher within a few rounds. Looking at Table 4.1 one is impressed that AES-128 uses
a simple round cipher and yet seems to produce a secure block cipher after only ten rounds.
1OpenSSL 1.0.1e on Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz (Haswell).
108
A word of caution. While this section explains the inner workings of several block ciphers, it
does not teach how to design new block ciphers. In fact, one of the main take-away messages from
this section is that readers should not design block ciphers on their own, but instead always use
the standard ciphers described here. Block-cipher design is non-trivial and many years of analysis
are needed before one gains conﬁdence in a speciﬁc proposal. Furthermore, readers should not even
implement block ciphers on their own since implementations of block-ciphers tend to be vulnerable
to timing and power attacks, as discussed in Section 4.3.2. It is much safer to use one of the standard
implementations freely available in crypto libraries such as OpenSSL. These implementations have
gone through considerable analysis over the years and have been hardened to resist attack.
4.2.1 Case study: DES
The Data Encryption Standard (DES) was developed at IBM in response to a solicitation for
proposals from the National Bureau of Standards (now the National Institute of Standards). It
was published in the Federal Register in 1975 and was adopted as a standard for “unclassiﬁed”
applications in 1977. The DES algorithm single-handedly jump started the ﬁeld of cryptanalysis;
everyone wanted to break it. Since inception, DES has undergone considerable analysis that led to
the development of many new tools for analyzing block ciphers.
The precursor to DES is an earlier IBM block cipher called Lucifer. Certain variants of Lucifer
operated on 128-bit blocks using 128-bit keys. The National Bureau of Standards, however, asked
for a block cipher that used shorter blocks (64 bits) and shorter keys (56 bits). In response, the IBM
team designed a block cipher that met these requirements and eventually became DES. Setting the
DES key size to 56 bits was widely criticized and lead to speculation that DES was deliberately
made weak due to pressure from US intelligence agencies. In the coming chapters, we will see that
reducing the block size to 64 bits also creates problems.
Due to its short key size, the DES algorithm is now considered insecure and should not be
used. However, a strengthened version of DES called Triple-DES (3DES) was reaﬃrmed as a US
standard in 1998. The National Institute of Standards, NIST, has approved Triple-DES through
the year 2030 for government use. In 2002 DES was superseded by a new and more eﬃcient block
cipher standard called AES that uses 128-bit (or longer) keys, and operates on 128-bit blocks.
4.2.1.1 The DES algorithm
The DES algorithm consists of 16 iterations of a simple round cipher. To describe DES it suﬃces
to describe the DES round cipher and the DES key expansion function. We describe each in turn.
The Feistel permutation. One of the key innovations in DES, invented by Horst Feistel at
IBM, builds a permutation from an arbitrary function. Let f : X→X be a function. We construct
a permutations π: X2 →X2 as follows (Fig. 4.7):
π(x,y) :=
(
y, x⊕f(y)
)
To show that π is one-to-one we construct its inverse, which is given by:
π−1(u,v) =
(
v⊕f(u), u
)
The function π is called a Feistel permutation and is used to build the DES round cipher.
The composition of n Feistel permutations is called an n-round Feistel network. Block ciphers
109
x y
u v
⨁ f
π(x,y)
u v
x y
⨁f
π−1(u,v)
Figure 4.7: The Feistel permutation
designed as a Feistel network are called Feistel ciphers. For DES, the function f takes 32-bit
inputs and the resulting permutation π operates on 64-bit blocks.
Note that the Feistel inverse functionπ−1 is almost identical toπ. As a result the same hardware
can be used for evaluating both π and π−1. This in turn means that the encryption and decryption
circuits can use the same hardware.
The DES round function F(k,x). The DES encryption algorithm is a 16-round Feistel network
where each round uses a diﬀerent function f : X→X . In round number ithe function f is deﬁned
as
f(x) := F(ki,x)
where ki is a 48-bit key for round number i and F is a ﬁxed function called the DES round
function. The function F is the centerpiece of the DES algorithm and is shown in Fig. 4.8. F
uses several auxiliary functions E,P , and S1,...,S 8 deﬁned as follows:
• The function E expands a 32-bit input to a 48-bit output by rearranging and replicating the
input bits. For example, E maps input bit number 1 to output bits 2 and 48; it maps input
bit 2 to output bit number 3, and so on.
• The function P, called the mixing permutation, maps a 32-bit input to a 32-bit output
by rearranging the bits of the input. For example, P maps input bit number 1 to output bit
number 9; input bit number 2 to output number 15, and so on.
• At the heart of the DES algorithm are the functions S1,...,S 8 called S-boxes. Each S-box
Si maps a 6-bit input to a 4-bit output by a lookup table. The DES standard lists these 8
look-up tables, where each table contains 64 entries.
Given these functions, the DES round function F(k,x) works as follows:
110
32-bit x 48-bit k
E
48 bits
⨁
S1 S2 S3 S4 S5 S6 S7 S8
32 bits
P
output
6
4
6
4
6
4
6
4
6
4
6
4
6
4
6
4
Figure 4.8: The DES round function F(k,x)
111
input: k∈{0,1}48 and x∈{0,1}32
output: y∈{0,1}32
F(k,x):
t←E(x) ⊕k ∈{0,1}48
separate t into 8 groups of 6-bits each: t:= t1 ∥···∥ t8
for i= 1 to 8 : si ←Si(ti)
s←s1 ∥···∥ s8 ∈{0,1}32
y←P(s) ∈{0,1}32
output y
Except for the S-boxes, the DES round cipher is made up entirely of XORs and bit permutations.
The eight S-boxes are the only components that introduce non-linearity into the design. IBM
published the criteria used to design the S-boxes in 1994 [44], after the discovery of a powerful
attack technique called “diﬀerential cryptanalysis” in the open literature. This IBM report makes
it clear that the designers of DES knew in 1973 of attack techniques that would only become known
in the open literature many years later. They designed DES to resist these attacks. The reason for
keeping the S-box design criteria secret is explained in the following quote [44]:
The design [of DES] took advantage of knowledge of certain cryptanalytic techniques,
most prominently the technique of “diﬀerential cryptanalysis,” which were not known in
the published literature. After discussions with the NSA, it was decided that disclosure
of the design considerations would reveal the technique of diﬀerential cryptanalysis, a
powerful technique that can be used against many ciphers. This in turn would weaken
the competitive advantage that the United States enjoyed over other countries in the
ﬁeld of cryptography.
Once diﬀerential cryptanalysis became public, there was no longer any reason to keep the design
of DES secret. Due to the importance of the S-boxes we list a few of the criteria that went into
their design, as explained in [44].
1. The size of the look-up tables, mapping 6-bits to 4-bits, was the largest that could be accom-
modated on a single chip using 1974 technology.
2. No output bit of an S-box should be close to a linear function of the input bits. That is, if
we select any output bit and any subset of the 6 input bits, then the fraction of inputs for
which this output bit equals the XOR of these input bits should be close to 1 /2.
3. If we ﬁx the leftmost and rightmost bits of the input to an S-box then the resulting 4-bit to
4-bit function is one-to-one. In particular, this implies that each S-box is a 4-to-1 map.
4. Changing one bit of the input to an S-box changes at least two bits of the output.
5. For each ∆ ∈{0,1}6, among the 64 pairs x,y ∈{0,1}6 such that x⊕y = ∆, the quantity
Si(x) ⊕Si(y) must not attain a single value more than eight times.
These criteria were designed to make DES as strong as possible, given the 56-bit key-size constraints.
It is now known that if the S-boxes were simply chosen at random, then with high probability the
resulting DES cipher would be insecure. In particular, the secret key could be recovered after only
several million queries to the challenger.
112
IP FP
64 bits
64 bits16 round Feistel network
56 bit key
k1 k2 k3 k16···
Figure 4.9: The complete DES circuit
Beyond the S-boxes, the mixing permutation P also plays an important role. It ensures that
the S-boxes do not always operate on the same group of 6 bits. Again, [44] lists a number of criteria
used to choose the permutation P. If the permutation P was simply chosen at random then DES
would be far less secure.
The key expansion function. The DES key expansion function G takes as input the 56-bit
key k and outputs 16 keys k1,...,k 16, each 48-bits long. Each key ki consists of 48 bits chosen
from the 56-bit key, with each ki using a diﬀerent subset of bits from k.
The DES algorithm. The complete DES algorithm is shown in Fig. 4.9. It consists of 16
iterations of the DES round cipher plus initial and ﬁnal permutations called IP and FP. These
permutations simply rearrange the 64 incoming and outgoing bits. The permutation FP is the
inverse of IP.
IP and FP have no cryptographic signiﬁcance and were included for unknown reasons. Since bit
permutations are slow in software, but fast in hardware, one theory is that IP and FP are intended
to deliberately slow down software implementations of DES.
4.2.2 Exhaustive search on DES: the DES challenges
Recall that an exhaustive search attack on a block cipher ( E,D) (Section 4.1.1.2) refers to the
following attack: the adversary is given a small number of plaintext blocks x1,...,x Q ∈X and
their encryption y1,...,y Q using a block cipher key k in K. The adversary ﬁnds k by trying all
possible keys k ∈ Kuntil it ﬁnds a key that maps all the given plaintext blocks to the given
ciphertext blocks. If enough ciphertext blocks are given, then k is the only such key, and it will be
found by the adversary.
For block ciphers like DES and AES-128 three blocks are enough to ensure that with high
probability there is a unique key mapping the given plaintext blocks to the given ciphertext blocks.
We will see why in Section 4.7.2 where we discuss ideal ciphers and their properties. For now it
suﬃces to know that given three plaintext/ciphertext blocks an attacker can use exhaustive search
to ﬁnd the secret key k.
In 1974, when DES was designed, an exhaustive search attack on a key space of size 2 56 was
believed to be infeasible. With improvements in computer hardware it was shown that a 56-bit key
is woefully inadequate.
113
To prove that exhaustive search on DES is feasible, RSA data security set up a sequence of
challenges, called the DES challenges. The rules were simple: on a pre-announced date RSA data
security posted three input/output pairs for DES. The ﬁrst group to ﬁnd the corresponding key
wins ten thousand US dollars. To make the challenge more entertaining, the challenge consisted
of nDES outputs y1,y2,...,y n where the ﬁrst three outputs, y1,y2,y3, were the result of applying
DES to the 24-byte plaintext message:
The unknown message is:
x1 x2 x3
which consists of three DES blocks: each block is 8 bytes which is 64 bits, a single DES block. The
goal was to ﬁnd a DES key that maps xi to yi for all i = 1,2,3 and then use this key to decrypt
the secret message encoded in y4 ...y n.
The ﬁrst challenge was posted in January 1997. It was solved by the deschall project in 96
days. The team used a distributed Internet search with the help of 78,000 volunteers who con-
tributed idle cycles on their machines. The person whose machine found the secret-key received
40% of the prize money. Once decrypted, the secret message encoded in y4 ...y n was “Strong
cryptography makes the world a safer place.”
A second challenge, posted in January 1998, was solved by the distributed.net project in only
41 days by conducting a similar Internet search, but on a larger scale.
In early 1998, the Electronic Frontiers Foundation (EFF) contracted Paul Kocher to construct
a dedicated machine to do DES exhaustive key search. The machine, called DeepCrack, cost
250,000 US dollars and contained about 1900 dedicated DES chips housed in six cabinets. The
chips worked in parallel, each searching through an assigned segment of the key space. When RSA
data security posted the next challenge in July 1998, DeepCrack solved it in 56 hours and easily
won the ten thousand dollar prize: not quite enough to cover the cost of the machine, but more
than enough to make an important point about DES.
The ﬁnal challenge was posted in January 1999. It was solved within 22 hours using a combined
DeepCrack and distributed.net eﬀort. This put the ﬁnal nail in DES’s coﬃn showing that a 56-bit
secret key can be recovered in just a few hours.
To complete the story, in 2007 the copacobana team built a cluster of 120 oﬀ the shelf FPGA
boards at a total cost of about ten thousand US dollars. The cluster can search through the entire
256 DES key space in about 12.8 days [81].
The conclusion from all this work is that a 56-bit key is way too short. The minimum safe key
size these days is 128 bits.
4.2.2.1 Is AES-128 vulnerable to exhaustive search?
Let us extrapolate the DES results to AES. While these estimates are inherently imprecise, they
give some indication as to the complexity of exhaustive search on AES. The minimum AES key
space size is 2 128. If scanning a space of size 2 56 takes 22 hours then scanning a space of size 2 128
will take time:
(22 hours) ×2128−56 ≈1.18 ·1020 years.
Even allowing for a billion fold improvement in computing speed and computing resources and
accounting for the fact that evaluating AES is faster than evaluating DES, the required time far
exceeds our capabilities. It is fair to conclude that a brute-force exhaustive search attack on AES
114
will never be practical. However, more sophisticated brute-force attacks on AES-128 exploiting
time-space tradeoﬀs may come within reach, as discussed in Section 18.7.
4.2.3 Strengthening ciphers against exhaustive search: the 3Econstruction
The DES cipher has proved to be remarkably resilient to sophisticated attacks. Despite many years
of analysis the most practical attack on DES is a brute force exhaustive search over the entire key
space. Unfortunately, the 56-bit key space is too small.
A natural question is whether we can strengthen the cipher against exhaustive search without
changing its inner structure. The simplest solution is to iterate the cipher several time using
independent keys.
Let E= (E,D) be a block cipher deﬁned over (K,X). We deﬁne the block cipher 3E= (E3,D3)
as
E3( (k1,k2,k3), x) := E
(
k3, E(k2, E(k1,x))
)
The 3Eblock cipher takes keys in K3. For DES the 3 Eblock cipher, called Triple-DES, uses keys
whose length is 3 ×56 = 168 bits.
Security. To analyze the security of 3 Ewe will need a framework called the ideal cipher model
which we present at the end of this chapter. We analyze the security of 3 Ein that section.
The Triple-DES standard. NIST approved Triple-DES for government use through the
year 2030. Strictly speaking, the NIST version of Triple-DES is deﬁned as
E3( (k1,k2,k3), x) := E
(
k3, D(k2, E(k1,x))
)
.
The reason for this is that setting k1 = k2 = k3 reduces the NIST Triple-DES to ordinary DES
and hence Triple-DES hardware can be used to implement single DES. This will not aﬀect our
discussion of security of Triple-DES. Another variant of Triple-DES is discussed in Exercise 4.5.
4.2.3.1 The 2Econstruction is insecure
While Triple-DES is not vulnerable to exhaustive search, its performance is three times slower than
single DES, as shown in Table 4.1.
Why not use Double-DES? Its key size is 2 ×56 = 112 bits, which is already suﬃcient to defeat
exhaustive search. Its performance is much better than Triple-DES.
Unfortunately, Double-DES is no more secure than single DES. More generally, let E= (E,D)
be a block cipher with key space K. We show that the 2 E= (E2,D2) construction, deﬁned as
E2( (k1,k2), x) := E
(
k2, E(k1,x)
)
is no more secure than E. The attack strategy is called meet in the middle .
We are given Q plaintext blocks x1,...,x Q and their 2Eencryptions yi = E2
(
(k1,k2), xi
)
for
i= 1,...,Q . We show how to recover the secret key (k1,k2) in time proportional to|K|, even though
the key space has size |K|2. As with exhaustive search, a small number of plaintext/ciphertext pairs
is suﬃcient to ensure that there is a unique key ( k1,k2) with high probability. Ten pairs are more
than enough to ensure uniqueness for block ciphers like Double-DES.
115
E(k 1,·) E(k 2,·)¯x ¯y
0 E(0,¯x)
1 E(1,¯x)
2 E(2,¯x)
... ...
step 1:
build table of all
E(k 1,¯x)
step 2:
for every k 2 in K
lookup D(k 2,¯y) in table
Figure 4.10: Meet in the middle attack on 2 E
Theorem 4.2. Let E= (E,D) be a block cipher deﬁned over (K,X). There is an algorithm AEX
that takes as input Q plaintext/ciphertext pairs (xi,yi) ∈X 2 for i = 1,...,Q and outputs a key
pair (k1,k2) ∈K2 such that
yi = E2
(
(k1,k2), xi
)
for all i= 1,...,Q. (4.9)
Its running time is dominated by a total of 2Q·|K| evaluations of algorithms E and D.
Proof. Let ¯x := (x1,...,x Q) and ¯y := (y1,...,y Q). We can capture the Q relations in (4.9) by
writing
¯y= E2
(
(k1,k2),¯x
)
= E(k2, E(k1,¯x)).
This is equivalent to
D(k2,¯y) = E(k1,¯x). (4.10)
To ﬁnd a pair (k1,k2) satisfying (4.10) the algorithm AEX does the following:
step 1: construct a table T containing all pairs
(
k 1, E(k 1,¯x)
)
for all k 1 ∈K
step 2: for all k 2 ∈K do:
¯z←D(k 2,¯y)
table lookup: if T contains a pair (·, ¯z) then
let (k 1,¯z) be that pair, output ( k 1,k 2) and halt
This meet in the middle attack is depicted in Fig. 4.10. By construction, the pair ( k 1,k 2) output
by the algorithm must satisfy D(k 2,¯y) = E(k 1,¯x), as required.
Step 1 requires Q·|K| evaluations of E. Step 2 similarly requires Q·|K| evaluations of D.
Therefore, the total number of evaluation of E and Dis 2Q·|K|. We assume that the time to insert
and look-up elements in the data structure holding the table T is less than the time to evaluate
algorithms E and D. 2
As discussed above, for relatively small values of Q, with overwhelming probability there will
be only one key pair satisfying (4.9), and this will be the output of Algorithm AEX in Theorem 4.2.
The running time of algorithm Ain Theorem 4.2 is about the same as the time to do exhaustive
search on E, suggesting that 2 Edoes not strengthen Eagainst exhaustive search. The theorem,
116
however, only considers the running time of A. Notice that Amust keep a large table in memory
which can be diﬃcult. To attack Double-DES, Awould need to store a table of size 2 56 where
each table entry contains a DES key and a short ciphertext. Overall this amounts to at least 2 60
bytes, which is about a million Terrabytes. While not impossible, obtaining suﬃcient storage can
be diﬃcult. Alternatively an attacker can trade-oﬀ storage space for running time — it is easy to
modify Aso that at any given time it only stores an ϵfraction of the table at the cost of increasing
the running time by a factor of 1 /ϵ.
A meet in the middle attack on Triple-DES. A similar meet in the middle attack applies
to the 3 E construction from the previous section. While 3 E has key space K3, the meet in the
middle attack on 3 Eruns in time about |K|2 and takes space |K|. In the case of Triple-DES, the
attack requires about |K|2 = 2112 evaluations of DES which is too long to run in practice. Hence,
Triple-DES resists this meet in the middle attack and is the reason why Triple-DES is used in
practice.
4.2.4 Case study: AES
Although Triple-DES is a NIST approved cipher, it has a number of signiﬁcant drawbacks. First,
Triple-DES is three times slower than DES and performs poorly when implemented in software.
Second, the 64-bit block size is problematic for a number of important applications, such as those
discussed in Chapter 6. By the mid-1990s it became apparent that a new federal block cipher
standard was needed.
The AES process. In 1997 NIST put out a request for proposals for a new block cipher standard
to be called the Advanced Encryption Standard or AES. The AES block cipher had to operate
on 128-bit blocks and support three key sizes: 128, 192, and 256 bits. In September of 1997,
NIST received 15 submissions, many of which were developed outside of the United States. After
holding two open conferences to discuss the proposals, in 1999 NIST narrowed down the list to ﬁve
candidates. A further round of intense cryptanalysis followed, culminating in the AES3 conference
in April of 2000, at which a representative of each of the ﬁnal ﬁve teams made a presentation arguing
why their submission should be chosen as the standard. In October of 2000, NIST announced that
Rijndael, a Belgian block cipher, had been selected as the AES cipher. AES became an oﬃcial
standard in November of 2001 when it was published as a NIST standard in FIPS 197. This
concluded a ﬁve year process to standardize a replacement to DES.
Rijndael was designed by Belgian cryptographers Joan Daemen and Vincent Rijmen [49]. AES
is slightly diﬀerent from the original Rijndael cipher. For example, Rijndael supports blocks of size
128, 192, or 256 bits while AES only supports 128-bit blocks.
4.2.4.1 The AES algorithm
Like many real-world block ciphers, AES is an iterated cipher that iterates a simple round cipher
several times. The number of iterations depends on the size of the secret key:
117
input ⨁ ΠAES :
ByteSub
ShiftRow
MixColumns
⨁ ⨁ ΠAES :
ByteSub
ShiftRow
MixColumns
⨁ ˆΠAES :
ByteSub
ShiftRow
⨁ output
128 bit key
k0 k1 k8 k9 k10
round 1 round 9 round 10
Figure 4.11: Schematic of the AES-128 block cipher
cipher key-size block-size number of
name (bits) (bits) rounds
AES-128 128 128 10
AES-192 192 128 12
AES-256 256 128 14
For example, the structure of the cipher AES-128 with its ten rounds is shown in Fig. 4.11. Here
ΠAES is a ﬁxed permutation (a one-to-one function) on {0,1}128 that does not depend on the key.
The last step of each round is to XOR the current round key with the output of Π AES. This is
repeated 9 times until in the last round a slightly modiﬁed permutation ˆΠAES is used. Inverting
the AES algorithm is done by running the entire structure in the reverse direction. This is possible
because every step is easily invertible.
Ciphers that follow the structure shown in Fig. 4.11 are called alternating key ciphers .
They are also known as iterated Even-Mansour ciphers . They can be proven secure under
certain “ideal” assumptions about the permutation Π AES in each round. We present this analysis
in Theorem 4.14 later in this chapter.
To complete the description of AES it suﬃces to describe the permutation Π AES, and the AES
key expansion PRG. We describe each in turn.
The AES round permutation. The permutation Π AES is made up of a sequence of three
invertible operations on the set {0,1}128. The 128 bits are organized as a 4 ×4 array of cells, where
each cell is made up of eight bits. The following three invertible operations are then carried out in
sequence, one after the other, on this 4 ×4 array:
1. SubBytes: Let S : {0,1}8 →{0,1}8 be a ﬁxed permutation (a one-to-one function). This
permutation is applied to each of the 16 cells, one cell at a time. The permutation S is
speciﬁed in the AES standard as a hard-coded table of 256 entries. It is designed to have
no ﬁxed points, namely S(x) ̸= x for all x ∈{0,1}8, and no inverse ﬁxed points, namely
S(x) ̸= ¯x where ¯x is the bit-wise complement of x. These requirements are needed to defeat
certain attacks discussed in Section 4.3.1.
2. ShiftRows: This step performs a cyclic shift on the four rows of the input 4 ×4 array: the
ﬁrst row is unchanged, the second row is cyclically shifted one byte to the left, the third row is
118
cyclically shifted two bytes, and the fourth row is cyclically shifted three bytes. In a diagram,
this step performs the following transformation:


a0 a1 a2 a3
a4 a5 a6 a7
a8 a9 a10 a11
a12 a13 a14 a15

=⇒


a0 a1 a2 a3
a5 a6 a7 a4
a10 a11 a8 a9
a15 a12 a13 a14

 (4.11)
3. MixColumns: In this step the 4 ×4 array is treated as a matrix and this matrix is multiplied
by a ﬁxed matrix where arithmetic is interpreted in the ﬁnite ﬁeld GF(2 8). Elements in
the ﬁeld GF(2 8) are represented as polynomials over GF(2) of degree less than eight where
multiplication is done modulo the irreducible polynomial x8 + x4 + x3 + x+ 1. Speciﬁcally,
the MixColumns transformation does:


02 03 01 01
01 02 03 01
01 01 02 03
03 01 01 02

×


a0 a1 a2 a3
a5 a6 a7 a4
a10 a11 a8 a9
a15 a12 a13 a14

=⇒


a′
0 a′
1 a′
2 a′
3
a′
4 a′
5 a′
6 a′
7
a′
8 a′
9 a′
10 a′
11
a′
12 a′
13 a′
14 a′
15

 (4.12)
Here the scalars 01 ,02,03 are interpreted as elements of GF(2 8) using their binary represen-
tation (e.g., 03 represents the element x+ 1 in GF(28)). This ﬁxed matrix is invertible over
GF(28) so that the entire transformation is invertible.
The permutation ΠAES used in the AES circuit of Fig. 4.11 is the sequential composition of the
three permutation SubBytes, ShiftRows, and MixColumns in that order. In the very last round
AES uses a slightly diﬀerent function we call ˆΠAES. This function is the same as Π AES except
that the MixColumns step is omitted. This omission is done so that the AES decryption circuit
looks somewhat similar to the AES encryption circuit. Security implications of this omission are
discussed in [57].
Because each step in Π AES is easily invertible, the entire permutation Π AES is easily invertible,
as required for decryption.
Implementing AES using pre-computed tables. The AES round function is built from
a permutation we called Π AES deﬁned as a sequence of three steps: SubBytes, ShiftRows, and
MixColumns. The designers of AES did not intend for AES to be implemented that way on modern
processors. Instead, they proposed an implementation of ΠAES the does all three steps at once using
four ﬁxed lookup tables called T0,T1,T2,T3.
To explain how this works, recall that Π AES takes as input a 4 ×4 matrix A= (ai)i=0,...,15 and
outputs a matrix A′ := ΠAES(A) of the same dimensions. Let us use S[a] to denote the result of
applying SubBytes to an input a ∈{0,1}8. Similarly, recall that the MixColumns step multiplies
the current state by a ﬁxed 4 ×4 matrix M. Let us use M[i] to denote column number iof M, and
A′[i] to denote column number i of A′.
Now, looking at (4.12), we can write the four columns of the output of Π AES(A) as:
A′[0] = M[0] ·S[a0] + M[1] ·S[a5] + M[2] ·S[a10] + M[3] ·S[a15]
A′[1] = M[0] ·S[a1] + M[1] ·S[a6] + M[2] ·S[a11] + M[3] ·S[a12]
A′[2] = M[0] ·S[a2] + M[1] ·S[a7] + M[2] ·S[a8] + M[3] ·S[a13]
A′[3] = M[0] ·S[a3] + M[1] ·S[a4] + M[2] ·S[a9] + M[3] ·S[a14]
(4.13)
119
where addition and multiplication is done in GF(2 8). Each column M[i], i= 0,1,2,3, is a vector
of four bytes over GF(28), while the quantities S[ai] are 1-byte scalars in GF(2 8).
Every term in (4.13) can be evaluated quickly using a ﬁxed pre-computed table. Fori= 0,1,2,3
let us deﬁne a table Ti with 256 entries as follows:
for a∈{0,1}8: Ti[a] := M[i] ·S[a] ∈{0,1}32 .
Plugging these tables into (4.13) gives a fast way to evaluate Π AES(A):
A′[0] = T0[a0] + T1[a5] + T2[a10] + T3[a15]
A′[1] = T0[a1] + T1[a6] + T2[a11] + T3[a12]
A′[2] = T0[a2] + T1[a7] + T2[a8] + T3[a13]
A′[3] = T0[a3] + T1[a4] + T2[a9] + T3[a14]
The entire AES circuit written this way is a simple sequence of table lookups. Since each table Ti
contains 256 entries, four bytes each, the total size of all four tables is 4KB. The circular structure
of the matrix M makes it possible to compress the four tables to only 2KB with little impact on
performance.
The one exception to (4.13) is the very last round of AES where theMixColumnsstep is omitted.
To evaluate the last round we need a ﬁfth 256-byte table S that only implements the SubBytes
operation.
This optimization of AES is optional. Implementations in constrained environments where
there is no room to store a 4KB table can choose to implement the three steps of Π AES in code,
which takes less than 4KB, but is not as fast. Thus AES can be adapted for both constrained and
unconstrained environments.
As a word of caution, we note that a simplistic implementation of AES using this table lookup
optimization is most likely vulnerable to cache timing attacks discussed in Section 4.3.2.
The AES-128 key expansion method. Looking back at Fig. 4.11 we see that key expansion
for AES-128 needs to generate 11 rounds keys k0,...,k 10 where each round key is 128 bits. To do
so, the 128-bit AES key is partitioned into four 32-bit words w0,0,w0,1,w0,2,w0,3 and these form
the ﬁrst round key k0. The remaining ten round keys are generated sequentially: for i= 1,..., 10,
the 128-bit round key ki = (wi,0,wi,1,wi,2,wi,3) is generated from the preceding round key ki−1 =
(wi−1,0,wi−1,1,wi−1,2,wi−1,3) as follows:
wi,0 ←wi−1,0 ⊕gi(wi−1,3)
wi,1 ←wi−1,1 ⊕wi,0
wi,2 ←wi−1,2 ⊕wi,1
wi,3 ←wi−1,3 ⊕wi,2 .
Here the function gi : {0,1}32 →{0,1}32 is a ﬁxed function speciﬁed in the AES standard. It
operates on its four byte input in three steps: (1) perform a one-byte left circular rotation on the
4-byte input, (2) apply SubBytesto each of the four bytes obtained, and (3) XOR the left most byte
with a ﬁxed round constant ci. The round constants c1,...,c 10 are speciﬁed in the AES standard:
round constant number i is the element xi−1 of the ﬁeld GF(2 8) treated as an 8-bit string.
The key expansion procedures for AES-192 and AES-256 are similar to those of AES-128. For
AES-192 each iteration generates six 32-bit words (192 bits total) in a similar manner to AES-128,
120
but only the ﬁrst four 32-bit words (128 bits total) are used as the AES round key. For AES-256
each iteration generates eight 32-bit words (256 bits total) in a similar manner to AES-128, but
only the ﬁrst four 32-bit words (128 bits total) are used as the AES round key.
The AES key expansion method is intentionally designed to be invertible: given the last round
key, one can work backwards to recover the full AES secret key k. The reason for this is to ensure
that every AES-128 round key, on its own, has the same amount of entropy as the AES-128 secret
key k. If AES-128 key expansion were not invertible then the last round key would not be uniform
in {0,1}128. Unfortunately, invertability also aids attacks: it is used in related key attacks and in
side-channel attacks on AES, discussed next.
Security of AES. The AES algorithm withstood fairly sophisticated attempts at cryptanalysis
lobbed at it. At the time of this writing, the best known attacks are as follows:
• Key recovery: Key recovery attacks refer to an adversary who is given multiple plain-
text/ciphertext pairs and is able to recover the secret key from these pairs, as in an exhaustive
search attack. The best known key recovery attack on AES-128 takes 2 126.1 evaluations of
AES [26]. This is about four times faster than exhaustive search and takes a prohibitively
long time. Therefore this attack has little impact on the security of AES-128.
The best known attack on AES-192 takes 2189.74 evaluation of AES, which is again only about
four times faster than exhaustive search. The best known attack on AES-256 takes 2 254.42
evaluation of AES, which is about three times faster than exhaustive search. These attacks
have little impact on the security of AES-192 or AES-256.
• Related key attacks: In an ℓ-way related key attack, the adversary is given ℓ lists of
plaintext/ciphertext pairs: for i = 1 ,...,ℓ , list number i is generated using key ki. The
point is that all ℓ keys k1,...,k ℓ must satisfy some ﬁxed relation chosen by the adversary.
The attacker’s goal is to recover one of the keys, say k1. In well-implemented cryptosystems,
keys are always generated independently at random and are unlikely to satisfy the required
relation. Therefore related key attacks do not typically aﬀect correct crypto implementations.
AES-256 is vulnerable to a related key attack that exploits its relatively simple key expansion
mechanism [21]. The attack requires four related keys k1,k2,k3,k4 where the relation is a
simple XOR relation: it requires that certain bits of the quantities k1 ⊕k2, k1 ⊕k3, and k2 ⊕k4
are set to speciﬁc values. Then, given lists of plaintext/ciphertext pairs generated for each
of the four keys, the attacker can recover the four keys in time 2 99.5. This is far faster than
the time it would take to mount an exhaustive search on AES-256. While the attack is quite
interesting, it does not aﬀect the security of AES-256 in well-implemented systems.
Hardware implementation of AES. At the time AES was standardized as a federal encryption
standard, most implementations were software based. The wide-spread adoption of AES in software
products prompted all major processor vendors to extend their instruction set to add support for
a hardware implementation of AES.
Intel, for example, added new instructions to its Xeon and Core families of processors called
AES-NI (AES new instructions) that speed-up and simplify the process of using AES in software.
The new instructions work as follows:
• AESKEYGENASSIST: runs the key expansion procedure to generate the AES round keys from
the AES key.
121
• AESENC: runs one round of the AES encryption algorithm. The instruction is called as:
AESENC xmm15, xmm1
where the xmm15 register holds the 128-bit data block and the xmm1 register holds the 128-
bit round key for that round. The resulting 128-bit data block is written to register xmm15.
Running this instruction nine times with the appropriate round keys loaded into registers
xmm1, . . . ,xmm9 executes the ﬁrst nine rounds of AES encryption.
• AESENCLAST: invoked similar to AESENC to run last round of the AES algorithm. Recall that
the last round function is diﬀerent from the others: it omits the MixColumns step.
• AESDEC and AESDECLAST: runs one round of the AES decryption algorithm, analogous to the
encryption instructions.
These AES-NI hardware instructions provide a signiﬁcant speed-up over a heavily optimized soft-
ware implementation of AES. Experiments by Emilia K¨ asper in 2009 show that on an Intel Core 2
processor, AES using the AES-NI instructions takes 1.35 cycles/byte (pipelined), while an opti-
mized software implementation takes 7.59 cycles/byte.
In Intel’s Skylake processors introduced in 2015 the AESENC, AESDEC and AESENCLAST instruc-
tions each take four cycles to complete. These instructions are fully pipelined so that a new in-
struction can be dispatched every cycle. In other words, Intel partitioned the execution of AESENC
into a pipeline of four stages. Four AES blocks can be processed concurrently by diﬀerent stages of
the pipeline. While processing a single AES-128 block takes (4 cycles) ×(10 rounds) = 40 cycles
(or 2.5 cycles/byte), processing four blocks in a pipeline takes only 44 cycles (or 0.69 cycles/byte).
Hence, pipelining can speed up AES by almost a factor of four. As we will see in the next chapter,
this plays an important role in choosing the exact method we use to encrypt long messages: it is
best to choose an encryption method that can leverage the available parallelism to keep the pipeline
busy.
Beyond speed, the hardware implementation of AES oﬀers better security because it is resistant
to the side-channel attacks discussed in the next section.
4.3 Sophisticated attacks on block ciphers
Widely deployed block ciphers like AES go through a lengthy selection process before they are
standardized and continue to be subjected to cryptanalysis. In this section we survey some attack
techniques that have been developed over the years.
In Section 4.3.1, we begin with attacks on the design of the cipher that may result in key com-
promise from observing plaintext/ciphertext pairs. Unlike brute-force exhaustive search attacks,
these algorithmic attacks rely on clever analysis of the internal structure of a particular block
cipher.
In Section 4.3.2, we consider a very diﬀerent class of attacks, called side-channel attacks. In
analyzing any cryptosystem, we consider scenarios in which an adversary interacts with the users
of a cryptosystem. During the course of these interactions, the adversary collects information that
may help it break the system. Throughout this book, we generally assume that this information
is limited to the input/output behavior of the users (for example, plaintext/ciphertext pairs).
However, this assumption ignores the fact that computation is a physical process. As we shall
122
see, in some scenarios it is possible for the adversary to break a cryptosystem by measuring physical
characteristics of the users’ computations, for example, running time or power consumption.
Another class of attacks on the physical implementation of a cryptosystem is a fault injection
attack, which is discussed in Section 4.3.3. Finally, in Section 4.3.4, we consider another class of
algorithmic attacks, in which the adversary can harness the laws of quantum mechanics to speed
up its computations.
These clever attacks make two very important points:
1. Casual users of cryptography should only ever use standardized algorithms like AES, and not
design their own block ciphers.
2. It is best to not implement algorithms on your own since, most likely the resulting imple-
mentations will be vulnerable to side-channel attacks; instead, it is better to use vetted
implementations in widely used crypto libraries.
To further emphasize these points we encourage anyone who ﬁrst learns about the inner-workings
of AES to take the following entertaining pledge (originally due to Jeﬀ Moser):
I promise that once I see how simple AES really is, I will not implement it in production
code even though it will be really fun. This agreement will remain in eﬀect until I learn
all about side-channel attacks and countermeasures to the point where I lose all interest
in implementing AES myself.
4.3.1 Algorithmic attacks
Attacking the design of block ciphers is a vast ﬁeld with many sophisticated techniques: linear
cryptanalysis, diﬀerential cryptanalysis, slide attacks, boomerang attacks, and many others. We
refer to [150] for a survey of the many elegant ideas that have been developed. Here we brieﬂy
describe a technique called linear cryptanalysis that has been used successfully against the DES
block cipher. This technique, due to Matsui [109, 108], illustrates why designing eﬃcient block-
ciphers is so challenging. This method has been shown to not work against AES.
Linear cryptanalysis. Let (E,D) be a block cipher where data blocks and keys are bit strings.
That is, M= C= {0,1}n and K= {0,1}h.
For a bit string m∈{0,1}n and a set of bit positions S ⊆{0,...,n −1}we use m[S] to denote
the XOR of the bits in positions in S. That is, if S = {i1,...,i ℓ}then m[S] := m[i1] ⊕···⊕ m[iℓ].
We say that the block cipher ( E,D) has a linear relation if there exist sets of bit positions
S0,S1 ⊆{0,...,n −1}and S2 ⊆{0,...,h −1}, such that for all keys k ∈K and for randomly
chosen m∈M, we have
Pr
[
m[S0] ⊕E(k,m)[S1] = k[S2]
]
≥1
2 + ϵ (4.14)
for some non-negligible ϵcalled the bias. For an “ideal” cipher the plaintext and ciphertext behave
like independent strings so that the relation m[S0] ⊕E(k,m)[S1] = k[S2] in (4.14) holds with
probability exactly 1 /2, and therefore ϵ = 0. Surprisingly, the DES block cipher has a linear
relation with a small, but non-negligible bias.
123
Let us see how a linear relation leads to an attack. Consider a cipher ( E,D) that has a linear
relation as in (4.14) for some non-negligible ϵ> 0. We assume the linear relation is explicit so that
the attacker knows the sets S0,S1 and S2 used in the relation. Suppose that for some unknown
secret key k∈K the attacker obtains many plaintext/ciphertext pairs ( mi,ci) for i= 1,...,t . We
assume that the messages m1,...,m t are sampled uniformly and independently from Mand that
ci = E(k,mi) for i= 1,...,t . Using this information the attacker can learn one bit of information
about the secret keyk, namely the bitk[S2] ∈{0,1}assuming suﬃciently many plaintext/ciphertext
pairs are given. The following lemma shows how.
Lemma 4.3. Let (E,D) be a block cipher for which (4.14) holds. Let m1,...,m t be messages
sampled uniformly and independently from the message space Mand let ci := E(k,mi) for i =
1,...,t . Then
Pr
[
k[S2] = Majorityt
i=1(mi[S0] ⊕ci[S1])
]
≥1 −e−tϵ2/2 . (4.15)
Here, Majority takes a majority vote on the given bits; for example, on input (0 ,0,1), the
majority is 0, and on input (0 ,1,1), the majority is 1. The proof of the lemma is by a direct
application of the classic Chernoﬀ bound.
The bound in (4.15) shows that once the number of known plaintext/ciphertext pairs ex-
ceeds 4 /ϵ2, the output of the majority function equals k[S2] with more than 86% probability.
Hence, the attacker can compute k[S2] from the given plaintext/ciphertext pairs and obtain one
bit of information about the secret key. While this single key bit may not seem like much, it is a
stepping stone towards a more powerful attack that can expose the entire key.
Linear cryptanalysis of DES. Matsui showed that 14-rounds of the DES block cipher has a
linear relation where the bias is at least ϵ≥2−21. In fact, two linear relations are obtained: one by
exploiting linearity in the DES encryption circuit and another from linearity in the DES decryption
circuit. For a 64-bit plaintext m let mL and mR be the left and right 32-bits of m respectively.
Similarly, for a 64-bit ciphertext clet cL and cR be the left and right 32-bits of crespectively. Then
two linear relations for 14-rounds of DES are:
mR[17,18,24] ⊕cL[7,18,24,29] ⊕cR[15] = k[Se]
cR[17,18,24] ⊕mL[7,18,24,29] ⊕mR[15] = k[Sd] (4.16)
for some bit positions Se,Sd ⊆{0,..., 55}in the 56-bit key k. Both relations have a bias of ϵ≥2−21
when applied to 14-rounds of DES.
These relations are extended to the entire 16-round DES by incorporating the ﬁrst and last
rounds of DES — rounds number 1 and 16 — into the relations. Let k1 be the ﬁrst round key and
let k16 be the last round key. Then by deﬁnition of the DES round function we obtain from (4.16)
the following relations on the entire 16-round DES circuit:
(
mL ⊕F(k1,mR)
)
[17,18,24] ⊕cR[7,18,24,29] ⊕
(
cL ⊕F(k16,cR)
)
[15] = k[S′
e] (4.17)
(
cL ⊕F(k16,cR)
)
[17,18,24] ⊕mR[7,18,24,29] ⊕
(
mL ⊕F(k1,mR)
)
[15] = k[S′
d] (4.18)
for appropriate bit positions S′
e,S′
d ⊆{0,..., 55}in the 56-bit key.
Let us ﬁrst focus on relation (4.17). Bits 17,18,24 of F(k1,mR) are the result of a single S-box
and therefore they depend on only six bits of k1. Similarly F(k16,cR)[15] depends on six bits of k16.
124
Hence, the left hand side of (4.17) depends on only 12 bits of the secret key k. Let us denote these
12 bits by k(12). We know that when the 12 bits are set to their correct value, the left hand side
of (4.17), evaluated at a random plaintext/ciphertext pair, exhibits a bias of about 2 −21 towards
the bit k[S′
e]. When the 12 key bits of the key are set incorrectly one assumes that the bias in (4.17)
is far less. As we will see, this has been veriﬁed experimentally.
This observation lets an attacker recover the 12 bits k(12) of the secret key k as follows. Given
a list L of t plaintext/ciphertext pairs (e.g., t= 243) do:
• Step 1: for each of the 2 12 candidates for the key bits k(12) compute the bias in (4.17).
That is, evaluate the left hand side of (4.17) on all t plaintext/ciphertext pairs in L and
let t0 be the number of times that the expression evaluates to 0. The bias is computed as
ϵ = |(t0/t) −(1/2)|. This produces a vector of 2 12 biases, one for each candidate 12 bits
for k(12).
• Step 2: sort the 2 12 candidates by their bias, from largest to smallest. If the list L of given
plaintext/ciphertext pairs is suﬃciently large then the 12-bit candidate producing the highest
bias is the most likely to be equal to k(12). This recovers 12 bits of the key. Once k(12) is
known we can determine the bit k[S′
e] using Lemma 4.3, giving a total of 13 bits of k.
The relation (4.18) can be used to recover an additional 13 bits of the keykin exactly the same way.
This gives the attacker a total 26 bits of the key. The remaining 56 −26 = 30 bits are recovered
by exhaustive search.
Naively computing the biases in Step 1 takes time 2 12 ×t: for each candidate for k(12) one has
to evaluate (4.17) on all tplaintext/ciphertext pairs in L. The following insight reduces the work to
approximately time t. For a given pair (m,c), the left hand side of (4.17) can be computed from only
thirteen bits of ( m,c): six bits of m are needed to compute F(k1,mR)[17,18,24], six bits of c are
needed to compute F(k16,cR)[15], and ﬁnally the single bit mL[17,18,24] ⊕cR[7,18,24,29] ⊕cL[15]
is needed. These 13 bits are suﬃcient to evaluate the left hand side of (4.17) for any candidate
key. Two plaintext/ciphertext pairs that agree on these 13 bits will always result in the same value
for (4.17). We refer to these 13 bits as the type of the plaintext/ciphertext pair.
Before computing the biases in Step 1 we build a table of size 2 13 that counts the number
of plaintext/ciphertext pairs in L of each type. For b ∈{0,1}13 table entry b is the number of
plaintext/ciphertext pairs of type b. Constructing this table takes time t, but once the table is
constructed computing all the biases in Step 1 can be done in time 2 12 ×213 = 225 which is much
less than t. Therefore, the bulk of the work in Step 1 is counting the number of plaintext/ciphertext
pairs of each type.
Matsui shows that given a list of 243 plaintext/ciphertext pairs this attack succeeds with proba-
bility 85% using about 243 evaluations of the DES circuit. Experimental results by Junod [97] show
that with 2 43 plaintext/ciphertext pairs, the correct 26 bits of the key are among the 2700 most
likely candidates from Step 1 on average. In other words, the exhaustive search for the remaining
30 bits is carried out on average 2700 ≈211.4 times to recover the entire 56-bit key. Overall, the
attack is dominated by the time to evaluate the DES circuit 230 ×211.4 = 241.4 times on average [97].
Lesson. Linear cryptanalysis of DES is possible because the ﬁfth S-box, S5, happens to be some-
what approximated by a linear function. The linearity of S5 introduced a linear relation on the
cipher that could be exploited to recover the secret key using 241 DES evaluations, far less than the
125
256 evaluations that would be needed in an exhaustive search. However, unlike exhaustive search,
this attack requires a large number of plaintext/ciphertext pairs: the required 2 43 pairs correspond
to 64 terabytes of plaintext data. Nevertheless, this is a good illustration of how diﬃcult it is to
design secure block ciphers and why one should only use standardized and well-studied ciphers.
Linear cryptanalysis has been generalized over the years to allow for more complex non-linear
relations among plaintext, ciphertext, and key bits. These generalizations have been used against
other block ciphers such as LOKI91 and Q.
4.3.2 Side-channel attacks
Side-channel attacks do not attack the cryptosystem as a mathematical object. Instead, they
exploit information inadvertently leaked by its physical implementation.
Consider an attacker who observes a cryptosystem as it operates on secret data, such as a
secret key. The attacker can learn far more information than just the input/output behavior of the
system. Two important examples are:
• Timing side channel: In a vulnerable implementation, the time it takes to encrypt a block
of plaintext may depend on the value of the secret key. An attacker who measures encryption
time can learn information about the key, as shown below.
• Power side channel: In a vulnerable implementation, the amount of power used by the
hardware as it encrypts a block of plaintext can depend on the value of the secret key. An
attacker who wants to extract a secret key from a device like a smartcard can measure the
device’s power usage as it operates and learn information about the key.
Many other side channels have been used to attack implementations: electromagnetic radiation
emanating from a device as it encrypts, heat emanating from a device as it encrypts [117], and even
sound [71].
4.3.2.1 Timing attacks
Timing attacks are a signiﬁcant threat to crypto implementations. Timing information can be
measured by a remote network attacker who interacts with a victim server and measures the
server’s response time to certain requests. For a vulnerable implementation, the response time can
leak information about a secret key. Timing information can also be obtained by a local attacker
on the same machine as the victim, for example, when a low-privilege process tries to extract a
secret key from a high-privilege process. In this case, the attacker obtains very accurate timing
measurements about its target. Timing attacks have been demonstrated in both the local and
remote settings.
In this section, we describe a timing attack on AES that exploits memory caching behavior
on the victim machine. We will assume that the adversary can accurately measure the victim’s
running time as it encrypts a block of plaintext with AES. The attack we present exploits timing
variations due to caching in the machine’s memory hierarchy.
Modern processors use a hierarchy of caches to speed up reads and writes to memory. The
fastest layer, called the L1 cache, is relatively small (e.g. 64KB). Data is loaded into the L1 cache
in blocks (called lines) of 64 bytes. Loading a line into L1 cache takes considerably more time than
reading a line already in cache.
126
This cache-induced diﬀerence in timing leads to a devastating key recovery attack against the
fast table-based implementation of AES presented on page 119. An implementation that ignores
these caching eﬀects will be easily broken by a timing attack.
Recall that the table-based implementation of AES uses four tables T0,T1,T2,T3 for all but the
last round. The last round does not include the MixColumns step and evaluation of this last round
uses an explicit S table instead of the tables T0,T1,T2,T3. Suppose that when each execution of
AES begins, the S table is not in the L1 cache. The ﬁrst time a table entry is read, that part of
the table will be loaded into L1 cache. Consequently, this ﬁrst read will be slow, but subsequent
reads to the same entry will be much faster since the data is already cached. Since the S table is
only used in the last round of AES no parts of the table will be loaded in cache prior to the last
round.
Letting A= (ai)i=0,...,15 denote the 4×4 input to the last round, and letting (wi)i=0,...,15 denote
the 4 ×4 last round key, the ﬁnal AES output is computed as the 4 ×4 matrix:
C = (ci,j) =


S[a0] + w0 S[a1] + w1 S[a2] + w2 S[a3] + w3
S[a5] + w4 S[a6] + w5 S[a7] + w6 S[a4] + w7
S[a10] + w8 S[a11] + w9 S[a8] + w10 S[a9] + w11
S[a15] + w12 S[a12] + w13 S[a13] + w14 S[a14] + w15

 (4.19)
The attacker is given this ﬁnal output C.
To mount the attack, consider two consecutive entries in the output matrixC, say c0 = S[a0]+w0
and c1 = S[a1]+w1. Subtracting one equation from the other we see that whena0 = a1 the following
relation holds:
c0 −c1 = w0 −w1 .
Therefore, with ∆ := w0 −w1 we have that c0 −c1 = ∆ whenever a0 = a1. Moreover, when a0 ̸= a1
the structure of the S table ensures that c0 −c1 ̸= ∆.
The key insight is that whenever a0 = a1, reading S[a0] loads the a0 entry of S into the L1
cache so that the second access to this entry via S[a1] is much faster. However, when a0 ̸= a1 it
is possible that both reads miss the L1 cache so that both are slow. Therefore, when a0 = a1 the
expected running time of the entire AES cipher is slightly less than when a0 ̸= a1.
The attacker’s plan now is to run the victim AES implementation on many random input blocks
and measure the running time. For each value of ∆ ∈{0,1}8 the attacker creates a list L∆ of all
output ciphertexts where c0 −c1 = ∆. For each ∆-value it computes the average running time
among all ciphertexts in L∆. Given enough samples, the lowest average running time is obtained
for the ∆-value satisfying ∆ = w0 −w1. Hence, timing information reveals one linear relation about
the last round key: w0 −w1 = ∆.
Suppose the implementation evaluates the terms of (4.19) in some sequential order. Repeating
the timing procedure above for diﬀerent consecutive pairs ci and ci+1 in C reveals the diﬀerence
in GF(2 8) between every two consecutive bytes of the last round key. Then if the ﬁrst byte of
the last round key is known, all remaining bytes of the last round key can be computed from the
known diﬀerences. Moreover, since key expansion in AES-128 is invertible, it is a simple matter to
reconstruct the AES-128 secret key from the last round key.
To complete the attack, the attacker simply tries all 256 possible values for the ﬁrst byte of last
round key. For each candidate value the attacker obtains a candidate AES-128 key. This key can
be tested by trying it out on a few known plaintext/ciphertext pairs. Once a correct AES-128 key
is found, the attacker has obtained the desired key.
127
This attack, due to Bonneau and Mironov [36], works quite well in practice. Their experiments
on a Pentium IV Xeon successfully recovered the AES secret key using about 2 20 timing measure-
ments of the encryption algorithm. The attack only takes a few minutes to run. We note that the
Pentium IV Xeon uses 32-byte cache lines so that the S table is split across eight lines.
Mitigations. The simplest approach to defeat timing attacks on AES is to use the AES-NI
instructions that implement AES in hardware. These instructions are faster than a software im-
plementation and always take the same amount of time, independent of the key or input data.
On processors that do not have built-in AES instructions one is forced to use a software imple-
mentation. One approach to mitigate cache-timing attacks is to use a table-free implementation of
AES. Several such implementations of AES using a technique called bit-slicing provide reasonable
performance in software and are supposedly resistant to timing attacks.
Another approach is to pre-load the tables T0,T1,T2,T3 and S into L1 cache before every
invocation of AES. This prevents the cache-based timing attack, but only if the tables are not evicted
from L1 cache while AES is executing. Ensuring that the tables stay in L1 cache is non-trivial on a
modern processor. Interrupts during AES execution can evict cache lines. Similarly, hyperthreading
allows for multiple threads to execute concurrently on the same core. While one thread pre-loads
the AES tables into L1 cache another thread executing concurrently can inadvertently evict them.
Yet another approach is to pad AES execution to the maximum possible time to prevent timing
attacks, but this has a non-negligible impact on performance.
To conclude, we emphasize that the following mitigation does not work: adding a random
number of instructions at the end of every AES execution to randomly pad the running time does
not prevent the attack. The attacker can overcome this by simply obtaining more samples and
averaging out the noise.
4.3.2.2 Power attacks on AES implementations
The amount of power consumed by a device as it operates can leak information about the inner-
workings of the device, including secret keys stored on the device. Let us see how an attacker can
use power measurements to quickly extract secret keys from a physical device.
As an example, consider a credit-card with an embedded chip where the chip contains a secret
AES key. To make a purchase the user plugs the credit-card into a point-of-sale terminal. The
terminal provides the card with the transaction details and the card authorizes the transaction
using the secret embedded AES key. We leave the exact details for how this works to a later
chapter.
Since the embedded chip must draw power from the terminal (it has no internal power source),
it is quite easy for the terminal to measure the amount of power consumed by the chip at any
given time. In particular, an attacker can measure the amount of power consumed as the AES
algorithm is evaluated. Fig. 4.12a shows a test device’s power consumption as it evaluates the
AES-128 algorithm four times (the x-axis is time and y-axis is power). Each hump is one run of
AES and within each hump the ten rounds of AES-128 are clearly visible.
Simple power analysis. Suppose an implementation contains a branch instruction that depends
on a bit of the secret key. Say, the branch is taken when the least signiﬁcant bit of the key is ‘1’ and
not taken otherwise. Since taking a branch requires more power than not taking it, the power trace
will show a spike at the branch point when the key bit is one and no spike otherwise. An attacker
128
time
power
(a) Power used by four iterations of AES
count
power (b) S-box LSB output is 0 vs. 1
(c) Power diﬀerential
k=101
k=102
k=103
k=104
k=105 (d) Diﬀerential for keys k= 101,..., 105
Figure 4.12: AES diﬀerential power analysis (source: Kocher et al. [99])
can simply look for a spike at the appropriate point in the power trace and learn that bit of the
key. With multiple key-dependent branch instructions the entire secret key can be extracted. This
works quite well against simple implementations of certain cryptosystems (such as RSA, which is
covered in a later chapter).
The attack of the previous paragraph, called simple power analysis (SPA), will not work
on AES: during encryption the secret AES round keys are simply XORed into the cipher state.
The power used by the XOR instruction only marginally depends on its operands and therefore
the power used by the XOR reveals no useful information about the secret key. This resistance to
simple power analysis was an attractive feature of AES.
Diﬀerential power analysis. Despite AES’s resistance to SPA, a more sophisticated power
analysis attack successfully extracts the AES secret key from simple implementations. Choose an
AES key k at random and encrypt 4000 random plaintexts using the key k. For our test device the
resulting 4000 power traces look quite diﬀerent from each other indicating that the power trace is
input dependent, the input being the random plaintext.
Next, consider the output of the ﬁrst S-box in the ﬁrst round. Call this output T. We hypothe-
size that the power consumed by the S-box lookup depends on the index being looked up. That is,
we guess that the value of T is correlated with the power consumed by the table lookup instruction.
To test the hypothesis, let us split the 4000 traces into two piles according to the least signiﬁcant
bit of T: pile 1 contains traces where the LSB of T is 1 and pile 0 contains traces where the bit
is 0. Consider the power consumed by traces in each pile at the moment in time when the card
computes the output of the ﬁrst S-box:
pile 1 (LSB = 1): mean power 116.9 units, standard deviation 10.7
pile 0 (LSB = 0): mean power 121.9 units, standard deviation 9.7
The two power distributions are shown in Fig. 4.12b. The distributions are close, but clearly
129
diﬀerent. Hence, with enough independent samples we can distinguish one distribution from the
other.
To exploit this observation, consider Fig. 4.12c. The top line shows the power trace averaged
over all traces in pile 1. The second line shows the power trace averaged over all traces in pile 0.
The bottom line shows the diﬀerence between the two top traces, magniﬁed by a factor of 15. The
ﬁrst spike in the bottom line is exactly at the time when the card computed the output of the ﬁrst
S-box. The size of the spike corresponds exactly to the diﬀerence in averages shown in Fig. 4.12b.
This bottom line is called the power diﬀerential.
To attack a target device the attacker must ﬁrst experiment with a clean device: the attacker
loads a chosen secret key into the device and computes the power diﬀerential curve for the device
as shown in Fig. 4.12c. Next, suppose the attacker obtains a device with an unknown embedded
key. It can extract the key as follows:
ﬁrst, measure the power trace for 4000 random plaintexts
next, for each candidate ﬁrst byte k∈{0,1}8 of the key do:
split the 4000 samples into two piles according to the ﬁrst bit of T
(this is done using the current guess for k and the 4000 known plaintexts)
if the resulting power diﬀerential curve matches the pre-computed curve:
output k as the ﬁrst byte of the key and stop
Fig. 4.12d shows this attack in action. When using the correct value for the ﬁrst byte of the
key ( k = 103) we obtain the correct power diﬀerential curve. When the wrong guess is used
(k= 101,102,104,105) the power diﬀerential does not match the expected curve.
Iterating this procedure for all 16 bytes of the AES-128 key recovers the entire key.
Mitigations. A common defense against power analysis uses hardware tweaks. Conceptually,
prior to executing AES the hardware draws a ﬁxed amount of power to charge a capacitor and then
runs the entire AES algorithm using power in the capacitor. Once AES is done the excess power
left in the capacitor is discarded. The next application of AES again charges the capacitor and so
on. This conceptual design (which takes some eﬀort to implement correctly in practice) ensures
that the device’s power consumption is independent of secret keys embedded in the device.
Another mitigation approach concedes that some limited information about the secret key
leaks every time the decryption algorithm runs. The goal is to then preemptively re-randomize the
secret key after each invocation of the algorithm so that the attacker cannot combine the bits of
information he learns from each execution. This approach is studied in an area called leakage-
resilient cryptography.
4.3.3 Fault injection attacks on AES
Another class of implementation attacks, called fault injection attacks, attempt to deliberately
cause the hardware to introduce errors while running the cryptosystem. An attacker can exploit
the malformed output to learn information about the secret key. Injecting faults can be done
by over-clocking the target hardware, by heating it using a laser, or by directing electromagnetic
interference at the target chip [96].
Fault injection attacks have been used to break vulnerable implementations of AES by causing
the AES engine to malfunction during encryption of a plaintext block. The resulting malformed
ciphertext can reveal information about the secret key [96]. Fault attacks are easiest to describe in
130
the context of public-key systems and we will come back and discuss them in detail in Section 16.4.3
where we show how they result in a complete break of some implementations of RSA.
One defense against fault injection attacks is to always check the result of the computation. For
example, an AES engine could check that the computed AES ciphertext correctly decrypts to the
given input plaintext. If the check fails, the hardware outputs an error and discards the computed
ciphertext. Unfortunately this slows down AES performance by a factor of two and is hardly done
in practice.
4.3.4 Quantum exhaustive search attacks
All the attacks described so far work on classical computers available today. Our physical world,
however, is governed by the laws of quantum mechanics. In theory, computers can be built to use
these laws to solve problems in much less time than would be required on a classical computer.
Although no one has yet succeeded in building quantum computers, it could be just be a matter
of time before the ﬁrst quantum computer is built.
Quantum computers have signiﬁcant implications to cryptography because they can be used to
speed up certain attacks and even completely break some systems. Consider again a block cipher
(E,D) with key space K. Recall that in a classical exhaustive search the attacker is given a few
plaintext/ciphertext pairs created with some key k∈K and the attacker tries all keys until he ﬁnds
a key that maps the given plaintexts to the given ciphertexts. On a classical computer this takes
time proportional to |K|.
Quantum exhaustive search. Surprisingly, on a quantum computer the same exhaustive search
problem can be solved in time proportional to only
√
|K|. For block ciphers like AES-128 this means
that exhaustive search will only require about
√
2128 = 264 steps. Computations involving 264 steps
can already be done in a reasonable amount of time on a classical computer. If a quantum computer
is built that could handle a computation of that size, then AES-128 will no longer be secure.
The above discussion suggests that for a block cipher to resist a quantum exhaustive search
attack, its key space |K|must have at least 2 256 keys so that the time for quantum exhaustive
search is on the order of 2128. This threat of a quantum computer is one reason why AES supports
a 256-bits key. Of course, we have no guarantees that there is not a faster quantum algorithm for
breaking the AES-256 block cipher, but at least quantum exhaustive search is out of the question
for that size key.
Grover’s algorithm. The algorithm for quantum exhaustive search is a special case of a more
general result in quantum computing due to Lov Grover [80]. The result says the following: suppose
we are given a function f : K→{ 0,1}deﬁned as follows
f(k) =
{
1 if k= k0
0 otherwise (4.20)
for some k0 ∈K. The goal is to ﬁnd k0 given only “black-box” access to f, namely by only querying
f at diﬀerent inputs. On a classical computer it is clear that the best algorithm is to try all possible
k∈K and this takes |K|queries to f in the worst case.
Grover’s algorithm shows thatk0 can be found on a quantum computer in onlyO
(√
|K|·time(f)
)
steps, where time( f) is the time to evaluate f(x). This is a very general result that holds for all
131
functions f of the form shown in (4.20). This can be used to speed-up general hard optimization
problems and is the “killer app” for quantum computers.
To break a block cipher like AES-128 given a few plaintext/ciphertext pairs, we would deﬁne
the function:
fAES(k) =
{
1 if AES(k,m) = c
0 otherwise
where m = (m0,...,m Q) and c = (c0,...,c Q) are the given ciphertext blocks. Assuming enough
blocks are given, there is a unique key k0 ∈K that satisﬁes AES(k,m) = c and this key can be
found in time proportional to
√
|K|using Grover’s algorithm.
4.4 Pseudo-random functions: basic deﬁnitions and properties
While secure block ciphers are the building block of many cryptographic systems, a closely related
concept, called a pseudo-random function (or PRF), turns out to be the right tool in many appli-
cations. PRFs are conceptually simpler objects than block ciphers and, as we shall see, they have
a broad range of applications. PRFs and block ciphers are so closely related that we can use secure
block ciphers as a stand in for secure pseudo-random functions (under certain assumptions). This
is quite nice, because as we saw in the previous section, we have available to us a number of very
practical, and plausibly secure block ciphers.
4.4.1 Deﬁnitions
A pseudo-random function (PRF) F is a deterministic algorithm that has two inputs: a key k
and an input data block x; its output y := F(k,x) is called an output data block . As usual,
there are associated, ﬁnite spaces: the key space K, in which k lies, the input space X, in which x
lies, and the output space Y, in which y lies. We say that F is deﬁned over (K,X,Y).
Intuitively, our notion of security for a pseudo-random function says that for a randomly chosen
key k, the function F(k,·) should — for all practical purposes — “look like” a random function
from Xto Y. To make this idea more precise, let us ﬁrst introduce some notation:
Funs[X,Y]
denotes the set of all functions f : X→Y . This is a very big set:
|Funs[X,Y]|= |Y||X|.
We also introduce an attack game:
Attack Game 4.2 (PRF). For a given PRF F, deﬁned over (K,X,Y), and for a given adversary
A, we deﬁne two experiments, Experiment 0 and Experiment 1. For b= 0,1, we deﬁne:
Experiment b:
• The challenger selects f ∈Funs[X,Y] as follows:
if b= 0: k←R K, f ←F(k,·);
if b= 1: f ←R Funs[X,Y].
132
• The adversary submits a sequence of queries to the challenger.
For i= 1,2,..., the ith query is an input data block xi ∈X.
The challenger computes yi ←f(xi) ∈Y, and gives yi to the adversary.
• The adversary computes and outputs a bit ˆb∈{0,1}.
For b= 0,1, let Wb be the event that Aoutputs 1 in Experiment b. We deﬁne A’s advantage
with respect to F as
PRFadv[A,F] :=
⏐⏐⏐Pr[W0] −Pr[W1]
⏐⏐⏐. (4.21)
Finally, we say that Ais a Q-query PRF adversary if Aissues at most Q queries. 2
Deﬁnition 4.2 (secure PRF). A PRF F is secure if for all eﬃcient adversaries A, the value
PRFadv[A,F] is negligible.
Again, we stress that the queries made by the adversary in Attack Game 4.2 are allowed to be
adaptive: the adversary is allowed to concoct each query in a way that depends on the previous
responses from the challenger (see Exercise 4.6).
As discussed in Section 2.2.5, Attack Game 4.2 can be recast as a “bit guessing” game, where
instead of having two separate experiments, the challenger chooses b∈{0,1}at random, and then
runs Experiment b against the adversary A. In this game, we measure A’s bit-guessing advantage
PRFadv∗[A,F] as |Pr[ˆb = b] −1/2|. The general result of Section 2.2.5 (namely, (2.11)) applies
here as well:
PRFadv[A,F] = 2 ·PRFadv∗[A,F]. (4.22)
Weakly secure PRFs. For certain constructions that use PRFs it suﬃces that the PRF satisfy
a weaker security property than Deﬁnition 4.2. We say that a PRF is weakly secure if no eﬃcient
adversary can distinguish the PRF from a random function when its queries are severely restricted:
it can only query the function at random points in the domain. Restricting the adversary’s queries
to random inputs makes it potentially easier to build weakly secure PRFs. In Exercise 4.2 we
examine natural PRF constructions that are weakly secure, but not fully secure.
We deﬁne weakly secure PRFs by slightly modifying Attack Game 4.2. Let F be a PRF deﬁned
over (K,X,Y). We modify the way in which an adversaryAinteracts with the challenger: whenever
the adversary queries the function, the challenger chooses a random x∈X and sends both x and
f(x) to the adversary. In other words, the adversary sees evaluations of the function f at random
points in Xand needs to decide whether the function is truly random or pseudorandom. We deﬁne
the adversary’s advantage in this game, denoted wPRF adv[A,F], as in (4.21).
Deﬁnition 4.3 (weakly secure PRF). A PRF F is weakly secure if for all eﬃcient adver-
saries A, the value wPRFadv[A,F] is negligible.
4.4.2 Eﬃcient implementation of random functions
Just as in Section 4.1.2, we can implement the random function chosen from Funs[ X,Y] used by
the challenger in Experiment 1 of Attack Game 4.2 by a faithful gnome. Just as in the block
cipher case, the challenger keeps track of input/output pairs ( xi,yi). When the challenger receives
the ith query xi, he tests whether xi = xj for some j <i; if so, he sets yi ←yj (this ensures that
133
the challenger implements a function); otherwise, he chooses yi at random from the set Y; ﬁnally,
he sends yi to the adversary. We can write the logic of this implementation of the challenger as
follows:
upon receiving the ith query xi ∈X from Ado:
if xi = xj for some j <i
then yi ←yj
else yi ←R Y
send yi to A.
4.4.3 When is a secure block cipher a secure PRF?
In this section, we ask the question: when is a secure block cipher a secure PRF? In answering this
question, we introduce a proof technique that is used heavily throughout cryptography.
Let E= (E,D) be a block cipher deﬁned over ( K,X), and let N := |X|. We may naturally
view E as a PRF, deﬁned over ( K,X,X). Now suppose that Eis a secure block cipher; that is,
no eﬃcient adversary can eﬀectively distinguish E from a random permutation. Does this imply
that E is also a secure PRF? That is, does this imply that no eﬃcient adversary can eﬀectively
distinguish E from a random function?
The answer to this question is “yes,” provided N is super-poly. Before arguing this, let us argue
that the answer is “no” when N is small.
Consider a PRF adversary playing Attack Game 4.2 with respect to E. Let f be the function
chosen by the challenger: in Experiment 0, f = E(k,·) for random k ∈K, while in Experiment 1,
f is randomly chosen from Funs[X,X]. Suppose that N is so small that an eﬃcient adversary can
aﬀord to obtain the value of f(x) for all x∈X. Moreover, our adversary Aoutputs 1 if it sees that
f(x) = f(x′) for two distinct valuesx,x′∈X, and outputs 0 otherwise. Clearly, in Experiment 0, A
outputs 1 with probability 0, since E(k,·) is a permutation. However, in Experiment 1, Aoutputs
1 with probability 1 −N!/NN ≥1/2. Thus, PRF adv[A,E] ≥1/2, and so E is not a secure PRF.
The above argument can be reﬁned using the Birthday Paradox (see Section B.1). For any poly-
bounded Q, we can deﬁne an eﬃcient PRF adversary Athat plays Attack Game 4.2 with respect
to E, as follows. Adversary Asimply makes Q distinct queries to its challenger, and outputs 1 iﬀ
it sees that f(x) = f(x′) for two distinct values x,x′∈X (from among the Q values given to the
challenger). Again, in Experiment 0, Aoutputs 1 with probability 0; however, by Theorem B.1, in
Experiment 1, Aoutputs 1 with probability at least min
{
Q(Q−1)
/
4N,0.63
}
. Thus, by making
just O(N1/2) queries, an adversary can easily see that a permutation does not behave like a random
function.
It turns out that the “birthday attack” is about the best that any adversary can do, and when
N is super-poly, this attack becomes infeasible:
Theorem 4.4 (PRF Switching Lemma). Let E= (E,D) be a block cipher deﬁned over (K,X),
and let N := |X|. Let Abe an adversary that makes at most Q queries to its challenger. Then
⏐⏐⏐BCadv[A,E] −PRFadv[A,E]
⏐⏐⏐≤Q2/2N.
Before proving this theorem, we derive the following simple corollary:
134
Corollary 4.5. Let E= (E,D) be a block cipher deﬁned over (K,X), and assume that N := |X|
is super-poly. Then Eis a secure block cipher if and only if E is a secure PRF.
Proof. By deﬁnition, if Ais an eﬃcient adversary, the maximum number of queries Q it makes to
its challenger is poly-bounded. Therefore, by Theorem 4.4, we have
⏐⏐⏐BCadv[A,E] −PRFadv[A,E]
⏐⏐⏐≤Q2/2N
Since N is super-poly and Q is poly-bounded, the value Q2/2N is negligible (see Fact 2.6). It
follows that BCadv[A,E] is negligible if and only if PRF adv[A,E] is negligible. 2
Actually, the proof of Theorem 4.4 has nothing to do with block ciphers and PRFs — it is
really an argument concerning random permutations and random functions. Let us deﬁne a new
attack game that tests an adversary’s ability to distinguish a random permutation from a random
function.
Attack Game 4.3 (permutation vs. function). For a given ﬁnite set X, and for a given
adversary A, we deﬁne two experiments, Experiment 0 and Experiment 1. For b= 0,1, we deﬁne:
Experiment b:
• The challenger selects f ∈Funs[X,X] as follows:
if b= 0: f ←R Perms[X];
if b= 1: f ←R Funs[X,X].
• The adversary submits a sequence of queries to the challenger.
For i= 1,2,..., the ith query is an input data block xi ∈X.
The challenger computes yi ←f(xi) ∈Y, and gives yi to the adversary.
• The adversary computes and outputs a bit ˆb∈{0,1}.
For b= 0,1, let Wb be the event that Aoutputs 1 in Experiment b. We deﬁne A’s advantage
with respect to Xas
PFadv[A,X] :=
⏐⏐⏐Pr[W0] −Pr[W1]
⏐⏐⏐. 2
Theorem 4.6. Let Xbe a ﬁnite set of size N. Let Abe an adversary that makes at most Qqueries
to its challenger. Then
PFadv[A,X] ≤Q2/2N.
We ﬁrst show that the above theorem easily implies Theorem 4.4:
Proof of Theorem 4.4. Let E= (E,D) be a block cipher deﬁned over (K,X). Let Abe an adversary
that makes at most Q queries to its challenger. We deﬁne Games 0, 1, and 2, played between A
and a challenger. For j = 0,1,2, we deﬁne pj to be the probability that Aoutputs 1 in Game j.
In each game, the challenger chooses a function f : X→X according to a particular distribution,
and responds to each query x∈X made by Awith the value f(x).
Game 0: The challenger in this game chooses f := E(k,·), where k∈K is chosen at random.
135
Game 1: The challenger in this game chooses f ∈Perms[X] at random.
Game 2: The challenger in this game chooses f ∈Funs[X,X] at random.
Observe that by deﬁnition,
|p1 −p0|= BCadv[A,E],
|p2 −p0|= PRFadv[A,E],
and that by Theorem 4.6,
|p2 −p1|= PFadv[A,X] ≤Q2/2N.
Putting these together, we get
⏐⏐BCadv[A,E] −PRFadv[A,E]
⏐⏐=
⏐⏐|p1 −p0|−|p2 −p0|
⏐⏐≤|p2 −p1|≤ Q2/2N,
which proves the theorem. 2
So it remains to prove Theorem 4.6. Before doing so, we state and prove a simple, but extremely
useful fact:
Theorem 4.7 (Diﬀerence Lemma). Let Z,W0,W1 be events deﬁned over some probability space,
and let ¯Z denote the complement of the event Z. Suppose that W0 ∧¯Z occurs if and only if W1 ∧¯Z
occurs. Then we have ⏐⏐Pr[W0] −Pr[W1]
⏐⏐≤Pr[Z].
Proof. This is a simple calculation. We have
⏐⏐Pr[W0] −Pr[W1]
⏐⏐=
⏐⏐Pr[W0 ∧Z] + Pr[W0 ∧¯Z] −Pr[W1 ∧Z] −Pr[W1 ∧¯Z]
⏐⏐
=
⏐⏐Pr[W0 ∧Z] −Pr[W1 ∧Z]
⏐⏐
≤Pr[Z].
The second equality follows from the assumption that W0 ∧¯Z ⇐⇒W1 ∧¯Z, and so in particular,
Pr[W0 ∧¯Z] = Pr[ W1 ∧¯Z]. The ﬁnal inequality follows from the fact that both Pr[ W0 ∧Z] and
Pr[W1 ∧Z] are numbers between 0 and Pr[ Z]. 2
In most of our applications of the Diﬀerence Lemma, W0 will represent the event that a given
adversary outputs 1 in some game against a certain challenger, while W1 will be the event that the
same adversary outputs 1 in a game played against a diﬀerent challenger. To apply the Diﬀerence
Lemma, we deﬁne these two games so that they both operate on the same underlying probability
space. This means that we view the random choices made by both the adversary and the challenger
as the same in both games — all that diﬀers between the two games is the rule used by the challenger
to compute its responses to the adversary’s queries.
Proof of Theorem 4.6. Consider an adversary Athat plays Attack Game 4.3 with respect to
X, where N := |X|, and assume that Amakes at most Q queries to the challenger. Consider
Experiment 0 of this attack game. Using the “faithful gnome” idea discussed in Section 4.4.2,
we can implement Experiment 0 by keeping track of input/output pairs ( xi,yi); moreover, it will
be convenient to choose initial “default” values zi for yi, where the values z1,...,z Q are chosen
uniformly and independently at random fromX; these “default” values are over-ridden, if necessary,
to ensure the challenger deﬁnes a random permutation. Here are the details:
136
z1,...,z Q ←R X
upon receiving the ith query xi from Ado:
if xi = xj for some j <ithen
yi ←yj
else
yi ←zi
(∗) if yi ∈{y1,...,y i−1}then yi ←R X\{ y1,...,y i−1}
send yi to A.
The line marked ( ∗) tests if the default value zi needs to be over-ridden to ensure that no output
is for two distinct inputs.
Let W0 be the event that Aoutputs 1 in this game, which we call Game 0.
We now obtain a diﬀerent game by modifying the above implementation of the challenger:
z1,...,z Q ←R X
upon receiving the ith query xi from Ado:
if xi = xj for some j <ithen
yi ←yj
else
yi ←zi
send yi to A.
All we have done is dropped line marked ( ∗) in the original challenger: our “faithful gnome”
becomes a “forgetful gnome,” and simply forgets to make the output consistency check.
Let W1 be the event that Aoutputs 1 in the game played against this modiﬁed challenger,
which we call Game 1.
Observe that Game 1 is equivalent to Experiment 1 of Attack Game 4.3; in particular, Pr[ W1]
is equal to the probability that Aoutputs 1 in Experiment 1 of Attack Game 4.3. Therefore, we
have
PFadv[A,X] = |Pr[W0] −Pr[W1]|.
We now want to apply the Diﬀerence Lemma. To do this, both games are understood to operate
on the same underlying probability space. All of the random choices made by the adversary and
challenger are the same in both games — all that diﬀers is the rule used by the challenger to
compute its responses. In particular, this means that the random choices made by A, as well as the
values z1,...,z Q chosen by the challenger, not only have identical distributions, but are literally
the same values in both games.
Deﬁne Z to be the event that zi = zj for some i ̸= j. Now suppose we run Game 0 and
Game 1, and event Z does not occur. This means that the zi values are all distinct. Now, since
the adversary’s random choices are the same in both games, its ﬁrst query in both games is the
same, and therefore the challenger’s response is the same in both games. The adversary’s second
query (which is a function of its random choices and the challenger’s ﬁrst response) is the same in
both games. By the assumption that Z does not occur, the challenger’s response is the same in
both games. Continuing this argument, one sees that each of the adversary’s queries and each of
the challenger’s responses are the same in both games, and therefore the adversary’s output is the
137
same in both games. Thus, if Z does not occur and the adversary outputs 1 in Game 0, then the
adversary also outputs 1 in Game 1. Likewise, if Z does not occur and the adversary outputs 1 in
Game 1, then the adversary outputs 1 in Game 0. More succinctly, we have W0 ∧¯Z occurs if and
only if W1 ∧¯Z occurs. So the Diﬀerence Lemma applies, and we obtain
|Pr[W0] −Pr[W1]|≤ Pr[Z].
It remains to bound Pr[ Z]. However, this follows from the union bound: for each pair ( i,j) of
distinct indices, Pr[zi = zj] = 1/N, and as there are less than Q2/2 such pairs, we have
Pr[Z] ≤Q2/2N.
That proves the theorem. 2
While there are other strategies one might use to prove the previous theorem (see Exercise 4.24),
the forgetful gnome technique that we used in the above proof is very useful and we will see it
again many times in the sequel.
4.4.4 Constructing PRGs from PRFs
It is easy to construct a PRG from a PRF. Let F be a PRF deﬁned over ( K,X,Y), let ℓ ≥1 be
a poly-bounded value, and let x1,...,x ℓ be any ﬁxed, distinct elements of X (this requires that
|X|≥ ℓ). We deﬁne a PRG G with seed space Kand output space Yℓ, as follows: for k∈K,
G(k) := (F(k, x1),...,F (k, xℓ)).
Theorem 4.8. If F is a secure PRF, then the PRG G described above is a secure PRG.
In particular, for every PRG adversary Athat plays Attack Game 3.1 with respect to G, there
is a PRF adversary Bthat plays Attack Game 4.2 with respect to F, where Bis an elementary
wrapper around A, such that
PRGadv[A,G] = PRFadv[B,F].
Proof. Let Abe an eﬃcient PRG adversary that plays Attack Game 3.1 with respect to G. We
describe a corresponding PRF adversaryBthat plays Attack Game 4.2 with respect toF. Adversary
Bworks as follows:
Bqueries its challenger at x1,...,x ℓ, obtaining responses y1,...,y ℓ. Adversary Bthen
plays the role of challenger to A, giving Athe value (y1,...,y ℓ). Adversary Boutputs
whatever Aoutputs.
It is obvious from the construction that for b = 0 ,1, the probability that B outputs 1 in
Experiment b of Attack Game 4.2 with respect to F is precisely equal to the probability that
Aoutputs 1 in Experiment b of Attack Game 3.1 with respect to G. The theorem then follows
immediately. 2
138
4.4.4.1 Deterministic counter mode
The above construction gives us another way to build a semantically secure cipher out of a secure
block cipher. Suppose E= (E,D) is a block cipher deﬁned over ( K,X), where X= {0,1}n. Let
N := |X| = 2 n. Assume that N is super-poly and that E is a secure block cipher. Then by
Theorem 4.4, the encryption function E is a secure PRF (deﬁned over ( K,X,X)). We can then
apply Theorem 4.8 to E to obtain a secure PRG, and ﬁnally apply Theorem 3.1 to this PRG to
obtain a semantically secure stream cipher.
Let us consider this stream cipher in detail. This cipher E′ = (E′,D′) has key space K, and
message and ciphertext space X≤ℓ, where ℓis a poly-bounded value, and in particular, ℓ≤N. We
can deﬁne x1,...,x ℓ to be any convenient elements of X; in particular, we can deﬁne xi to be the
n-bit binary encoding of i−1, which we denote ⟨i−1⟩n. Encryption and decryption for E′work as
follows.
• For k∈K and m∈X≤ℓ, with v:= |m|, we deﬁne
E′(k,m) :=
(
E(k,⟨0⟩n) ⊕m[0],...,E (k,⟨v−1⟩n) ⊕m[v−1]
)
.
• For k∈K and c∈X≤ℓ, with v:= |c|, we deﬁne
D′(k,c) :=
(
E(k,⟨0⟩n) ⊕c[0],...,E (k,⟨v−1⟩n) ⊕c[v−1]
)
.
This mode of operation of a block cipher is called deterministic counter mode . It is il-
lustrated in Fig. 4.13. Notice that unlike ECB mode, the decryption algorithm D is never used.
Putting together Theorems 4.4, 4.8, and 3.1, we see that cipher E′ is semantically secure; in par-
ticular, for any eﬃcient SS adversary A, there exists an eﬃcient BC adversary Bsuch that
SSadv[A,E′] ≤2 ·BCadv[B,E] + ℓ2/N. (4.23)
An advantage of deterministic counter mode over ECB mode is that it is semantically secure
without making any restrictions on the message space. The only disadvantage is that security
might degrade signiﬁcantly for very long messages, because of the ℓ2/N term in (4.23). Indeed,
it is essential that ℓ2/2N is very small. Consider the following attack on E′. Set m0 to be the
message consisting of ℓ zero blocks, and set m1 to be a message consisting of ℓ random blocks. If
the challenger in Attack Game 2.1 encrypts m0 using E′, then the ciphertext will not contain any
duplicate blocks. However, by the birthday paradox (see Theorem B.1), if the challenger encrypts
m1, the ciphertext will contain duplicate blocks with probability at least min
{
ℓ(ℓ−1)
/
4N,0.63
}
. So
the adversary Athat constructs m0 and m1 in this way, and outputs 1 if and only if the ciphertext
contains duplicate blocks, has an advantage that grows quadratically in ℓ, and is non-negligible for
ℓ≈N1/2.
4.4.5 Mathematical details
As usual, we give a more mathematically precise deﬁnition of a PRF, using the terminology deﬁned
in Section 2.3.
Deﬁnition 4.4 (pseudo-random function). A pseudo-random function consists of an algo-
rithm F, along with three families of spaces with system parameterization P:
K = {Kλ,Λ}λ,Λ, X = {Xλ,Λ}λ,Λ, and Y = {Yλ,Λ}λ,Λ,
139
E(k, ·)
E(k, ·)
E(k, ·)
· · ·
m[0]
m[1]
m[v − 1]
c[v − 1]
c[0]
c[1]
(a) encryption
(b) decryption
⟨0⟩n
⟨1⟩n
⟨v − 1⟩n
⊕
⊕
⊕
E(k, ·)
E(k, ·)
E(k, ·)
· · ·
m[0]
m[1]
m[v − 1]
c[v − 1]
c[0]
c[1]
⟨0⟩n
⟨1⟩n
⟨v − 1⟩n
⊕
⊕
⊕
Figure 4.13: Encryption and decryption for deterministic counter mode
140
such that
1. K, X, and Y are eﬃciently recognizable.
2. K and Y are eﬃciently sampleable.
3. Algorithm F is a deterministic algorithm that on input λ∈Z≥1, Λ ∈Supp(P(λ)), k∈Kλ,Λ,
and x∈Xλ,Λ, runs in time bounded by a polynomial in λ, and outputs an element of Yλ,Λ.
As usual, in deﬁning security, the attack game is parameterized by security and system param-
eters, and the advantage is a function of the security parameter.
4.5 Constructing block ciphers from PRFs
In this section, we show how to construct a secure block cipher from any secure PRF whose output
space and input space is {0,1}n, where 2 n is super-poly. The construction is called the Luby-
Rackoﬀ construction. The result itself is mainly of theoretical interest, as block ciphers that are
used in practice have a more ad hoc design; however, the result is sometimes seen as a justiﬁcation
for the design of practical block ciphers that have a Feistel network structure (see Section 4.2.1).
Let F be a PRF, deﬁned over ( K,X,X), where X = {0,1}n. We describe a block cipher
E= (E,D) whose key space is K3, and whose data block space is X2.
Given a key (k1,k2,k3) ∈K3 and a data block ( u,v) ∈X2, the encryption algorithm E runs as
follows:
w←u⊕F(k1, v)
x←v⊕F(k2, w)
y←w⊕F(k3, x)
output (x,y).
Given a key ( k1,k2,k3) ∈K3 and a data block ( x,y) ∈X 2, the decryption algorithm D runs as
follows:
w←y⊕F(k3, x)
v←x⊕F(k2, w)
u←w⊕F(k1, v)
output (u,v).
See Fig. 4.14 for an illustration of E. Notice that the cipher is a three round Feistel network.
It is easy to see that E is a block cipher. It is useful to see algorithm E as consisting of 3
“rounds.” For k∈K, let us deﬁne the “round function”
φk : X2 →X2
(a,b) ↦→(b,a ⊕F(k,b)).
It is easy to see that for any ﬁxed k, the function φk is a permutation on X2; indeed, if σ(a,b) :=
(b,a), then
φ−1
k = σ◦φk ◦σ.
Moreover, we see that
E((k1,k2,k3),·) = φk3 ◦φk2 ◦φk1
and
D((k1,k2,k3),·) = φ−1
k1 ◦φ−1
k2 ◦φ−1
k3 = σ◦φk1 ◦φk2 ◦φk3 ◦σ.
141
⊕
⊕
⊕
⊕
⊕
⊕
F (k1, ·)
F (k1, ·)
F (k2, ·)
F (k2, ·)
F (k3, ·)
F (k3, ·)
u
v
w
x
x
y
x
y
w
v
x
v
v
u
(a) Encryption
(b) Decryption
Figure 4.14: Encryption and decryption with Luby-Rackoﬀ
142
Theorem 4.9. If F is a secure PRF and N := |X| = 2 n is super-poly, then the Luby-Rackoﬀ
cipher E= (E,D) constructed from F is a secure block cipher.
In particular, for every Q-query BC adversary Athat attacks Eas in Attack Game 4.1, there
exists a PRF adversary Bthat plays Attack Game 4.2 with respect to F, where Bis an elementary
wrapper around A, such that
BCadv[A,E] ≤3 ·PRFadv[B,F] + Q2
N + Q2
2N2 .
Proof idea. By Corollary 4.5, and the assumption that N is super-poly, it suﬃces to show that E
is a secure PRF. So we want to show that if an adversary is playing in Experiment 0 of Attack
Game 4.2 with respect to E, the challenger’s responses eﬀectively “look like” completely random
bit strings. We may assume that the adversary never makes the same query twice. Moreover, as F
is a PRF, we can replace F(k1,·), F(k2,·), and F(k3,·) by truly random functions, f1, f2, and f3,
and the adversary should hardly notice the diﬀerence.
So now, given a query ( ui,vi), the challenger computes its response ( xi,yi) as follows:
wi ←ui ⊕f1(vi)
xi ←vi ⊕f2(wi)
yi ←wi ⊕f3(xi).
A rough, intuitive argument goes like this. Suppose that no two wi values are the same. Then
all of the outputs of f2 will be random and independent. From this, we can argue that the xi’s are
also random and independent. Then from this, it will follow that except with negligible probability,
the inputs to f3 will be distinct. From this, we can conclude that the yi’s are essentially random
and independent.
So we will be in good shape if we can show that all of the wi’s are distinct. But the wi’s are
obtained indirectly from the random function f1, and so with some care, one can indeed argue that
the wi will be distinct, except with negligible probability. 2
Proof. Let Abe an eﬃcient BC adversary that plays Attack Game 4.1 with respect to E, and which
makes at most Q queries to its challenger. We want to show that BC adv[A,E] is negligible. To do
this, we ﬁrst show that PRF adv[A,E] is negligible, and the result will then follow from the PRF
Switching Lemma (i.e., Theorem 4.4) and the assumption that N is super-poly.
To simplify things a bit, we replace Awith an adversary A0 with the following properties:
• A0 always makes exactly Q queries to its challenger;
• A0 never makes the same query more than once;
• A0 is just as eﬃcient as A(more precisely, A0 is an elementary wrapper around A);
• PRFadv[A0,E] = PRFadv[A,E].
Adversary A0 simply runs the same protocol as A; however, it keeps a table of query/response
pairs so as to avoid making duplicate queries; moreover, it “pads” the execution of Aif necessary,
so as to make exactly Q queries.
The overall strategy of the proof is as follows. First, we deﬁne Game 0 to be the game played
between A0 and the challenger of Experiment 0 of Attack Game 4.2 with respect to E. We then
143
deﬁne several more games: Game 1, Game 2, and Game 3. Each of these games is played between
A0 and a diﬀerent challenger; moreover, the challenger in Game 3 is equivalent to the challenger
of Experiment 1 of Attack Game 4.2. Also, for j = 0,..., 3, we deﬁne Wj to be the event that
A0 outputs 1 in Game j. We will show that for j = 1,..., 3 that the value |Pr[Wj] −Pr[Wj−1]|is
negligible, from which it will follow that
|Pr[W3] −Pr[W0]|= PRFadv[A0,E]
is also negligible.
Game 0. Let us begin by giving a detailed description of the challenger in Game 0 that is convenient
for our purposes:
k1,k2,k3 ←R K
upon receiving the ith query (ui,vi) ∈X2 (for i= 1,...,Q ) do:
wi ←ui ⊕F(k1, vi)
xi ←vi ⊕F(k2, wi)
yi ←wi ⊕F(k3, xi)
send (xi,yi) to the adversary.
Recall that the adversary A0 is guaranteed to always make Qdistinct queries (u1,v1),..., (uQ,vQ);
that is, the ( ui,vi) values are distinct as pairs, so that for i̸= j, we may have ui = uj or vi = vj,
but not both.
Game 1. We next play the “PRF card,” replacing the three functions F(k1,·),F(k2,·),F(k3,·) by
truly random functions f1,f2,f3. Intuitively, since F is a secure PRF, the adversary A0 should not
notice the diﬀerence. Our challenger in Game 1 thus works as follows:
f1,f2,f3 ←R Funs[X,X]
upon receiving the ith query (ui,vi) ∈X2 (for i= 1,...,Q ) do:
wi ←ui ⊕f1(vi)
xi ←vi ⊕f2(wi)
yi ←wi ⊕f3(xi)
send (xi,yi) to the adversary.
As discussed in Exercise 4.26, we can model the three PRFs F(k1,·),F(k2,·),F(k3,·) as a single
PRF F′, called the 3-wise parallel composition of F: the PRF F′ is deﬁned over ( K3,{1,2,3}×
X,X), and F′((k1,k2,k3),(s,x)) := F(ks,x). We can easily construct an adversary B′, just as
eﬃcient as A0, such that
|Pr[W1] −Pr[W0]|= PRFadv[B′,F′]. (4.24)
Adversary B′ simply runs A0 and outputs whatever A0 outputs; when A0 queries its challenger
with a pair ( ui,vi), adversary B′computes the response ( xi,yi) for A0 by computing
wi ←ui ⊕f′(1,vi)
xi ←vi ⊕f′(2,wi)
yi ←wi ⊕f′(3,xi).
Here, the f′denotes the function chosen by B′’s challenger in Attack Game 4.2 with respect to F′.
It is clear that B′outputs 1 with probability Pr[ W0] in Experiment 0 of that attack game, while it
outputs 1 with probability Pr[ W1] in Experiment 1, from which (4.24) follows.
144
By Exercise 4.26, there exists an adversary B, just as eﬃcient as B′, such that
PRFadv[B′,F′] = 3 ·PRFadv[B,F]. (4.25)
Game 2. We next make a purely conceptual change: we implement the random functions f2 and
f3 using the “faithful gnome” idea discussed in Section 4.4.2. This is not done for eﬃciency, but
rather, to set us up so as to be able to make (and easily analyze) a more substantive modiﬁcation
later, in Game 3. Our challenger in this game works as follows:
f1 ←R Funs[X,X]
X1,...,X Q ←R X
Y1,...,Y Q ←R X
upon receiving the ith query (ui,vi) ∈X2 (for i= 1,...,Q ) do:
wi ←ui ⊕f1(vi)
x′
i ←Xi; if wi = wj for some j <ithen x′
i ←x′
j; xi ←vi ⊕x′
i
y′
i ←Yi; if xi = xj for some j <ithen y′
i ←y′
j; yi ←wi ⊕y′
i
send (xi,yi) to the adversary.
The idea is that the value x′
i represents f2(wi). By default, x′
i is equal to the random value Xi;
however, the boxed code over-rides this default value if wi is the same as wj for some j < i.
Similarly, the value y′
i represents f3(xi). By default, y′
i is equal to the random value Yi, and the
boxed code over-rides the default if necessary.
Since the challenger in Game 2 is completely equivalent to that of Game 1, we have
Pr[W2] = Pr[W1]. (4.26)
Game 3. We now employ the “forgetful gnome” technique, which we already saw in the proof
of Theorem 4.6. The idea is to simply eliminate the consistency checks made by the challenger in
Game 2. Here is the logic of the challenger in Game 3:
f1 ←R Funs[X,X]
X1,...,X Q ←R X
Y1,...,Y Q ←R X
upon receiving the ith query (ui,vi) ∈X2 (for i= 1,...,Q ) do:
wi ←ui ⊕f1(vi)
x′
i ←Xi; xi ←vi ⊕x′
i
y′
i ←Yi; yi ←wi ⊕y′
i
send (xi,yi) to the adversary.
Note that this description is literally the same as the description of the challenger in Game 2,
except that we have simply erased the underlined code in the latter.
For the purposes of analysis, we view Games 2 and 3 as operating on the same underlying
probability space. This probability space is determined by
• the random choices made by the adversary, which we denote by Coins, and
• the random choices made by the challenger, namely, f1, X1,...,X Q, and Y1,...,Y Q.
What diﬀers between the two games is the rule that the challenger uses to compute its responses
to the queries made by the adversary.
145
Claim 1: in Game 3, the random variables Coins ,f1,x1,y1,...,x Q,yQ are mutually independent.
To prove this claim, observe that by construction, the random variables
Coins, f 1, X 1,...,X Q, Y 1,...,Y Q
are mutually independent. Now condition on any ﬁxed values of Coins and f1. The ﬁrst query
(u1,v1) is now ﬁxed, and hence so is w1; however, in this conditional probability space, X1 and Y1
are still uniformly and independently distributed over X, and so x1 and y1 are also uniformly and
independently distributed. One continues the argument, conditioning on ﬁxed values of x1,y1 (in
addition to ﬁxed values of Coins and f1), observing that now u2,v2,and w2 are also ﬁxed, and that
x2 and y2 are uniformly and independently distributed. It should be clear how the claim follows
by induction.
Let Z1 be the event that wi = wj for some i̸= j in Game 3. Let Z2 be the event that xi = xj
for some i ̸= j in Game 3. Let Z := Z1 ∨Z2. Note that the event Z is deﬁned in terms of the
variables wi and xi values in Game 3. Indeed, the variables wi and zi may not be computed in the
same way in Games 2 and 3, and so we have explicitly deﬁned the event Z in terms of their values
in Game 3. Nevertheless, it is straightforward to see that Games 2 and 3 proceed identically if Z
does not occur. In particular:
Claim 2: the event W2 ∧¯Z occurs if and only if the event W3 ∧¯Z occurs. To prove this claim,
consider any ﬁxed values of the variables
Coins, f 1, X 1,...,X Q, Y 1,...,Y Q
for which Z does not occur. It will suﬃce to show that the output of A0 is the same in both
Games 2 and 3. Since the query ( u1,v1) depends only on Coins, we see that the variables u1,v1,
and hence also w1,x1,y1 have the same values in both games. Since the query ( u2,v2) depends
only on Coins and (x1,y1), it follows that the variables u2,v2 and hence w2 have the same values
in both games; since Z does not occur, we see w2 ̸= w1 and hence the variable x2 has the same
value in both games; again, since Z does not occur, it follows that x2 ̸= x1, and hence the variable
y2 has the same value in both games. Continuing this argument, we see that for i= 1,...,Q , the
variables ui,vi,wi,xi,yi have the same values in both games. Since the output of A0 is a function
of these variables and Coins, the output is the same in both games. That proves the claim.
Claim 2, together with the Diﬀerence Lemma (i.e., Theorem 4.7) and the Union Bound, implies
|Pr[W3] −Pr[W2]|≤ Pr[Z] ≤Pr[Z1] + Pr[Z2]. (4.27)
By the fact that x1,...,x Q are mutually independent (see Claim 1), it is obvious that
Pr[Z2] ≤Q2
2 · 1
N, (4.28)
since Z2 is the union of less than Q2/2 events, each of which occurs with probability 1 /N.
Let us now analyze the event Z1. We claim that
Pr[Z1] ≤Q2
2 · 1
N. (4.29)
146
To prove this, it suﬃces to prove it conditioned on any ﬁxed values of Coins,x1,y1,...,x Q,yQ.
If these values are ﬁxed, then so are u1,v1,...,u Q,vQ. However, by independence (see Claim 1),
the variable f1 is still uniformly distributed over Funs[ X,X] in this conditional probability space.
Now consider any ﬁxed pair of indices i,j, with i̸= j. Suppose ﬁrst that vi = vj. Then since A0
never makes the same query twice, we must have ui ̸= uj, and it is easy to see that wi ̸= wj for
any choice of f1. Next suppose that vi ̸= vj. Then the values f1(vi) and f1(vj) are uniformly and
independently distributed over Xin this conditional probability space, and
Pr[f1(vi) ⊕f1(vj) = ui ⊕uj] = 1
N
in this conditional probability space.
Thus, we have shown that in Game 3, for all pairs i,j with i̸= j,
Pr[wi = wj] ≤ 1
N
The inequality (4.29) follows from the Union Bound.
As another consequence of Claim 1, we observe that Game 3 is equivalent to Experiment 1 of
Attack Game 4.2 with respect to E. From this, together with (4.24), (4.25), (4.26), (4.27), (4.28),
and (4.29), we conclude that
PRFadv[A0,E] ≤3 ·PRFadv[B,F] + Q2
N .
Finally, applying Theorem 4.4 to the cipher E, whose data block space has size N2, we have
BCadv[A,E] ≤3 ·PRFadv[B,F] + Q2
N + Q2
2N2 .
That concludes the proof of the theorem. 2
4.6 The tree construction: from PRGs to PRFs
It turns out that given a suitable, secure PRG, one can construct a secure PRF with a technique
called the tree construction. Combining this result with the Luby-Rackoﬀ construction in Sec-
tion 4.5, we see that from any secure PRG, we can construct a secure block cipher. While this
result is of some theoretical interest, the construction is not very eﬃcient, and is not really used
in practice. However, we note that a simple generalization of this construction plays an important
role in practical schemes for message authentication; we shall discuss this in Section 6.4.2.
Our starting point is a PRG G deﬁned over (S,S2); that is, the seed space is a set S, and the
output space is the set S2 of all seed pairs. For example, G might stretch n-bit strings to 2 n-bit
strings.2 It will be convenient to write G(s) = (G0(s),G1(s)); that is, G0(s) ∈S denotes the ﬁrst
component of G(s) and G1(s) denotes the second component of G(s). From G, we shall build a
PRF F with key space S, input space {0,1}ℓ (where ℓ is an arbitrary, poly-bounded value), and
output space S.
Let us ﬁrst deﬁne the algorithm G∗, that takes as input s∈S and x= (a1,...,a n) ∈{0,1}n,
where ai ∈{0,1}for i= 1,...,n , and outputs an element t∈S, computed as follows:
2Indeed, we could even start with a PRG that stretches n bit strings to ( n+ 1)-bit strings, and then apply the
n-wise sequential construction analyzed in Theorem 3.3 to obtain a suitable G.
147
1
0
1
Figure 4.15: Evaluation tree for ℓ= 3. The highlighted path corresponds to the input x= 101.
The root is shaded to indicate it is assigned a random label. All other nodes are assigned derived
labels.
t←s
for i←1 to n do
t←Gai(t)
output t.
For s∈S and x∈{0,1}ℓ, we deﬁne
F(s,x) := G∗(s,x).
We shall call the PRF F derived from G in this way the tree construction.
It is useful to envision the bits of an input x∈{0,1}ℓ as tracing out a path through a complete
binary tree of height ℓ and with 2 ℓ leaves, which we call the evaluation tree: a bit value of 0
means branch left and a bit value of 1 means branch right. In this way, any node in the tree can
be uniquely addressed by a bit string of length at most ℓ; strings of length j ≤ℓ address nodes
at level j in the tree: the empty string addresses the root (which is at level 0), strings of length 1
address the children of the root (which are at level 1), etc. The nodes in the evaluation tree are
labeled with elements of S, using the following rule:
• the root of the tree is labeled with s;
• the label of any other node is derived from the label tof its parent as follows: if the node is
a left child, its label is G0(t), and if the node is a right child, its label is G1(t).
The value of F(s,x) is then the label on the leaf addressed by x. See Fig. 4.15.
Theorem 4.10. If Gis a secure PRG, then the PRF F obtained from Gusing the tree construction
is a secure PRF.
In particular, for every PRF adversary Athat plays Attack Game 4.2 with respect to F, and
which makes at most Q queries to its challenger, there exists a PRG adversary Bthat plays
Attack Game 3.1 with respect to G, where Bis an elementary wrapper around A, such that
PRFadv[A,F] = ℓQ·PRGadv[B,G].
148
0
0
0
0
1
1
1
0
1
0
1
1
1
Figure 4.16: Evaluation tree for Hybrid 2 with ℓ = 4. The shaded nodes are assigned random
labels, while the unshaded nodes are assigned derived labels. The highlighted paths correspond to
inputs 0000, 0011, 1010, and 1111.
Proof idea. The basic idea of the proof is a hybrid argument. We build a sequence of games,
Hybrid 0, . . . , Hybrid ℓ. Each of these games is played between a given PRF adversary, attacking
F, and a challenger whose behavior is slightly diﬀerent in each game. In Hybrid j, the challenger
builds an evaluation tree whose nodes are labeled as follows:
• nodes at levels 0 through j are assigned random labels;
• the nodes at levels j+ 1 through ℓ are assigned derived labels.
In response to a query x ∈{0,1}ℓ in Hybrid j, the challenger sends to the adversary the label of
the leaf addressed by x. See Fig. 4.16
Clearly, Hybrid 0 is equivalent to Experiment 0 of Attack Game 4.2, while Hybridℓis equivalent
to Experiment 1. Intuitively, under the assumption that G is a secure PRG, the adversary should
not be able to tell the diﬀerence between Hybrids j and j+ 1 for j = 0,...,ℓ −1. In making this
intuition rigorous, we have to be a bit careful: the evaluation tree is huge, and to build an eﬃcient
PRG adversary that attacks G, we cannot aﬀord to write down the entire tree (or even one level
of the tree). Instead, we use the fact that if the PRF adversary makes at most Q queries to its
challenger (which is a poly-bounded value), then at any level j in the evaluation tree, the paths
traced out by these Q queries touch at most Q nodes at level j (in Fig. 4.16, these would be the
ﬁrst, third, and fourth nodes at level 2 for the given inputs). The PRG adversary we construct
will use a variation of the faithful gnome idea to eﬀectively maintain the relevant random labels at
level j, as needed. 2
Proof. Let Abe an eﬃcient adversary that plays Attack Game 4.2 with respect toF. Let us assume
that Amakes at most a poly-bounded number Q of queries to the challenger.
As discussed above, we deﬁne ℓ+ 1 hybrid games, Hybrid 0, . . . , Hybridℓ, each played between
Aand a challenger. In Hybrid j, the challenger works as follows:
149
f ←R Funs[{0,1}j,S]
upon receiving a query x= (a1,...,a ℓ) ∈{0,1}ℓ from Ado:
u←(a1,...,a j), v←(aj+1,...,a ℓ)
y←G∗(f(u),v)
send y to A.
Intuitively, for u ∈{0,1}j, f(u) represents the random label at the node at level j addressed by
u. Thus, each node at level j is assigned a random label, while nodes at levels j + 1 through ℓ
are assigned derived labels. Note that in our description of this game, we do not explicitly assign
labels to nodes at levels 0 through j−1, as these labels do not aﬀect any outputs.
For j = 0,...,ℓ , let pj be the probability thatAoutputs 1 in Hybridj. As Hybrid 0 is equivalent
to Experiment 0 of Attack Game 4.2, and Hybrid ℓ is equivalent to Experiment 1, we have:
PRFadv[A,F] = |pℓ −p0|. (4.30)
Let G′ denote the Q-wise parallel composition of G, which we discussed in Section 3.4.1. G′
takes as input (s1,...,s Q) ∈SQ and outputs (G(s1),...,G (sQ)) ∈(S2)Q. By Theorem 3.2, if Gis
a secure PRG, then so is G′.
We now build an eﬃcient PRG adversary B′that attacks G′, such that
PRGadv[B′,G′] = 1
ℓ ·|pℓ −p0|. (4.31)
We ﬁrst give an overview of how B′ works. In playing Attack Game 3.1 with respect to G′, the
challenger presents to B′a vector
⃗ r= ((r10,r11),..., (rQ0,rQ1)) ∈(S2)Q. (4.32)
In Experiment 0 of the attack game, ⃗ r= G(⃗ s) for random ⃗ s∈SQ, while in Experiment 1, ⃗ ris
randomly chosen from (S2)Q. To distinguish these two experiments, B′plays the role of challenger
to Aby choosing ω∈{1,...,ℓ }at random, and uses the elements of ⃗ rto label nodes at level ω of
the evaluation tree in a consistent fashion. To do this, B′maintains a lookup table, which allows it
to associate with each preﬁx u∈{0,1}ω−1 of some query x∈{0,1}ℓ an index p, so that the children
of the node addressed by u are labeled by the seed pair ( rp0,rp1). Finally, when Aterminates and
outputs a bit, B′ outputs the same bit. As will be evident from the details of the construction of
B′, conditioned on ω= j for any ﬁxed j = 1,...,ℓ , the probability that B′outputs 1 is:
• pj−1, if B′is in Experiment 0 of its attack game, and
• pj, if B′is in Experiment 1 of its attack game.
Then by the usual telescoping sum calculation, we get (4.31).
Now the details. We implement our lookup table as an associative array Map : {0,1}∗→Z>0.
Here is the logic for B′:
upon receiving ⃗ ras in (4.32) from its challenger, B′plays the role of challenger to A, as
follows:
150
ω←R {1,...,ℓ }
initialize an empty associative array Map : {0,1}∗→Z>0
ctr ←0
upon receiving a query x= (a1,...,a ℓ) ∈{0,1}ℓ from Ado:
u←(a1,...,a ω−1), d←aω, v←(aω+1,...,a ℓ)
if u /∈Domain(Map) then
ctr ←ctr + 1, Map[u] ←ctr
p←Map[u], y←G∗(rpd,v)
send y to A.
Finally, B′outputs whatever Aoutputs.
For b = 0,1, let Wb be the event that B′ outputs 1 in Experiment b of Attack Game 3.1 with
respect to G′. We claim that for any ﬁxed j = 1,...,ℓ , we have
Pr[W0 |ω= j] = pj−1 and Pr[ W1 |ω= j] = pj.
Indeed, condition on ω= j for ﬁxed j, and consider how B′labels nodes in the evaluation tree. On
the one hand, when B′ is in Experiment 1 of its attack game, it eﬀectively assigns random labels
to nodes at level j, and the lookup table ensures that this is done consistently. On the other hand,
when B′is in Experiment 0 of its attack game, it eﬀectively assigns pseudo-random labels to nodes
at level j, which is the same as assigning random labels to the parents of these nodes at level j−1,
and assigning derived labels at level j; again, the lookup table ensures a consistent labeling.
From the above claim, equation (4.31) now follows by a familiar, telescoping sum calculation:
PRGadv[B′,G′] =
⏐⏐Pr[W1] −Pr[W0]
⏐⏐
=
⏐⏐⏐
ℓ∑
j=1
Pr[W1 |ω= j] ·Pr[ω= j] −
ℓ∑
j=1
Pr[W0 |ω= j] ·Pr[ω= j]
⏐⏐⏐
= 1
ℓ ·
⏐⏐⏐
ℓ∑
j=1
Pr[W1 |ω= j] −
ℓ∑
j=1
Pr[W0 |ω= j]
⏐⏐⏐
= 1
ℓ ·
⏐⏐⏐
ℓ∑
j=1
pj −
ℓ∑
j=1
pj−1
⏐⏐⏐
= 1
ℓ ·|pℓ −p0|.
Finally, by Theorem 3.2, there exists an eﬃcient PRG adversary Bsuch that
PRGadv[B′,G′] = Q·PRGadv[B,G]. (4.33)
The theorem now follows by combining equations (4.30), (4.31), and (4.33). 2
4.6.1 Variable length tree construction
It is natural to consider how the tree construction works on variable length inputs. Again, let G
be a PRG deﬁned over ( S,S2), and let G∗ be as deﬁned above. For any poly-bounded value ℓ we
151
deﬁne the PRF ˜F, with key space S, input space {0,1}≤ℓ, and output space S, as follows: for s∈S
and x∈{0,1}≤ℓ, we deﬁne
˜F(s,x) = G∗(s,x).
Unfortunately, ˜F is not a secure PRF. The reason is that there is a trivial extension attack.
Suppose u,v ∈{0,1}≤ℓ such that u is a proper preﬁx of v; that is, v = u∥w for some non-empty
string w. Then given u and v, along with y:= ˜F(s,u), we can easily compute F(s,v) as G∗(y,w).
Of course, for a truly random function, we could not predict its value at v, given its value at u,
and so it is easy to distinguish ˜F(s,·) from a random function.
Even though ˜F is not a secure PRF, we can still say something interesting about it. We show
that ˜F is a PRF against restricted set of adversaries called preﬁx-free adversaries.
Deﬁnition 4.5. Let F be a PRF deﬁned over (K,X≤ℓ,Y). We say that a PRF adversary Aplaying
Attack Game 4.2 with respect to F is a preﬁx-free adversary if all of its queries are non-empty
strings over X of length at most ℓ, no one of which is a proper preﬁx of another. 3 We denote A’s
advantage in winning the game by PRFpfadv[A,F]. Further, let us say that F is a preﬁx-free
secure PRF if PRFpfadv[A,F] is negligible for all eﬃcient, preﬁx-free adversaries A.
For example, if a preﬁx-free adversary issues a query for the sequence (a1,a2,a3) then it cannot
issue queries for ( a1) or for ( a1,a2).
Theorem 4.11. If G is a secure PRG, then the variable length tree construction ˜F derived from
G is a preﬁx-free secure PRF.
In particular, for every preﬁx-free adversary Athat plays Attack Game 4.2 with respect to ˜F,
and which makes at most Qqueries to its challenger, there exists a PRG adversary Bthat plays
Attack Game 3.1 with respect to G, where Bis an elementary wrapper A, such that
PRFpfadv[A, ˜F] = ℓQ·PRGadv[B,G].
Proof. The basic idea of the proof is exactly the same as that of Theorem 4.10. We sketch here the
main ideas, highlighting the diﬀerences from that proof.
Let Abe an eﬃcient, preﬁx-free adversary that plays Attack Game 4.2 with respect to ˜F.
Assume that Amakes at most Q queries to its challenger. Moreover, it will be convenient to
assume that Anever makes the same query twice. Thus, we are assuming that Anever makes two
queries, one of which is equal to, or is a preﬁx of, another. The challenger in Attack Game 4.2 will
not enforce this assumption — we simply assume that Ais playing by the rules.
As before, we view the evaluation of ˜F(s,·) in terms of an evaluation tree: the root is labeled
by s, and the labels on all other nodes are assigned derived labels. The only diﬀerence now is that
inputs to ˜F(s,·) may address internal nodes of the evaluation tree. However, the preﬁx-freeness
restriction means that no input can address a node that is an ancestor of a node addressed by a
diﬀerent input.
We again deﬁne hybrid games, Hybrid 0, . . . , Hybrid ℓ. In these games, the challenger uses an
evaluation tree labeled in exactly the same way as in the proof of Theorem 4.10: in Hybrid j, nodes
at levels 0 through j are assigned random labels, and nodes at other levels are assigned derived
labels. The challenger responds to a query xby returning the label of the node in the tree addressed
by x, which need not be a leaf. More formally, the challenger in Hybrid j works as follows:
3For sequences x = (a1 ...a s) and y = (b1 ...b t), if s ≤t and ai = bi for i = 1,...,s , then we say that x is a
preﬁx of y; moreover, if s<t , then we say x is a proper preﬁx of y.
152
f ←R Funs[{0,1}≤j,S]
upon receiving a query x= (a1,...,a n) ∈{0,1}≤ℓ from Ado:
if n<j
then y←f(x)
else u←(a1,...,a j), v←(aj+1,...,a n), y←G∗(f(u),v)
send y to A.
For j = 0,...,ℓ , deﬁne pj to be the probability that Aoutputs 1 in Hybrid j. As the reader may
easily verify, we have
PRFpfadv[A, ˜F] = |pℓ −p0|.
Next, we deﬁne an eﬃcient PRG adversary B′that attacks the Q-wise parallel composition G′
of G, such that
PRGadv[B′,G′] = 1
ℓ ·|pℓ −p0|.
Adversary B′runs as follows:
upon receiving ⃗ ras in (4.32) from its challenger, B′plays the role of challenger to A, as
follows:
ω←R {1,...,ℓ }
initialize an empty associative array Map : {0,1}∗→Z>0
ctr ←0
upon receiving a query x= (a1,...,a n) ∈{0,1}≤ℓ from Ado:
if n<ω then
(∗) y←R S
else
u←(a1,...,a ω−1), d←aω, v←(aω+1,...,n )
if u /∈Domain(Map) then
ctr ←ctr + 1, Map[u] ←ctr
p←Map[u], y←G∗(rpd,v)
send y to A.
Finally, B′outputs whatever Aoutputs.
For b = 0,1, let Wb be the event that B′ outputs 1 in Experiment b of Attack Game 4.2 with
respect to G′. It is not too hard to see that for any ﬁxed j = 1,...,ℓ , we have
Pr[W0 |ω= j] = pj−1 and Pr[ W1 |ω= j] = pj.
Indeed, condition on ω= j for ﬁxed j, and consider how B′labels nodes in the evaluation tree. At
the line marked (∗), B′assigns random labels to all nodes in the evaluation tree at levels 0 through
j−1, and the assumption that Anever makes the same query twice guarantees that these labels
are consistent (the same node does not receive two diﬀerent labels at diﬀerent times). Now, on the
one hand, when B′ is in Experiment 1 of its attack game, it eﬀectively assigns random labels to
nodes at level j as well, and the lookup table ensures that this is done consistently. On the other
hand, when B′is in Experiment 0 of its attack game, it eﬀectively assigns pseudo-random labels to
nodes at level j, which is the same as assigning random labels to the parents of these nodes at level
153
j −1; the preﬁx-freeness assumption ensures that none of these parent nodes are inconsistently
assigned random labels at the line marked ( ∗).
The rest of the proof goes through as in the proof of Theorem 4.10. 2
4.7 The ideal cipher model
Block ciphers are used in a variety of cryptographic constructions. Sometimes it is impossible
or diﬃcult to prove a security theorem for some of these constructions under standard security
assumptions. In these situations, a heuristic technique — called the ideal cipher model — is
sometimes employed. Roughly speaking, in this model, the security analysis is done by treating
the block cipher as if it were a family of random permutations. If E= (E,D) is a block cipher
deﬁned over (K,X), then the family of random permutations is {Πk }k ∈K, where each Πk is a truly
random permutation on X, and the Π k ’s collectively are mutually independent. These random
permutations are much too large to write down and cannot be used in a real construction. Rather,
they are used to model a construction based on a real block cipher, to obtain a heuristic security
argument for a given construction. We stress the heuristic nature of the ideal cipher model: while
a proof of security in this model is better than nothing, it does not rule out an attack by an
adversary that exploits the design of a particular block cipher, even one that is secure in the sense
of Deﬁnition 4.1.
4.7.1 Formal deﬁnitions
Suppose we have some type of cryptographic scheme Swhose implementation makes use of a block
cipher E= (E,D) deﬁned over ( K,X). Moreover, suppose the scheme Sevaluates E at various
inputs (k ,a) ∈K×X , and D at various inputs ( k ,b) ∈K×X , but does not look at the internal
implementation of E. In this case, we say that Suses Eas an oracle.
We wish to analyze the security of S. Let us assume that whatever security property we are
interested in, say “property X,” is modeled (as usual) as a game between a challenger (speciﬁc
to property X) and an arbitrary adversary A. Presumably, in responding to certain queries, the
challenger computes various functions associated with the scheme S, and these functions may in
turn require the evaluation of E and/or D at certain points. This game deﬁnes an advantage
Xadv[A,S], and security with respect to property X means that this advantage should be negligible
for all eﬃcient adversaries A.
If we wish to analyze Sin the ideal cipher model, then the attack game deﬁning security is
modiﬁed so that Eis eﬀectively replaced by a family of random permutations{Πk }k ∈K, as described
above, to which both the adversary and the challenger have oracle access. More precisely, the game
is modiﬁed as follows.
• At the beginning of the game, the challenger chooses Π k ∈Perms[K] at random, for each
k ∈K.
• In addition to its standard queries, the adversary Amay submit ideal cipher queries . There
are two types of queries: Π-queries and Π −1-queries.
– For a Π-query, the adversary submits a pair ( k ,a) ∈K×X , to which the challenger
responds with Πk (a).
154
– For a Π−1-query, the adversary submits a pair ( k ,b) ∈K×X , to which the challenger
responds with Π−1
k (b).
The adversary may make any number of ideal cipher queries, arbitrarily interleaved with
standard queries.
• In processing standard queries, the challenger performs its computations using Πk (a) in place
of E(k ,a) and Π−1
k (b) in place of D(k ,b).
The adversary’s advantage is deﬁned using the same rule as before, but is denoted X icadv[A,S] to
emphasize that this is an advantage in the ideal cipher model . Security in the ideal cipher model
means that Xicadv[A,S] should be negligible for all eﬃcient adversaries A.
It is important to understand the role of the ideal cipher queries. Essentially, they model the
ability of an adversary to make “oﬄine” evaluations of E and D.
Ideal permutation model. Some constructions, like Even-Mansour (discussed below), make
use of a permutation π : X →X, rather than a block cipher. In the security analysis, one might
heuristically model π as a random permutation Π, to which all parties in the attack game have
oracle access to Π and Π −1. We call this the ideal permutation model . One can view this as a
special case of the ideal cipher model by simply deﬁning Π = Π k 0 for some ﬁxed, publicly available
key k 0 ∈K.
4.7.2 Exhaustive search in the ideal cipher model
Let (E,D) be a block cipher deﬁned over (K,X) and let kbe some random secret key in K. Suppose
an adversary is able to intercept a small number of input/output pairs ( xi,yi) generated using k:
yi = E(k,xi) for all i= 1,...,Q .
The adversary can now recover k by trying all possible keys in k ∈K until a key k satisfying
yi = E(k ,xi) for all i = 1 ,...,Q is found. For block ciphers used in practice it is likely that
this k is equal to the secret key k used to generate the given pairs. This exhaustive search
over the key space recovers the block-cipher secret-key in time O(|K|) using a small number of
input/output pairs. We analyze the number of input/output pairs needed to mount a successful
attack in Theorem 4.12 below.
Exhaustive search is the simplest example of a key-recovery attack. Since we will present a
number of key-recovery attacks, let us ﬁrst deﬁne the key-recovery attack game in more detail. We
will primarily use the key-recovery game as means of presenting attacks.
Attack Game 4.4 (key-recovery). For a given block cipher E= (E,D), deﬁned over ( K,X),
and for a given adversary A, deﬁne the following game:
• The challenger picks a random k←R K.
• Aqueries the challenger several times. For i= 1,2,..., the ith query consists of a
message xi ∈M. The challenger, given xi, computes yi ←R E(k,xi), and gives yi
to A.
• Eventually Aoutputs a candidate key k ∈K.
155
We say that Awins the game if k = k. We let KRadv[A,E] denote the probability that Awins the
game. 2
The key-recovery game extends naturally to the ideal cipher model, where E(k ,a) = Πk (a) and
D(k ,b) = Π−1
k (b), and {Πk }k ∈K is a family of independent random permutations. In this model,
we allow the adversary to make arbitrary Π- and Π −1-queries, in addition to its standard queries
to E(k,·). We let KR icadv[A,E] denote the adversary’s key-recovery advantage when Eis an ideal
cipher.
It is worth noting that security against key-recovery attacks does not imply security in the
sense of indistinguishability (Deﬁnition 4.1). The simplest example is the constant block cipher
E(k,x) = xfor which key-recovery is not possible (the adversary obtains no information about k),
but the block cipher is easily distinguished from a random permutation.
Exhaustive search. The following theorem bounds the number of input/output pairs needed
for exhaustive search, assuming the cipher is an ideal cipher. For real-world parameters, taking
Q= 3 in the theorem is often suﬃcient to ensure success.
Theorem 4.12. Let E = ( E,D) be a block cipher deﬁned over (K,X). Then there exists an
adversary AEX that plays Attack Game 4.4 with respect to E, modeled as an ideal cipher, making
Q standard queries and Q|K|ideal cipher queries, such that
KRicadv[AEX,E] ≥(1 −ϵ) where ϵ:= |K|
(|X|− Q)Q (4.34)
Proof. In the ideal cipher model, we are modeling the block cipher E= (E,D) as a family {Πk }k ∈K
of random permutations on X. In Attack Game 4.4, the challenger chooses k ∈K at random. An
adversary may make standard queries to obtain the value E(k,x) = Πk(x) at points x∈X of his
choosing. An adversary may also make ideal cipher queries, obtaining the values Πk (a) and Π−1
k (b)
for points k ∈K and a,b ∈X of his choosing. These ideal cipher queries correspond to “oﬄine”
evaluations of E and D.
Our adversary AEX works as follows:
let {x1,...,x Q}be an arbitrary set of distinct messages in X
for i= 1,...,Q do:
make a standard query to obtain yi := E(k,xi) = Πk(xi)
for each k ∈K do:
for i= 1,...,Q do:
make an ideal cipher query to obtain bi := Πk (xi)
if yi = bi for all i= 1,...,Q then
output k and terminate
Let k be the challenger’s secret-key. We show that AEX outputs k with probability at least 1 −ϵ,
with ϵ deﬁned as in (4.34). Since AEX tries all keys, this amounts to showing that the probability
that there is more than one key consistent with the given ( xi,yi) pairs is at most ϵ. We shall show
that this holds for every possible choice of k, so for the remainder of the proof, we shall view k as
ﬁxed. We shall also view x1,...,x Q as ﬁxed, so all the probabilities are with respect to the random
permutations Πk for k ∈K.
156
For each k ∈K, let Wk be the event that yi = Π k (xi) for all i = 1 ,...,Q . Note that by
deﬁnition, Wk occurs with probability 1. Let W be the event that Wk occurs for some k ̸= k. We
want to show that Pr[W] ≤ϵ.
Fix k ̸= k. Since the permutation Π k is chosen independently of the permutation Π k , we know
that
Pr[Wk ] = 1
|X|· 1
|X|− 1 ··· 1
|X|− Q+ 1 ≤
( 1
|X|− Q
)Q
As this holds for all k ̸= k, the result follows from the union bound. 2
4.7.2.1 Security of the 3Econstruction
The attack presented in Theorem 4.2 works equally well against the 3Econstruction. The size of the
key space is |K|3, but one obtains a “meet in the middle” key recovery algorithm that runs in time
O
(
|K|2 ·Q
)
. For Triple-DES this algorithm requires more than 2 2×56 evaluations of Triple-DES,
which is far beyond our computing power.
One wonders whether better attacks against 3 Eexist. When Eis an ideal cipher we can prove
a lower bound on the amount of work needed to distinguish 3 Efrom a random permutation.
Theorem 4.13. Let E = ( E,D) be an ideal block cipher deﬁned over (K,X), and consider an
attack against the 3Econstruction in the ideal cipher model. If Ais an adversary that makes at
most Q queries (including both standard and ideal cipher queries) in the ideal cipher variant of
Attack Game 4.1, then
BCicadv[A,3E] ≤C1L Q2
|K|3 + C2
Q2/3
|K|2/3|X|1/3 + C3
1
|K|,
where L:= max(|K|/|X|,log2|X|), and C1,C2,C3 are constants (that do not depend on Aor E).
The statement of the theorem is easier to understand if we assume that |K|≤|X| , as is the case
with DES. In this case, the bound can be restated as
BCicadv[A,3E] ≤Clog2 |X| Q2
|K|3 ,
for a constant C. Ignoring the log X term, this says that an adversary must make roughly |K|1.5
queries to obtain a signiﬁcant advantage (say, 1/4). Compare this to the meet-in-the-middle attack.
To achieve a signiﬁcant advantage, that adversary must make roughly |K|2 queries. Thus, meet-in-
the-middle attack may not be the most powerful attack.
To conclude our discussion of Triple-DES, we note that the 3 Econstruction does not always
strengthen the cipher. For example, if E = ( E,D) is such that the set of |K| permutations
{E(k ,·) : k ∈ K}is a group, then 3 E would be no more secure than E. Indeed, in this case
π:= E3((k1,k2,k3),·) is identical to E(k,·) for some k∈K. Consequently, distinguishing 3 Efrom
a random permutation is no harder than doing so for E. Of course, block ciphers used in practice
are not groups (as far as we know).
157
4.7.3 The Even-Mansour block cipher and the EX construction
Let X= {0,1}n. Let π : X→X be a permutation and let π−1 be its inverse function. Even and
Mansour deﬁned the following simple block cipher EEM = (E,D) deﬁned over (X2,X):
E
(
(P1,P2), x
):= π(x⊕P1) ⊕P2 and D
(
(P1,P2), y
):= π−1(y⊕P2) ⊕P1 (4.35)
How do we analyze the security of this block cipher? Clearly for some π’s this construction is
insecure, for example when π is the identity function. For what π is EEM a secure block cipher?
The only way we know to analyze security of EEM is by modeling π as a random permutation
Π on the set X(i.e., in the ideal cipher model using a ﬁxed key). We show in Theorem 4.14 below
that in the ideal cipher model, for all adversaries A:
BCicadv[EEM,A] ≤2QsQic
|X| (4.36)
where Qs is the number of queries Amakes to EEM and Qic is the number of queries Amakes to Π
and Π−1. Hence, the Even-Mansour block cipher is secure (in the ideal cipher model) whenever |X|
is suﬃciently large. Exercise 4.21 shows that the bound (4.36) is tight. We discuss more attacks
on Even-Mansour in Exercise 18.20.
The Even-Mansour security theorem (Theorem 4.14) does not require the keys P1 and P2 to
be independent. In fact, the bounds in (4.36) remain unchanged if we set P1 = P2 so that the key
for EEM is a single element of X. However, we note that if one leaves out either of P1 or P2, the
construction is completely insecure (see Exercise 4.20).
Iterated Even-Mansour and AES. Looking back at our description of AES (Fig. 4.11) one
observes that the Even-Mansour cipher looks a lot like one round of AES where the round function
ΠAES plays the role of π. Of course one round of AES is not a secure block cipher: the bound
in (4.36) does not imply security because Π AES is not a random permutation.
Suppose one replaces each occurrence of Π AES in Fig. 4.11 by a diﬀerent permutation: one
function for each round of AES. The resulting structure, called iterated Even-Mansour, can be
analyzed in the ideal cipher model and the resulting security bounds are better than those stated
in (4.36).
These results suggest a theoretical justiﬁcation for the AES structure in the ideal cipher model.
The EX construction and DESX. If we apply the Even-Mansour construction to a full-ﬂedged
block cipher E= (E,D) deﬁned over ( K,X), we obtain a new block cipher called EX = (EX,DX)
where
EX
(
(k,P1,P2), x
):= E(k, x⊕P1) ⊕P2 , DX
(
(k,P1,P2), y
):= D(k, y⊕P2) ⊕P1. (4.37)
This new cipher EX has a key space K×X 2 which can be much larger than the key space for the
underlying cipher E.
Theorem 4.14 below shows that — in the ideal cipher model — this larger key space translates to
better security: the maximum advantage against EX is much smaller than the maximum advantage
against E, whenever |X|is suﬃciently large.
Applying EX to the DES block cipher gives an eﬃcient method to immunize DES against
exhaustive search attacks. With P1 = P2 we obtain a block cipher called DESX whose key size
158
is 56 + 64 = 120 bits: enough to resist exhaustive search. Theorem 4.14 shows that attacks in the
ideal cipher model on the resulting cipher are impractical. Since evaluating DESX requires only
one call to DES, the DESX block cipher is three times faster than the Triple-DES block cipher and
this makes it seem as if DESX is the preferred way to strengthen DES. However, non black-box
attacks like diﬀerential and linear cryptanalysis still apply to DESX whereas they are ineﬀective
against Triple-DES. Consequently, DESX should not be used in practice.
4.7.4 Proof of the Even-Mansour and EX theorems
We shall prove security of the Even-Mansour block cipher (4.35) in the ideal permutation model
and of the EX construction (4.37) in the ideal cipher model.
We prove their security in a single theorem below. Taking a single-key block cipher (i.e.,|K|= 1)
proves security of Even-Mansour in the ideal permutation model. Taking a block cipher with a
larger key space proves security of EX. Note that the pads P1 and P2 need not be independent and
the theorem holds if we set P2 = P1.
Theorem 4.14. Let E = (E,D) be a block cipher deﬁned over (K,X). Let EX = (EX,DX) be
the block cipher derived from E as in construction (4.37), where P1 and P2 are each uniformly
distributed over a subset X′ of X. If we model Eas an ideal cipher, and if Ais an adversary in
Attack Game 4.1 for EX that makes at most Qs standard queries (i.e., EX-queries) and Qic ideal
cipher queries (i.e., Π- or Π−1-queries), then we have
BCicadv[A,EX] ≤2QsQic
|K||X′|. 2 (4.38)
To understand the security beneﬁt of the EX construction consider the following: modeling Eas
an ideal cipher gives BC icadv[A,E] ≤Qic/|K|for all A. Hence, Theorem 4.14 shows that, in the
ideal cipher model, applying EX to Eshrinks the maximum advantage by a factor of 2 Qs/|X′|.
The bounds in Theorem 4.14 are tight: there is an adversary Athat achieves the advantage
shown in (4.38); see Exercise 4.21. The advantage of this Ais unchanged even when P1 and P2 are
chosen independently. Therefore, we might as well always choose P2 = P1.
We also note that it is actually no harder to prove that EX is a strongly secure block cipher (see
Section 4.1.3) in the ideal cipher model, with exactly the same security bounds as in Theorem 4.14.
Proof idea. The basic idea is to show that the ideal cipher queries and the standard queries do not
interact with each other, except with probability as bounded in (4.38). Indeed, to make the two
types of queries interact with each other, the adversary has to make
(k = k and a = x⊕P1) or ( k = k and b = y⊕P2)
for some input/output pair (x,y) corresponding to a standard query and some input/output triple
(k ,a,b) corresponding to an ideal cipher query. Essentially, the adversary will have to simultane-
ously guess the random key k as well as one of the random pads P1 or P2.
Assuming there are no such interactions, we can eﬀectively realize all of the standard queries
as Π(x⊕P1) ⊕P2 using a random permutation Π that is independent of the random permutations
used to realize the ideal cipher queries. But Π ′(x) := Π(x⊕P1) ⊕P2 is just a random permutation.
Before giving a rigorous proof of Theorem 4.14, we present a technical lemma, called the Do-
main Separation Lemma , that will greatly simplify the proof, and is useful in analyzing other
constructions.
159
To motivate the lemma, consider the following two experiments. In the one experiment, called
the “split experiment”, an adversary has oracle access to two random permutations Π 1,Π2 on a
set X. The adversary can make a series of queries, each of the form ( µ,d, z), where µ ∈{1,2}
speciﬁes which of the two permutations to evaluate, d∈{±1}speciﬁes the direction to evaluate the
permutation, and z ∈X the input to the permutation. On such a query, the challenger responds
with z′:= Πd
µ(z). Another experiment, called the “coalesced experiment”, is exactly the same as
the split experiment, except that there is only a single permutation Π, and the challenger answers
the query (µ,d, z) with z′:= Πd(z), ignoring completely the index µ. The question is: under what
condition can the adversary distinguish between these two experiments?
Obviously, if the adversary can submit a query (1,+1,a) and a query (2,+1,a), then in the split
experiment, the results will almost certainly be diﬀerent, while in the coalesced experiment, they
will surely be the same. Another type of attack is possible as well: the adversary could make a query
(1,+1,a) obtaining b, and then submit the query (2,−1,b), obtaining a′. In the split experiment, a
and a′will almost certainly be diﬀerent, while in the coalesced experiment, they will surely be the
same. Besides these two examples, one could get two more examples which reverse the direction of
all the queries. The Domain Separation Lemma will basically say that unless the adversary makes
queries of one of these four types, he cannot distinguish between these two experiments.
Of course, the Domain Separation Lemma is only useful in contexts where the adversary is
somehow constrained so that he cannot freely make queries of his choice. Indeed, we will only use
it inside of the proof of a security theorem where the “adversary” in the Domain Separation Lemma
comprises components of a challenger and an adversary in a more interesting attack game.
In the more general statement of the lemma, we replace Π1 and Π2 by a family of permutations
of permutations {Πµ}µ∈U, and we replace Π by a family {Πν}ν∈V. We also introduce a function
f : U → V that speciﬁes how several permutations in the split experiment are collapsed into
one permutation in the coalesced experiment: for each ν ∈V, all the permutations Π µ in the
split experiment for which f(µ) = ν are collapsed into the single permutation Πν in the coalesced
experiment.
In the generalized version of the distinguishing game, if the adversary makes a query ( µ,d, z),
then in the split experiment, the challenger responds with z′ := Π d
µ(z), while in the coalesced
experiment, the challenger responds with z′ := Π d
f(µ)(z). In the split experiment, we also keep
track of the subset of the domains and ranges of the permutations that correspond to actual
queries made by the adversary in the split experiment. That is, we build up sets Dom (d)
µ for each
µ ∈U and d ∈±1, so that a ∈Dom(+1)
µ if and only if the adversary issues a query of the form
(µ,+1,a) or a query of the form ( µ,−1,b) that yields a. Similarly, b ∈Dom(−1)
µ if and only if the
adversary issues a query of the form ( µ,−1,b) or a query of the form ( µ,+1,a) that yields b. We
call Dom(+1)
µ the sampled domain of Πµ and Dom(−1)
µ the sampled range of Πµ.
Attack Game 4.5 (domain separation). Let U,V, X be ﬁnite, nonempty sets, and let f :
U →V be a function. For a given adversary A, we deﬁne two experiments, Experiment 0 and
Experiment 1. For b= 0,1, we deﬁne:
Experiment b:
• For each µ ∈U, and each ν ∈V the challenger sets Π µ ←R Perms[X] and Πν ←R Perms[X]
Also, for each µ∈U and d∈{±1}the challenger sets Dom(d)
µ ←∅.
• The adversary submits a sequence of queries to the challenger.
160
For i= 1,2,..., the ith query is ( µi,di,zi) ∈U ×{±1}×X .
If b= 0: the challenger sets z′
i ←Π
di
f(µi)(zi).
If b = 1: the challenger sets z′
i ←Πdi
µi(zi); the challenger also adds the value zi to the set
Dom(di)
µi , and adds the value z′
i to the set Dom (−di)
µi .
In either case, the challenger then sends z′
i to the adversary.
• Finally, the adversary outputs a bit ˆb∈{0,1}.
For b = 0 ,1, let Wb be the event that Aoutputs 1 in Experiment b. We deﬁne A’s do-
main separation distinguishing advantage as |Pr[W0] −Pr[W1]|. We also deﬁne the domain
separation failure event Z to be the event that in Experiment 1 , at the end of the game we
have Dom(d)
µ ∩Dom(d)
µ′ ̸= ∅for some d ∈{±1}and some pair of distinct indices µ,µ′ ∈U with
f(µ) = f(µ′). Finally, we deﬁne the domain separation failure probability to be Pr[Z]. 2
Experiment 1 in the above game is the split experiment and Experiment 0 is the coalesced
experiment.
Theorem 4.15 (Domain Separation Lemma). In Attack Game 4.5, an adversary’s domain
separation distinguishing advantage is bounded by the domain separation failure probability.
In applying the Domain Separation Lemma, we will typically analyze some attack game in
which permutations start out as coalesced, and then force them to be separated. We can bound
the impact of this change on the outcome of the attack by analyzing the domain separation failure
probability in the attack game with the split permutations.
Before proving the Domain Separation Lemma, it is perhaps more instructive to see how it is
used in the proof of Theorem 4.14.
Proof of Theorem 4.14. Let Abe an adversary as in the statement of the theorem. For b = 0,1
let pb be the probability that Aoutputs 1 in Experiment b of the block cipher attack game in the
ideal cipher model (Attack Game 4.1). So by deﬁnition we have
BCicadv[A,EX] = |p0 −p1|. (4.39)
We shall prove the theorem using a sequence of two games, applying the Domain Separation
Lemma.
Game 0. We begin by describing Game 0, which corresponds to Experiment 0 of the block cipher
attack game in the ideal cipher model. Recall that in this model, we have a family of random
permutations, and the encryption function is implemented in terms of this family. Also recall that
in addition to standard queries that probe the function Ek(·), the adversary may also probe the
random permutations.
Initialize:
for each k ∈K, set Πk ←R Perms[X]
k←R K, choose P1,P2
161
standard EX-query x:
1. a ←x⊕P1
2. b ←Πk(a)
3. y←b ⊕P2
4. return y
ideal cipher Π-query k ,a:
1. b ←Πk (a)
2. return b
ideal cipher Π−1-query k ,b:
1. a ←Π−1
k (b)
2. return a
Let W0 be the event that Aoutputs 1 at the end of Game 0. It should be clear from construction
that
Pr[W0] = p0. (4.40)
Game 1. In this game, we apply the Domain Separation Lemma. The basic idea is that we
will declare “by ﬁat” that the random permutations used in processing the standard queries are
independent of the random permutations used in processing ideal cipher queries. Eﬀectively, each
permutation Πk gets split into two independent permutations: Π std,k , which is used by the chal-
lenger in responding to standard EX-queries, and Πic,k , which is used in responding to ideal cipher
queries. In detail (changes from Game 0 are highlighted):
Initialize:
for each k ∈K, set Πstd,k ←R Perms[X] and Πic,k ←R Perms[X]
k←R K, choose P1,P2
standard EX-query x:
1. a ←x⊕P1
2. b ←Πstd,k(a) / / add a to sampled domain of Πstd,k, add b to sampled range of Πstd,k
3. y←b ⊕P2
4. return y
ideal cipher Π-query k ,a:
1. b ←Πic,k (a) / / add a to sampled domain of Πic,k , add b to sampled range of Πic,k
2. return b
ideal cipher Π−1-query k ,b:
1. a ←Π−1
ic,k (b) / / add a to sampled domain of Πic,k , add b to sampled range of Πic,k
2. return a
Let W1 be the event that Aoutputs 1 at the end of Game 1. Let Z be the event that in Game 1
there exists k ∈K, such that the sampled domains of Πic,k and Πstd,k overlap or the sampled ranges
162
of Πic,k and Πstd,k overlap. The Domain Separation Lemma says that
|Pr[W0] −Pr[W1]|≤ Pr[Z]. (4.41)
In applying the Domain Separation Lemma, the “coalescing function” f maps from {std,ic}×K
to K, sending the pair ( ·,k ) to k . Observe that the challenger only makes queries to Π k, where k
is the secret key, and so such an overlap can occur only at k = k. Also observe that in Game 1,
the random variables k, P1, and P2 are completely independent of the adversary’s view.
So the event Z occurs if and only if for some input/output triple ( k ,a,b) triple arising from a
Π- or Π−1-query, and for some input/output pair ( x,y) arising from an EX-query, we have
(k = k and a = x⊕P1) or ( k = k and b = y⊕P2). (4.42)
Using the union bound, we can therefore bound Pr[ Z] as a sum of probabilities of 2 QsQic events,
each of the form k = k and a = x⊕P1, or of the form k = k and b = y⊕P2. By independence,
since kis uniformly distributed over a set of size |K|, and each of P1 and P2 is uniformly distributed
over a set of size |X′|, each such event occurs with probability at most 1 /(|K||X′|). It follows that
Pr[Z] ≤2QsQic
|K||X′|. (4.43)
Finally, observe that Game 1 is equivalent to Experiment 1 of the block cipher attack game in
the ideal cipher model: the EX-queries present to the adversary the random permutation Π ′(x) :=
Πstd,k(x⊕P1) ⊕P2 and this permutation is independent of the random permutations used in the
Π- and Π−1-queries. Thus,
Pr[W1] = p1. (4.44)
The bound (4.38) now follows from (4.39), (4.40), (4.41), (4.43), and (4.44). This completes the
proof of the theorem. 2
Finally, we turn to the proof of the Domain Separation Lemma, which is a simple (if tedious)
application of the Diﬀerence Lemma and the “forgetful gnome” technique.
Proof of Theorem 4.15. We deﬁne a sequence of games.
Game 0. This game will be equivalent to the coalesced experiment in Attack Game 4.5, but
designed in a way that will facilitate the analysis.
In this game, the challenger maintains various sets Π of pairs ( a,b). Each set Π represents a
function that can be extended to a permutation on X that sends a to b for every (a,b) in Π. We
call such a set Π a partial permutation on X. Deﬁne
Domain(Π) = {a ∈X : (a,b) ∈Π for some b ∈X} ,
Range(Π) = {b ∈X : (a,b) ∈Π for some a ∈X} .
Also, for a ∈Domain(Π), deﬁne Π( a) to be the unique b such that ( a,b) ∈Π. Likewise, for
b ∈Range(Π), deﬁne Π−1(b) to be the unique a such that (a,b) ∈Π.
Here is the logic of the challenger in Game 0:
Initialize:
for each ν ∈V, initialize the partial permutation Πν ←∅
163
Process query (µ,+1,a):
1. if a ∈Domain(Πf(µ)) then b ←Πf(µ)(a), return b
2. b ←R X\ Range(Πf(µ))
3. add ( a,b) to Πf(µ)
4. return b
Process query (µ,−1,b):
1. if b ∈Range(Πf(µ)) then a ←Π
−1
f(µ)(b), return a
2. a ←R X\ Domain(Πf(µ))
3. add ( a,b) to Πf(µ)
4. return a
This game is clearly equivalent to the coalesced experiment in Attack Game 4.5. Let W0 be the
event that the adversary outputs 1 in this game.
Game 1. Now we modify this game to get an equivalent game, but it will facilitate the application
of the Diﬀerence Lemma in moving to the next game. For µ,µ′ ∈ U, let us write µ ∼ µ′ if
f(µ) = f(µ′). This is an equivalence relation on U, and we write [ µ] for the equivalence class
containing µ.
Here is the logic of the challenger in Game 1:
Initialize:
for each µ∈U, initialize the partial permutation Π µ ←∅
Process query (µ,+1,a):
1a. if a ∈Domain(Πµ) then b ←Πµ(a), return b
∗1b. if a ∈Domain(Πµ′) for some µ′∈[µ] then b ←Πµ′(a), return b
2a. b ←R X\ Range(Πµ)
∗2b. if b ∈⋃
µ′∈[µ] Range(Πµ′) then b ←R X\ ⋃
µ′∈[µ] Range(Πµ′)
3. add ( a,b) to Πµ
4. return b
Process query (µ,−1,b):
1a. if b ∈Range(Πµ) then a ←Π−1
µ (b), return a
∗1b. if b ∈Range(Πµ′) for some µ′∈[µ] then a ←Π−1
µ′ (b), return a
2a. a ←R X\ Domain(Πµ)
∗2b. if a ∈⋃
µ′∈[µ] Domain(Πµ′) then a ←R X\ ⋃
µ′∈[µ] Domain(Πµ′)
3. add ( a,b) to Πµ
4. return a
Let W1 be the event that the adversary outputs 1 in this game.
It is not hard to see that the challenger’s behavior in this game is equivalent to that in Game 0,
and so Pr[ W0] = Pr[W1]. The idea is that for every ν ∈f(U) ⊆V, the partial permutation Πν in
Game 0 is partitioned into a family of disjoint partial permutations {Πµ}µ∈f−1(ν), so that
Πν =
⋃
µ∈f−1(ν)
Πµ,
164
and
Domain(Πµ) ∩Domain(Πµ′) = ∅ and Range(Π µ) ∩Range(Πµ′) = ∅
for all µ,µ′∈f−1(ν) with µ̸= µ′. (4.45)
Game 2. Now we simply delete the lines marked with a “ ∗” in Game 1. Let W2 be the event that
the adversary outputs 1 in this game.
It is clear that this game is equivalent to the split experiment in Attack Game 4.5, and so
|Pr[W2] −Pr[W1]|is equal to the adversary’s advantage in Attack Game 4.5. We want to use the
Diﬀerence Lemma to bound |Pr[W2] −Pr[W1]|. To make this entirely rigorous, one models both
games as operating on the same underlying probability space: we deﬁne a collection of random
variables representing the coins of the adversary, as well as the various random samples from
diﬀerent subsets of X made by the challenger. These random variables completely describe both
Games 1 and 2: the only diﬀerence between the two games are the deterministic computation rules
that determine the outcomes. Deﬁne Z be to be the event that at the end of Game 2, the condition
(4.45) does not hold. One can verify that Games 1 and 2 proceed identically unless Z holds, so
by the Diﬀerence Lemma, we have |Pr[W2] −Pr[W1]|≤ Pr[Z]. Moreover, it is clear that Pr[ Z] is
precisely the failure probability in Attack Game 4.5. 2
4.8 A fun application: comparing information without revealing
it
In this section we describe an important application for PRFs called sub-key derivation. Alice
and Bob have a shared key k for a PRF. They wish to generate a sequence of shared keys k1,k2,...
so that key number ican be computed without having to compute all earlier keys. Naturally, they
set ki := F(k,i) where F is a secure PRF whose input space is {1,2,...,B }for some bound B.
The generated sequence of keys is indistinguishable from random keys.
As a fun application of this, consider the following problem: Alice is on vacation at the Squaw
valley ski resort and wants to know if her friend Bob is also there. If he is they could ski together.
Alice could call Bob and ask him if he is on the slopes, but this would reveal to Bob where she is
and Alice would rather not do that. Similarly, Bob values his privacy and does not want to tell
Alice where he is, unless Alice happens to be close by.
Abstractly, this problem can be phrased as follows: Alice has a number a ∈Zp and Bob has
a number b∈Zp for some prime p. These numbers indicate their approximate positions on earth.
Think of dividing the surface of the earth into p squares and the numbers a and b indicate what
square Alice and Bob are currently at. If Bob is at the resort then a= b, otherwise a̸= b.
Alice wants to learn if a = b; however, if a ̸= b then Alice should learn nothing else about b.
Bob should learn nothing at all about a.
In a later chapter we will see how to solve this exact problem. Here, we make the problem
easier by allowing Alice and Bob to interact with a server, Sam, that will help Alice learn if a= b,
but will itself learn nothing at all. The only assumption about Sam is that it does not collude with
Alice or Bob, that is, it does not reveal private data that Alice or Bob send to it. Clearly, Alice
and Bob could send a and b to Sam and he will tell Alice if a= b, but then Sam would learn both
a and b. Our goal is that Sam learns nothing, not even if a= b.
To describe the basic protocol, suppose Alice and Bob have a shared secret key ( k0,k1) ∈Z2
p.
Moreover, Alice and Bob each have a private channel to Sam. The protocol for comparing aand b
165
Alice Server Bob
input: a Sam input: b
r, x b ←r(b+ k0) + k1←−−−−−−−−−−−−−−−−−−−−−−−−r←R Zpxa ←a+ k0−−−−−−−−−−−−−−−−−−−−−−−−→
x+ k1
?= 0
x←rxa −xb←−−−−−−−−−−−−−−−−−−−−−−−−
Figure 4.17: Comparing a and b without revealing them
is shown in Fig. 4.17. It begins with Bob choosing a random r in Zp and sending ( r,xb) to Sam.
Bob can do this whenever he wants, even before Alice initiates the protocol. When Alice wants to
test equality, she sends xa to Sam. Sam computes x←rxa −xb and sends x back to Alice. Now,
observe that
x+ k1 = r(a−b)
so that x+ k1 = 0 when a = b and x+ k1 is very likely to be non-zero otherwise (assuming p is
suﬃciently large so that r̸= 0 with high probability). This lets Alice learn if a= b.
What is revealed by this protocol? Clearly Bob learns nothing. Alice learns r(a−b), but if
a ̸= b this quantity is uniformly distributed in Zp. Therefore, when a ̸= b Alice just obtains a
uniform element in Zp and this reveals nothing beyond the fact that a̸= b. Sam sees r,xa,xb, but
all three values are independent of a and b: xa and xb are one-time pad encryptions under keys
k0 and k1, respectively. Therefore, Sam learns nothing. Notice that the only privacy assumption
about Sam is that it does not reveal ( r,xb) to Alice or xa to Bob.
The trouble, much like with the one-time pad, is that the shared key ( k0,k1) can only be used
for a single equality test, otherwise the protocol becomes insecure. If ( k0,k1) is used to test if a= b
and later the same key ( k0,k1) is used to test if a′= b′then Alice and Sam learn information they
are not supposed to. For example, Sam learns a−a′. Moreover, Alice can deduce ( a−b)/(a′−b′)
which reveals information about band b′(e.g., if a= a′= 0 then Alice learns the ratio of band b′).
Sub-key derivation. What if Alice wants to repeatedly test proximity to Bob? The solution
is to generate a new independent key ( k0,k1) for each invocation of the protocol. We do so by
deriving instance-speciﬁc sub-keys using a secure PRF.
Let F be a secure PRF deﬁned over (K, {1,...,B }, Z2
p) and suppose that Alice and Bob share
a long term key k ∈K. Bob maintains a counter cntb that is initially set to 0. Every time Bob
sends his encrypted location ( r,xb) to Sam he increments cntb and derives sub-keys ( k0,k1) from
the long-term key k as:
(k0,k1) ←F(k, cntb). (4.46)
He sends (r,xb,cntb) to Sam. Bob can do this whenever he wants, say every few minutes, or every
time he moves to a new location.
Whenever Alice wants to test proximity to Bob she ﬁrst asks Sam to send her the value of
the counter in the latest message from Bob. She makes sure the counter value is larger than the
previous value Sam sent her (to prevent a mischievous Sam or Bob from tricking Alice into re-using
an old counter value). Alice then computes (k0,k1) herself using (4.46) and carries out the protocol
with Sam in Fig. 4.17 using these keys.
166
Because F is a secure PRF, the sequence of derived sub-keys is indistinguishable from random
independently sampled keys. This ensures that the repeated protocol reveals nothing about the
tested values beyond equality. By using a PRF, Alice is able to quickly compute ( k0,k1) for the
latest value of cntb.
4.9 Notes
Citations to the literature to be added.
4.10 Exercises
4.1 (Exercising the deﬁnition of a secure PRF). Let F be a secure PRF deﬁned over
(K,X,Y), where K= X= Y= {0,1}n.
(a) Show that F1(k,x) = F(k,x) ∥0 is not a secure PRF.
(b) Show that F2
(
k, (x,y)
):= F(k,x) ⊕F(k,y) is insecure.
(c) Prove that F3(k,x) := F(k,x) ⊕x is a secure PRF.
(d) Prove that F4
(
(k1,k2),x
):= F(k1,x) ⊕F(k2,x) is a secure PRF.
(e) Show that F5
(
(k1,k2),(x1,x2)
):= F(k1,x1) ⊕F(k2,x2) is insecure.
(f) Show that F6(k,x) := F(k,x) ∥F(k,x ⊕1n) is insecure.
(g) Prove that F7(k,x) := F(F(k,0n),x) is a secure PRF.
(h) Show that F8(k,x) := F(F(k,0n),x) ∥F(k,x) is insecure.
(i) Show that F9(k,x) := F(k,x) ∥F
(
k,F (k,x)
)
is insecure.
4.2 (Weak PRFs). Let F be a PRF deﬁned over ( K,X,Y) where Y := {0,1}n and |X| is
super-poly. Deﬁne
F5
(
(k1,k2),(x1,x2)
):= F(k1,x1) ⊕F(k2,x2).
We showed in Exercise 4.1 part (e) that F5 is not a secure PRF.
(a) Show that F5 is a weakly secure PRF (as in Deﬁnition 4.3), assuming F is weakly secure. In
particular, for any Q-query weak PRF adversary Aattacking F5 (i.e., an adversary that only
queries the function at random points in X) there is a weak PRF adversary Battacking F,
where Bis an elementary wrapper around A, such that
wPRFadv[A,F5] ≤wPRFadv[B,F] + (Q/|X|)4.
(b) Suppose F is a secure PRF. Show that F5 is weakly secure even if we modify the weak PRF
attack game and allow the adversary Ato query F5 at one chosen point in addition to the Q
random points. A PRF that is secure in this sense is suﬃcient for a popular data integrity
mechanism discussed in Section 7.4.
167
(c) Show that F5 is no longer secure if we modify the weak PRF attack game and allow the
adversary Ato query F5 at two chosen points in addition to the Q random points.
4.3 (Format preserving encryption). Suppose we are given a block cipher ( E,D) operating
on domain X. We want a block cipher ( E′,D′) that operates on a smaller domain X′⊆X. Deﬁne
(E′,D′) as follows:
E′(k,x) := y←E(k,x)
while y̸∈X′do: y←E(k,y)
output y
D′(k,y) is deﬁned analogously, applying D(k,·) until the result falls in X′. Clearly ( E′,D′) are
deﬁned on domain X′.
(a) With t:= |X|/|X′|, how many evaluations of E are needed in expectation to evaluate E′(k,x)
as a function of t? You answer shows that when t is small (e.g., t ≤2) evaluating E′(k,x)
can be done eﬃciently.
(b) Show that if ( E,D) is a secure block cipher with domain X then (E′,D′) is a secure block
cipher with domain X′. Try proving security by induction on |X|−|X ′|.
Discussion: This exercise is used in the context of encrypted 16-digit credit card numbers where
the ciphertext also must be a 16-digit number. This type of encryption, called format preserving
encryption, amounts to constructing a block cipher whose domain size is exactly 10 16. This
exercise shows that it suﬃces to construct a block cipher ( E,D) with domain size 2 54 which is the
smallest power of 2 larger than 10 16. The procedure in the exercise can then be used to shrink the
domain to size 10 16.
4.4 (Truncating PRFs). Let F be a PRF whose range is Y= {0,1}n. For some ℓ<n consider
the PRF F′ with a range Y′ = {0,1}ℓ deﬁned as: F′(k,x) := F(k,x)[0 ...ℓ −1]. That is, we
truncate the output of F(k,x) to the ﬁrst ℓ bits. Show that if F is a secure PRF then so is F′.
4.5 (Two-key Triple-DES). Consider the following variant of the 3 E construction that uses
only two keys: for a block cipher ( E,D) with key space K deﬁne 3 E′ as E((k1,k2),m) :=
E(k1,E(k2,E(k1,m))). Show that this block cipher can be defeated by a meet in the middle
attack using O(|K|) evaluation of E and Dand using O(|K|) encryption queries to the block cipher
challenger. Further attacks on this method are discussed in [112, 105].
4.6 (adaptive vs non-adaptive security). This exercise develops an argument that shows that
a PRF may be secure against every adversary that makes its queries non-adaptively, (i.e., all at
once) but is insecure against adaptive adversaries (i.e., the kind allowed in Attack Game 4.2).
To be a bit more precise, we deﬁne the non-adaptive version of Attack Game 4.2 as follows. The ad-
versary submits all at once the query (x1,...,x Q) to the challenger, who responds with (y1,...,y Q),
where y:= f(xi). The rest of the attack game is the same: in Experiment 0,k←R Kand f ←R F(k,·),
while in Experiment 1, f ←R Funs[X,Y]. Security against non-adaptive adversaries means that all
eﬃcient adversaries have only negligible advantage; advantage is deﬁned as usual:|Pr[W0]−Pr[W1]|,
where Wb is the event that the adversary outputs 1 in Experiment b.
Suppose F is a secure PRF deﬁned over ( K,X,X), where N := |X| is super-poly. We proceed
to “sabotage” F, constructing a new PRF ˜F as follows. Let x′ be some ﬁxed element of X. For
x= F(k,x′) deﬁne ˜F(k,x) := x′, and for all other x deﬁne ˜F(k,x) := F(k,x).
168
(a) Show that ˜F is not a secure PRF against adaptive adversaries.
(b) Show that ˜F is a secure PRF against non-adaptive adversaries.
(c) Show that a similar construction is possible for block ciphers: given a secure block cipher
(E,D) deﬁned over ( K,X) where |X| is super-poly, construct a new, “sabotaged” block
cipher ( ˜E, ˜D) that is secure against non-adaptive adversaries, but insecure against adaptive
adversaries.
4.7 (PRF security deﬁnition). This exercise develops an alternative characterization of PRF
security for a PRF F deﬁned over (K,X,Y). As usual, we need to deﬁne an attack game between
an adversary Aand a challenger. Initially, the challenger generates
b←R {0,1}, k←R K, ˜y1 ←R Y
Then Amakes a series of queries to the challenger. There are two types of queries:
Function: In an function query , Asubmits an x ∈ Xto the challenger, who responds with
y←F(k,x). The adversary may make any (poly-bounded) number of function queries.
Test: In a test query , Asubmits an ˜x ∈X to the challenger, who computes ˜ y0 ←F(k,˜x) and
responds with ˜yb. The adversary is allowed to make only a single test query (with any number
of function queries before and after the test query). The test point ˜xis not allowed to among
the function the queries x.
At the end of the game, Aoutputs a bit ˆb∈{0,1}. As usual, we deﬁne A’s advantage in the above
attack game to be |Pr[ˆb= b] −1/2|. We say that F is Alt-PRF secure if this advantage is negligible
for all eﬃcient adversaries. Show that F is a secure PRF if and only if F is Alt-PRF secure.
Discussion: This characterization shows that the value of a secure PRF at a point ˜ x in Xlooks
like a random element of Y, even after seeing the value of the PRF at many other points of X.
4.8 (Key malleable PRFs). Let F be a PRF deﬁned over ( {0,1}n,{0,1}n,Y).
(a) We say that F is XOR-malleable if F(k, x⊕c) = F(k,x) ⊕c for all k,x,c in {0,1}n.
(b) We say that F is key XOR-malleable if F(k⊕c, x) = F(k,x) ⊕c for all k,x,c in {0,1}n.
Clearly an XOR-malleable PRF cannot be secure: malleability lets an attacker distinguish the PRF
from a random function. Show that the same holds for a key XOR-malleable PRF.
Remark: In contrast, we note that there are secure PRFs whereF(k1⊕k2, x) = F(k1,x)⊕F(k2,x).
See Exercise 11.2 for an example, where the xor on the left is replaced by addition, and the xor on
the right is replaced by multiplication.
4.9 (Strongly secure block ciphers). In Section 4.1.3 we sketched out the notion of a strongly
secure block cipher.
(a) Write out the complete deﬁnition of a strongly secure block cipher as a game between a
challenger and an adversary.
(b) Consider the following cipher E′ = ( E′,D′) built from a block cipher ( E,D) deﬁned over
(K,{0,1}n):
E′(k,m) := D(k, t⊕E(k,m) ) and D′(k,c) := D(k, t⊕E(k,c) )
169
where t ∈{0,1}n is a ﬁxed constant. For what values of t is this cipher E′ semantically
secure? Prove semantic security assuming the underlying block cipher is strongly secure.
4.10 (Meet-in-the-middle attacks). Let us study the security of the 4 E construction where
a block cipher ( E,D) is iterated four times using four diﬀerent keys: E4( (k1,k2,k3,k4), m) :=
E
(
k4, E(k3, E(k2, E(k1,m)))
)
where (E,D) is a block cipher deﬁned over ( K,X).
(a) Show that there is a meet in the middle attack on 4 Ethat recovers the secret key in time |K|2
and memory space |K|2.
(b) Suppose |K|= |X|. Show that there is a meet in the middle attack on 4 Ethat recovers the
secret key in time |K|2, but only uses memory space |K|. If you get stuck see [53].
4.11 (Tweakable block ciphers). A tweakable block cipher is a block cipher whose encryption
and decryption algorithm take an additional input t, called a “tweak”, which is drawn from a
“tweak space” T. As usual, keys come from a key space K, and data blocks from a data block
space X. The encryption and decryption functions operate as follows: for k ∈K,x ∈X ,t ∈T ,
we have y = E(k,x,t ) ∈X and x = D(k,y,t ). So for each k ∈K and t ∈T , E(k,·,t) deﬁnes a
permutation on Xand D(k,·,t) deﬁnes the inverse permutation. Unlike keys, tweaks are typically
publicly known, and may even be adversarially chosen.
Security is deﬁned by a game with two experiments. In both experiments, the challenger deﬁnes
a family of permutations {Πt}t∈T, where each Π t is a permutation on X. In Experiment 0, the
challenger sets k←R K, and
Πt := E(k,·,t) for all t∈T .
In Experiment 1, the challenger sets
Πt ←R Perms[X] for all t∈T .
Both experiments then proceed identically. The adversary issues a series of queries. Each query is
one of two types:
forward query: the adversary sends (x,t) ∈X×T , and the challenger responds with y:= Πt(x);
inverse queries: the adversary sends ( y,t) ∈ X×T , and the challenger responds with x :=
Π−1
t (y).
At the end of the game, the adversary outputs a bit. If pb is the probability that the adversary
outputs 1 in Experiment b, the adversary’s advantage is deﬁned to be |p0 −p1|. We say that (E,D)
is a secure tweakable block cipher if every eﬃcient adversary has negligible advantage.
This deﬁnition of security generalizes the notion of a strongly secure block cipher (see Section 4.1.3
and Exercise 4.9). In applications of tweakable block ciphers, this strong security notion is more
appropriate (e.g., see Exercise 9.17).
(a) Prove security of the construction ˜E(k,m,t ) := E(E(k,t),m) where ( E,D) is a strongly
secure block cipher deﬁned over ( K,K).
(b) Show that there is an attack on the construction from part (a) that achieves advantage ≥1/2
and which makes Q≈
√
|K|queries.
Hint: In addition to the ≈
√
|K|queries, your adversary should make an additional ≈
√
|K|
“oﬄine” evaluations of the cipher ( E,D).
170
(c) Prove security of the construction
E′(
(k0,k1),m,t
):=
{
p←F(k0,t); output p⊕E(k1, m⊕p)
}
,
where (E,D) is a strongly secure block cipher and F is a secure PRF. In Exercise 7.10 we
will see a more eﬃcient variant of this construction.
Hint: Use the assumption that (E,D) is a strongly secure block cipher to replace E(k1,·) in
the challenger by a truly random permutation ˜Π; then, use the Domain Separation Lemma
(see Theorem 4.15) to replace ˜Π by a family of independent permutations {˜Πt}t∈T, and
analyze the corresponding domain separation failure probability.
Discussion: Tweakable block ciphers are used in disk sector encryption where encryption must
not expand the data: the ciphertext size is required to have the same size as the input. The sector
number is used as the tweak to ensure that even if two sectors contain the same data, the resulting
encrypted sectors are diﬀerent. The construction in part (c) is usually more eﬃcient than that in
part (a), as the latter uses a diﬀerent block cipher key with every evaluation, which can incur extra
costs. See further discussion in Exercise 7.10.
4.12 (PRF combiners). We want to build a PRF F using two PRFs F1 and F2, so that if at
some future time one of F1 or F2 is broken (but not both) then F is still secure. Put another way,
we want to construct F from F1 and F2 such that F is secure if either F1 or F2 is secure.
Suppose F1 and F2 both have output spaces {0,1}n, and both have a common input space. Deﬁne
F( (k1,k2), x) := F1(k1,x) ⊕F2(k2,x).
Show that F is secure if either F1 or F2 is secure.
4.13 (Block cipher combiners). Continuing with Exercise 4.12, we want to build a block cipher
E= (E,D) from two block ciphers E1 = (E1,D1) and E2 = (E2,D2) so that if at some future time
one of E1 or E2 is broken (but not both) then Eis still secure. Suppose both E1 and E2 are deﬁned
over (K,X). Deﬁne Eas:
E( (k1,k2), x) := E1
(
k1, E2(k2,x)
)
and D( (k1,k2), y) := D2
(
k2, D1(k1,y)
)
.
(a) Show that Eis secure if either E1 or E2 is secure.
(b) Show that this is not a secure combiner for PRFs. That is, F( (k1,k2), x) := F1
(
k1, F2(k2,x)
)
need not be a secure PRF even if one of F1 or F2 is.
4.14 (Key leakage). Let F be a secure PRF deﬁned over (K,X,Y), where K= X= Y= {0,1}n.
(a) Let K1 = {0,1}n+1. Construct a new PRF F1, deﬁned over ( K1,X,Y), with the following
property: the PRF F1 is secure; however, if the adversary learns the last bit of the key then
the PRF is no longer secure. This shows that leaking even a single bit of the secret key can
completely destroy the PRF security property.
Hint: Let k1 = k ∥b where k ∈{0,1}n and b ∈{0,1}. Set F1(k1,x) to be the same as
F(k,x) for all x ̸= 0 n. Deﬁne F1(k1,0n) so that F1 is a secure PRF, but becomes easily
distinguishable from a random function if the last bit of the secret key k1 is known to the
adversary.
171
(b) Construct a new PRF F2, deﬁned over ( K×K ,X,Y), that remains secure if the attacker
learns any single bit of the key. Your function F2 may only call F once.
4.15 (Variants of Luby-Rackoﬀ). Let F be a secure PRF deﬁned over ( K,X,X).
(a) Show that two-round Luby-Rackoﬀ is not a secure block cipher.
(b) Show that three-round Luby-Rackoﬀ is not a strongly secure block cipher.
4.16 (Insecure tree construction). In the tree construction for building a PRF from a PRG
(Section 4.6), the secret key is used at the root of the tree and the input is used to trace a path
through the tree. Show that a construction that does the opposite is not a secure PRF. That is,
using the input as the root and using the key to trace through the tree is not a secure PRF.
4.17 (Truncated tree construction). Suppose we cut oﬀ the tree construction from Section 4.6
after only three levels of the tree, so that there are only eight leaves, as in Fig. 4.15. Give a direct
proof, using a sequence of seven hybrids, that outputting the values at all eight leaves gives a secure
PRG deﬁned over (S,S8), assuming the underlying PRG is secure.
4.18 (Augmented tree construction). Suppose we are given a PRG Gdeﬁned over (K×S,S2).
Write G(k,s) = (G0(k,s),G1(k,s)). Let us deﬁne the PRF G∗ with key space Kn ×S and input
space {0,1}n as follows:
G∗(
(k0,...,k n−1,s), x∈{0,1}n):=
t←s
for i←0 to n−1 do
b←x[i]
t←Gb(ki,t)
output t.
(a) Given an example secure PRG G for which G∗is insecure as a PRF.
(b) Show that G∗is a secure PRF if for every poly-bounded Q the following PRG is secure:
G′(k,s0,...,s Q−1) := (G(k,s0),...,G (k,sQ−1)) .
4.19 (Synthesizers and parallel PRFs). For a secure PRG G deﬁned over (S,R) we showed
that Gn(s1,...,s n) :=
(
G(s1),...,G (sn)
)
is a secure PRG over ( Sn,Rn). The proof requires that
the components s1,...,s n of the seed be chosen uniformly and independently over Sn. A secure
synthesizer is a PRG for which this holds even if s1,...,s n are not independent of one another.
Speciﬁcally, a synthesizer is an eﬃcient function S : X2 →X. The synthesizer is said to be n-way
secure if
Sn(x1,y1,...,x n,yn) :=
(
S(xi,yj)
)
i,j=1,...,n ∈X(n2)
is a secure PRG deﬁned over ( X2n,X(n2)). Here S is being evaluated at n2 inputs that are not
independent of one another and yet Sn is a secure PRG.
(a) Not every secure PRG is a secure synthesizer. Let G be a secure PRG over ( S,R). Show
that S(x,y) :=
(
G(x),y
)
is a secure PRG deﬁned over ( S2, R×S), but is an insecure 2-way
synthesizer.
172
k0
0
k1
0
k0
1
k1
1
k0
2
k1
2
k0
3
k1
3
k0
4
k1
4
k0
5
k1
5
k0
6
k1
6
k0
7
k1
7
k0
0
k1
0
k0
0
k0
1
k1
1
k0
1
k0
2
k1
2
k1
2
k0
3
k1
3
k0
3
k0
4
k1
4
k1
4
k0
5
k1
5
k0
5
k0
6
k1
6
k1
6
k0
7
k1
7
k1
7
S S S S
S S
S
F(¯k, 00101011)
key ¯k∈X16
Figure 4.18: A PRF built from a synthesizer S. The PRF input in {0,1}n is used to select n
components from the key ¯k ∈X2n. The selected components, shown as shaded squares, are used
as shown in the ﬁgure.
(b) A secure synthesizer lets us build a large domain PRF that can be evaluated quickly on
a parallel computer. Show that if S : X2 → Xis a Q-way secure synthesizer, for poly-
bounded Q, then the PRF in Fig. 4.18 is a secure PRF deﬁned over ( X2n, {0,1}n, X). For
simplicity, assume that n is a power of 2. Observe that the PRF can be evaluated in only
log2 n steps on a parallel computer.
4.20 (Insecure variants of Even-Mansour). In Section 4.7.3 we discussed the Even-Mansour
block cipher ( E,D) built from a permutation π : X → Xwhere X = {0,1}n. Recall that
E
(
(P0,P1), m
):= π(m⊕P0) ⊕P1.
(a) Show that E1(P0,m) := π(m⊕P0) is not a secure block cipher.
(b) Show that E2(P1,m) := π(m) ⊕P1 is not a secure block cipher.
4.21 (Birthday attack on Even-Mansour). Let’s show that the bounds in the Even-Mansour
security theorem (Theorem 4.14) are tight. For X:= {0,1}n, recall that the Even-Mansour block
cipher (E,D), built from a permutation π: X→X , is deﬁned as: E
(
(k0,k1), m
):= π(m⊕k0)⊕k1.
We show how to break this block cipher in time approximately 2 n/2.
(a) Show that for all a,m, ∆ ∈X and ¯k:= (k0,k1) ∈X2, whenever a= m⊕k0, we have
E
(¯k, m
)
⊕E
(¯k, m⊕∆
)
= π(a) ⊕π(a⊕∆)
(b) Use part (a) to construct an adversary Athat wins the block cipher security game against
(E,D) with advantage close to 1, in the ideal cipher model. Withq:= 2n/2 and some non-zero
173
∆ ∈X, the adversary Aqueries the cipher at 2q random points mi, mi⊕∆ ∈X and queries
the permutation π at 2q random points ai, ai ⊕∆ ∈X, for i= 1,...,q .
4.22 (A variant of the Even-Mansour cipher). Let M:= {0,1}m, K:= {0,1}n, and X:=
{0,1}n+m. Consider the following cipher ( E,D) deﬁned over ( K,M,X) built from a permutation
π: X→X :
E(k,x) := (k∥0m) ⊕π(k∥x) (4.47)
D(k,c) is deﬁned analogously. Show that if we model π as an ideal permutation Π, then for every
block cipher adversary Aattacking (E,D) we have
BCicadv[A,E] ≤2Qic
|K| . (4.48)
Here Qic is the number of queries Amakes to Π- and Π −1-oracles.
4.23 (Analysis of Salsa and ChaCha). In this exercise we analyze the Salsa and ChaCha
stream ciphers from Section 3.6 in the ideal permutation model. Let π: X→X be a permutation,
where X= {0,1}n+m. Let K:= {0,1}n and deﬁne the PRFF, which is deﬁned over (K,{0,1}m,X),
as
F(k,x) := (k∥x) ⊕π(k∥x) . (4.49)
This PRF is an abstraction of the PRF underlying the Salsa and ChaCha stream ciphers. Use
Exercise 4.22 to show that if we model π as an ideal permutation Π, then for every PRF adversary
Aattacking F we have
PRFicadv[A,F] ≤2Qic
|K| + Q2
F
2|X| (4.50)
where QF is the number of queries that Amakes to an F(k,·) oracle and Qic is the number of
queries Amakes to Π- and Π−1-oracles. In Salsa and ChaCha, QF is at most |X|1/4 so that Q2
F
2|X| is
“negligible.”
Discussion: The speciﬁc permutation πused in the Salsa and ChaCha stream ciphers is not quite
an ideal permutation. For example, π(0n+m) = 0n+m. Hence, your analysis applies to the general
framework, but not speciﬁcally to Salsa and ChaCha.
4.24 (Alternative proof of Theorem 4.6). Let X and Y be random variables as deﬁned in
Exercise 3.14. Consider an adversary Ain Attack Game 4.3 that makes at most Q queries to its
challenger. Show that PF adv[A,X] ≤∆[X,Y] ≤Q2/2N.
4.25 (A one-sided switching lemma). Following up on the previous exercise, one can use
part (b) of Exercise 3.14 to get a “one sided” version of Theorem 4.6, which can be useful in
some settings. Consider an adversary Ain Attack Game 4.3 that makes at most Q queries to its
challenger. Let W0 and W1 be as deﬁned in that game: W0 is the event that Aoutputs 1 when
probing a random permutation, and W1 is the event that Aoutputs 1 when probing a random
function. Assume Q2 <N . Show that Pr[ W0] ≤ρ[X,Y] ·Pr[W1] ≤2 Pr[W1].
4.26 (Parallel composition of PRFs). Just as we can compose PRGs in parallel, while main-
taining security (see Section 3.4.1), we can also compose PRFs in parallel, while maintaining secu-
rity.
174
Suppose we have a PRF F, deﬁned over ( K,X,Y). We want to model the situation where
an adversary is given n black boxes (where n ≥1 is poly-bounded): the boxes either contain
F(k1,·),...,F (kn,·), where the ki are random (and independent) keys, or they contain f1,...,f n,
where the fi are random elements of Funs[ X,Y], and the adversary should not be able to tell the
diﬀerence.
A convenient way to model this situation is to consider the n-wise parallel composition of F,
which is a PRF F′whose key space is Kn, whose input space is {1,...,n }×X , and whose output
space is Y. Given a key k′= (k1,...,k n), and an input x′= (s,x), with s∈{1,...,n }and x∈X,
we deﬁne F′(k′,x′) := F(ks,x).
Show that if F is a secure PRF, then so is F′. In particular, show that for every PRF adver-
sary A, then exist a PRF adversary B, where Bis an elementary wrapper around A, such that
PRFadv[A,F′] = n·PRFadv[B,F].
4.27 (A universal attacker on PRFs). Let F be a PRF deﬁned over ( K,X,Y), and let Q be
some integer where 0 <Q ≤|K|.
(a) Suppose that |Y|≥|K| . Show that there is a PRF adversary Athat runs in time linear in Q,
makes one query to the PRF challenger, and has advantage
PRFadv[A,F] ≥ Q
|K|− Q
|Y|. (4.51)
Hint: Try to ﬁrst devise your PRF adversary Awhen Q= |K|.
(b) When |K|= |Y| the bound in (4.51) is meaningless. To obtain a meaningful bound, use
part (a) to show that when |Y|≥|K| 1/2 there is a PRF adversary Athat makes two queries
to the PRF challenger, runs in time linear in Q, and has advantage
PRFadv[A,F] ≥ Q
|K|− Q
|Y|2 . (4.52)
4.28 (Distributed PRFs). Let F be a secure PRF deﬁned over ( K,X,Y) where Y:= {0,1}n.
In Exercise 4.1 part (d) we showed that if F is secure then so is
F′(
(k1,k2),x) := F(k1,x) ⊕F(k2,x).
This F′ has a useful property: the PRF key ( k1,k2) can be split into two shares, k1 and k2. If
Alice is given one share and Bob the other share, then both Alice and Bob are needed to evaluate
the PRF, and neither can evaluate the PRF on its own. Moreover, the PRF can be evaluated
distributively, that is, without re-constituting the key ( k1,k2): to evaluate the PRF at a point x0,
Alice simply sends F(k1,x0) to Bob.
(a) To show that Alice cannot evaluate F′ by herself, show that F′ is a secure PRF even if the
adversary is given k1. Argue that the same holds for k2.
(b) Construct a PRF where the key can be split into three shares s1,s2,s3 so that any two shares
can be used evaluate the PRF distributively, but no single share is suﬃcient to evaluate the
PRF on its own.
Hint: Consider the PRF F′′(
(k1,k2,k3),x) := F(k1,x) ⊕F(k2,x) ⊕F(k3,x) and show how
to construct the shares s1,s2,s3 from the keys k1,k2,k3. Make sure to prove that the F′′ is
a secure PRF when the adversary is given a single share, namely si for some i∈{1,2,3}.
175
(c) Generalize the construction from part (b) to construct a PRF F′′′supporting three-out-of-ﬁve
sharing of the key: any three shares can be used to evaluate the PRF distributively, but no
two shares can.
Hint: The key space for F′′′is K10.
176
Chapter 5
Chosen Plaintext Attack
This chapter focuses on the problem of securely encrypting several messages in the presence of an
adversary who eavesdrops, and who may even inﬂuence the choice of some messages in order to
glean information about other messages. This leads us to the notion of semantic security against a
chosen plaintext attack .
5.1 Introduction
In Chapter 2, we focused on the problem of encrypting a single message. Now we consider the
problem of encrypting several messages. To make things more concrete, suppose Alice wants to use
a cipher to encrypt her ﬁles on some ﬁle server, while keeping her secret keys for the cipher stored
securely on her USB memory stick.
One possible approach is for Alice to encrypt each individual ﬁle using a diﬀerent key. This
entails that for each ﬁle, she stores an encryption of that ﬁle on the ﬁle server, as well as a
corresponding secret key on her memory stick. As we will explore in detail in Section 5.2, this
approach will provide Alice with reasonable security, provided she uses a semantically secure cipher.
Now, although a ﬁle may be several megabytes long, a key for any practical cipher is just a few bytes
long. However, if Alice has many thousands of ﬁles to encrypt, she must store many thousands of
keys on her memory stick, which may not have suﬃcient storage for all these keys.
As we see, the above approach, while secure, is not very space eﬃcient, as it requires one key per
ﬁle. Faced with this problem, Alice may simply decide to encrypt all her ﬁles with the same key.
While more eﬃcient, this approach may be insecure. Indeed, if Alice uses a cipher that provides only
semantic security (as in Deﬁnition 2.2), this may not provide Alice with any meaningful security
guarantee, and may very well expose her to a realistic attack.
For example, suppose Alice uses the stream cipher Ediscussed in Section 3.2. Here, Alice’s key
is a seed s for a PRG G, and viewing a ﬁle m as a bit string, Alice encrypts m by computing the
ciphertext c:= m⊕∆, where ∆ consists of the ﬁrst |m|bits of the “key stream” G(s). But if Alice
uses this same seed sto encrypt many ﬁles, an adversary can easily mount an attack. For example,
if an adversary knows some of the bits of one ﬁle, he can directly compute the corresponding bits
of the key stream, and hence obtain the corresponding bits of any ﬁle. How might an adversary
know some bits of a given ﬁle? Well, certain ﬁles, like email messages, contain standard header
information (see Example 2.6), and so if the adversary knows that a given ciphertext is an encryption
of an email, he can get the bits of the key stream that correspond to the location of the bits in this
177
standard header. To mount an even more devastating attack, the adversary may try something even
more devious: he could simply send Alice a large email, say one megabyte in length; assuming that
Alice’s software automatically stores an encryption of this email on her server, when the adversary
snoops her ﬁle server, he can recover a corresponding one megabyte chunk of the key stream, and
now he can decrypt any one megabyte ﬁle stored on Alice’s server! This email may even be caught
in Alice’s spam ﬁlter, and never actually be seen by Alice, although her encryption software may
very well diligently encrypt this email along with everything else. This type of an attack is called
a chosen plaintext attack , because the adversary forces Alice to give him the encryption of one or
more plaintexts of his choice during his attack on the system.
Clearly, the stream cipher above is inadequate for the job. In fact, the stream cipher, as well
as any other deterministic cipher , should not be used to encrypt multiple ﬁles with the same key.
Why? Any deterministic cipher that is used to encrypt several ﬁles with the same key will suﬀer
from an inherent weakness: an adversary will always be able to tell when two ﬁles are identical
or not. Indeed, with a deterministic cipher, if the same key is used to encrypt the same message,
the resulting ciphertext will always be the same (and conversely, for any cipher, if the same key
is used to encrypt two diﬀerent messages, the resulting ciphertexts must be diﬀerent). While this
type of attack is certainly not as dramatic as those discussed above, in which the adversary can
read Alice’s ﬁles almost at will, it is still a serious vulnerability. For example, while the discussion
in Section 4.1.4 about ECB mode was technically about encrypting a single message consisting of
many data blocks, it applies equally well to the problem of encrypting many single-block messages
under the same key.
In fact, it is possible for Alice to use a cipher to securely encrypt all of her ﬁles under a single,
short key, but she will need to use a cipher that is better suited to this task. In particular, because of
the above inherent weakness of any deterministic cipher, she will have to use a probabilistic cipher,
that is, a cipher that uses a probabilistic encryption algorithm, so that diﬀerent encryptions of the
same plaintext under the same key will (generally) produce diﬀerent encryptions. For her task, she
will want a cipher that achieves a level of security stronger than semantic security. The appropriate
notion of security is called semantic security against chosen plaintext attack . In Section 5.3 and the
sections following, we formally deﬁne this concept, look at some constructions based on semantically
secure ciphers, PRFs, and block ciphers, and look at a few case studies of “real world” systems.
While the above discussion motivated the topics in this chapter using the example of the “ﬁle
encryption” problem, one can also motivate these topics by considering the “secure network com-
munication” problem. In this setting, one considers the situation where Alice and Bob share a
secret key (or keys), and Alice wants to secretly transmit several messages to Bob over an insecure
network. Now, if Alice can conveniently concatenate all of her messages into one long message,
then she can just use a stream cipher to encrypt the whole lot, and be done with it. However, for
a variety of technical reasons, this may not be feasible: if she wants to be able to transmit the
messages in an arbitrary order and at arbitrary times, then she is faced with a problem very similar
to that of the “ﬁle encryption” problem. Again, if Alice and Bob want to use a single, short key,
the right tool for the job is a cipher semantically secure against chosen plaintext attack.
We stress again that just like in Chapter 2, the techniques covered in this chapterdo not provide
any data integrity, nor do they address the problem of how two parties come to share a secret key
to begin with. These issues are dealt with in coming chapters.
178
5.2 Security against multi-key attacks
Consider again the “ﬁle encryption” problem discussed in the introduction to this chapter. Suppose
Alice chooses to encrypt each of her ﬁles under diﬀerent, independently generated keys using a
semantically secure cipher. Does semantic security imply a corresponding security property in this
“multi-key” setting?
The answer to this question is “yes.” We begin by stating the natural security property corre-
sponding to semantic security in the multi-key setting.
Attack Game 5.1 (multi-key semantic security). For a given cipher E= (E,D), deﬁned over
(K,M,C), and for a given adversaryA, we deﬁne two experiments, Experiment 0 and Experiment 1.
For b= 0,1, we deﬁne
Experiment b:
• The adversary submits a sequence of queries to the challenger.
For i= 1,2,..., the ith query is a pair of messages, mi0,mi1 ∈M, of the same length.
The challenger computes ki ←R K, ci ←R E(ki,mib), and sends ci to the adversary.
• The adversary outputs a bit ˆb∈{0,1}.
For b= 0,1, let Wb be the event that Aoutputs 1 in Experiment b. We deﬁne A’s advantage
with respect to Eas
MSSadv[A,E] := |Pr[W0] −Pr[W1]|. 2
We stress that in the above attack game, the adversary’s queries are adaptively chosen, in the
sense that for each i= 1,2,..., the message pair ( mi0,mi1) may be computed by the adversary in
some way that depends somehow on the previous encryptions c1,...,c i−1 output by the challenger.
Deﬁnition 5.1 (Multi-key semantic security). A cipher Eis called multi-key semantically
secure if for all eﬃcient adversaries A, the value MSSadv[A,E] is negligible.
As discussed in Section 2.2.5, Attack Game 5.1 can be recast as a “bit guessing” game, where
instead of having two separate experiments, the challenger chooses b∈{0,1}at random, and then
runs Experiment b against the adversary A. In this game, we measure A’s bit-guessing advantage
MSSadv∗[A,E] as |Pr[ˆb= b] −1/2|, and as usual (by (2.11)), we have
MSSadv[A,E] = 2 ·MSSadv∗[A,E]. (5.1)
As the next theorem shows, semantic security implies multi-key semantic security.
Theorem 5.1. If a cipher Eis semantically secure, it is also multi-key semantically secure.
In particular, for every MSS adversary Athat attacks E as in Attack Game 5.1, and which
makes at most Q queries to its challenger, there exists an SS adversary Bthat attacks Eas in
Attack Game 2.1, where Bis an elementary wrapper around A, such that
MSSadv[A,E] = Q·SSadv[B,E].
179
Proof idea. The proof is a straightforward hybrid argument, which is a proof technique we intro-
duced in the proofs of Theorem 3.2 and 3.3 (the reader is advised to review those proofs, if neces-
sary). In Experiment 0 of the MSS attack game, the challenger is encrypting m10,m20,...,m Q0.
Intuitively, since the key k1 is only used to encrypt the ﬁrst message, and Eis semantically secure,
if we modify the challenger so that it encrypts m11 instead of m10, the adversary should not behave
signiﬁcantly diﬀerently. Similarly, we may modify the challenger so that it encrypts m21 instead
of m20, and the adversary should not notice the diﬀerence. If we continue in this way, making a
total of Q modiﬁcations to the challenger, we end up in Experiment 1 of the MSS game, and the
adversary should not notice the diﬀerence. 2
Proof. Suppose E= (E,D) is deﬁned over (K,X,Y). Let Abe an MSS adversary that plays Attack
Game 5.1 with respect to E, and which makes at most Q queries to its challenger in that game.
First, we introduce Q+ 1 hybrid games, Hybrid 0, . . . , Hybrid Q, played between a challenger
and A. For j = 0 ,1,...,Q , when Amakes its ith query ( mi0,mi1), the challenger in Hybrid j
computes its response ci as follows:
ki ←R K
if i>j then ci ←R E(ki,mi0) else ci ←R E(ki,mi1).
Put another way, the challenger in Hybrid j encrypts
m11,...,m j1, m (j+1)0,...,m Q0,
generating diﬀerent keys for each of these encryptions.
For j = 0,1,...,Q , let pj denote the probability that Aoutputs 1 in Hybrid j. Observe that
p0 is equal to the probability that Aoutputs 1 in Experiment 0 of Attack Game 5.1 with respect
to E, while pQ is equal to the probability that Aoutputs 1 in Experiment 1 of Attack Game 5.1
with respect to E. Therefore, we have
MSSadv[A,E] = |pQ −p0|. (5.2)
We next devise an SS adversary Bthat plays Attack Game 2.1 with respect to E, as follows:
First, Bchooses ω∈{1,...,Q }at random.
Then, Bplays the role of challenger to A— when Amakes its ith query (mi0,mi1), B
computes its response ci as follows:
if i>ω then
ki ←R K, ci ←R E(ki,mi0)
else if i= ω then
Bsubmits (mi0,mi1) to its own challenger
ci is set to the challenger’s response
else / / i<ω
ki ←R K, ci ←R E(ki,mi1).
Finally, Boutputs whatever Aoutputs.
Put another way, adversary Bencrypts
m11,...,m (ω−1)1,
180
generating its own keys for this purpose, submits ( mω0,mω1) to its own encryption oracle, and
encrypts
m(ω+1)0,...,m Q0,
again, generating its own keys.
We claim that
MSSadv[A,E] = Q·SSadv[B,E]. (5.3)
To prove this claim, for b = 0 ,1, let Wb be the event that Boutputs 1 in Experiment b of its
attack game. If ω denotes the random number chosen by B, then the key observation is that for
j = 1,...,Q , we have:
Pr[W0 |ω= j] = pj−1 and Pr[ W1 |ω= j] = pj.
Equation (5.3) now follows from this observation, together with (5.2), via the usual telescoping sum
calculation:
SSadv[B,E] = |Pr[W1] −Pr[W0]|
= 1
Q ·
⏐⏐⏐⏐
Q∑
j=1
Pr[W1 |ω= j] −
Q∑
j=1
Pr[W0 |ω= j]
⏐⏐⏐⏐
= 1
Q ·|pQ −p0|
= 1
Q ·MSSadv[A,E],
and the claim, and hence the theorem, is proved. 2
Let us return now to the “ﬁle encryption” problem discussed in the introduction to this chapter.
What this theorem says is that if Alice uses independent keys to encrypt each of her ﬁles with a
semantically secure cipher, then an adversary who sees the ciphertexts stored on the ﬁle server will
eﬀectively learn nothing about Alice’s ﬁles (except possibly some information about their lengths).
Notice that this holds even if the adversary plays an active role in determining the contents of some
of the ﬁles (e.g., by sending Alice an email, as discussed in the introduction).
5.3 Semantic security against chosen plaintext attack
Now we consider the problem that Alice faced in the introduction of this chapter, where she wants
to encrypt all of her ﬁles on her system using a single, and hopefully short, secret key. The right
notion of security for this task is semantic security against chosen plaintext attack, or CPA
security for short.
Attack Game 5.2 (CPA security). For a given cipher E= (E,D), deﬁned over (K,M,C), and
for a given adversary A, we deﬁne two experiments, Experiment 0 and Experiment 1. For b= 0,1,
we deﬁne
Experiment b:
• The challenger selects k←R K.
181
• The adversary submits a sequence of queries to the challenger.
For i= 1,2,..., the ith query is a pair of messages, mi0,mi1 ∈M, of the same length.
The challenger computes ci ←R E(k,mib), and sends ci to the adversary.
• The adversary outputs a bit ˆb∈{0,1}.
For b= 0,1, let Wb be the event that Aoutputs 1 in Experiment b. We deﬁne A’s advantage
with respect to Eas
CPAadv[A,E] := |Pr[W0] −Pr[W1]|. 2
The only diﬀerence between the CPA attack game and the MSS Attack Game 5.1 is that in the
CPA game, the same key is used for all encryptions, whereas in the MSS attack game, a diﬀerent
key is chosen for each encryption. In particular, the adversary’s queries may be adaptively chosen
in the CPA game, just as in the MSS game.
Deﬁnition 5.2 (CPA security). A cipher E is called semantically secure against chosen
plaintext attack, or simply CPA secure, if for all eﬃcient adversaries A, the value CPAadv[A,E]
is negligible.
As in Section 2.2.5, Attack Game 5.2 can be recast as a “bit guessing” game, where instead
of having two separate experiments, the challenger chooses b ∈{0,1}at random, and then runs
Experiment b against the adversary A; we deﬁne A’s bit-guessing advantage as CPAadv∗[A,E] :=
|Pr[ˆb= b] −1/2|, and as usual (by (2.11)), we have
CPAadv[A,E] = 2 ·CPAadv∗[A,E]. (5.4)
Again, we return to the “ﬁle encryption” problem discussed in the introduction to this chapter.
What this deﬁnition says is that if Alice uses just a single key to encrypt each of her ﬁles with
a CPA secure cipher, then an adversary who sees the ciphertexts stored on the ﬁle server will
eﬀectively learn nothing about Alice’s ﬁles (except possibly some information about their lengths).
Again, notice that this holds even if the adversary plays an active role in determining the contents
of some of the ﬁles.
Example 5.1. Just to exercise the deﬁnition a bit, let us show that no deterministic cipher can
possibly satisfy the deﬁnition of CPA security. Suppose that E= (E,D) is a deterministic cipher.
We construct a CPA adversary Aas follows. Let m,m′ be any two, distinct messages in the
message space of E. The adversary Amakes two queries to its challenger: the ﬁrst is ( m,m′),
and the second is ( m,m). Suppose c1 is the challenger’s response to the ﬁrst query and c2 is the
challenger’s response to the second query. Adversary Aoutputs 1 if c1 = c2, and 0 otherwise.
Let us calculate CPA adv[A,E]. On the one hand, in Experiment 0 of Attack Game 5.2, the
challenger encrypts min responding to both queries, and so c1 = c2; hence, Aoutputs 1 with prob-
ability 1 in this experiment (this is precisely where we need the assumption that Eis deterministic).
On the other hand, in Experiment 1, the challenger encrypts m′ and m, and so c1 ̸= c2; hence, A
outputs 1 with probability 0 in this experiment. It follows that CPA adv[A,E] = 1.
The attack in this example can be generalized to show that not only must a CPA-secure cipher
be probabilistic, but it must be very unlikely that two encryptions of the same message yield the
same ciphertext — see Exercise 5.12. 2
Remark 5.1. Analogous to Theorem 5.1, it is straightforward to show that if a cipher is CPA-
secure, it is also CPA-secure in the multi-key setting. See Exercise 5.2. 2
182
5.4 Building CPA secure ciphers
In this section, we describe a number of ways of building ciphers that are semantically secure
against chosen plaintext attack. As we have already discussed in Example 5.1, any such cipher
must be probabilistic. We begin in Section 5.4.1 with a generic construction that combines any
semantically secure cipher with a pseudo-random function (PRF). The PRF is used to generate
“one time” keys. Next, in Section 5.4.2, we develop a probabilistic variant of the counter mode
cipher discussed in Section 4.4.4. While this scheme can be based on any PRF, in practice, the
PRF is usually instantiated with a block cipher. Finally, in Section 5.4.3, we present a cipher that
is constructed from a block cipher using a method called cipher block chaining (CBC) mode .
These last two constructions, counter mode and CBC mode, are called modes of operation of a
block cipher. Another mode of operation we have already seen in Section 4.1.4 is electronic codebook
(ECB) mode. However, because of the lack of security provided by this mode of operation, it is
seldom used. There are other modes of operations that provide CPA security, which we develop in
the exercises.
5.4.1 A generic hybrid construction
In this section, we show how to turn any semantically secure cipher E= (E,D) into a CPA secure
cipher E′using an appropriate PRF F.
The basic idea is this. A key for E′is a key k′for F. To encrypt a single message m, a random
input x for F is chosen, and a key k for E is derived by computing k ←F(k′,x). Then m is
encrypted using this key k: c ←R E(k,m). The ciphertext is c′ := (x,c). Note that we need to
include x as part of c′so that we can decrypt: the decryption algorithm ﬁrst derives the key k by
computing k←F(k′,x), and then recovers m by computing m←D(k,c).
For all of this to work, the output space of F must match the key space of E. Also, the input
space of F must be super-poly, so that the chances of accidentally generating the same x value
twice is negligible.
Now the details. Let E= (E,D) be a cipher, deﬁned over ( K,M,C). Let F be a PRF deﬁned
over (K′,X,K); that is, the output space of F should be equal to the key space of E. We deﬁne a
new cipher E′= (E′,D′), deﬁned over (K′,M,X×C ), as follows:
• for k′∈K′and m∈M, we deﬁne
E′(k′,m) := x←R X, k←F(k′,x), c←R E(k,m)
output (x,c);
• for k′∈K′and c′= (x,c) ∈X×C , we deﬁne
D′(k′,c′) := k←F(k′,x), m←D(k,c)
output m.
It is easy to verify that E′is indeed a cipher, and is our ﬁrst example of a probabilistic cipher.
Example 5.2. Before proving CPA security ofE′let us ﬁrst see the construction in action. Suppose
Eis the one-time pad, namely E(k,m) := k⊕mwhere K= M= C= {0,1}L. Applying the generic
hybrid construction above to the one-time pad results in the following popular cipherE0 = (E0,D0):
• for k′∈K′and m∈M, deﬁne
183
E0(k′,m) := x←R X, output ( x, F(k′,x) ⊕m)
• for k′∈K′and c′= (x,c) ∈X×C , deﬁne
D0(k′,c′) := output F(k′,x) ⊕c
CPA security of this cipher follows from the CPA security of the generic hybrid construction E′
which is proved in Theorem 5.2 below. 2
Theorem 5.2. If F is a secure PRF, Eis a semantically secure cipher, and N := |X|is super-poly,
then the cipher E′ described above is a CPA secure cipher.
In particular, for every CPA adversary Athat attacks E′as in the bit-guessing version of Attack
Game 5.2, and which makes at most Q queries to its challenger, there exists a PRF adversary
BF that attacks F as in Attack Game 4.2, and an SS adversary BE that attacks Eas in the bit-
guessing version of Attack Game 2.1, where both BF and BE are elementary wrappers around
A, such that
CPAadv[A,E′] ≤Q2
N + 2 ·PRFadv[BF,F] + Q·SSadv[BE,E]. (5.5)
Proof idea. First, using the assumption that F is a PRF, we can eﬀectively replace F by a truly
random function. Second, using the assumption that N is super-poly, we argue that except with
negligible probability, no two x-values are ever the same. But in this scenario, the challenger’s keys
are now all independently generated, and so the challenger is really playing the same role as the
challenger in the Attack Game 5.1. The result then follows from Theorem 5.1. 2
Proof. Let Abe an eﬃcient CPA adversary that attacks E′ as in Attack Game 5.2. Assume that
Amakes at most Q queries to its challenger. Our goal is to show that CPA adv[A,E′] is negligible,
assuming that F is a secure PRF, that N is super-poly, and that Eis semantically secure.
It is convenient to use the bit-guessing versions of the CPA and semantic security attack games.
We prove:
CPAadv∗[A,E′] ≤Q2
2N + PRFadv[BF,F] + Q·SSadv∗[BE,E] (5.6)
for eﬃcient adversaries BF and BE. Then (5.5) follows from (5.4) and Theorem 2.10.
The basic strategy of the proof is as follows. First, we deﬁne Game 0 to be the game played
between Aand the challenger in the bit-guessing version of Attack Game 5.2 with respect to E′.
We then deﬁne several more games: Game 1, Game 2, and Game 3. Each of these games are
played between Aand a diﬀerent challenger; moreover, as we shall see, Game 3 is equivalent to the
bit-guessing version of Attack Game 5.1 with respect to E. In each of these games, b denotes the
random bit chosen by the challenger, while ˆb denotes the bit output by A. Also, for j = 0,..., 3,
we deﬁne Wj to be the event that ˆb= b in Game j. We will show that for j = 1,..., 3, the value
|Pr[Wj] −Pr[Wj−1]|is negligible; moreover, from the assumption that E is semantically secure,
and from Theorem 5.1, it will follow that |Pr[W3] −1/2|is negligible; from this, it follows that
CPAadv∗[A,E′] := |Pr[W0] −1/2|is negligible.
Game 0. Let us begin by giving a detailed description of the challenger in Game 0 that is convenient
for our purposes:
184
b←R {0,1}
k′←R K′
for i←1 to Q do
xi ←R X
ki ←F(k′,xi)
upon receiving the ith query (mi0,mi1) ∈M2:
ci ←R E(ki,mib)
send (xi,ci) to the adversary.
By construction, we have
CPAadv∗[A,E′] =
⏐⏐Pr[W0] −1/2
⏐⏐. (5.7)
Game 1. Next, we play our “PRF card,” replacing F(k′,·) by a truly random function f ∈
Funs[X,K]. The challenger in this game looks like this:
b←R {0,1}
f ←R Funs[X,K]
for i←1 to Q do
xi ←R X
ki ←f(xi)
upon receiving the ith query (mi0,mi1) ∈M2:
ci ←R E(ki,mib)
send (xi,ci) to the adversary.
We claim that ⏐⏐Pr[W1] −Pr[W0]
⏐⏐= PRFadv[BF,F], (5.8)
where BF is an eﬃcient PRF adversary; moreover, since we are assuming that F is a secure PRF,
it must be the case that PRF adv[BF,F] is negligible.
The design of BF is naturally suggested by the syntax of Games 0 and 1. If f ∈Funs[X,K]
denotes the function chosen by its challenger in Attack Game 4.2 with respect to F, adversary BF
runs as follows:
First, BF makes the following computations:
b←R {0,1}
for i←1 to Q do
xi ←R X
ki ←R f(xi).
Here, BF obtains the value f(xi) by querying its own challenger with xi.
Next, adversary BF plays the role of challenger to A; speciﬁcally, when Amakes its ith
query (mi0,mi1), adversary BF computes
ci ←R E(ki,mib)
and sends (xi,ci) to A.
185
PRF Challenger
ki
b R
 {0, 1}
mi0, mi1
ci
R
 E(ki, mib)
ˆb
 (ˆb, b)
xi
R
 X
xi, ci
BF
A
Figure 5.1: Adversary BF in the proof of Theorem 5.2
Eventually, Ahalts and outputs a bit ˆb, at which time adversary BF halts and outputs
1 if ˆb= b, and outputs 0 otherwise.
See Fig. 5.1 for a picture of adversary BF. As usual, δ(x,y) is deﬁned to be 1 if x = y, and 0
otherwise.
Game 2. Next, we use our “faithful gnome” idea (see Section 4.4.2) to implement the random
function f. Our “gnome” has to keep track of the inputs to f, and detect if the same input is used
twice. In the following logic, our gnome uses a truly random key as the “default” value for ki, but
over-rides this default value if necessary, as indicated in the line marked ( ∗):
b←R {0,1}
for i←1 to Q do
xi ←R X
ki ←R K
(∗) if xi = xj for some j <ithen ki ←kj
upon receiving the ith query (mi0,mi1) ∈M2:
ci ←R E(ki,mib)
send (xi,ci) to the adversary.
As this is a faithful implementation of the random function f, we have
Pr[W2] = Pr[W1]. (5.9)
186
Game 3. Next, we make our gnome “forgetful,” simply dropping the line marked ( ∗) in the
previous game:
b←R {0,1}
for i←1 to Q do
xi ←R X
ki ←R K
upon receiving the ith query (mi0,mi1) ∈M2:
ci ←R E(ki,mib)
send (xi,ci) to the adversary.
To analyze the quantity|Pr[W3]−Pr[W2]|, we use the Diﬀerence Lemma (Theorem 4.7). To this
end, we view Games 2 and 3 as operating on the same underlying probability space: the random
choices made by the adversary and the challenger are identical in both games — all that diﬀers is
the rule used by the challenger to compute its responses. In particular, the variables xi are identical
in both games. Deﬁne Z to be the event that xi = xj for some i ̸= j. Clearly, Games 2 and 3
proceed identically unless Z occurs; in particular, W2 ∧¯Z occurs if and only if W3 ∧¯Z occurs.
Applying the Diﬀerence Lemma, we therefore have
⏐⏐Pr[W3] −Pr[W2]
⏐⏐≤Pr[Z]. (5.10)
Moreover, it is easy to see that
Pr[Z] ≤Q2
2N, (5.11)
since Z is the union of less than Q2/2 events, each of which occurs with probability 1 /N.
Observe that in Game 3, independent encryption keys ki are used to encrypt each message. So
next, we play our “semantic security card,” claiming that
|Pr[W3] −1/2|= MSSadv∗[ ¯BE,E], (5.12)
where ¯BE is an eﬃcient adversary that plays the bit-guessing version of Attack Game 5.1 with
respect to E, making at most Q queries to its challenger in that game.
The design of ¯BE is naturally suggested by the syntactic form of Game 3. It works as follows:
Playing the role of challenger to A, upon receiving the ith query ( mi0,mi1) from A,
adversary ¯BE submits (mi0,mi1) to its own challenger, obtaining a ciphertext ci ∈C;
then ¯BE selects xi at random from X, and sends (xi,ci) to Ain response to the latter’s
query.
When Aﬁnally outputs a bit ˆb, ¯BE outputs this same bit.
See Fig. 5.2 for a picture of adversary ¯BE.
It is evident from the construction (and (2.11)) that (5.12) holds. Moreover, by Theorem 5.1
and (5.1), we have
MSSadv∗[ ¯BE,E] = Q·SSadv∗[BE,E], (5.13)
where BE is an eﬃcient adversary playing the bit-guessing version of Attack Game 2.1 with respect
to E.
187
mi0, mi1
ˆb
MSS Challenger
mi0, mi1
ci
xi
R
 X
xi, ci
A
¯BE
Figure 5.2: Adversary ¯BE in the proof of Theorem 5.2
188
Putting together (5.7) through (5.13), we obtain (5.6). Also, one can check that the running
times of both BF and BE are roughly the same as that of A; indeed, they are elementary wrappers
around A, and (5.5) holds regardless of whether Ais eﬃcient. 2
While the above proof was a bit long, we hope the reader agrees that it was in fact quite natural,
and that all of the steps were fairly easy to follow. Also, this proof illustrates how one typically
employs more than one security assumption in devising a security proof as a sequence of games.
Remark 5.2. We brieﬂy mention that the hybrid construction E′ in Theorem 5.2 is CPA secure
even if the PRF F used in the construction is only weakly secure (as in Deﬁnition 4.3). To prove
Theorem 5.2 under this weaker assumption observe that in both Games 0 and 1 the challenger only
evaluates the PRF at random points in X. Therefore, the adversary’s advantage in distinguishing
Games 0 and 1 is negligible even if F is only weakly secure. 2
5.4.2 Randomized counter mode
We can build a CPA secure cipher directly out of a secure PRF, as follows. Suppose F is a PRF
deﬁned over (K,X,Y). We shall assume that X= {0,...,N −1}, and that Y= {0,1}n.
For any poly-bounded ℓ≥1, we deﬁne a cipher E= (E,D), with key space K, message space
Y≤ℓ, and ciphertext space X×Y ≤ℓ, as follows:
• for k∈K and m∈Y≤ℓ, with v:= |m|, we deﬁne
E(k,m) :=
x←R X
compute c∈Yv as follows:
for j ←0 to v−1 do
c[j] ←F(k,x + j mod N) ⊕m[j]
output (x,c);
• for k∈K and c′= (x,c) ∈X×Y ≤ℓ, with v:= |c|, we deﬁne
D(k,c′) :=
compute m∈Yv as follows:
for j ←0 to v−1 do
m[j] ←F(k,x + j mod N) ⊕c[j]
output m.
This cipher is much like the stream cipher one would get by building a PRG out of F using
the construction in Section 4.4.4. The diﬀerence is that instead of using a ﬁxed sequence of inputs
to F to derive a key stream, we use a random starting point, which we then increment to obtain
successive inputs to F. The x component of the ciphertext is typically called an initial value, or
IV for short.
In practice, F is typically implemented using the encryption function of a block cipher, and
X= Y= {0,1}n, where we naturally view n-bit strings as numbers in the range 0 ,..., 2n −1. As
it happens, the decryption function of the block cipher is not needed at all in this construction.
See Fig. 5.3 for an illustration of this mode.
It is easy to verify that Eis indeed a (probabilistic) cipher. Also, note that the message space
of Eis variable length, and that for the purposes of deﬁning CPA security using Attack Game 5.2,
the length of a message m∈Y≤ℓ is its natural length |m|.
189
E(k, ·) E(k, ·)E(k, ·)
m[0] m[1]
c[0] c[1]
(a) encryption
(b) decryption
     
E(k, ·) E(k, ·)E(k, ·)
m[0] m[1]
c[0] c[1]
     
m[2]
m[2]
c[2]
c[2]
hx +2 inhx +1 inhx +0 in
x
x
hx +0 in hx +1 in hx +2 in
Figure 5.3: Randomized counter mode ( v= 3)
190
Theorem 5.3. If F is a secure PRF and N is super-poly, then for any poly-bounded ℓ ≥1, the
cipher Edescribed above is a CPA secure cipher.
In particular, for every CPA adversary Athat attacks E as in Attack Game 5.2, and which
makes at most Q queries to its challenger, there exists a PRF adversary Bthat attacks F as in
Attack Game 4.2, where Bis an elementary wrapper around A, such that
CPAadv[A,E] ≤2Q2ℓ
N + 2 ·PRFadv[B,F]. (5.14)
Proof idea. Suppose we start with an adversary that plays the CPA attack game with respect to
E. First, using the assumption that F is a PRF, we can eﬀectively replace F by a truly random
function f. Second, using the assumption that N is super-poly, and the fact that each IV is chosen
at random, we can argue that except with negligible probability, the challenger never evaluates f
at the same point twice. But in this case, the challenger is eﬀectively encrypting each message
using an independent one-time pad, and so we can conclude that the adversary’s advantage in the
original CPA attack game is negligible. 2
Proof. Let Abe an eﬃcient adversary that plays Attack Game 5.2 with respect to E, and which
makes at most Q queries to its challenger in that game. We want to show that CPA adv[A,E] is
negligible, assuming that F is a secure PRF and that N is super-poly.
It is convenient to use the bit-guessing version of the CPA attack game. We prove:
CPAadv∗[A,E] ≤Q2ℓ
N + PRFadv[B,F] (5.15)
for an eﬃcient adversary B. Then (5.14) follows from (5.4).
The basic strategy of the proof is as follows. First, we deﬁne Game 0 to be the game played
between Aand the challenger in the bit-guessing version of Attack Game 5.2 with respect to
E. We then deﬁne several more games: Game 1, Game 2, and Game 3. Each of these games
is played between Aand a diﬀerent challenger. In each of these games, b denotes the random
bit chosen by the challenger, while ˆb denotes the bit output by A. Also, for j = 0 ,..., 3, we
deﬁne Wj to be the event that ˆb = b in Game j. We will show that for j = 1,..., 3, the value
|Pr[Wj] −Pr[Wj−1]|is negligible; moreover, it will be evident that Pr[W3] = 1/2, from which it will
follow that CPAadv∗[A,E] := |Pr[W0] −1/2|is negligible.
Game 0. We may describe the challenger in Game 0 as follows:
b←R {0,1}
k←R K
for i←1 to Q do
xi ←R X
for j ←0 to ℓ−1 do
x′
ij ←xi + j mod N
yij ←F(k,x′
ij)
upon receiving the ith query (mi0,mi1), with vi := |mi0|= |mi1|:
compute ci ∈Yvi as follows:
for j ←0 to vi −1 do: ci[j] ←yij ⊕mib[j]
send (xi,ci) to the adversary.
191
By construction, we have
CPAadv∗[A,E] =
⏐⏐Pr[W0] −1/2
⏐⏐. (5.16)
Game 1. Next, we play our “PRF card,” replacing F(k,·) by a truly random function f ∈
Funs[X,Y]. The challenger in this game looks like this:
b←R {0,1}
f ←R Funs[X,Y]
for i←1 to Q do
xi ←R X
for j ←0 to ℓ−1 do
x′
ij ←xi + j mod N
yij ←f(x′
ij)
···
We have left out part of the code for the challenger, as it will not change in any of our games.
We claim that ⏐⏐Pr[W1] −Pr[W0]
⏐⏐= PRFadv[B,F], (5.17)
where Bis an eﬃcient adversary; moreover, since we are assuming that F is a secure PRF, it must
be the case that PRF adv[B,F] is negligible. This is hopefully (by now) a routine argument, and
we leave the details of this to the reader.
Game 2. Next, we use our “faithful gnome” idea to implement the random function f. In
describing the logic of our challenger in this game, we use the standard lexicographic ordering on
pairs of indices ( i,j); that is, ( i′,j′) <(i,j) if and only if
i′<i or i′= i and j′<j.
In the following logic, our “gnome” uses a truly random value as the “default” value for each yij,
but over-rides this default value if necessary, as indicated in the line marked ( ∗):
b←R {0,1}
for i←1 to Q do
xi ←R X
for j ←0 to ℓ−1 do
x′
ij ←xi + j mod N
yij ←R Y
(∗) if x′
ij = x′
i′j′ for some (i′,j′) <(i,j) then yij ←yi′j′
···
As this is a faithful implementation of the random function f, we have
Pr[W2] = Pr[W1]. (5.18)
Game 3. Now we make our gnome “forgetful,” dropping the line marked (∗) in the previous game:
192
b←R {0,1}
for i←1 to Q do
xi ←R X
for j ←0 to ℓ−1 do
x′
ij ←xi + j mod N
yij ←R Y
···
To analyze the quantity|Pr[W3]−Pr[W2]|, we use the Diﬀerence Lemma (Theorem 4.7). To this
end, we view Games 2 and 3 as operating on the same underlying probability space: the random
choices made by the adversary and the challenger are identical in both games — all that diﬀers
is the rule used by the challenger to compute its responses. In particular, the variables x′
ij are
identical in both games. Deﬁne Z to be the event that x′
ij = x′
i′j′ for some (i,j) ̸= (i′,j′). Clearly,
Games 2 and 3 proceed identically unless Z occurs; in particular, W2 ∧¯Z occurs if and only if
W3 ∧¯Z occurs. Applying the Diﬀerence Lemma, we therefore have
⏐⏐Pr[W3] −Pr[W2]
⏐⏐≤Pr[Z]. (5.19)
We claim that
Pr[Z] ≤Q2ℓ
N . (5.20)
To prove this claim, we may assume thatN ≥2ℓ(this should generally hold, since we are assuming
that ℓ is poly-bounded and N is super-poly). Observe that Z occurs if and only if
{xi,...,x i + ℓ−1}∩{xi′,...,x i′+ ℓ−1}̸= ∅
for some pair of indices i and i′ with i ̸= i′ (and arithmetic is done mod N). Consider any ﬁxed
such pair of indices. Conditioned on any ﬁxed value of xi, the value xi′ is uniformly distributed
over {0,...,N −1}, and the intervals overlap if and only if
xi′ ∈{xi + j : −ℓ+ 1 ≤j ≤ℓ−1},
which happens with probability (2ℓ−1)/N. The inequality (5.20) now follows, as there are Q(Q−
1)/2 ways to choose i and i′.
Finally, observe that in Game 3 the yij values are uniformly and independently distributed over
Y, and thus the challenger is essentially using independent one-time pads to encrypt. In particular,
it is easy to see that the adversary’s output in this game is independent of b. Therefore,
Pr[W3] = 1/2. (5.21)
Putting together (5.16) through (5.21), we obtain (5.15), and the theorem follows. 2
Remark 5.3. One can also view randomized counter mode as a special case of the generic hybrid
construction in Section 5.4.1. See Exercise 5.5. 2
193
5.4.2.1 Case study: AES counter mode
The IPsec protocol uses a particular variant of AES counter mode, as speciﬁed in RFC 3686.
Recall that AES uses a 128 bit block. Rather than picking a random 128-bit IV for every message,
RFC 3686 picks the IV as follows:
• The most signiﬁcant 32 bits are chosen at random at the time that the secret key is generated
and are ﬁxed for the life of the key. The same 32 bit value is used for all messages encrypted
using this key.
• The next 64 bits are chosen at random in {0,1}64.
• The least signiﬁcant 32 bits are set to the number 1.
This resulting 128-bit IV is used as the initial value of the counter. When encrypting a message,
the least signiﬁcant 32 bits are incremented by one for every block of the message. Consequently,
the maximum message length that can be encrypted is 2 32 AES blocks or 2 36 bytes.
With this choice of IV the decryptor knows the 32 most signiﬁcant bits of the IV as well as
the 32 least signiﬁcant bits. Hence, only 64 bits of the IV need to be sent with the ciphertext.
The proof of Theorem 5.3 can be adapted to show that this method of choosing IVs is secure.
The slight advantage of this method over picking a random 128-bit IV is that the resulting ciphertext
is a little shorter. A random IV forces the encryptor to include all 128 bits in the ciphertext. With
the method of RFC 3686 only 64 bits are needed, thus shrinking the ciphertext by 8 bytes.
5.4.3 CBC mode
A historically important encryption method is to use a block cipher in cipher block chaining (CBC)
mode. This method is used in older versions of the TLS protocol (e.g., TLS 1.0). It is inferior to
counter mode encryption as discussed in the next section.
Suppose E= (E,D) is a block cipher deﬁned over ( K,X), where X= {0,1}n. Let N := |X|=
2n. For any poly-bounded ℓ ≥1, we deﬁne a cipher E′ = ( E′,D′), with key space K, message
space X≤ℓ, and ciphertext space X≤ℓ+1 \X0; that is, the ciphertext space consists of all nonempty
sequences of at most ℓ+ 1 data blocks. Encryption and decryption are deﬁned as follows:
• for k∈K and m∈X≤ℓ, with v:= |m|, we deﬁne
E′(k,m) :=
compute c∈Xv+1 as follows:
c[0] ←R X
for j ←0 to v−1 do
c[j+ 1] ←E(k, c[j] ⊕m[j])
output c;
• for k∈K and c∈X≤ℓ+1 \X0, with v:= |c|−1, we deﬁne
D′(k,c) :=
compute m∈Xv as follows:
for j ←0 to v−1 do
m[j] ←D(k, c[j+ 1]) ⊕c[j]
output m.
194
See Fig. 5.4 for an illustration of the encryption and decryption algorithm in the case |m|= 3.
Here, the ﬁrst component c[0] of the ciphertext is also called an initial value, or IV. Note that
unlike the counter mode construction in Section 5.4.2, in CBC mode, we must use a block cipher,
as we actually need to use the decryption algorithm of the block cipher.
It is easy to verify that E′is indeed a (probabilistic) cipher. Also, note that the message space
of Eis variable length, and that for the purposes of deﬁning CPA security using Attack Game 5.2,
the length of a message m∈X≤ℓ is its natural length |m|.
Theorem 5.4. If E = ( E,D) is a secure block cipher deﬁned over (K,X), and N := |X| is
super-poly, then for any poly-bounded ℓ≥1, the cipher E′ described above is a CPA secure cipher.
In particular, for every CPA adversary Athat attacks E′as in the bit-guessing version of Attack
Game 5.2, and which makes at most Q queries to its challenger, there exists BC adversary B
that attacks Eas in Attack Game 4.1, where Bis an elementary wrapper around A, such that
CPAadv[A,E′] ≤2Q2ℓ2
N + 2 ·BCadv[B,E]. (5.22)
Proof idea. The basic idea of the proof is very similar to that of Theorem 5.3. We start with an
adversary that plays the CPA attack game with respect toE′. We then replace Eby a truly random
function f. Then we argue that except with negligible probability, the challenger never evaluates f
at the same point twice. But then what the adversary sees is nothing but a bunch of random bits,
and so learns nothing at all about the message being encrypted. 2
Proof. Let Abe an eﬃcient CPA adversary that attacks E′ as in Attack Game 5.2. Assume that
Amakes at most Q queries to its challenger in that game. We want to show that CPA adv∗[A,E′]
is negligible, assuming that E is a secure block cipher and that N is super-poly. Under these
assumptions, by Corollary 4.5, the encryption function E is a secure PRF, deﬁned over ( K,X,X).
It is convenient to use the bit-guessing version of the CPA attack game, We prove:
CPAadv∗[A,E′] ≤Q2ℓ2
N + BCadv[B,E] (5.23)
for an eﬃcient adversary B. Then (5.22) follows from (5.4).
As usual, we deﬁne a sequence of games: Game 0, Game 1, Game 2, Game 3. Each of these
games are played between Aand a challenger. The challenger in Game 0 is the one from the
bit-guessing version of Attack Game 5.2 with respect to E′. In each of these games, b denotes the
random bit chosen by the challenger, while ˆb denotes the bit output by A. Also, for j = 0,..., 3,
we deﬁne Wj to be the event that ˆb= b in Game j. We will show that for j = 1,..., 3, the value
|Pr[Wj] −Pr[Wj−1]|is negligible; moreover, it will be evident that Pr[W3] = 1/2, from which it will
follow that |Pr[W0] −1/2|is negligible.
Here we go!
Game 0. We may describe the challenger in Game 0 as follows:
195
⊕
⊕
⊕
m[0]
m[1]
m[2]
c[1]
c[2]
c[3]
c[0]
E(k, ·)
E(k, ·)
E(k, ·)
(a) encryption
⊕
⊕
⊕
m [0]
m [1]
m [2 ]
c[1]
c[2]
c[3 ]
c[0 ]
D (k , ·)
D (k , ·)
D (k , ·)
(b) decryption
Figure 5.4: Encryption and decryption for CBC mode with ℓ= 3
196
b←R {0,1}, k←R K
upon receiving the ith query (mi0,mi1), with vi := |mi0|= |mi1|:
compute ci ∈Xvi+1 as follows:
ci[0] ←R X
for j ←0 to vi −1 do
xij ←ci[j] ⊕mib[j]
ci[j+ 1] ←E(k,xij)
send ci to the adversary.
By construction, we have
CPAadv∗[A,E′] =
⏐⏐Pr[W0] −1/2
⏐⏐. (5.24)
Game 1. We now play the “PRF card,” replacing E(k,·) by a truly random function f ∈
Funs[X,X]. Our challenger in this game looks like this:
b←R {0,1}, f ←R Funs[X,X]
upon receiving the ith query (mi0,mi1), with vi := |mi0|= |mi1|:
compute ci ∈Xvi+1 as follows:
ci[0] ←R X
for j ←0 to vi −1 do
xij ←ci[j] ⊕mib[j]
ci[j+ 1] ←f(xij)
send ci to the adversary.
We claim that ⏐⏐Pr[W1] −Pr[W0]
⏐⏐= PRFadv[B,E], (5.25)
where Bis an eﬃcient adversary; moreover, since we are assuming that Eis a secure block cipher,
and that N is super-poly, it must be the case that PRF adv[B,E] is negligible. This is hopefully
(by now) a routine argument, and we leave the details of this to the reader.
Game 2. The next step in this dance should by now be familiar: we implement f using a faithful
gnome. We do so by introducing random variables yij which represent the “default” values for ci[j],
which get over-ridden if necessary in the line marked ( ∗) below:
b←R {0,1}
set yij ←R Xfor i= 1,...,Q and j = 0,...,ℓ
upon receiving the ith query (mi0,mi1), with vi := |mi0|= |mi1|:
compute ci ∈Xvi+1 as follows:
ci[0] ←yi0
for j ←0 to vi −1 do
xij ←ci[j] ⊕mib[j]
ci[j+ 1] ←yi(j+1)
(∗) if xij = xi′j′ for some (i′,j′) <(i,j) then ci[j+ 1] ←ci′[j′+ 1]
send ci to the adversary.
We clearly have
Pr[W2] = Pr[W1]. (5.26)
Game 3. Now we make the gnome forgetful, removing the check in the line marked ( ∗):
197
b←R {0,1}
set yij ←R Xfor i= 1,...,Q and j = 0,...,ℓ
upon receiving the ith query (mi0,mi1), with vi := |mi0|= |mi1|:
compute ci ∈Xvi+1 as follows:
ci[0] ←yi0
for j ←0 to vi −1 do
xij ←ci[j] ⊕mib[j]
ci[j+ 1] ←yi(j+1)
send ci to the adversary.
To analyze the quantity|Pr[W3]−Pr[W2]|, we use the Diﬀerence Lemma (Theorem 4.7). To this
end, we view Games 2 and 3 as operating on the same underlying probability space: the random
choices made by the adversary and the challenger are identical in both games — all that diﬀers is
the rule used by the challenger to compute its responses.
We deﬁne Z to be the event that xij = xi′j′ in Game 3. Note that the event Z is deﬁned in
terms of the xij values in Game 3. Indeed, the xij values may not be computed in the same way in
Games 2 and 3, and so we have explicitly deﬁned the event Z in terms of their values in Game 3.
Nevertheless, it is clear that Games 2 and 3 proceed identically unless Z occurs; in particular,
W2 ∧¯Z occurs if and only if W3 ∧¯Z occurs. Applying the Diﬀerence Lemma, we therefore have
⏐⏐Pr[W3] −Pr[W2]
⏐⏐≤Pr[Z]. (5.27)
We claim that
Pr[Z] ≤Q2ℓ2
2N . (5.28)
To prove this, let Coins denote the random choices made by A. Observe that in Game 3, the values
Coins, b, y ij (i= 1,...Q, j = 0,...,ℓ )
are independently distributed.
Consider any ﬁxed index i = 1,...,Q . Let us condition on any ﬁxed values of Coins, b, and
yi′j for i′ = 1 ,...,i −1 and j = 0 ,...,ℓ . In this conditional probability space, the values of
mi0, mi1, and vi are completely determined, as are the values vi′ and xi′j for i′= 1,...,i −1 and
j = 0,...,v i′−1; however, the values ofyi0,...,y iℓ are still uniformly and independently distributed
over X. Moreover, as xij = yij⊕mib[j] for j = 0,...,v i−1, it follows that these xij values are also
uniformly and independently distributed over X. Thus, for any ﬁxed index j = 0,...,v i −1, and
any ﬁxed indices i′ and j′, with ( i′,j′) < (i,j), the probability that xij = xi′j′ in this conditional
probability space is 1/N. The bound (5.28) now follows from an easy calculation.
Finally, we claim that
Pr[W3] = 1/2. (5.29)
This follows from the fact that
Coins, b, y ij (i= 1,...Q, j = 0,...,ℓ )
are independently distributed, and the fact that the adversary’s output ˆb is a function of
Coins, yij (i= 1,...Q, j = 0,...,ℓ ).
198
From this, we see that ˆb and b are independent, and so (5.29) follows immediately.
Putting together (5.24) through (5.29), we have
CPAadv∗[A,E′] ≤Q2ℓ2
2N + PRFadv[B,E].
By Theorem 4.4, we have
⏐⏐BCadv[B,E] −PRFadv[B,E]
⏐⏐≤Q2ℓ2
2N ,
and (5.23) follows, which proves the theorem. 2
5.4.4 Case study: CBC padding in TLS 1.0
Let E= (E,D) be a block cipher with domain X. Our description of CBC mode encryption using E
assumes that messages to be encrypted are elements of X≤ℓ. When the domain is X= {0,1}128,
as in the case of AES, this implies that we can only encrypt messages whose length is a multiple
of 16 bytes. But what if the message length is not a multiple of the block size?
Suppose we wish to encrypt a v-byte message m using AES in CBC mode when v is not
necessarily a multiple of 16. The ﬁrst thing that comes to mind is to pad the message m so that
its length in bytes is a multiple of 16. Clearly the padding function must be invertible so that the
padding can be removed during decryption.
The TLS 1.0 protocol deﬁnes the following padding function for encrypting a v-byte message
with AES in CBC mode: let p:= 16 −(vmod 16), then append p bytes to the message m, where
each byte has value p−1. For example, consider the following two cases:
• if m is 29 bytes long then p = 3 and the pad consists of the three bytes “222” so that the
padded message is 32 bytes long which is exactly two AES blocks.
• if the length of m is a multiple of the block size, say 32 bytes, then p = 16 and the pad
consists of 16 bytes. The padded message is then 48 bytes long which is three AES blocks.
It may seem odd that when the message is a multiple of the block size we add a full dummy block at
the end. This is necessary so that the decryption procedure can properly remove the pad. Indeed,
it should be clear that this padding method is invertible for all input message lengths.
It is an easy fact to prove that every invertible padding scheme for CBC mode encryption built
from a secure block cipher gives a CPA secure cipher for messages of arbitrary length.
Padding in CBC mode can be avoided using a method called ciphertext stealing as long as
the plaintext is longer than a single block. The ciphertext stealing variant of CBC is the topic
of Exercise 5.16. When encrypting messages whose length is less than a block, say single byte
messages, there is still a need to pad.
5.4.5 Concrete parameters and a comparison of counter and CBC modes
We conclude this section with a comparison of the counter and CBC mode constructions. We
assume that counter mode is implemented with a PRF F that maps n-bit blocks to n-bit blocks,
and that CBC is implemented with an n-bit block cipher. In each case, the message space consists
199
of sequences of at most ℓ n-bit data blocks. With the security theorems proved in this section, we
have the following bounds:
CPAadv[A,Ectr] ≤4Q2ℓ
2n + 2 ·PRFadv[BF,F],
CPAadv[A,Ecbc] ≤2Q2ℓ2
2n + 2 ·BCadv[BE,E].
Here, Ais any CPA adversary making at most Qqueries to its challenger, ℓis the maximum length
(in data blocks) of any one message. For the purposes of this discussion, let us simply ignore the
terms PRFadv[BF,F] and BCadv[BE,E].
One can immediately see that counter mode has a quantitative security advantage. To make
things more concrete, suppose the block size is n= 128, and that each message is 1MB (2 23 bits)
so that ℓ= 216 blocks. If we want to keep the adversary’s advantage below 2 −32, then for counter
mode, we can encrypt up to Q = 239.5 messages, while for CBC we can encrypt only up to 2 32
messages. Once Q messages are encrypted with a given key, a fresh key must be generated and
used for subsequent messages. Therefore, with counter mode a single key can be used to securely
encrypt many more messages than with CBC.
We should warn that this quantitative advantage disappears if we use a block cipher to imple-
ment F, as we get the same quadratic dependence on ℓdue to the error term in the PRF switching
lemma (Theorem 4.4). However, for a custom built PRF, this quantitative advantage applies.
Counter mode has several other advantages over CBC:
• Parallelism and pipelining. Encryption and decryption for counter mode is trivial to paral-
lelize, whereas encryption in CBC mode is inherently sequential (decryption in CBC mode
is parallelizable). Modes that support parallelism greatly improve performance when the un-
derlying hardware can execute many instructions in parallel as is often the case in modern
processors. More importantly, consider a hardware implementation of a single block cipher
round that supports pipelining, as in Intel’s implementation of AES-128 (page 121). Pipelin-
ing enables multiple encryption instructions to execute at the same time. A parallel mode
such as counter mode keeps the pipeline busy, whereas in CBC encryption the pipeline is
mostly unused due to the sequential nature of this mode. As a result, counter mode encryp-
tion on Intel’s Haswell processors is about seven times faster than CBC mode encryption,
assuming the plaintext data is already loaded into L1 cache.
• Shorter ciphertext length. For very short messages, counter mode ciphertexts are signiﬁcantly
shorter than CBC mode ciphertexts. Consider, for example, a one-byte plaintext (which arises
naturally when encrypting individual key strokes as in SSH). A counter mode ciphertext need
only be one block plus one byte: one block for the random IV plus one byte for the encrypted
plaintext. In contrast, a CBC ciphertext is two full blocks. This results in 15 redundant bytes
per CBC ciphertext assuming 128-bit blocks.
• Encryption only. CBC mode uses both algorithms E and D of the block cipher whereas
counter mode uses only algorithm E. This can reduce an implementation code size.
Remark 5.4. Both randomized counter mode and CBC require a random IV. Some crypto libraries
actually leave it to the higher-level application to supply the IV. This can lead to problems if the
200
higher-level applications do not take pains to ensure the IVs are suﬃciently random. For example,
for counter mode, it is necessary that the IVs are suﬃciently spread out, so that the corresponding
intervals do not overlap. In fact, this property is suﬃcient as well. In contrast, for CBC mode,
more is required: it is essential that IVs be unpredictable — see Exercise 5.13.
Leaving it to the higher-level application to supply the IV is actually an example of nonce-based
encryption, which we will explore in detail next, in Section 5.5. 2
5.5 Nonce-based encryption
All of the CPA-secure encryption schemes we have seen so far suﬀer from ciphertext expansion :
ciphertexts are longer than plaintexts. For example, the generic hybrid construction in Section 5.4.1
generates ciphertexts ( x,c), where x belongs to the input space of some PRF and c encrypts
the actual message; the counter mode construction in Section 5.4.2 generates ciphertexts of the
essentially same form (x,c); similarly, the CBC mode construction in Section 5.4.3 includes the IV
as a part of the ciphertext.
For very long messages, the expansion is not too bad. For example, with AES and counter
mode or CBC mode, a 1MB message results is a ciphertext that is just 16 bytes longer, which may
be a perfectly acceptable expansion rate. However, for messages of 16 bytes or less, ciphertexts are
at least twice as long as plaintexts.
The bad news is, some amount of ciphertext expansion is inevitable for any CPA-secure encryp-
tion scheme (see Exercise 5.10). The good news is, in certain settings, one can get by without any
ciphertext expansion. For example, suppose Alice and Bob are fully synchronized, so that Alice
ﬁrst sends an encryption of m1, then an encryption of m2, and so on, while Bob ﬁrst decrypts the
encryption of m1, then decrypts the encryption of m2, and so on. For concreteness, assume Alice
and Bob are using the generic hybrid construction of Section 5.4.1. Recall that the encryption of
message mi is ( xi,ci), where ci := E(ki,mi) and ki := F(k′,xi). The essential property of the
xi’s needed to ensure security was simply that they are distinct. When Alice and Bob are fully
synchronized (i.e., ciphertexts sent by Alice reach Bob in-order), they simply have to agree on a
ﬁxed sequence x1,x2,..., of distinct elements in the input space of the PRF F. For example, xi
might simply be the binary encoding of i.
This mode of operation of an encryption scheme does not really ﬁt into our deﬁnitional frame-
work. Historically, there are two ways to modify the framework to allow for this type of operation.
One approach is to allow for stateful encryption schemes, where both the encryption and decryption
algorithms maintain some internal state that evolves with each application of the algorithm. In the
example of the previous paragraph, the state would just consist of a counter that is incremented
with each application of the algorithm. This approach requires encryptor and decryptor to be fully
synchronized, which limits its applicability, and we shall not discuss it further.
The second, and more popular, approach is called nonce-based encryption. Instead of main-
taining internal states, both the encryption and decryption algorithms take an additional input N ,
called a nonce. The syntax for nonce-based encryption becomes
c= E(k,m, N ),
where c∈C is the ciphertext, k ∈K is the key, m∈M is the message, and N ∈N is the nonce.
Moreover, the encryption algorithm E is required to be deterministic. Likewise, the decryption
201
syntax becomes
m= D(k,c, N ).
The intention is that a message encrypted with a particular nonce should be decrypted with the
same nonce — it is up to the application using the encryption scheme to enforce this. More formally,
the correctness requirement is that
D(k, E(k,m, N ), N ) = m
for all k ∈K, m∈M, and N ∈N . We say that such a nonce-based cipher E= (E,D) is deﬁned
over (K,M,C,N ).
Intuitively, a nonce-based encryption scheme is CPA secure if it does not leak any useful in-
formation to an eavesdropper, assuming that no nonce is used more than once in the encryption
process — again, it is up to the application using the scheme to enforce this. Note that this require-
ment on how nonces are used is very weak, much weaker than requiring that they are unpredictable,
let alone randomly chosen. Of course, since nonces are not meant to be secret, their value must
not reveal anything about the plaintext.
We can readily formalize this notion of security by slightly tweaking our original deﬁnition of
CPA security.
Attack Game 5.3 (nonce-based CPA security). For a given cipher E = ( E,D), deﬁned
over (K,M,C,N ), and for a given adversary A, we deﬁne two experiments, Experiment 0 and
Experiment 1. For b= 0,1, we deﬁne
Experiment b:
• The challenger selects k←R K.
• The adversary submits a sequence of queries to the challenger.
For i= 1,2,..., the ith query is a pair of messages, mi0,mi1 ∈M, of the same length, and
a nonce N i ∈N \{N 1,..., N i−1}.
The challenger computes ci ←E(k,mib,N i), and sends ci to the adversary.
• The adversary outputs a bit ˆb∈{0,1}.
For b= 0,1, let Wb be the event that Aoutputs 1 in Experiment b. We deﬁne A’s advantage
with respect to Eas
nCPAadv[A,E] := |Pr[W0] −Pr[W1]|. 2
Note that in the above game, the nonces are completely under the adversary’s control, subject
only to the constraint that they are unique.
Deﬁnition 5.3 (nonce-based CPA security). A nonce-based cipher Eis called semantically
secure against chosen plaintext attack , or simply CPA secure, if for all eﬃcient adversaries
A, the value nCPAadv[A,E] is negligible.
As usual, as in Section 2.2.5, Attack Game 5.3 can be recast as a “bit guessing” game, and we
have
nCPAadv[A,E] = 2 ·nCPAadv∗[A,E], (5.30)
where nCPAadv∗[A,E] := |Pr[ˆb = b] −1/2|in a version of Attack Game 5.3 where the challenger
just chooses b at random.
202
5.5.1 Nonce-based generic hybrid encryption
Let us recast the generic hybrid construction in Section 5.4.1 as a nonce-based encryption scheme.
As in that section, Eis a cipher, which we shall now insist is deterministic, deﬁned over ( K,M,C),
and F is a PRF deﬁned over (K′,X,K). We deﬁne the nonce-based cipher E′, which is deﬁned over
(K′,M,C,X), as follows:
• for k′∈K′, m∈M, and x∈X, we deﬁne E′(k′,m,x ) := E(k,m), where k:= F(k′,x);
• for k′∈K′, c∈C, x∈X, we deﬁne D′(k′,c,x ) := D(k,c), where k:= F(k′,x).
All we have done is to treat the value x∈X as a nonce; otherwise, the scheme is exactly the same
as that deﬁned in Section 5.4.1.
One can easily verify the correctness requirement for E′. Moreover, one can easily adapt the
proof of Theorem 5.2 to prove the following:
Theorem 5.5. If F is a secure PRF and E is a semantically secure cipher, then the cipher E′
described above is a CPA secure cipher.
In particular, for every nCPA adversary Athat attacks E′ as in the bit-guessing version of
Attack Game 5.3, and which makes at most Q queries to its challenger, there exists a PRF
adversary BF that attacks F as in Attack Game 4.2, and an SS adversary BE that attacks Eas
in the bit-guessing version of Attack Game 2.1, where both BF and BE are elementary wrappers
around A, such that
nCPAadv[A,E′] ≤2 ·PRFadv[BF,F] + Q·SSadv[BE,E]. (5.31)
We leave the proof as an exercise for the reader. Note that the term Q2
N in (5.5), which represent
the probability of a collision on the input to F, is missing from (5.31), simply because by deﬁnition,
no collisions can occur.
5.5.2 Nonce-based Counter mode
Next, we recast the counter-mode cipher from Section 5.4.2 to the nonce-based encryption setting.
Let us make a ﬁrst attempt, by simply treating the value x∈X in that construction as a nonce.
Unfortunately, this scheme cannot satisfy the deﬁnition of nonce-based CPA security. The
problem is, an attacker could choose two distinct nonces x1,x2 ∈ X, such that the intervals
{x1,...,x 1 + ℓ−1}and {x2,...,x 2 + ℓ−1}overlap (again, arithmetic is done mod N). In this
case, the security proof will break down; indeed, it is easy to mount a quite devastating attack, as
discussed in Section 5.1, since that attacker can essentially force the encryptor to re-use some of
the same bits of the “key stream”.
Fortunately, the ﬁx is easy. Let us assume that ℓ divides N (in practice, both ℓ and N will be
powers of 2, so this is not an issue). Then we use as the nonce space {0,...,N/ℓ −1}, and translate
the nonce N to the PRF input x := N ℓ. It is easy to see that for any two distinct nonces N 1 and
N 2, for x1 := N 1ℓ and x2 := N 2ℓ, the intervals {x1,...,x 1 + ℓ−1}and {x2,...,x 2 + ℓ−1}do not
overlap.
With Emodiﬁed in this way, we can easily adapt the proof of Theorem 5.3 to prove the following:
Theorem 5.6. If F is a secure PRF, then the nonce-based cipher Edescribed above is CPA secure.
203
In particular, for every nCPA adversary Athat attacks Eas in Attack Game 5.3, there exists
a PRF adversary Bthat attacks F as in Attack Game 4.2, where Bis an elementary wrapper
around A, such that
nCPAadv[A,E] ≤2 ·PRFadv[B,F]. (5.32)
We again leave the proof as an exercise for the reader.
5.5.3 Nonce-based CBC mode
Finally, we consider how to recast the CBC-mode encryption scheme in Section 5.4.3 as a nonce-
based encryption scheme. As a ﬁrst attempt, one might simply try to view the IV c[0] as a nonce.
Unfortunately, this does not yield a CPA secure nonce-based encryption scheme. In the nCPA
attack game, the adversary could make two queries:
(m10,m11,N 1),
(m20,m21,N 2),
where
m10 = N 1 ̸= N 2 = m20, m11 = m21.
Here, all messages are one-block messages. In Experiment 0 of the attack game, the resulting
ciphertexts will be the same, whereas in Experiment 1, they will be diﬀerent. Thus, we can
perfectly distinguish between the two experiments.
Again, the ﬁx is fairly straightforward. The idea is to map nonces to pseudo-random IV’s by
passing them through a PRF. So let us assume that we have a PRF F deﬁned over ( K′,N ,X).
Here, the key space K′ and input space N of F may be arbitrary sets, but the output space Xof
F must match the block space of the underlying block cipher E= (E,D), which is deﬁned over
(K,X). In the nonce-based CBC scheme E′, the key space is K×K ′, and in the encryption and
decryption algorithms, the IV is computed from the nonce N and key k′as c[0] := F(k′,N ).
With these modiﬁcations, we can now prove the following variant of Theorem 5.4:
Theorem 5.7. If E = ( E,D) is a secure block cipher deﬁned over (K,X), and N := |X| is
super-poly, and F is a secure PRF deﬁned over (K′,N ,X), then for any poly-bounded ℓ ≥1, the
nonce-based cipher E′ described above is CPA secure.
In particular, for every nCPA adversary Athat attacks E′as in the bit-guessing version of Attack
Game 5.3, and which makes at most Q queries to its challenger, there exists BC adversary B
that attacks E as in Attack Game 4.1, and a PRF adversary BF that attacks F as in Attack
Game 4.2, where Band BF are elementary wrappers around A, such that
nCPAadv[A,E′] ≤2Q2ℓ2
N + 2 ·PRFadv[BF,F] + 2·BCadv[B,E]. (5.33)
Again, we leave the proof as an exercise for the reader. Note that in the above construction,
we may use the underlying block cipher Efor the PRF F; however, it is essential that independent
keys k and k′are used (see Exercise 5.14).
Nonce-based CBC mode as described above is called CBC-ESSIV (encrypted salt-sector IV) and
is used in full disk encryption systems, such as dm-crypt. When encrypting a sector on disk, the
sector number is used as the nonce. Other full disk encryption systems use tweakable encryption,
as discussed in Exercise 4.11).
204
5.6 A fun application: revocable broadcast encryption
Movie studios spend a lot of eﬀort making blockbuster movies, and then sell the movies (on DVDs)
to millions of customers who purchase them to watch at home. A customer should be able to watch
movies on a stateless standalone movie player, that has no network connection.
The studios are worried about piracy, and do not want to send copyrighted digital content in the
clear to millions of users. A simple solution could work as follows. Every authorized manufacturer
is given a device key kd ∈K, and it embeds this key in every device that it sells. If there are a
hundred authorized device manufacturers, then there are a hundred device keys k(1)
d ,...,k (100)
d . A
movie m is encrypted as:
cm :=



k←R K
for i= 1,..., 100 : ci ←R E(k(i)
d , k)
c←R E′(k,m)
output (c1,...,c 100, c)



where (E,D) is a CPA secure cipher, and ( E′,D′) is semantically secure with key space K. We
analyze this construction in Exercise 5.4, where we show that it is CPA secure. We refer to
(c1,...,c 100) as the ciphertext header, and refer to c as the body.
Now, every authorized device can decrypt the movie using its embedded device key. First,
decrypt the appropriate ciphertext in the header, and then use the obtained key k to decrypt the
body. This mechanism forms the basis of the content scrambling system (CSS) used to encrypt
DVDs. We previously encountered CSS in Section 3.8.
The trouble with this scheme is that once a single device is compromised, and its device key
kd is extracted and published, then anyone can use this kd to decrypt every movie ever published.
There is no way to revoke kd without breaking many consumer devices in the ﬁeld. In fact, this is
exactly how CSS was broken: the device key was extracted from an authorized player, and then
used in a system called DeCSS to decrypt encrypted DVDs.
The lesson from CSS is that global unrevocable device keys are a bad idea. Once a single key
is leaked, all security is lost. When the DVD format was updated to a new format called Blu-ray,
the industry got a second chance to design the encryption scheme. In the new scheme, called the
Advanced Access Content System (AACS), every device gets a random device key unique to
that device. The system is designed to support billions of devices, each with its own key.
The goals of the system are twofold. First, every authorized device should be able to decrypt
every Blu-ray disk. Second, whenever a device key is extracted and published, it should be possible
to revoke that key, so that this device key cannot be used to decrypt future Blu-ray disks, but
without impacting any other devices in the ﬁeld.
A revocable broadcast system. Suppose there are ndevices in the system, where for simplicity,
let us assume nis a power of two. We treat these ndevices as the leaves of a complete binary tree,
as shown in Fig. 5.5. Every node in the tree is assigned a random key in the key space K. The
keys embedded in device number i ∈{1,...,n }is the set of keys on the path from leaf number i
to the root. This way, every device is given exactly log 2 n keys in K.
When the system is ﬁrst launched, and no device keys are yet revoked, all content is encrypted
using the key at the root (key number 15 in Fig. 5.5). More precisely, we encrypt a movie m as:
cm :=
{
k←R K, c1 ←R E(kroot, k), c←R E′(k,m), output (c1,c)
}
205
k1 k2 k3 k4 k5 k6 k7 k8
k9 k10 k11 k12
k13 k14
k15
Figure 5.5: The tree of keys for n= 8 devices; shaded nodes are the keys embedded in device 3.
Because all devices have the root key kroot, all devices can decrypt.
Revoking devices. Now, suppose device number i is attacked, and all the keys stored on it are
published. Then all future content will be encrypted using the keys associated with the siblings of
the log2 nnodes on the path from leaf ito the root. For example, when device number 3 in Fig. 5.5
is revoked, all future content is encrypted using the three keys k4,k9,k14 as
cm :=



k←R K
c1 ←R E(k4, k), c2 ←R E(k9, k), c3 ←R E(k14, k)
c←R E′(k,m)
output (c1,c2,c3,c)



(5.34)
Again, ( c1,c2,c3) is the ciphertext header, and c is the ciphertext body. Observe that device
number 3 cannot decrypt cm, because it cannot decrypt any of the ciphertexts in the header.
However, every other device can easily decrypt using one of the keys at its disposal. For example
device number 6 can use k14 to decrypt c3. In eﬀect, changing the encryption scheme to encrypt
as in (5.34) revokes device number 3, without impacting any other device. The cost to this is that
the ciphertext header now contains log 2 n blocks, as opposed to a single block before the device
was revoked.
More generally, suppose r devices have been compromised and need to be revoked. Let S ⊆
{1,...,n }be the set of non-compromised devices, so that that |S|= n−r. New content will be
encrypted using keys in the tree so that devices in S can decrypt, but all devices outside of S
cannot. The set of keys that makes this possible is characterized by the following deﬁnition:
Deﬁnition 5.4. Let T be a complete binary tree with n leaves, where n is a power of two. Let
S ⊆{1,...,n }be a set of leaves. We say that a set of nodes W ⊆{1,..., 2n−1}covers the set
S if every leaf in S is a descendant of some node in W, and leaves outside of S are not. We use
cover(S) to denote the smallest set of nodes that covers S.
Fig. 5.6 gives an example of a cover of the set of leaves {1,2,4,5,6}. The ﬁgure captures a
setting where devices number 3, 7, and 8 are revoked. It should be clear that if we use keys in
cover(S) to encrypt a movie m, then devices in S can decrypt, but devices outside of S cannot. In
206
particular, we encrypt m as follows:
cm :=



k←R K
for u∈cover(S) : cu ←R E(ku, k)
c←R E′(k,m)
output ({cu}u∈cover(S), c)



. (5.35)
Security of this scheme is discussed in Exercise 5.21.
The more devices are revoked, the larger the header of cm becomes. The following theorem
shows how big the header gets in the worst case. The proof is an induction argument that also
suggests an eﬃcient recursive algorithm to compute an optimal cover.
Theorem 5.8. Let T be a complete binary tree with nleaves, where nis a power of two. For every
1 ≤r≤n, and every set S of n−r leaves, we have
|cover(S)|≤ r·log2(n/r)
Proof. We prove the theorem by induction on log2 n. For n= 1 the theorem is trivial. Now, assume
the theorem holds for a tree with n/2 leaves, and let us prove it for a tree T with n leaves. The
tree T is made up of a root node, and two disjoint sub-trees, T1 and T2, each with n/2 leaves. Let
us split the set S ⊆{1,...,n }in two: S = S1 ∪S2, where S1 is contained in {1,...,n/ 2}, and S2
is contained in {n/2 + 1,...,n }. That is, S1 are the elements of S that are leaves in T1, and S2 are
the elements of S that are leaves in T2. Let r1 := (n/2) −|S1|and r2 := (n/2) −|S2|. Then clearly
r= r1 + r2.
First, suppose both r1 and r2 are greater than zero. By the induction hypothesis, we know that
for i= 1,2 we have |cover(Si)|≤ rilog2(n/2ri). Therefore,
|cover(S)|= |cover(S1)|+ |cover(S2)|≤ r1 log2(n/2r1) + r2 log2(n/2r2)
= rlog2(n/r) +
(
rlog2 r−r1 log2(2r1) −r2 log2(2r2)
)
≤rlog2(n/r),
which is what we had to prove in the induction step. The last inequality follows from a simple fact
about logarithms, namely that for all numbers r1 ≥1 and r2 ≥1, we have
(r1 + r2) log2(r1 + r2) ≤r1 log2(2r1) + r2 log2(2r2).
Second, if r1 = 0 then r2 = r≥1. The induction step now follows from:
|cover(S)|= |cover(S2)|≤ rlog2(n/2r) = rlog2(n/r) −r≤rlog2(n/r),
as required. The case r2 = 0 follows similarly. This completes the induction step, and the proof. 2
Theorem 5.8 shows that rdevices can be revoked at the cost of increasing the ciphertext header
size to rlog2(n/r) blocks. For moderate values of r this is not too big. Nevertheless, this general
approach can be improved [121, 83, 77]. The best system using this approach embeds O(log n) keys
in every device, same as here, but the header size is only O(r) blocks. The AACS system uses the
subset-tree diﬀerence method [121], which has a worst case header of size 2 r−1 blocks, but stores
1
2 log2 n keys per device.
While AACS is a far better designed than CSS, it too has been attacked. In particular, the
process of a revoking an AACS key is fairly involved and can take several months. Hackers showed
that they can extract new device keys from unrevoked players faster than the industry can revoke
them.
207
k1 k2 k3 k4 k5 k6 k7 k8
k9 k10 k11 k12
k13 k14
k15
Figure 5.6: The three shaded nodes are the minimal cover for leaves {1,2,4,5,6}.
5.7 Notes
Citations to the literature to be added.
5.8 Exercises
5.1 (Double encryption). Let E= (E,D) be a cipher. Consider the cipher E2 = (E2,D2), where
E2(k,m) = E(k,E(k,m)). One would expect that if encrypting a message once with E is secure
then encrypting it twice as in E2 should be no less secure. However, that is not always true.
(a) Show that there is a semantically secure cipher Esuch that E2 is not semantically secure.
(b) Prove that for every CPA secure ciphers E, the cipher E2 is also CPA secure. That is, show
that for every CPA adversary Aattacking E2 there is a CPA adversary Battacking Ewith
about the same advantage and running time.
5.2 (Multi-key CPA security). Generalize the deﬁnition of CPA security to the multi-key
setting, analogous to Deﬁnition 5.1. In this attack game, the adversary gets to obtain encryptions
of many messages under many keys. The game begins with the adversary outputting a number Q
indicating the number of keys it wants to attack. The challenger chooses Q random keys. In
every subsequent encryption query, the adversary submits a pair of messages and speciﬁes under
which of the Q keys it wants to encrypt; the challenger responds with an encryption of either the
ﬁrst or second message under the speciﬁed key (depending on whether the challenger is running
Experiment 0 or 1). Flesh out all the details of this attack game, and prove, using a hybrid
argument, that (single-key) CPA security implies multi-key CPA security. You should show that
security degrades linearly in Q. That is, the advantage of any adversaryAin breaking the multi-key
CPA security of a scheme is at most Q·ϵ, where ϵis the advantage of an adversary B(which is an
elementary wrapper around A) in attacking the scheme’s (single-key) CPA security.
5.3 (An alternate deﬁnition of CPA security). This exercise develops an alternative char-
acterization of CPA security for a cipher E= (E,D), deﬁned over (K,M,C). As usual, we need to
deﬁne an attack game between an adversary Aand a challenger. Initially, the challenger generates
b←R {0,1}, k←R K.
Then Amakes a series of queries to the challenger. There are two types of queries:
208
Encryption: In an encryption query, Asubmits a message m∈M to the challenger, who responds
with a ciphertext c ←R E(k,m). The adversary may make any (poly-bounded) number of
encryption queries.
Test: In a test query, Asubmits a pair of messages m0,m1 ∈M to the challenger, who responds
with a ciphertext c ←R E(k,mb). The adversary is allowed to make only a single test query
(with any number of encryption queries before and after the test query).
At the end of the game, Aoutputs a bit ˆb∈{0,1}.
As usual, we deﬁne A’s advantage in the above attack game to be |Pr[ˆb= b] −1/2|. We say that
Eis Alt-CPA secure if this advantage is negligible for all eﬃcient adversaries.
Show that Eis CPA secure if and only if Eis Alt-CPA secure.
5.4 (Hybrid CPA construction). Let ( E0,D0) be a semantically secure cipher deﬁned over
(K0,M,C0), and let ( E1,D1) be a CPA secure cipher deﬁned over ( K,K0,C1).
(a) Deﬁne the following hybrid cipher ( E,D) as:
E(k,m) :=
{
k0 ←R K0, c1 ←R E1(k,k0), c0 ←R E0(k0,m), output (c1,c0)
}
D
(
k, (c1,c0)
):=
{
k0 ←D1(k,c1), m←D0(k0,c0), output m
}
Here c1 is called the ciphertext header, and c0 is called the ciphertext body. Prove that (E,D)
is CPA secure.
(b) Suppose m is some large copyrighted content. A nice feature of ( E,D) is that the content
owner can make the long ciphertext body c0 public for anyone to download at their leisure.
Suppose both Alice and Bob take the time to download c0. When later Alice, who has key ka,
pays for access to the content, the content owner can quickly grant her access by sending her
the short ciphertext header ca ←R E1(ka,k0). Similarly, when Bob, who has key kb, pays for
access, the content owner grants him access by sending him the short header cb ←R E1(kb,k0).
Now, an eavesdropper gets to see
E′(
(ka,kb), m
):= (ca,cb,c0)
Generalize your proof from part (a) to show that this cipher is also CPA secure.
5.5 (A simple proof of randomized counter mode security). As mentioned in Remark 5.3,
we can view randomized counter mode as a special case of the generic hybrid construction in
Section 5.4.1. To this end, let F be a PRF deﬁned over ( K,X,Y), where X= {0,...,N −1}and
Y= {0,1}n, where N is super-poly. For poly-bounded ℓ ≥1, consider the PRF F′ deﬁned over
(K,X,Yℓ) as follows:
F′(k,x) :=
(
F(k,x), F(k,x + 1 mod N), ..., F (k,x + ℓ−1 mod N)
)
.
(a) Show that F′is a weakly secure PRF, as in Deﬁnition 4.3.
(b) Using part (a) and Remark 5.2, give a short proof that randomized counter mode is CPA
secure.
209
5.6 (CPA security from a block cipher). Let E = ( E,D) be a block cipher deﬁned over
(K,M×R). Consider the cipher E′= (E′,D′), where
E′(k,m) :=
{
r←R R, c←R E
(
k, (m,r)
)
, output c
}
D′(k,c) :=
{
(m,r′) ←D(k,c), output m
}
This cipher is deﬁned over ( K,M,M×R). Show that if Eis a secure block cipher, and 1 /|R|is
negligible, then E′is CPA secure.
5.7 (pseudo-random ciphertext security). In Exercise 3.4, we developed a notion of security
called pseudo-random ciphertext security. This notion naturally extends to multiple ciphertexts.
For a cipher E = ( E,D) deﬁned over ( K,M,C), we deﬁne two experiments: in Experiment 0
the challenger ﬁrst picks a random key k ←R Kand then the adversary submits a sequence of
queries, where the ith query is a message mi ∈M, to which the challenger responds with E(k,mi).
Experiment 1 is the same as Experiment 0 except that the challenger responds to the adversary’s
queries with random, independent elements of C. We say that Eis pseudo-random multi-ciphertext
secure if no eﬃcient adversary can distinguish between these two experiments with a non-negligible
advantage.
(a) Consider the counter-mode construction in Section 5.4.2, based on a PRF F deﬁned over
(K,X,Y), but with a ﬁxed-length plaintext space Yℓ and a corresponding ﬁxed-length ci-
phertext space X×Y ℓ. Under the assumptions that F is a secure PRF, |X| is super-poly,
and ℓ is poly-bounded, show that this cipher is pseudo-random multi-ciphertext secure.
(b) Consider the CBC construction Section 5.4.3, based on a block cipher E= (E,D) deﬁned over
(K,X), but with a ﬁxed-length plaintext space Xℓ and corresponding ﬁxed-length ciphertext
space Xℓ+1. Under the assumptions that Eis a secure block cipher, |X|is super-poly, and ℓ
is poly-bounded, show that this cipher is pseudo-random multi-ciphertext secure.
(c) Show that a pseudo-random multi-ciphertext secure cipher is also CPA secure.
(d) Give an example of a CPA secure cipher that is not pseudo-random multi-ciphertext secure.
5.8 (Deterministic CPA and SIV). We have seen that any cipher that is CPA secure must
be probabilistic, since for a deterministic cipher, an adversary can always see if the same message
is encrypted twice. We may deﬁne a relaxed notion of CPA security that says that this is the only
thing the adversary can see. This is easily done by placing the following restriction on the adversary
in Attack Game 5.2: for all indices i,j, we insist that mi0 = mj0 if and only if mi1 = mj1. We say
that a cipher is deterministic CPA secure if every eﬃcient adversary has negligible advantage
in this restricted CPA attack game. In this exercise, we develop a general approach for building
deterministic ciphers that are deterministic CPA secure.
Let E= (E,D) be a CPA-secure cipher deﬁned over ( K,M,C). We let E(k,m; r) denote running
algorithm E(k,m) with randomness r ←R R(for example, if Eimplements counter mode or CBC
encryption then r is the random IV used by algorithm E). Let F be a secure PRF deﬁned over
(K′,M,R). Deﬁne the deterministic cipher E′= (E′,D′), deﬁned over (K×K′,M,C) as follows:
E′(
(k,k′), m
) := E(k,m; F(k′,m)),
D′(
(k,k′), c
) := D(k,c) .
210
Show that E′ is deterministic CPA secure. This construction is known as the Synthetic IV (or
SIV) construction.
5.9 (Generic nonce-based encryption and nonce re-use resilience). In the previous exer-
cise, we saw how we could generically convert a probabilistic CPA-secure cipher into a deterministic
cipher that satisﬁes a somewhat weaker notion of security called deterministic CPA security.
(a) Show how to modify that construction so that we can convert any CPA-secure probabilistic
cipher into a nonce-based CPA-secure cipher.
(b) Show how to combine the two approaches to get a cipher that is nonce-based CPA secure,
but also satisﬁes the deﬁnition of deterministic CPA security if we drop the uniqueness re-
quirement on nonces.
Discussion: This is an instance of a more general security property called nonce re-use
resilience: the scheme provides full security if nonces are unique, and even if they are not,
a weaker and still useful security guarantee is provided.
5.10 (Ciphertext expansion vs. security). Let E = (E,D) be an encryption scheme where
messages and ciphertexts are bit strings.
(a) Suppose that for all keys and all messages m, the encryption of m is the exact same length
as m. Show that ( E,D) cannot be semantically secure under a chosen plaintext attack.
(b) Suppose that for all keys and all messages m, the encryption of m is exactly ℓ bits longer
than the length of m. Show an attacker that can win the CPA security game using ≈2ℓ/2
queries and advantage ≈1/2. You may assume the message space contains many more than
2ℓ/2 messages.
5.11 (CBC encryption with small blocks is insecure). Let’s see a concrete example of the
attack in the previous exercise. Let ( E,D) be a CBC cipher, as in Section 5.4.3, built from a block
cipher that operates on ℓbit blocks. Construct an attacker that wins the CPA game against (E,D)
that makes ≈2ℓ/2 queries to its challenger and has advantage ≈1/2. This attack was used to show
that 3DES-CBC is no longer secure for Internet use due to its small 64-bit block size [19].
5.12 (Repeating ciphertexts). Let E= (E,D) be a cipher deﬁned over (K,M,C). Assume that
there are at least two messages in M, that all messages have the same length, and that we can
eﬃciently generate messages in Muniformly at random. Show that if Eis CPA secure, then it is
infeasible for an adversary to make an encryptor generate the same ciphertext twice. The precise
attack game is as follows. The challenger chooses k∈K at random and the adversary makes a series
of queries; the ith query is a message mi, to which the challenger responds with ci ←R E(k,mi).
The adversary wins the game if any two ci’s are the same. Show that if E is CPA secure, then
every eﬃcient adversary wins this game with negligible probability. In particular, show that the
advantage of any adversary Ain winning the repeated-ciphertext attack game is at most 2 ϵ, where
ϵ is the advantage of an adversary B(which is an elementary wrapper around A) that breaks the
scheme’s CPA security.
5.13 (Predictable IVs). Let us see why in CBC mode an unpredictable IV is necessary for CPA
security. Suppose a defective implementation of CBC encrypts a sequence of messages by always
using the last ciphertext block of the ith message as the IV for the (i+ 1)-st message. The TLS 1.0
211
protocol, used to protect Web traﬃc, implements CBC encryption this way. Construct an eﬃcient
adversary that wins the CPA game against this implementation with advantage close to 1. We note
that the Web-based BEAST attack [58] exploits this defect to completely break CBC encryption
in TLS 1.0.
5.14 (An insecure nonce-based CBC mode). Consider the nonce-based CBC scheme E′ de-
scribed in Section 5.5.3. Suppose that the nonce space N is equal to block spaceXof the underlying
block cipher E= (E,D), and the PRF F is just the encryption algorithm E. If the two keys k and
k′in the construction are chosen independently, the scheme is secure. Your task is to show that if
only one key k is chosen, and the other key k′is set to k, then the scheme is insecure.
5.15 (Output feedback mode). Suppose F is a PRF deﬁned over ( K,X), and ℓ ≥1 is poly-
bounded.
(a) Consider the following PRG G : K→X ℓ. Let x0 be an arbitrary, ﬁxed element of X. For
k ∈K, let G(k) := (x1,...,x ℓ), where xi := F(k,xi−1) for i = 1,...,ℓ . Show that G is a
secure PRG, assuming F is a secure PRF and that |X|is super-poly.
(b) Next, assume that X= {0,1}n. We deﬁne a cipher E= (E,D), deﬁned over ( K,Xℓ,Xℓ+1),
as follows. Given a key k∈K and a message (m1,...,m ℓ) ∈Xℓ, the encryption algorithm E
generates the ciphertext ( c0,c1,...,c ℓ) ∈Xℓ+1 as follows: it chooses x0 ∈X at random, and
sets c0 = x0; it then computes xi = F(k,xi−1) and ci = mi ⊕xi for i = 1,...,ℓ . Describe
the corresponding decryption algorithm D, and show that Eis CPA secure, assuming F is a
secure PRF and that |X|is super-poly.
Note: This construction is called output feedback mode (or OFB).
5.16 (CBC ciphertext stealing). One problem with CBC encryption is that messages need to
be padded to a multiple of the block length and sometimes a dummy block needs to be added. The
following ﬁgure describes a variant of CBC that eliminates the need to pad:
The method pads the last block with zeros if needed (a dummy block is never added), but the
output ciphertext contains only the shaded parts of C1,C2,C3,C4. Note that, ignoring the IV, the
ciphertext is the same length as the plaintext. This technique is called ciphertext stealing.
(a) Explain how decryption works.
(b) Can this method be used if the plaintext contains only one block?
5.17 (Single ciphertext block corruption in CBC mode). Let cbe an ℓblock CBC-encrypted
ciphertext, for some ℓ >3. Suppose that exactly one block of c is corrupted, and the result is
decrypted using the CBC decryption algorithm. How many blocks of the decrypted plaintext are
corrupted?
212
5.18 (The malleability of CBC mode). Let cbe the CBC encryption of some message m∈Xℓ,
where X := {0,1}n. You do not know m. Let ∆ ∈X . Show how to modify the ciphertext c to
obtain a new ciphertext c′ that decrypts to m′, where m′[0] = m[0] ⊕∆, and m′[i] = m[i] for
i= 1,...,ℓ −1. That is, by modifying c appropriately, you can ﬂip bits of your choice in the ﬁrst
block of the decryption of c, without aﬀecting any of the other blocks.
5.19 (Online ciphers). In practice there is a strong desire to encrypt one block of plaintext at
a time, outputting the corresponding block of ciphertext right away. This lets the system transmit
ciphertext blocks as soon as they are ready without having to wait until the entire message is
processed by the encryption algorithm.
(a) Deﬁne a CPA-like security game that captures this method of encryption. Instead of forcing
the adversary to submit a complete pair of messages in every encryption query, the adversary
should be allowed to issue a query indicating the beginning of a message, then repeatedly
issue more queries containing message blocks, and ﬁnally issue a query indicating the end of a
message. Responses to these queries will include all ciphertext blocks that can be computed
given the information given.
(b) Show that randomized CBC encryption is not CPA secure in this model.
(c) Show that randomized counter mode is online CPA secure.
5.20 (Redundant bits do not harm CPA security). Let E= (E,D) be a CPA-secure cipher
deﬁned over ( K,M,C). Show that appending to a ciphertext additional data that is computed
from the ciphertext does not damage CPA security. Speciﬁcally, let g : C→Y be some eﬃciently
computable function. Show that the following modiﬁed cipher E′= (E′,D′) is CPA-secure:
E′(k,m) :=
{
c←E(k,m), t←g(c), output (c,t)
}
D′(
k, (c,t)
):= D(k,c)
5.21 (Broadcast encryption). In a broadcast encryption system, a sender can encrypt a message
so that only a speciﬁed set of recipients can decrypt. Such a system is made up of three eﬃcient
algorithms ( G,E,D ): algorithm G is invoked as G(n) and outputs an encryptor key ek, and n
keys k1,...,k n, one key for each recipient; algorithm E is invoked as c ←R E(ek,m,S ), where m
is the message and S ⊆{1,...,n }is the intended set of recipients; algorithm D is invoked as
m ←D(ki,c) for some 1 ≤i ≤n, and correctly decrypts the given c whenever i is in the set S.
More precisely, for all m and all subsets S of {1,...,n }, we have that D
(
ki, E(ek,m,S )
)
= m for
all i∈S.
(a) Describe the revocation scheme described in (5.35) in Section 5.6 as a broadcast encryption
system. How do algorithms G,E,D work and what are ek and k1,...,k n?
(b) A broadcast encryption scheme is secure if a set of colluding recipients Blearns nothing about
plaintexts encrypted for subsets of {1,...,n }\B, namely plaintexts that are not intended for
the members of B. More precisely, CPA security of a broadcast encryption system is deﬁned
using the following two experiments, Experiment 0 and Experiment 1: In Experiment b, for
b = 0,1, the adversary begins by outputing a subset B of {1,...,n }. The challenger then
runs G(n) and sends to the adversary all the keys named in B, namely {ki}i∈B. Now the
adversary issues chosen plaintext queries, where query number j is a triple (Sj,mj,0,mj,1) for
213
some set Sj in {1,...,n }\B. The challenger sends back cj ←R E(ek,mj,b,Sj). The system is
secure if the adversary cannot distinguish these two experiments.
Show that the scheme from part (a) is a secure broadcast encryption system, assuming the
underlying header encryption scheme is CPA secure, and the body encryption scheme (E′,D′)
is semantically secure.
Hint: Use a sequence of 2 n−1 hybrids, one for each key in the tree of Fig. 5.5
214
Chapter 6
Message integrity
In previous chapters we focused on security against an eavesdropping adversary. The adversary
had the ability to eavesdrop on transmitted messages, but could not change messages en-route.
We showed that chosen plaintext security is the natural security property needed to defend against
such attacks.
In this chapter we turn our attention to active adversaries. We start with the basic question
of message integrity: Bob receives a message m from Alice and wants to convince himself that the
message was not modiﬁed en-route. We will design a mechanism that lets Alice compute a short
message integrity tag t for the message m and send the pair ( m,t) to Bob, as shown in Fig. 6.1.
Upon receipt, Bob checks the tag t and rejects the message if the tag fails to verify. If the tag
veriﬁes then Bob is assured that the message was not modiﬁed in transmission.
We emphasize that in this chapter the message itself need not be secret. Unlike previous
chapters, our goal here is not to conceal the message. Instead, we only focus on message integrity.
In Chapter 9 we will discuss the more general question of simultaneously providing message secrecy
and message integrity. There are many applications where message integrity is needed, but message
secrecy is not. We give two examples.
Example 6.1. Consider the problem of delivering ﬁnancial news or stock quotes over the Internet.
Although the news items themselves are public information, it is vital that no third party modify
the data on its way to the user. Here message secrecy is irrelevant, but message integrity is critical.
Our constructions will ensure that if user Bob rejects all messages with an invalid message integrity
tag then an attacker cannot inject modiﬁed content that will look legitimate. One caveat is that
an attacker can still change the order in which news reports reach Bob. For example, Bob might
see report number 2 before seeing report number 1. In some settings this may cause the user to
take an incorrect action. To defend against this, the news service may wish to include a sequence
number with each report so that the user’s machine can buﬀer reports and ensure that the user
always sees news items in the correct order. 2
In this chapter we are only concerned with attacks that attempt to modify data. We do not
consider Denial of Service (DoS) attacks, where the attacker delays or prevents news items from
reaching the user. DoS attacks are often handled by ensuring that the network contains redundant
paths from the sender to the receiver so that an attacker cannot block all paths. We will not discuss
these issues here.
Example 6.2. Consider an application program — such as a word processor or mail client —
215
Alice Bob
m t
Generate tag t
t←S(k,m)
Verify message-tag pair (m,t)
V(k,m,t )
?
= accept
m m
Figure 6.1: Short message integrity tag added to messages
stored on disk. Although the application code is not secret (it might even be in the public domain),
its integrity is important. Before running the program the user wants to ensure that a virus did not
modify the code stored on disk. To do so, when the program is ﬁrst installed, the user computes a
message integrity tag for the code and stores the tag on disk alongside the program. Then, every
time, before starting the application the user can validate this message integrity tag. If the tag is
valid, the user is assured that the code has not been modiﬁed since the tag was initially generated.
Clearly a virus can overwrite both the application code and the integrity tag. Nevertheless, our
constructions will ensure that no virus can fool the user into running unauthenticated code. As
in our ﬁrst example, the attacker can swap two authenticated programs — when the user starts
application A he will instead be running application B. If both applications have a valid tag the
system will not detect the swap. The standard defense against this is to include the program name
in the executable ﬁle. That way, when an application is started the system can display to the user
an authenticated application name. 2
The question, then, is how to design a secure message integrity mechanism. We ﬁrst argue the
following basic principle:
Providing message integrity between two communicating parties requires that the send-
ing party has a secret key unknown to the adversary.
Without a secret key, ensuring message integrity is not possible: the adversary has enough infor-
mation to compute tags for arbitrary messages of its choice — it knows how the message integrity
algorithm works and needs no other information to compute tags. For this reason all cryptographic
message integrity mechanisms require a secret key unknown to the adversary. In this chapter,
we will assume that both sender and receiver will share the secret key; later in the book, this
assumption will be relaxed.
We note that communication protocols not designed for security often use keyless integrity
mechanisms. For example, the Ethernet protocol uses CRC32 as its message integrity algorithm.
This algorithm, which is publicly available, outputs 32-bit tags embedded in every Ethernet frame.
The TCP protocol uses a keyless 16-bit checksum which is embedded in every packet. We emphasize
that these keyless integrity mechanisms are designed to detect random transmission errors, not
malicious errors. The argument in the previous paragraph shows that an adversary can easily defeat
these mechanisms and generate legitimate-looking traﬃc. For example, in the case of Ethernet, the
adversary knows exactly how the CRC32 algorithm works and this lets him compute valid tags for
arbitrary messages. He can then tamper with Ethernet traﬃc without being detected.
216
6.1 Deﬁnition of a message authentication code
We begin by deﬁning a message integrity system based on a shared secret key. For historical reasons
such systems are called Message Authentication Codes or MACs for short.
Deﬁnition 6.1. A MAC system I= (S,V ) is a pair of eﬃcient algorithms, S and V, where S
is called a signing algorithm and V is called a veriﬁcation algorithm. Algorithm S is used to
generate tags and algorithm V is used to verify tags.
• S is a probabilistic algorithm that is invoked as t←RS(k,m), where kis a key, mis a message,
and the output t is called a tag.
• V is a deterministic algorithm that is invoked as r ←V(k,m,t ), where k is a key, m is a
message, t is a tag, and the output r is either accept or reject.
• We require that tags generated by S are always accepted by V; that is, the MAC must satisfy
the following correctness property: for all keys k and all messages m,
Pr[V(k, m, S(k, m) ) = accept] = 1.
As usual, we say that keys lie in some ﬁnite key space K, messages lie in a ﬁnite message space
M, and tags lie in some ﬁnite tag space T. We say that I= (S,V ) is deﬁned over (K,M,T).
Fig. 6.1 illustrates how algorithms S and V are used for protecting network communications
between two parties. Whenever algorithm V outputs accept for some message-tag pair ( m,t), we
say that t is a valid tag for m under key k, or that ( m,t) is a valid pair under k. Naturally, we
want MAC systems where tags are as short as possible so that the overhead of transmitting the
tag is minimal.
We will explore a variety of MAC systems. The simplest type of system is one in which the
signing algorithm S is deterministic, and the veriﬁcation algorithm is deﬁned as
V(k,m,t ) =
{
accept if S(k,m) = t,
reject otherwise.
We shall call such a MAC system adeterministic MAC system. One property of a deterministic
MAC system is that it has unique tags: for a given key k, and a given message m, there is a
unique valid tag for m under k. Not all MAC systems we explore will have such a simple design:
some have a randomized signing algorithm, so that for a given key k and message m, the output of
S(k,m) may be one of many possible valid tags, and the veriﬁcation algorithm works some other
way. As we shall see, such randomized MAC systems are not necessary to achieve security, but
they can yield better eﬃciency/security trade-oﬀs.
Secure MACs. Next, we turn to describing what it means for a MAC to be secure. To construct
MACs that remain secure in a variety of applications we will insist on security in a very hostile
environment. Since most real-world systems that use MACs operate in less hostile settings, our
conservative security deﬁnitions will imply security for all these systems.
We ﬁrst intuitively explain the deﬁnition and then motivate why this conservative deﬁnition
makes sense. Suppose an adversary is attacking a MAC system I = ( S,V ). Let k be some
217
MAC Challenger Adversary A
k←
R
K mi
ti ←S(k,mi)
(m,t)
Figure 6.2: MAC attack game (Attack Game 6.1)
randomly chosen MAC key, which is unknown to the attacker. We allow the attacker to request
tags t := S(k,m) for arbitrary messages m of its choice. This attack, called a chosen message
attack, enables the attacker to collect millions of valid message-tag pairs. Clearly we are giving
the attacker considerable power — it is hard to imagine that a user would be foolish enough to sign
arbitrary messages supplied by an attacker. Nevertheless, we will see that chosen message attacks
come up in real world settings. We refer to message-tag pairs ( m,t) that the adversary obtains
using the chosen message attack as signed pairs.
Using the chosen message attack we ask the attacker to come up with an existential MAC
forgery. That is, the attacker need only come up with some new valid message-tag pair ( m,t).
By “new”, we mean a message-tag pair that is diﬀerent from all of the signed pairs. The attacker
is free to choose marbitrarily; indeed, mneed not have any special format or meaning and can be
complete gibberish.
We say that a MAC system is secure if even an adversary who can mount a chosen message
attack cannot create an existential forgery. This deﬁnition gives the adversary more power than it
typically has in the real world and yet we ask it to do something that will normally be harmless;
forging the MAC for a meaningless message seems to be of little use. Nevertheless, as we will
see, this conservative deﬁnition is very natural and enables us to use MACs for lots of diﬀerent
applications.
More precisely, we deﬁne secure MACs using an attack game between a challenger and an
adversary A. The game is described below and in Fig. 6.2.
Attack Game 6.1 (MAC security). For a given MAC system I = ( S,V ), deﬁned over
(K,M,T), and a given adversary A, the attack game runs as follows:
• The challenger picks a random k←R K.
• Aqueries the challenger several times. For i = 1,2,..., the ith signing query is
a message mi ∈M. Given mi, the challenger computes a tag ti ←R S(k,mi), and
then gives ti to A.
• Eventually Aoutputs a candidate forgery pair ( m,t) ∈M×T that is not among
the signed pairs, i.e.,
(m,t) ̸∈
{
(m1,t1),(m2,t2),...
}
.
218
We say that Awins the above game if ( m,t) is a valid pair under k (i.e., V(k,m,t ) = accept).
We deﬁne A’s advantage with respect to I, denoted MACadv[A,I], as the probability that Awins
the game. Finally, we say that Ais a Q-query MAC adversary if Aissues at most Q signing
queries. 2
Deﬁnition 6.2. We say that a MAC system Iis secure if for all eﬃcient adversaries A, the value
MACadv[A,I] is negligible.
In case the adversary wins Attack Game 6.1, the pair ( m,t) that it sends to the challenger is
called an existential forgery. MAC systems that satisfy Deﬁnition 6.2 are said to beexistentially
unforgeable under a chosen message attack .
In the case of a deterministic MAC system, the only way for Ato win Attack Game 6.1 is to
produce a valid message-tag pair ( m,t) for some new message m /∈{m1,m2,... }. Indeed, security
in this case just means that S is unpredictable, in the sense described in Section 4.1.1; that is, given
S(k,m1),S(k,m2),..., it is hard to predict S(k,m) for any m /∈{m1,m2,... }.
In the case of a randomized MAC system, our security deﬁnition captures a stronger property.
There may be many valid tags for a given message. Let m be some message and suppose the
adversary requests one or more valid tags t1,t2,... for m. Can the adversary produce a new valid
tag t′ for m? (i.e. a tag satisfying t′ /∈{t1,t2,... }). Our deﬁnition says that a valid pair ( m,t′),
where t′is new, is a valid existential forgery. Therefore, for a MAC to be secure it must be diﬃcult
for an adversary to produce a new valid tagt′for a previously signed messagem. This may seem like
an odd thing to require of a MAC. If the adversary already has valid tags form, why should we care
if it can produce another one? As we will see in Chapter 9, our security deﬁnition, which prevents
the adversary from producing new tags on signed messages, is necessary for the applications we
have in mind.
Going back to the examples in the introduction, observe that existential unforgeability implies
that an attacker cannot create a fake news report with a valid tag. Similarly, the attacker cannot
tamper with a program on disk without invalidating the tag for the program. Note, however, that
when using MACs to protect application code, users must provide their secret MAC key every time
they want to run the application. This will quickly annoy most users. In Chapter 8 we will discuss
a keyless method to protect public application code.
To exercise the deﬁnition of secure MACs let us ﬁrst see a few consequences of it. LetI= (S,V )
be a MAC deﬁned over ( K,M,T), and let k be a random key in K.
Example 6.3. Suppose m1 and m2 are almost identical messages. Say m1 is a money transfer
order for $100 and m2 is a transfer order for $101. Clearly, an adversary who intercepts a valid
tag for m1 should not be able to deduce from it a valid tag for m2. A MAC system that satisﬁes
Deﬁnition 6.2 ensures this. To see why, suppose an adversary Acan forge the tag for m2 given the
tag for m1. Then Acan win Attack Game 6.1: it uses the chosen message attack to request a tag
for m1, deduces a forged tag t2 for m2, and outputs ( m2,t2) as a valid existential forgery. Clearly
Awins Attack Game 6.1. Hence, existential unforgeability captures the fact that a tag for one
message m1 gives no useful information for producing a tag for another message m2, even when m2
is almost identical to m1. 2
Example 6.4. Our deﬁnition of secure MACs gives the adversary the ability to obtain the tag for
arbitrary messages. This may seem like giving the adversary too much power. In practice, however,
there are many scenarios where chosen message attacks are feasible. The reason is that the MAC
219
signer often does not know the source of the data being signed. For example, consider a backup
system that dumps the contents of disk to backup tapes. Since backup integrity is important, the
system computes an integrity tag on every disk block that it writes to tape. The tag is stored on
tape along with the data block. Now, suppose an attacker writes data to a low security part of disk.
The attacker’s data will be backed up and the system will compute a tag over it. By examining
the resulting backup tape the attacker obtains a tag on its chosen message. If the MAC system is
secure against a chosen message attack then this does not help the attacker break the system. 2
Remark 6.1. Just as we did for other security primitives, one can generalize the notion of a secure
MAC to the multi-key setting, and prove that a secure MAC is also secure in the multi-key setting.
See Exercise 6.3. 2
6.1.1 Mathematical details
As usual, we give a more mathematically precise deﬁnition of a MAC, using the terminology deﬁned
in Section 2.3. This section may be safely skipped on ﬁrst reading.
Deﬁnition 6.3 (MAC). A MAC system is a pair of eﬃcient algorithms, S and V, along with
three families of spaces with system parameterization P:
K = {Kλ,Λ}λ,Λ, M = {Mλ,Λ}λ,Λ, and T = {Tλ,Λ}λ,Λ,
As usual, λ∈Z≥1 is a security parameter and Λ ∈Supp(P(λ)) is a domain parameter. We require
that
1. K, M, and T are eﬃciently recognizable.
2. K is eﬃciently sampleable.
3. Algorithm S is an eﬃcient probabilistic algorithm that on input λ,Λ,k,m , where λ ∈Z≥1,
Λ ∈Supp(P(λ)), k∈Kλ,Λ, and m∈Mλ,Λ, outputs an element of Tλ,Λ.
4. Algorithm V is an eﬃcient deterministic algorithm that on input λ,Λ,k,m,t , where λ∈Z≥1,
Λ ∈Supp(P(λ)), k∈Kλ,Λ, m∈Mλ,Λ, and t∈Tλ,Λ, outputs either accept or reject.
In deﬁning security, we parameterize Attack Game 6.1 by the security parameter λ, which is
given to both the adversary and the challenger. The advantage MAC adv[A,I] is then a function
of λ. Deﬁnition 6.2 should be read as saying that MAC adv[A,I](λ) is a negligible function.
6.2 MAC veriﬁcation queries do not help the attacker
In our deﬁnition of secure MACs (Attack Game 6.1) the adversary has no way of testing whether a
given message-tag pair is valid. In fact, the adversary cannot even tell if it wins the game, since only
the challenger has the secret key needed to run the veriﬁcation algorithm. In real life, an attacker
capable of mounting a chosen message attack can probably also test whether a given message-tag
pair is valid. For example, the attacker could build a packet containing the message-tag pair in
question and send this packet to the victim’s machine. Then, by examining the machine’s behavior
the attacker can tell whether the packet was accepted or dropped, indicating whether the tag was
valid or not.
220
Consequently, it makes sense to extend Attack Game 6.1 by giving the adversary the extra
power to verify message-tag pairs. Of course, we continue to allow the adversary to request tags
for arbitrary messages of its choice.
Attack Game 6.2 (MAC security with veriﬁcation queries). For a given MAC system
I= (S,V ), deﬁned over (K,M,T), and a given adversary A, the attack game runs as follows:
• The challenger picks a random k←R K.
• Aqueries the challenger several times. Each query can be one of two types:
– Signing query: for i = 1 ,2,..., the ith signing query consists of a message
mi ∈M. The challenger computes a tag ti ←R S(k,mi), and gives ti to A.
– Veriﬁcation query: for j = 1 ,2,..., the jth veriﬁcation query consists of a
message-tag pair ( ˆmj,ˆtj) ∈M×T that is not among the previously signed
pairs, i.e.,
( ˆmj,ˆtj) ̸∈
{
(m1,t1),(m2,t2),...
}
.
The challenger responds to Awith V(k, ˆmj,ˆtj).
We say that Awins the above game if the challenger ever responds to a veriﬁcation query with
accept. We deﬁne A’s advantage with respect to I, denoted MAC vqadv[A,I], as the probability
that Awins the game. 2
The two deﬁnitions are equivalent. Attack Game 6.2 is essentially the same as the original
Attack Game 6.1, except that Acan issue MAC veriﬁcation queries. We prove that this extra
power does not help the adversary.
Theorem 6.1. If Iis a secure MAC system, then it is also secure in the presence of veriﬁcation
queries.
In particular, for every MAC adversary Athat attacks I as in Attack Game 6.2, and which
makes at most Qv veriﬁcation queries and at most Qs signing queries, there exists a Qs-query
MAC adversary Bthat attacks I as in Attack Game 6.1, where Bis an elementary wrapper
around A, such that
MACvqadv[A,I] ≤MACadv[B,I] ·Qv.
Proof idea. Let Abe a MAC adversary that attacks Ias in Attack Game 6.2, and which makes
at most Qv veriﬁcation queries and at most Qs signing queries. From adversary A, we build an
adversary Bthat attacks Ias in Attack Game 6.1 and makes at mostQs signing queries. Adversary
Bcan easily answer A’s signing queries by forwarding them to B’s challenger and relaying the
resulting tags back to A.
The question is how to respond to A’s veriﬁcation queries. By deﬁnition, Aonly submits
veriﬁcation queries on message pairs that are not among the previously signed pairs. So Badopts a
simple strategy: it responds with reject to all veriﬁcation queries from A. If Banswers incorrectly,
it has a forgery which lets it win Attack Game 6.1. Unfortunately, Bdoes not know which of these
veriﬁcation queries is a forgery, so it simply guesses, choosing one at random. Since Amakes at
most Qv veriﬁcation queries, Bwill guess correctly with probability at least 1 /Qv. This is the
source of the Qv factor in the error term. 2
221
Proof. In more detail, adversary Bplays the role of challenger to Ain Attack Game 6.2, while
at the same time, it plays the role of adversary in Attack Game 6.1, interacting with the MAC
challenger in that game. The logic is as follows:
initialization:
ω←R {1,...,Q v}
upon receiving a signing query mi ∈M from Ado:
forward mi to the MAC challenger, obtaining the tag ti
send ti to A
upon receiving a veriﬁcation query ( ˆmj,ˆtj) ∈M×T from Ado:
if j = ω
then output ( ˆmj,ˆtj) as a candidate forgery pair and halt
else send reject to A
To rigorously justify the construction of adversary B, we analyze the behavior of Ain three
closely related games.
Game 0. This is the original attack game, as played between the challenger in Attack Game 6.2
and adversary A. Here is the logic of the challenger in this game:
initialization:
k←R K
upon receiving a signing query mi ∈M from Ado:
ti ←R S(k,mi)
send ti to A
upon receiving a veriﬁcation query ( ˆmj,ˆtj) ∈M×T from Ado:
rj ←V(k, ˆmj,ˆtj)
(∗) send rj to A
Let W0 be the event that in Game 0, rj = accept for some j. Evidently,
Pr[W0] = MACvqadv[A,I]. (6.1)
Game 1. This is the same as Game 0, except that the line marked ( ∗) above is changed to:
send reject to A
That is, when responding to a veriﬁcation query, the challenger always responds to Awith reject.
We also deﬁne W1 to be the event that in Game 1, rj = accept for some j. Even though the
challenger does not notify Athat W1 occurs, both Games 0 and 1 proceed identically until this
event happens, and so events W0 and W1 are really the same; therefore,
Pr[W1] = Pr[W0]. (6.2)
Also note that in Game 1, although the rj values are used to deﬁne the winning condition, they
are not used for any other purpose, and so do not inﬂuence the attack in any way.
222
Game 2. This is the same as Game 1, except that at the beginning of the game, the challenger
chooses ω ←R {1,...,Q v}. We deﬁne W2 to be the event that in Game 2, rω = accept. Since the
choice of ω is independent of the attack itself, we have
Pr[W2] ≥Pr[W1]/Qv. (6.3)
Evidently, by construction, we have
Pr[W2] = MACadv[B,I]. (6.4)
The theorem now follows from (6.1)–(6.3). 2
In summary, we showed that Attack Game 6.2, which gives the adversary more power, is
equivalent to Attack Game 6.1 used in deﬁning secure MACs. The reduction introduces a factor of
Qv in the error term. Throughout the book we will make use of both attack games:
• When constructing secure MACs it is easier to use Attack Game 6.1 which restricts the
adversary to signing queries only. This makes it easier to prove security since we only have
to worry about one type of query. We will use this attack game throughout the chapter.
• When using secure MACs to build higher level systems (such as authenticated encryption) it
is more convenient to assume that the MAC is secure with respect to the stronger adversary
described in Attack Game 6.2.
We also point out that if we had used a weaker notion of security, in which the adversary only
wins by presenting a valid tag on a new message (rather than new valid message-tag pair), then
the analogs of Attack Game 6.1 and Attack Game 6.2 are not equivalent (see Exercise 6.7).
6.3 Constructing MACs from PRFs
We now turn to constructing secure MACs using the tools at our disposal. In previous chapters we
used pseudo random functions (PRFs) to build various encryption systems. We gave examples of
practical PRFs such as AES (while AES is a block cipher it can be viewed as a PRF thanks to the
PRF switching lemma, Theorem 4.4). Here we show that any secure PRF can be directly used to
build a secure MAC.
Recall that a PRF is an algorithm F that takes two inputs, a key k and an input data block
x, and outputs a value y:= F(k,x). As usual, we say that F is deﬁned over (K,X,Y), where keys
are in K, inputs are in X, and outputs are in Y. For a PRF F we deﬁne the deterministic MAC
system I= (S,V ) derived from F as:
S(k, m) := F(k, m);
V(k, m, t) :=
{
accept if F(k,m) = t,
reject otherwise.
As already discussed, any PRF with a large (i.e., super-poly) output space is unpredictable (see
Section 4.1.1), and therefore, as discussed in Section 6.1, the above construction yields a secure
MAC. For completeness, we state this as a theorem:
223
Theorem 6.2. Let F be a secure PRF deﬁned over (K,X,Y), where |Y|is super-poly. Then the
deterministic MAC system Iderived from F is a secure MAC.
In particular, for every Q-query MAC adversary Athat attacks Ias in Attack Game 6.1, there
exists a (Q+ 1)-query PRF adversary Bthat attacks F as in Attack Game 4.2, where Bis an
elementary wrapper around A, such that
MACadv[A,I] ≤PRFadv[B,F] + 1/|Y|
Proof idea. Let Abe an eﬃcient MAC adversary. We derive an upper bound on MAC adv[A,I]
by bounding A’s ability to generate forged message-tag pairs. As usual, replacing the underlying
secure PRF F with a truly random function f in Funs[X,Y] does not change A’s advantage much.
But now that the adversary Ais interacting with a truly random function it is faced with a hopeless
task: using the chosen message attack it obtains the value of f at a few points of its choice. He then
needs to guess the value of f(m) ∈Y at some new point m. But since f is a truly random function,
Ahas no information about f(m), and therefore has little chance of guessing f(m) correctly. 2
Proof. We make this intuition rigorous by letting Ainteract with two closely related challengers.
Game 0. As usual, we begin by reviewing the challenger in the MAC Attack Game 6.1 as it applies
to I. We implement the challenger in this game as follows:
(∗) k←R K, f ←F(k,·)
upon receiving the ith signing query mi ∈M (for i= 1,2,... ) do:
ti ←f(mi)
send ti to the adversary
At the end of the game, the adversary outputs a message-tag pair ( m,t). We deﬁne W0 to be the
event that the condition
t= f(m) and m̸∈{m1,m2,... } (6.5)
holds in Game 0. Clearly, Pr[ W0] = MACadv[A,I].
Game 1. We next play the usual “PRF card,” replacing the function F(k,·) by a truly random
function f in Funs[X,Y]. Intuitively, since F is a secure PRF, the adversary Ashould not notice
the diﬀerence. Our challenger in Game 1 is the same as in Game 0 except that we change line (*)
as follows:
(∗) f ←R Funs[X,Y]
Let W1 to be the event that condition (6.5) holds in Game 1. We construct a ( Q+ 1)-query PRF
adversary Bsuch that: ⏐⏐Pr[W1] −Pr[W0]
⏐⏐= PRFadv[B,F]. (6.6)
Adversary Bresponds to A’s chosen message queries by querying its own PRF challenger. Eventu-
ally Aoutputs a candidate MAC forgery ( m,t) where m is not one of its chosen message queries.
Now Bqueries its PRF challenger at m and gets back some t′ ∈Y. If t = t′ then Boutputs 0;
otherwise it outputs 1. A simple argument shows that this Bsatisﬁes (6.6).
Next, we directly bound Pr[ W1]. The adversary Asees the values of f at various points
m1,m2,... and is then required to guess the value of f at some new point m. But since f is a
truly random function, the value f(m) is independent of its value at all other points. Hence, since
224
m̸∈{m1,m2,... }, adversary Awill guess f(m) with probability 1/|Y|. Therefore, Pr[ W1] ≤1/|Y|.
Putting this together with (6.6), we obtain
MACadv[A,I] = Pr[W0] ≤
⏐⏐Pr[W0] −Pr[W1]
⏐⏐+ Pr[W1] ≤PRFadv[B,F] + 1
|Y|
as required. 2
Concrete tag lengths. The theorem shows that to ensure MAC adv[A,I] < 2−128 we need a
PRF whose output space Ysatisﬁes |Y|>2128. If the output space Yis {0,1}n for some n, then
the resulting tags must be at least 128 bits long.
6.4 Preﬁx-free PRFs for long messages
In the previous section we saw that any secure PRF is also a secure MAC. However, the concrete
examples of PRFs from Chapter 4 only take short inputs and can therefore only be used to provide
integrity for very short messages. For example, viewing AES as a PRF gives a MAC for 128-bit
messages. Clearly, we want to build MACs for much longer messages.
All the MAC constructions in this chapter follow the same paradigm: they start from a PRF
for short inputs (like AES) and produce a PRF, and therefore a MAC, for much longer inputs.
Hence, our goal for the remainder of the chapter is the following:
given a secure PRF on short inputs construct a secure PRF on long inputs.
We solve this problem in three steps:
• First, in this section we construct preﬁx-free secure PRFs for long inputs. More precisely,
given a secure PRF that operates on single-block (e.g., 128-bit) inputs, we construct a preﬁx-
free secure PRF that operates on variable-length sequences of blocks. Recall that a preﬁx-free
secure PRF (Deﬁnition 4.5) is only secure in a limited sense: we only require that preﬁx-free
adversaries cannot distinguish the PRF from a random function. A preﬁx-free PRF adversary
issues queries that are non-empty sequences of blocks, and no query can be a proper preﬁx
of another.
• Second, in the next few sections we show how to convert preﬁx-free secure PRFs for long
inputs into fully secure PRFs for long inputs. Thus, by the end of these sections we will have
several secure PRFs, and therefore secure MACs, that operate on long inputs.
• Third, in Section 6.8 we show how to convert a PRF that operates on messages that are
strings of blocks into a PRF that operates on strings of bits.
Preﬁx-free PRFs. We begin with two classic constructions for preﬁx-free secure PRFs. The
CBC construction is shown in Fig. 6.3a. The cascade construction is shown in Fig. 6.3b. We
show that when the underlying F is a secure PRF, both CBC and cascade are preﬁx-free secure
PRFs.
225
a1
F(k,·)
a2
F(k,·)
⨁
a3
F(k,·)
⨁
aℓ
F(k,·)
⨁
tag
···
(a) The CBC construction FCBC(k,m)
a1
Fk
a2
F
a3
F
aℓ
F
···
tag
(b) The cascade construction F∗(k,m)
Figure 6.3: Two preﬁx-free secure PRFs
6.4.1 The CBC preﬁx-free secure PRF
Let F be a PRF that maps n-bit inputs to n-bit outputs. In symbols, F is deﬁned over (K,X,X)
where X= {0,1}n. For any poly-bounded value ℓ, we build a new PRF, denoted FCBC, that maps
messages in X≤ℓ to outputs in X. The function FCBC, described in Fig. 6.3a, works as follows:
input: k∈K and m= (a1,...,a v) ∈X≤ℓ for some v∈{0,...,ℓ }
output: a tag in X
t←0n
for i←1 to v do:
t←F(k, ai ⊕t )
output t
FCBC is similar to CBC mode encryption from Fig. 5.4, but with two important diﬀerences. First,
FCBC does not output any intermediate values along the CBC chain. Second, FCBC uses a ﬁxed IV,
namely 0n, where as CBC mode encryption uses a random IV per message.
The following theorem shows that FCBC is a preﬁx-free secure PRF deﬁned over ( K,X≤ℓ,X).
Theorem 6.3. Let F be a secure PRF deﬁned over (K,X,X) where X = {0,1}n and |X| = 2n
is super-poly. Then for any poly-bounded value ℓ, we have that FCBC is a preﬁx-free secure PRF
deﬁned over (K,X≤ℓ,X).
In particular, for every preﬁx-free PRF adversary Athat attacks FCBC as in Attack Game 4.2,
and issues at most Q queries, there exists a PRF adversary B that attacks F as in Attack
226
Game 4.2, where Bis an elementary wrapper around A, such that
PRFpfadv[A,FCBC] ≤PRFadv[B,F] + (Qℓ)2
2|X| . (6.7)
Exercise 6.6 develops an attack on ﬁxed-length FCBC that demonstrates that security degrades
quadratically in Q. This shows that the quadratic dependence on Q in (6.7) is necessary. A more
diﬃcult proof of security shows that security only degrades linearly in ℓ (see Section 6.13). In
particular, the error term in (6.7) can be reduced to an expression dominated by O(Q2ℓ/|X|)
Proof idea. We represent the adversary’s queries in a rooted tree, where edges in the tree are labeled
by message blocks (i.e., elements of X). A query for FCBC(k,m), where m= (a1,...,a v) ∈Xv and
1 ≤v≤ℓ, deﬁnes a path in the tree, starting at the root, as follows:
root a1
−→p1
a2
−→p2
a3
−→··· av
−→pv. (6.8)
Thus, two messages m and m′ correspond to paths in the tree which both start at the root; these
two paths may share a common initial subpath corresponding to the longest common preﬁx of m
and m′.
With each node pin this tree, we associate a value γp ∈X which represents the computed value
in the CBC chain. More precisely, we deﬁne γroot := 0n, and for any non-root node q with parent
p, if the corresponding edge in the tree is p a− →q, then γq := F(k,γp ⊕a). With these conventions,
we see that if a message m traces out a path as in (6.8), then γpv = FCBC(k,m).
The crux of the proof is to argue that if F behaves like a random function, then for every
pair of distinct edges in the tree, say p a− →q and p′ a′
− →q′, we have γp ⊕a ̸= γp′ ⊕a′ with
overwhelming probability. To prove that there are no collisions of this type, the preﬁx-freeness
restriction is critical, as it guarantees that the adversary never sees γp and γp′, and hence a and
a′ are independent of these values. Once we have established that there are no collisions of these
types, it will follow that all values associated with non-root nodes are random and independent,
and this holds in particular for the values associated with the leaves, which represent the outputs
of FCBC seen by the adversary. Therefore, the adversary cannot distinguish FCBC from a random
function. 2
Proof. We make this intuition rigorous by letting Ainteract with four closely related challengers
in four games. For j = 0,1,2,3, we let Wj be the event that Aoutputs 1 at the end of Game j.
Game 0. This is Experiment 0 of Attack Game 4.2.
Game 1. We next play the usual “PRF card,” replacing the function F(k,·) by a truly random
function f in Funs[X,X]. Clearly, we have
⏐⏐Pr[W1] −Pr[W0]
⏐⏐= PRFadv[B,F] (6.9)
for an eﬃcient adversary B.
Game 2. We now make a purely conceptual change, implementing the random function f as a
“faithful gnome” (as in Section 4.4.2). However, it will be convenient for us to do this in a particular
way, using the “query tree” discussed above.
To this end, ﬁrst let B := Qℓ, which represents an upper bound on the number of points at
which f will be evaluated. Our challenger ﬁrst prepares random values
βi ←R X (i= 1,...,B ).
227
These will be the only random values used by our challenger.
As the adversary makes queries, our challenger will dynamically build up the query tree. Ini-
tially, the tree contains only the root. Whenever the adversary makes a query, the challenger traces
out the corresponding path in the existing query tree; at some point, this path will extend beyond
the existing query tree, and our challenger adds the necessary nodes and edges so that the query
tree grows to include the new path.
Our challenger must also compute the values γp associated with each node. Initially, γroot = 0n.
When adding a new edge p a− →q to the tree, if this is the ith edge being added (for i= 1,...,B ),
our challenger does the following:
γq ←βi
(∗) if ∃another edge p′ a′
− →q′with γp′⊕a′= γp ⊕a then γq ←γq′
The idea is that we use the next unused value in our prepared list β1,...,β B as the “default”
value for γq. The line marked ( ∗) performs the necessary consistency check, which ensures that our
gnome is indeed faithful.
Because this change is purely conceptual, we have
Pr[W2] = Pr[W1]. (6.10)
Game 3. Next, we make our gnome forgetful, by removing the consistency check marked ( ∗) in
the logic in Game 2.
To analyze the eﬀect of this change, let Z be the event that in Game 3 , for some distinct pair
of edges p a− →q and p′ a′
− →q′, we have γp′⊕a′= γp ⊕a.
Now, the only randomly chosen values in Games 2 and 3 are the random choices of the ad-
versary, Coins, and the list of values β1,...,β B. Observe that for any ﬁxed choice of values
Coins,β1,...,β B, if Z does not occur, then in fact Games 2 and 3 proceed identically. Therefore,
we may apply the Diﬀerence Lemma (Theorem 4.7), obtaining
⏐⏐Pr[W3] −Pr[W2]
⏐⏐≤Pr[Z]. (6.11)
We next bound Pr[Z]. Consider two distinct edges p a− →q and p′ a′
− →q′. We want to bound the
probability that γp′⊕a′= γp ⊕a, which is equivalent to
γp′⊕γp = a′⊕a. (6.12)
There are two cases to consider.
Case 1: p= p′. Since the edges are distinct, we must have a′̸= a, and hence (6.12) holds with
probability 0.
Case 2: p ̸= p′. The requirement that the adversary’s queries are preﬁx free implies that in
Game 3, the adversary never sees — or learns anything about — the values γp and γp′. One of por
p′ could be the root, but not both. It follows that the value γp ⊕γp′ is uniformly distributed over
Xand is independent of a⊕a′. From this, it follows that (6.12) holds with probability 1 /|X|.
By the union bound, it follows that
Pr[Z] ≤ B2
2|X|. (6.13)
228
Combining (6.9), (6.10), (6.11), and (6.13), we obtain
PRFpfadv[A,FCBC] =
⏐⏐Pr[W3] −Pr[W0]
⏐⏐≤PRFadv[B,F] + B2
2|X|. (6.14)
Moreover, Game 3 corresponds exactly to Experiment 1 of Attack Game 4.2, from which the
theorem follows. 2
6.4.2 The cascade preﬁx-free secure PRF
Let F be a PRF that takes keys in Kand produces outputs in K. In symbols, F is deﬁned over
(K,X,K). For any poly-bounded value ℓ, we build a new PRF F∗, called the cascade of F, that
maps messages in X≤ℓ to outputs in K. The function F∗, illustrated in Fig. 6.3b, works as follows:
input: k∈K and m= (a1,...,a v) ∈X≤ℓ for some v∈{0,...,ℓ }
output: a tag in K
t←k
for i←1 to v do:
t←F(t, ai)
output t
The following theorem shows that F∗is a preﬁx-free secure PRF.
Theorem 6.4. Let F be a secure PRF deﬁned over (K,X,K). Then for any poly-bounded value ℓ,
the cascade F∗ of F is a preﬁx-free secure PRF deﬁned over (K,X≤ℓ,K).
In particular, for every preﬁx-free PRF adversary Athat attacks F∗as in Attack Game 4.2, and
issues at most Qqueries, there exists a PRF adversary Bthat attacks F as in Attack Game 4.2,
where Bis an elementary wrapper around A, such that
PRFpfadv[A,F∗] ≤Qℓ·PRFadv[B,F]. (6.15)
Exercise 6.6 develops an attack on ﬁxed-length F∗ that demonstrates that security degrades
quadratically in Q. This is disturbing as it appears to contradict the linear dependence on Q in
(6.15). However, rest assured there is no contradiction here. The adversary Afrom Exercise 6.6,
which uses ℓ = 3, has advantage about 1 /2 when Q is about
√
|K|. Plugging Ainto the proof of
Theorem 6.4 we obtain a PRF adversary Bthat attacks the PRF F by making about Qqueries to
gain an advantage about 1 /Q. Note that 1 /Q≈Q/|K|when Q is close to
√
|K|. There is nothing
surprising about this adversary B: it is essentially the universal PRF attacker from Exercise 4.27.
Hence, (6.15) is consistent with the attack from Exercise 6.6. Another way to view this is that
the quadratic dependence on Qis already present in (6.15) because there is an implicit factor of Q
hiding in the quantity PRF adv[B,F].
The proof of Theorem 6.4 is similar to the proof that the variable-length tree construction in
Section 4.6 is a preﬁx-free secure PRF (Theorem 4.11). Let us brieﬂy explain how to extend the
proof of Theorem 4.11 to prove Theorem 6.4.
229
Relation to the tree construction. The cascade construction is a generalization of the variable-
length tree construction of Section 4.6. Recall that the tree construction builds a secure PRF from
a secure PRG that maps a seed to a pair of seeds. It is easy to see that when F is a PRF deﬁned
over (K,{0,1},K) then Theorem 6.4 is an immediate corollary of Theorem 4.11: simply deﬁne the
PRG G mapping k∈K to G(k) := (F(k,0),F(k,1)) ∈K2, and observe that cascade applied to F
is the same as the variable-length tree construction applied to G.
The proof of Theorem 4.11 generalizes easily to prove Theorem 6.4 for any PRF. For example,
suppose that F is deﬁned over ( K,{0,1,2},K). This corresponds to a PRG G mapping k ∈K to
G(k) := (F(k,0),F(k,1),F(k,2)) ∈K3. The cascade construction applied to F can be viewed as a
ternary tree, instead of a binary tree, and the proof of Theorem 4.11 carries over with no essential
changes.
But why stop at width three? We can make the tree as wide as we wish. The cascade construc-
tion using a PRF F deﬁned over ( K,X,K) corresponds to a tree of width |X|. Again, the proof
of Theorem 4.11 carries over with no essential changes. We leave the details as an exercise for the
interested reader (Exercise 4.26 may be convenient here).
Comparing the CBC and cascade PRFs. Note that CBC uses a ﬁxed keykfor all applications
of F while cascade uses a diﬀerent key in each round. Since block ciphers are typically optimized
to encrypt many blocks using the same key, the constant re-keying in cascade may result in worse
performance than CBC. Hence, CBC is the more natural choice when using an oﬀ the shelf block
cipher like AES.
An advantage of cascade is that there is no additive error term in Theorem 6.4. Consequently,
the cascade construction remains secure even if the underlying PRF has a small domain X. CBC,
in contrast, is secure only when Xis large. As a result, cascade can be used to convert a PRG into
a PRF for large inputs while CBC cannot.
6.4.3 Extension attacks: CBC and cascade are insecure MACs
We show that the MACs derived from CBC and cascade are insecure. This will imply that CBC
and cascade are not secure PRFs. All we showed in the previous section is that CBC and cascade
are preﬁx-free secure PRFs.
Extension attack on cascade. Given F∗(k,m) for some messagemin X≤ℓ, anyone can compute
t′:= F∗(k, m ∥m′) (6.16)
for any m′∈X∗, without knowledge of k. Once F∗(k,m) is known, anyone can continue evaluating
the chain using blocks of the messagem′and obtain t′. We refer to this as the extension property
of cascade.
The extension property immediately implies that the MAC derived from F∗is terribly insecure.
The forger can request the MAC on message m and then deduce the MAC on m∥m′ for any m′
of its choice. It follows, by Theorem 6.2, that F∗is not a secure PRF.
An attack on CBC. We describe a simple MAC forger on the MAC derived from CBC. The
forger works as follows:
230
F
PF
m
k1
tag
k2
t∈Y
Figure 6.4: The encrypted PRF construction EF (k,m)
1. pick an arbitrary a1 ∈X;
2. request the tag t on the one-block message ( a1);
3. deﬁne a2 := a1 ⊕t and output t as a MAC forgery for the two-block message ( a1,a2) ∈X2.
Observe that t= F(k,a1) and a1 = F(k,a1) ⊕a2. By deﬁnition of CBC we have:
FCBC
(
k, (a1,a2)
)
= F
(
k, F (k, a1) ⊕a2
)
= F(k, a1
)
= t.
Hence,
(
(a1,a2), t
)
is an existential forgery for the MAC derived from CBC. Consequently, FCBC
cannot be a secure PRF. Note that the attack on the cascade MAC is far more devastating than
on the CBC MAC. But in any case, these attacks show that neither CBC nor cascade should be
used directly as MACs.
6.5 From preﬁx-free secure PRF to fully secure PRF (method 1):
encrypted PRF
We show how to convert the preﬁx-free secure PRFsFCBC and F∗into secure PRFs, which will give
us secure MACs for variable length inputs. More generally, we show how to convert a preﬁx-free
secure PRF PF to a secure PRF. We present three methods:
• Encrypted PRF: encrypt the short output of PF with another PRF.
• Preﬁx-free encoding: encode the input to PF so that no input is a preﬁx of another.
• CMAC: a more eﬃcient preﬁx-free encoding using randomization.
In this section we discuss the encrypted PRF method. The construction is straightforward. Let
PF be a PRF mapping X≤ℓ to Yand let F be a PRF mapping Yto T. Deﬁne
EF
(
(k1,k2), m
):= F
(
k2, PF (k1, m)
)
(6.17)
The construction is shown in Fig. 6.4.
We claim that when PF is either CBC or cascade then EF is a secure PRF. More generally, we
show that EF is secure whenever PF is an extendable PRF, deﬁned as follows:
231
Deﬁnition 6.4. Let PF be a PRF deﬁned over (K,X≤ℓ,Y). We say that PF is an extendable
PRF if for all k∈K, x,y ∈X≤ℓ−1, and a∈X we have:
if PF (k,x) = PF (k,y) then PF (k, x∥a) = PF (k, y∥a).
It is easy to see that both CBC and cascade are extendable PRFs. The next theorem shows
that when PF is an extendable, preﬁx-free secure PRF then EF is a secure PRF.
Theorem 6.5. Let PF be an extendable and preﬁx-free secure PRF deﬁned over (K1,X≤ℓ+1,Y),
where |Y|is super-poly and ℓis poly-bounded. Let F be a secure PRF deﬁned over (K2,Y,T). Then
EF , as deﬁned in (6.17), is a secure PRF deﬁned over (K1 ×K2,X≤ℓ,T).
In particular, for every PRF adversary Athat attacks EF as in Attack Game 4.2, and issues
at most Q queries, there exist a PRF adversary B1 attacking F as in Attack Game 4.2, and
a preﬁx-free PRF adversary B2 attacking PF as in Attack Game 4.2, where B1 and B2 are
elementary wrappers around A, such that
PRFadv[A,EF ] ≤PRFadv[B1,F] + PRFpfadv[B2,PF ] + Q2
2|Y|. (6.18)
We prove Theorem 6.5 in the next chapter (Section 7.3.1) after we develop the necessary tools.
Note that to make EF a secure PRF on inputs of length up to ℓ, this theorem requires that PF is
preﬁx-free secure on inputs of length ℓ+ 1.
The bound in (6.18) is tight. Although not entirely necessary, let us assume that Y = T,
that F is a block cipher, and that |X|is not too small. These assumptions will greatly simplify the
argument. We exhibit an attack that breaks EF with constant probability after Q≈
√
|Y|queries.
Our attack will, in fact, breakEF as a MAC. The adversary picksQrandom inputs x1,...,x Q ∈X2
and queries its MAC challenger at all Q inputs to obtain t1,...,t Q ∈T . By the birthday paradox
(Corollary B.2), for any ﬁxed key k1, with constant probability there will be distinct indices i,j
such that xi ̸= xj and PF (k1,xi) = PF (k1,xj). On the one hand, if such a collision occurs, we will
detect it, because ti = tj for such a pair of indices. On the other hand, if ti = tj for some pair of
indices i,j, then our assumption that F is a block cipher guarantees that PF (k1,xi) = PF (k1,xj).
Now, assuming that xi ̸= xj and PF (k1,xi) = PF (k1,xj), and since PF is extendable, we know
that for all a ∈X , we have PF
(
k1,(xi ∥a)
)
= PF
(
k1,(xj ∥a)
)
. Therefore, our adversary can
obtain the MAC tag tfor xi ∥a, and this tag twill also be a valid tag for xj ∥a. This attack easily
generalizes to show the necessity of the term Q2/(2|Y|) in (6.18).
6.5.1 ECBC and NMAC: MACs for variable length inputs
Figures 6.5a and 6.5b show the result of applying the EF construction (6.17) to CBC and cascade.
6.5.1.1 The Encrypted-CBC PRF
Applying EF to CBC results in a classic PRF (and hence a MAC) called encrypted-CBC or
ECBC for short. This MAC is standardized by ANSI (see Section 6.9) and is used in the banking
industry. The ECBC PRF uses the same underlying PRF F for both CBC and the ﬁnal encryption.
Consequently, ECBC is deﬁned over (K2, X≤ℓ, X).
232
a1
F(k1,·)
a2
F(k1,·)
⨁
a3
F(k1,·)
⨁
aℓ
F(k1,·)
⨁
···
F(k2,·) tag
CBC
(a) The ECBC construction ECBC( k,m) (encrypted CBC)
a1
Fk1
a2
F
aℓ
F
···
F
t∥fpadt∈K
tagk2cascade
(b) The NMAC construction NMAC( k,m) (encrypted cascade)
Figure 6.5: Secure PRF constructions for variable length inputs
233
Theorem 6.6 (ECBC security). Let F be a secure PRF deﬁned over (K,X,X). Suppose X is
super-poly, and let ℓbe a poly-bounded length parameter. Then ECBC is a secure PRF deﬁned over
(K2,X≤ℓ,X).
In particular, for every PRF adversary Athat attacks ECBC as in Attack Game 4.2, and issues
at most Q queries, there exist PRF adversaries B1,B2 that attack F as in Attack Game 4.2,
and which are elementary wrappers around A, such that
PRFadv[A,ECBC] ≤PRFadv[B1,F] + PRFadv[B2,F] + (Q(ℓ+ 1))2 + Q2
2|X| . (6.19)
Proof. CBC is clearly extendable and is a preﬁx-free secure PRF by Theorem 6.3. Hence, if the
underlying PRF F is secure, then ECBC is a secure PRF by Theorem 6.5. 2
The argument given after Theorem 6.5 shows that there is an attacker that after Q ≈
√
|X|
queries breaks this PRF with constant advantage. Recall that for 3DES we have X = {0,1}64.
Hence, after about a billion queries (or more precisely, 2 32 queries) an attacker can break the
ECBC-3DES MAC with constant probability.
6.5.1.2 The NMAC PRF
Applying EF to cascade results in a PRF (and hence a MAC) called Nested MAC or NMAC
for short. A variant of this MAC is standardized by the IETF (see Section 8.7.2) and is widely
used in Internet protocols.
We wish to use the same underlying PRF F for the cascade construction and for the ﬁnal
encryption. Unfortunately, the output of cascade is in Kwhile the message input to F is in X. To
solve this problem we need to embed the output of cascade into X. More precisely, we assume that
|K|≤|X| and that there is an eﬃciently computable one-to-one function g that maps Kinto X.
For example, suppose K:= {0,1}κ and X:= {0,1}n where κ ≤n. Deﬁne g(t) := t ∥fpad where
fpad is a ﬁxed pad of length n−κ bits. This fpad can be as simple as a string of 0s. With this
translation, all of NMAC can be built from a single secure PRF F, as shown in Fig. 6.5b.
Theorem 6.7 (NMAC security). Let F be a secure PRF deﬁned over (K,X,K), where Kcan
be embedded into X. Then NMAC is a secure PRF deﬁned over (K2,X≤ℓ,K).
In particular, for every PRF adversary Athat attacks NMAC as in Attack Game 4.2, and issues
at most Q queries, there exist PRF adversaries B1,B2 that attack F as in Attack Game 4.2,
and which are elementary wrappers around A, such that
PRFadv[A,NMAC] ≤(Q(ℓ+ 1)) ·PRFadv[B1,F] + PRFadv[B2,F] + Q2
2|K|. (6.20)
Proof. NMAC is clearly extendable and is a preﬁx-free secure PRF by Theorem 6.4. Hence, if the
underlying PRF F is secure, then NMAC is a secure PRF by Theorem 6.5. 2
ECBC and NMAC are streaming MACs. Both ECBC and NMAC can be used to authenticate
variable size messages in X≤ℓ. Moreover, there is no need for the message length to be known ahead
of time. A MAC that has this property is said to be a streaming MAC. This property enables
applications to feed message blocks to the MAC one block at a time and at some arbitrary point
234
decide that the message is complete. This is important for applications like streaming video, where
the message length may not be known ahead of time.
In contrast, some MAC systems require that the message length be prepended to the message
body (see Section 6.6). Such MACs are harder to use in practice since they require applications to
determine the message length before starting the MAC calculations.
6.6 From preﬁx-free secure PRF to fully secure PRF (method 2):
preﬁx-free encodings
Another approach to converting a preﬁx-free secure PRF into a secure PRF is to encode the input
to the PRF so that no encoded input is a preﬁx of another. We use the following terminology:
• We say that a set S ⊆X≤ℓ is a preﬁx-free set if no element in S is a proper preﬁx of any
other. For example, if ( x1,x2,x3) belongs to a preﬁx-free set S, then neither x1 nor (x1,x2)
are in S.
• Let X≤ℓ
>0 denote the set of all non-empty strings over X of length at most ℓ. We say that a
function pf : M→X ≤ℓ
>0 is a preﬁx-free encoding if pf is injective (i.e., one-to-one) and the
image of pf is a preﬁx-free set.
Let PF be a preﬁx-free secure PRF deﬁned over (K,X≤ℓ,Y) and pf : M→X ≤ℓ
>0 be a preﬁx-free
encoding. Deﬁne the derived PRF F as
F(k,m) := PF (k,pf (m)).
Then F is deﬁned over (K,M,Y). We obtain the following trivial theorem.
Theorem 6.8. If PF is a preﬁx-free secure PRF and pf is a preﬁx-free encoding then F is a secure
PRF.
6.6.1 Preﬁx free encodings
To construct PRFs using Theorem 6.8 we describe two preﬁx-free encodings pf : M→X ≤ℓ. We
assume that X= {0,1}n for some n.
Method 1: prepend length. Set M:= X≤ℓ−1 and let m= (a1,...,a v) ∈M. Deﬁne
pf (m) := (⟨v⟩,a1,...,a v) ∈X≤ℓ
>0
where ⟨v⟩∈X is the binary representation of v, the length of m. We assume that ℓ <2n so that
the message length can be encoded as an n-bit binary string.
We argue that pf is a preﬁx-free encoding. Clearly pf is injective. To see that the image of
pf is a preﬁx-free set let pf (x) and pf (y) be two elements in the image of pf . If pf (x) and pf (y)
contain the same number of blocks, then neither is a proper preﬁx of the other. Otherwise, pf (x)
and pf (y) contain a diﬀerent number of blocks and must therefore diﬀer in the ﬁrst block. But
then, again, neither is a proper preﬁx of the other. Hence, pf is a preﬁx-free encoding.
This preﬁx-free encoding is not often used in practice since the resulting MAC is not a streaming
MAC: an application using this MAC must commit to the length of the message to MAC ahead of
time. This is undesirable for streaming applications such as streaming video where the length of
packets may not be known ahead of time.
235
Method 2: stop bits. Let ¯X:= {0,1}n−1 and let M= ¯X≤ℓ
>0 . For m= (a1,...,a v) ∈M, deﬁne
pf (m) :=
(
(a1 ∥0), (a2 ∥0), ..., (av−1 ∥0), (av ∥1)
)
∈X≤ℓ
>0
Clearly pf is injective. To see that the image of pf is a preﬁx-free set let pf (x) and pf (y) be two
elements in the image of pf . Let v be the number of blocks in pf (x). If pf (y) contains v or fewer
blocks then pf (x) is not a proper preﬁx of pf (y). If pf (y) contains more than v blocks then block
number v in pf (y) ends in 0, but block number v in pf (x) ends in 1. Hence, pf (x) and pf (y) diﬀer
in block v and therefore pf (x) is not a proper preﬁx of pf (y).
The MAC resulting from this preﬁx-free encoding is a streaming MAC. This encoding, however,
increases the length of the message to MAC byvbits. When computing the MAC on a long message
using either CBC or cascade, this encoding will result in additional evaluations of the underlying
PRF (e.g. AES). In contrast, the encrypted PRF method of Section 6.5 only adds one additional
application of the underlying PRF. For example, to MAC a megabyte message (2 20 bytes) using
ECBC-AES and pf one would need an additional 511 evaluations of AES beyond what is needed
for the encrypted PRF method. In practice, things are even worse. Since computers prefer byte-
aligned data, one would most likely need to append an entire byte to every block, rather than just
a bit. Then to MAC a megabyte message using ECBC-AES and pf would result in 4096 additional
evaluations of AES over the encrypted PRF method — an overhead of about 6%.
6.7 From preﬁx-free secure PRF to fully secure PRF (method 3):
CMAC
Both preﬁx free encoding methods from the previous section are problematic. The ﬁrst resulted in
a non-streaming MAC. The second required more evaluations of the underlying PRF for long mes-
sages. We can do better by randomizing the preﬁx free encoding. We build a streaming secure PRF
that introduces no overhead beyond the underlying preﬁx-free secure PRF. The resulting MACs,
shown in Fig. 6.6, are superior to those obtained from encrypted PRFs and deterministic encodings.
This approach is used in a NIST MAC standard called CMAC and described in Section 6.10.
First, we introduce some convenient notation:
Deﬁnition 6.5. For two strings x,y ∈X≤ℓ, let us write x∼y if x is a preﬁx of y or y is a preﬁx
of x.
Deﬁnition 6.6. Let ϵ be a real number, with 0 ≤ϵ ≤1. A randomized ϵ-preﬁx-free encoding
is a function rpf : K×M→X ≤ℓ
>0 such that for all m0,m1 ∈M with m0 ̸= m1, we have
Pr
[
rpf (k,m0) ∼rpf (k,m1)
]
≤ϵ,
where the probability is over the random choice of k in K.
Note that the image of rpf (k,·) need not be a preﬁx-free set. However, without knowledge of k it
is diﬃcult to ﬁnd messages m0,m1 ∈M such that rpf (k,m0) is a proper preﬁx of rpf (k,m1) (or
vice versa). The function rpf (k,·) need not even be injective.
A simple rpf . Let K:= Xand M:= X≤ℓ
>0 . Deﬁne
rpf (k, (a1,...,a v)) :=
(
a1,...,a v−1,(av ⊕k)
)
∈X≤ℓ
>0
236
It is easy to see that rpf is a randomized (1 /|X|)-preﬁx-free encoding. Let m0,m1 ∈M with
m0 ̸= m1. Suppose that |m0|= |m1|. Then it is clear that for all choices of k, rpf (k,m0) and
rpf (k,m1) are distinct strings of the same length, and so neither is a preﬁx of the other. Next,
suppose that |m0| < |m1|. If v := |rpf (k,m0)|, then clearly rpf (k,m0) is a proper preﬁx of
rpf (k,m1) if and only if block number v satisﬁes
m0,v ⊕k= m1,v.
But this holds with probability 1 /|X| over the random choice of k, as required. Finally, the case
|m0|>|m1|is handled by a symmetric argument.
Using rpf . Let PF be a preﬁx-free secure PRF deﬁned over (K,X≤ℓ,Y) and rpf : K1 ×M→X ≤ℓ
>0
be a randomized preﬁx-free encoding. Deﬁne the derived PRF F as
F
(
(k,k1), m) := PF
(
k,rpf (k1,m)
)
. (6.21)
Then F is deﬁned over ( K×K 1,M,Y). We obtain the following theorem, which is analogous to
Theorem 6.8.
Theorem 6.9. If PF is a preﬁx-free secure PRF, ϵis negligible, and rpf a randomized ϵ-preﬁx-free
encoding, then F deﬁned in (6.21) is a secure PRF.
In particular, for every PRF adversary Athat attacks F as in Attack Game 4.2, and issues at
most Q queries, there exist preﬁx-free PRF adversaries B1 and B2 that attack PF as in Attack
Game 4.2, where B1 and B2 are elementary wrappers around A, such that
PRFadv[A,F] ≤PRFpfadv[B1,PF ] + PRFpfadv[B2,PF ] + Q2ϵ/2. (6.22)
Proof idea. If the adversary’s set of inputs to F give rise to a preﬁx-free set of inputs to PF , then
the adversary sees just some random looking outputs. Moreover, if the adversary sees random
outputs, it obtains no information about the rpf key k1, which ensures that the set of inputs to
PF is indeed preﬁx free (with overwhelming probability). Unfortunately, this argument is circular.
However, we will see in the detailed proof how to break this circularity. 2
Proof. Without loss of generality, we assume thatAnever issues the same query twice. We structure
the proof as a sequence of three games. For j = 0,1,2, we let Wj be the event that Aoutputs 1 at
the end of Game j.
Game 0. The challenger in Experiment 0 of the PRF Attack Game 4.2 with respect to F works
as follows.
k←R K, k 1 ←R K1
upon receiving a signing query mi ∈M (for i= 1,2,... ) do:
xi ←rpf (k1,mi) ∈X≤ℓ
>0
yi ←PF (k,xi)
send yi to A
Game 1. We change the challenger in Game 0 to ensure that all queries to PF are preﬁx free.
Recall the notation x∼y, which means that x is a preﬁx of y or y is a preﬁx of x.
237
k←R K, k 1 ←R K1, r1,...,r Q ←R Y
upon receiving a signing query mi ∈M (for i= 1,2,... ) do:
xi ←rpf (k1,mi) ∈X≤ℓ
>0
(1) if xi ∼xj for some j <i
then yi ←ri
(2) else yi ←PF (k,xi)
send yi to A
Let Z1 be the event that the condition on line (1) holds at some point during Game 1. Clearly,
Games 1 and 2 proceed identically until event Z1 occurs; in particular, W0 ∧ ¯Z1 occurs if and only
if W1 ∧ ¯Z1 occurs. Applying the Diﬀerence Lemma (Theorem 4.7), we obtain
⏐⏐Pr[W1] −Pr[W0]
⏐⏐≤Pr[Z1]. (6.23)
Unfortunately, we are not quite in a position to bound Pr[ Z1] at this point. At this stage in the
analysis, we cannot say that the evaluations of PF at line (2) do not leak some information about
k1 that could help Amake Z1 happen. This is the circularity problem we alluded to above. To
overcome this problem, we will delay the analysis of Z1 to the next game.
Game 2. Now we play the usual “PRF card,” replacing the function PF (k,·) by a truly random
function. This is justiﬁed, since by construction, in Game 1, the set of inputs to PF (k,·) is preﬁx-
free. To implement this change, we may simply replace the line marked (2) by
(2) else yi ←ri
After making this change, we see that yi gets assigned the random value ri, regardless of whether
the condition on line (1) holds or not.
Now, let Z2 be the event that the condition on line (1) holds at some point during Game 2. It
is not hard to see that
|Pr[Z1] −Pr[Z2]|≤ PRFpfadv[B1,F] (6.24)
and
|Pr[W1] −Pr[W2]|≤ PRFpfadv[B2,F] (6.25)
for eﬃcient preﬁx-free PRF adversaries B1 and B2. These two adversaries are basically the same,
except that B1 outputs 1 if the condition on line (1) holds, while B2 ouputs whatever Aoutputs.
Moreover, in Game 2, the value of k1 is clearly independent of A’s queries, and so by making
use of the ϵ-preﬁx-free property of rpf , and the union bound we have
Pr[Z2] ≤Q2ϵ/2 (6.26)
Finally, Game 2 perfectly emulates for Aa random function in Funs[M,Y]. Game 2 is therefore
identical to Experiment 1 of the PRF Attack Game 4.2 with respect to F, and hence
|Pr[W0] −Pr[W2]|= PRFadv[A,F]. (6.27)
Now combining (6.23)–(6.27) proves the theorem. 2
238
a1
F(k,·)
a2
F(k,·)
⨁
a3
F(k,·)
⨁
aℓ
F(k,·)
⨁
tag
···
k1
(a) rpf applied to CBC
a1
Fk
a2
F
a3
F
aℓ
F
⨁ k1
···
tag
(b) rpf applied to cascade
Figure 6.6: Secure PRFs using random preﬁx-free encodings
6.8 Converting a block-wise PRF to bit-wise PRF
So far we constructed a number of PRFs for variable length inputs in X≤ℓ. Typically X= {0,1}n
where nis the block size of the underlying PRF from which CBC or cascade are built (e.g., n= 128
for AES). All our MACs so far are designed to authenticate messages whose length is a multiple of
n bits.
In this section we show how to convert these PRFs into PRFs for messages of arbitrary bit
length. That is, given a PRF for messages in X≤ℓ we construct a PRF for messages in {0,1}≤nℓ.
Let F be a PRF taking inputs in X≤ℓ+1. Let inj : {0,1}≤nℓ →X ≤ℓ+1 be an injective (i.e.,
one-to-one) function. Deﬁne the derived PRF Fbit as
Fbit(k,x) := F(k, inj (x)).
Then we obtain the following trivial theorem.
Theorem 6.10. If F is a secure PRF deﬁned over (K,X≤ℓ+1,Y) then Fbit is a secure PRF deﬁned
over (K, {0,1}≤nℓ, Y).
An injective function. For X:= {0,1}n, a standard example of an injective inj from {0,1}≤nℓ
to X≤ℓ+1 works as follows. If the input message length is not a multiple of n then inj appends
100 ... 00 to pad the message so its length is the next multiple of n. If the given message length
is a multiple of n then inj appends an entire n-bit block (1 ∥0n−1). Fig. 6.7 describes this in a
picture. More precisely, the function works as follows:
239
a1 a2
a1 a2 −→
−→ a1 a2 1000
a1 a2 1000000
case 1:
case 2:
Figure 6.7: An injective function inj : {0,1}≤nℓ →X≤ℓ+1
input: m∈{0,1}≤nℓ
u←|m|mod n, m ′←m∥1 ∥0n−u−1
output m′as a sequence of n-bit message blocks
To see that inj is injective we show that it is invertible. Given y ←inj (m) scan y from right to
left and remove all the 0s until and including the ﬁrst 1. The remaining string is m.
A common mistake is to pad the given message to a multiple of a block size using an all-0 pad.
This pad is not injective and results in an insecure MAC: for any message m whose length is not
a multiple of the block length, the MAC on m is also a valid MAC for m ∥0. Consequently, the
MAC is vulnerable to existential forgery.
Injective functions must expand. When we feed an n-bit single block message into inj , the
function adds a “dummy” block and outputs a two-block message. This is unfortunate for appli-
cations that MAC many single block messages. When using CBC or cascade, the dummy block
forces the signer and veriﬁer to evaluate the underlying PRF twice for each message, even though all
messages are one block long. Consequently, inj forces all parties to work twice as hard as necessary.
It is natural to look for injective functions from {0,1}≤nℓ to X≤ℓ that never add dummy blocks.
Unfortunately, there are no such functions simply because the set {0,1}≤nℓ is larger than the set
X≤ℓ. Hence, all injective functions must occasionally add a “dummy” block to the output.
The CMAC construction described in Section 6.10 provides an elegant solution to this problem.
CMAC avoids adding dummy blocks by using a randomized injective function.
6.9 Case study: ANSI CBC-MAC
When building a MAC from a PRF, implementors often shorten the ﬁnal tag by only outputting
the w most signiﬁcant bits of the PRF output. Exercise 4.4 shows that truncating a secure PRF
has no eﬀect on its security as a PRF. Truncation, however, aﬀects the derived MAC. Theorem 6.2
shows that the smaller w is, the less secure the MAC becomes. In particular, the theorem adds a
1/2w error in the concrete security bounds.
Two ANSI standards (ANSI X9.9 and ANSI X9.19) and two ISO standards (ISO 8731-1 and
ISO/IEC 9797) specify variants of ECBC for message authentication using DES as the underlying
PRF. These standards truncate the ﬁnal 64-bit output of the ECBC-DES and use only the leftmost
wbits of the output, where w= 32,48,or 64 bits. This reduces the tag length at the cost of reduced
security.
Both ANSI CBC-MAC standards specify a padding scheme to be used for messages whose
length is not a multiple of the DES or AES block size. The padding scheme is identical to the
240
a1
F(k,·)
a2
F(k,·)
⨁
au
F(k,·)
⨁
···
k1
tag
a1
F(k,·)
a2
F(k,·)
⨁
au∥100
F(k,·)
⨁
···
k2
tag
(a) when length(m) is a positive multiple of n (b) otherwise
Figure 6.8: CMAC signing algorithm
function inj described in Section 6.8. The same padding scheme is used when signing a message
and when verifying a message-tag pair.
6.10 Case study: CMAC
Cipher-based MAC — CMAC — is a variant of ECBC adopted by the National Institute of Stan-
dards (NIST) in 2005. It is based on a proposal due to Black and Rogaway and an extension due to
Iwata and Kurosawa. CMAC improves over ECBC used in the ANSI standard in two ways. First,
CMAC uses a randomized preﬁx-free encoding to convert a preﬁx-free secure PRF to a secure PRF.
This saves the ﬁnal encryption used in ECBC. Second, CMAC uses a “two key” method to avoid
appending a dummy message block when the input message length is a multiple of the underlying
PRF block size.
CMAC is the best approach to building a bit-wise secure PRF from the CBC preﬁx-free secure
PRF. It should be used in place of the ANSI method. In Exercise 6.14 we show that the CMAC
construction applies equally well to cascade.
The CMAC bit-wise PRF. The CMAC algorithm consists of two steps. First, a sub-key
generation algorithm is used to derive three keys k0,k1,k2 from the MAC key k. Then the three
keys k0,k1,k2 are used to compute the MAC.
Let F be a PRF deﬁned over ( K,X,X) where X= {0,1}n. The NIST standard uses AES as
the PRF F. The CMAC signing algorithm is given in Table 6.1 and is illustrated in Fig. 6.8. The
ﬁgure on the left is used when the message length is a multiple of the block size n. The ﬁgure on
the right is used otherwise. The standard allows for truncating the ﬁnal output to w bits by only
outputting the w most signiﬁcant bits of the ﬁnal value t.
Security. The CMAC algorithm described in Fig. 6.8 can be analyzed using the randomized
preﬁx-free encoding paradigm. In eﬀect, CMAC converts the CBC preﬁx-free secure PRF directly
241
input: Key k∈K and m∈{0,1}∗
output: tag t∈{0,1}w for some w≤n
Setup:
Run a sub-key generation algorithm
to generate keys k0,k1,k2 ∈X from k∈K
ℓ←length(m)
u←max(1,⌈ℓ/n⌉)
Break m into consecutive n-bit blocks so that
m= a1 ∥a2 ∥···∥ au−1 ∥a∗
u where a1,...,a u−1 ∈{0,1}n.
(∗) If length( a∗
u) = n
then au = k1 ⊕a∗
u
else au = k2 ⊕(a∗
u ∥1 ∥0j) where j = nu−ℓ−1
CBC:
t←0n
for i←1 to u do:
t←F(k0, t ⊕ai)
Output t[0 ...w −1] / / Output w most signiﬁcant bits of t.
Table 6.1: The CMAC signing algorithm
into a bit-wise secure PRF using a randomized preﬁx-free encoding rpf : K×M→X ≤ℓ
>0 where
K:= X2 and M:= {0,1}≤nℓ. The encoding rpf is deﬁned as follows:
input: m∈M and (k1,k2) ∈X2
if |m|is not a positive multiple of n then
u←|m|mod n
partition m into a sequence of bit strings a1,...,a v ∈X,
so that m= a1 ∥···∥ av and a1,...,a v−1 are n-bit strings
if |m|is a positive multiple of n
then output
(
a1, ..., av−1, (av ⊕k1)
)
else output
(
a1, ..., av−1, ((av ∥1 ∥0n−u−1) ⊕k2)
)
The argument that rpf is a randomized 2−n-preﬁx-free encoding is similar to the one is Section 6.7.
Hence, CMAC ﬁts the randomized preﬁx-free encoding paradigm and its security follows from
Theorem 6.9. The keys k1,k2 are used to resolve collisions between a message whose length is a
positive multiple of nand a message that has been padded to make it a positive multiple of n. This
is essential for the analysis of the CMAC rpf .
Sub-key generation. The sub-key generation algorithm generates the keys ( k0,k1,k2) from k.
It uses a ﬁxed mask string Rn that depends on the block size of F. For example, for a 128-bit
block size, the standard speciﬁes R128 := 012010000111. For a bit string X we denote by X <<1
the bit string that results from discarding the leftmost bit X and appending a 0-bit on the right.
The sub-key generation algorithm works as follows:
242
input: key k∈K
output: keys k0,k1,k2 ∈X
k0 ←k
L←F(k,0n)
(1) if msb( L) = 0 then k1 ←(L<< 1) else k1 ←(L<< 1) ⊕Rn
(2) if msb( k1) = 0 then k2 ←(k1 <<1) else k2 ←(k1 <<1) ⊕Rn
output k0,k1,k2.
where msb(L) refers to the most signiﬁcant bit of L. The lines marked (1) and (2) may look a
bit mysterious, but in eﬀect, they simply multiply L by X and by X2 (respectively) in the ﬁnite
ﬁeld GF(2n). Here we are treating the elements of GF(2 n) as polynomials in F2[X] modulo a ﬁxed
polynomial g(X). For a 128-bit block size, the deﬁning polynomial g(X) that corresponds to R128
is g(X) := X128 + X7 + X2 + X + 1. Exercise 6.16 explores some insecure variants of sub-key
generation.
The three keys (k0,k1,k2) output by the sub-key generation algorithm can be used for authen-
ticating multiple messages. Hence, its running time is amortized across many messages.
Clearly the keys k0, k1, and k2 are not independent. If they were, or if they were derived as,
say, ki := F(k,αi) for constants α0,α1,α2, the security of CMAC would follow directly from the
arguments made here and our general framework. Nevertheless, a more intricate analysis allows
one to prove that CMAC is indeed secure [93].
6.11 PMAC: a parallel MAC
The MACs we developed so far, ECBC, CMAC, and NMAC, are inherently sequential: block
number icannot be processed before block number i−1 is ﬁnished. This makes it diﬃcult to exploit
hardware parallelism or pipelining to speed up MAC generation and veriﬁcation. In this section
we construct a secure MAC that is well suited for a parallel architecture. The best construction is
called PMAC. We present PMAC0 which is a little easier to describe.
Let F1 be a PRF deﬁned over ( K1,Zp,Y), where p is a prime and Y:= {0,1}n. Let F2 be a
PRF deﬁned over (K2,Y,Z).
We build a new PRF, called PMAC 0, which takes as input a key and a message in Z≤ℓ
p for
some ℓ. It outputs a value in Z. The PMAC 0 construction works as follows:
input: m= (a1,...,a v) ∈Zv
p for some 0 ≤v≤ℓ, and
key ⃗k= (k,k1,k2) where k∈Zp, k1 ∈K1, and k2 ∈K2
output: tag in Z
PMAC0(⃗k,m):
t←0n ∈Y, mask ←0 ∈Zp
for i←1 to v do:
mask ←mask + k / / mask = i·k∈Zp
r←ai + mask / / r= ai + i·k∈Zp
t←t⊕F1
(
k1,r)
output F2(k2, t)
The main loop adds the masks k,2k,3k,... to message blocks prior to evaluating the PRF F1. On
a sequential machine this requires two additions modulo p per iteration. On a parallel machine
243
a1
a1 + k
F1(k1,·)
a2
a2 + 2k
F1(k1,·)
a3
a3 + 3k
F1(k1,·)
av
av + vk
F1(k1,·)
···
···
···
⨁
F2(k2,·) tag
Figure 6.9: PMAC0 construction
each processor can independently compute ai + ik and then apply F1. See Fig. 6.9.
PMAC0 is a secure PRF and hence gives a secure MAC for large messages. The proof will
follow easily from Theorem 7.7 developed in the next chapter. For now we state the theorem and
delay its proof to Section 7.3.3.
Theorem 6.11. If F1 and F2 are secure PRFs, and |Y| and the prime p are super-poly, then
PMAC0 is a secure PRF for any poly-bounded ℓ.
In particular, for every PRF adversary Athat attacks PMAC0 as in Attack Game 4.2, and
issues at most Qqueries, there exist PRF adversaries B1 and B2, which are elementary wrappers
around A, such that
PRFadv[A,PMAC0] ≤PRFadv[B1,F1] + PRFadv[B2,F2] + Q2
2|Y|+ Q2ℓ2
2p . (6.28)
When using PMAC0, the input message must be partitioned into blocks, where each block is an
element of Zp. In practice, that is inconvenient. It is much easier to break the message into blocks,
where each block is an n-bit string in {0,1}n, for some n. A better parallel MAC construction,
presented next, does exactly that by using the ﬁnite ﬁeld GF(2 n) instead of Zp. This is a good
illustration for why GF(2 n) is so useful in cryptography. We often need to work in a ﬁeld for
security reasons, but a prime ﬁnite ﬁeld like Zp is inconvenient to use in practice. Instead, we use
GF(2n) where arithmetic operations are much faster. GF(2n) also lets us naturally operate on n-bit
blocks.
PMAC: better than PMAC0. Although PMAC0 is well suited for a parallel architecture, there
is room for improvement. Better implementations of the PMAC0 approach are available. Examples
244
include PMAC [23] and XECB [73], both of which are parallelizable. PMAC, for example, provides
the following improvements over PMAC0:
• PMAC uses arithmetic in the ﬁnite ﬁeld GF(2n) instead of in Zp. Elements of GF(2 n) can be
represented as n-bit strings, and addition in GF(2 n) is just a bit-wise XOR. Because of this,
PMAC just uses F1 = F2 = F, where F is a PRF deﬁned over (K,Y,Y), and the input space
of PMAC consists of sequences of elements of Y= {0,1}n, rather than elements of Zp.
• The PMAC mask for block iis deﬁned as γi·k where γ1,γ2,... are ﬁxed constants in GF(2n)
and multiplication is deﬁned in GF(2 n). The γi’s are specially chosen so that computing
γi+1 ·k from γi ·k is very cheap.
• PMAC derives the key k as k ←F(k1,0n) and sets k2 ←k1. Hence PMAC uses a shorter
secret key than PMAC0.
• PMAC uses a trick to save one application of F.
• PMAC uses a variant of the CMAC rpf to provide a bit-wise PRF.
The end result is that PMAC is as eﬃcient as ECBC and NMAC on a sequential machine, but
has much better performance on a parallel or pipelined architecture. PMAC is the best PRF
construction in this chapter; it works well on a variety of computer architectures and is eﬃcient for
both long and short messages.
PMAC0 is incremental. Suppose Bob computes the tag tfor some long message m. Some time
later he changes one block in m and wants to recompute the tag of this new message m′. When
using CBC-MAC the tag tis of no help — Bob must recompute the tag for m′from scratch. With
PMAC0 we can do much better. Suppose the PRF F2 used in the construction of PMAC 0 is the
encryption algorithm of a block cipher such as AES, and let D be the corresponding decryption
algorithm. Let m′ be the result of changing block number i of m from ai to a′
i. Then the tag
t′:= PMAC0(k,m′) for m′can be easily derived from the tag t:= PMAC0(k,m) for m as follows:
t1 ←D(k2,t)
t2 ←t1 ⊕ F1(k1, ai + ik) ⊕ F1(k1, a′
i + ik)
t′←F2(k2,t2)
Hence, given the tag on some long message m (as well as the MAC secret key) it is easy to derive
tags for local edits of m. MACs that have this property are said to be incremental. We just
showed that the PMAC0, implemented using a block cipher, is incremental.
6.12 A fun application: searching on encrypted data
To be written.
6.13 Notes
Citations to the literature to be added.
245
6.14 Exercises
6.1 (The 802.11b insecure MAC). Consider the following MAC (a variant of this was used for
WiFi encryption in 802.11b WEP). Let F be a PRF deﬁned over ( K,R,X) where X:= {0,1}32.
Let CRC32 be a simple and popular error-detecting code meant to detect random errors; CRC32 is
a function that takes as input m∈{0,1}≤ℓ and outputs a 32-bit string. Deﬁne the following MAC
system (S,V ):
S(k,m) :=
{
r←R R, t←F(k,r) ⊕CRC32(m), output (r,t)
}
V(k,m, (r,t)) :={accept if t= F(k,r) ⊕CRC32(m) and reject otherwise}
Show that this MAC system is insecure.
6.2 (Tighter bounds with veriﬁcation queries). Let F be a PRF deﬁned over ( K,X,Y), and
let I be the MAC system derived from F, as discussed in Section 6.3. Let Abe an adversary
that attacks I as in Attack Game 6.2, and which makes at most Qv veriﬁcation queries and
at most Qs signing queries. Theorem 6.1 says that there exists a Qs-query MAC adversary B
that attacks I as in Attack Game 6.1, where Bis an elementary wrapper around A, such that
MACvqadv[A,I] ≤MACadv[B,I] ·Qv. Theorem 6.2 says that there exists a ( Qs + 1)-query PRF
adversary B′that attacks F as in Attack Game 4.2, where B′is an elementary wrapper around B,
such that MACadv[B,I] ≤PRFadv[B′,F] + 1/|Y|. Putting these two statements together, we get
MACvqadv[A,I] ≤(PRFadv[B′,F] + 1/|Y|) ·Qv
This bound is not the best possible. Give a direct analysis that shows that there exists a ( Qs +Qv)-
query PRF adversary B′′, where B′′is an elementary wrapper around A, such that
MACvqadv[A,I] ≤PRFadv[B′′,F] + Qv/|Y|.
6.3 (Multi-key MAC security). Just as we did for semantically secure encryption in Exer-
cise 5.2, we can extend the deﬁnition of a secure MAC from the single-key setting to the multi-key
setting. In this exercise, you will show that security in the single-key setting implies security in the
multi-key setting.
(a) Show how to generalize Attack Game 6.2 so that an attacker can submit both signing queries
and veriﬁcation queries with respect to several MAC keys k1,...,k Q. At the beginning of the
game the adversary outputs a number Q indicating the number of keys it wants to attack
and the challenger chooses Q random keys k1,...,k Q. Subsequently, every query from the
attacker includes an index j ∈{1,...,Q }. The challenger uses the key kj to respond to the
query.
(b) Show that every eﬃcient adversary Athat wins your multi-key MAC attack game with
probability ϵ can be transformed into an eﬃcient adversary Bthat wins Attack Game 6.2
with probability ϵ/Q.
Hint: This is not done using a hybrid argument, but rather a “guessing” argument, somewhat
analogous to that used in the proof of Theorem 6.1. Adversary Bplays the role of challenger
to adversary A. Once Aoutputs a number Q, Bchooses Q random keys k1,...,k Q and a
random index ω ∈{1,...,Q }. When Aissues a query for key number j ̸= ω, adversary B
246
uses its key kj to answer the query. When Aissues a query for the key kω, adversary B
answers the query by querying its MAC challenger. If Aoutputs a forgery under key kω then
Bwins the MAC forgery game. Show that Bwins Attack Game 6.2 with probability ϵ/Q.
We call this style of argument “plug-and-pray:” B“plugs” the key he is challenged on at a
random index ω and “prays” that Auses the key at index ω to form his existential forgery.
6.4 (Multicast MACs). Consider a scenario in which Alice wants to broadcast the same message
to nusers, U1,...,U n. She wants the users to be able to authenticate that the message came from
her, but she is not concerned about message secrecy. More generally, Alice may wish to broadcast
a series of messages, but for this exercise, let us focus on just a single message.
(a) In the most trivial solution, Alice shares a MAC keyki with each userUi. When she broadcasts
a message m, she appends tags t1,...,t n to the message, where ti is a valid tag for m under
key ki. Using its shared key ki, every user Ui can verify m’s authenticity by verifying that ti
is a valid tag for m under ki.
Assuming the MAC is secure, show that this broadcast authentication scheme is secure even
if users collude . For example, users U1,...,U n−1 may collude, sharing their keys k1,...,k n−1
among each other, to try to make user Un accept a message that is not authentic.
(b) While the above broadcast authentication scheme is secure, even in the presence of collusions,
it is not very eﬃcient; the number of keys and tags grows linearly in n.
Here is a more eﬃcient scheme, but with a weaker security guarantee. We illustrate it with
n= 6. The goal is to get by with ℓ< 6 keys and tags. We will use just ℓ= 4 keys, k1,...,k 4.
Alice stores all four of these keys. There are 6 =
(4
2
)
subsets of {1,..., 4}of size 2. Let us
number these subsets J1,...,J 6. For each user Ui, if Ji = {v,w}, then this user stores keys
kv and kw.
When Alice broadcasts a message m, she appends tags t1,...,t 4, corresponding to keys
k1,...,k 4. User Ui veriﬁes tags tv and tw, using its keys kv,kw, where Ji = {v,w}as above.
Assuming the MAC is secure, show that this broadcast authentication scheme is secure pro-
vided no two users collude . For example, using the keys that he has, user U1 may attempt
to trick user U6 into accepting an inauthentic message, but users U1 and U2 may not collude
and share their keys in such an attempt.
(c) Show that the scheme presented in part (b) is completely insecure if two users are allowed to
collude.
6.5 (MAC combiners). We want to build a MAC systemIusing two MAC systemsI1 = (S1,V1)
and I2 = (S2,V2), so that if at some time one of I1 or I2 is broken (but not both) then Iis still
secure. Put another way, we want to construct Ifrom I1 and I2 such that Iis secure if either I1
or I2 is secure.
(a) Deﬁne I= (S,V ), where
S( (k1,k2), m) := ( S1(k1,m), S2(k2,m) ),
and V is deﬁned in the obvious way: on input ( k,m, (t1,t2)), V accepts iﬀ both V1(k1,m,t 1)
and V2(k2,m,t 2) accept. Show that Iis secure if either I1 or I2 is secure.
247
(b) Suppose that I1 and I2 are deterministic MAC systems (see the deﬁnition on page 217), and
that both have tag space {0,1}n. Deﬁne the deterministic MAC system I= (S,V ), where
S( (k1,k2), m) := S1(k1,m) ⊕S2(k2,m).
Show that Iis secure if either I1 or I2 is secure.
6.6 (Concrete attacks on CBC and cascade). We develop attacks on FCBC and F∗as preﬁx-
free PRFs to show that for both security degrades quadratically with number of queries Qthat the
attacker makes. For simplicity, let us develop the attack when inputs are exactly three blocks long.
(a) Let F be a PRF deﬁned over (K,X,X) where X= {0,1}n, where |X|is super-poly. Consider
the FCBC preﬁx-free PRF with input space X3. Suppose an adversary queries the challenger
at points ( x1,y1,z), (x2,y2,z), ... (xQ,yQ,z), where the xi’s, the yi’s, and z are chosen
randomly from X. Show that if Q ≈
√
|X|, the adversary can predict the PRF at a new
point in X3 with probability at least 1 /2.
(b) Show that a similar attack applies to the three-block cascade F∗preﬁx-free PRF built from a
PRF deﬁned over (K,X,K). Assume X= Kand |K|is super-poly. After making Q≈
√
|K|
queries in X3, your adversary should be able to predict the PRF at a new point in X3 with
probability at least 1/2.
6.7 (Weakly secure MACs). In Attack Game 6.1 the adversary can issue signing queries and
obtain message-tag pairs ( mi,ti) for i = 1,...,Q . It can win the game in one of two ways: (i) it
can produce a new valid tag t for one of m1,...,m Q, or (ii) it can produce a forgery pair ( m,t)
for an entirely new message m, namely, an m that is not one of m1,...,m Q. In this exercise we
consider a weaker notion of security for a MAC where the adversary can only win the game by
producing a forgery of type (ii). One can modify the winning condition in Attack Games 6.1 and 6.2
to reﬂect this weaker security notion. In Attack Game 6.1, this means that to win, in addition to
being a valid pair, the adversary’s candidate forgery pair ( m,t) must satisfy the constraint that m
is not among the signing queries. In Attack Game 6.2, this means that the adversary wins if the
challenger ever responds to a veriﬁcation query ( ˆmj,ˆtj) with accept, where ˆmj is not among the
signing queries made prior to this veriﬁcation query. These two modiﬁed attack games correspond
to notions of security that we call weak security without veriﬁcation queries and weak security with
veriﬁcation queries.
(a) The analog of Theorem 6.1 does not hold for these weaker security notions. Your goal is to
develop an explicit counter-example. Let F be a secure PRF deﬁned over ( K,X,T) where
K= {0,1}n and Tis super-poly. Consider the following “sabotaged” MAC systemI= (S,V ),
deﬁned over (K,X,T′), where T′:= T ×{0,1,...,n }:
S(k,m) :=
(
F(k,m),n
)
V
(
k,m, (t,i)
):=
{
output accept iﬀ t= F(k,m) and k[i] = 0
}
where k[i] is bit iof kfor i= 0,1,...,n −1 and k[n] := 0. Show that Iprovides weak security
without veriﬁcation queries, but does not provide weak security with veriﬁcation queries.
(b) Show that Ifrom part (a) is insecure with respect to either Attack Games 6.1 or 6.2.
248
6.8 (Fixing CBC: a bad idea). We showed that CBC is a preﬁx-free secure PRF but not a
secure PRF. We showed that prepending the length of the message makes CBC a secure PRF. Show
that appending the length of the message prior to applying CBC does not make CBC a secure PRF.
6.9 (Fixing CBC: a really bad idea). To avoid extension attacks on CBC, one might be tempted
to deﬁne a CBC-MAC with a randomized IV. This is a MAC with a probabilistic signing algorithm
that on input k ∈K and (x1,...,x v) ∈X≤ℓ, works as follows: choose IV ∈X at random; output
(IV ,t), where t := FCBC(x1 ⊕IV ,x2,...,x v). On input ( k,(x1,...,x v),(IV ,t)), the veriﬁcation
algorithms tests if t= FCBC(x1 ⊕IV ,x2,...,x v). Show that this MAC is completely insecure, and
is not even a preﬁx-free secure PRF.
6.10 (Truncated CBC). Prove that truncating the output of CBC gives a secure PRF for variable
length messages. More speciﬁcally, if CBC is instantiated with a block cipher that operates on n-bit
blocks, and we truncate the output of CBC to w <n bits, then this truncated version is a secure
PRF on variable length inputs, provided 1 /2n−w is negligible.
Hint: Adapt the proof of Theorem 6.3.
6.11 (Truncated cascade). In the previous exercise, we saw that truncating the output of the
CBC construction yields a secure PRF. In this exercise, you are to show that the same does not
hold for the cascade construction, by giving an explicit counter-example. For your counter-example,
you may assume a secure PRF F′ (deﬁned over any convenient input, output, and key spaces, of
your choosing). Using F′, construct another PRF F, such that (a) F is a secure PRF, but (b) the
corresponding truncated version of F∗is not a secure PRF.
6.12 (Truncated cascade in the ideal cipher model). In the previous exercise, we saw that
the truncated cascade may not be secure when instantiated with certain PRFs. However, in your
counter-example, that PRF was constructed precisely to make cascade fail — intuitively, for “typ-
ical” PRFs, one would not expect this to happen. To substantiate this intuition, this exercise
asks you to prove that in the ideal cipher model (see Section 4.7), the cascade construction is a
secure PRF. More precisely, if we model F as the encryption function of an ideal cipher, then the
truncated version of F∗ is a secure PRF. Here, you may assume that F operates on n-bit blocks
and n-bit keys, and that the output of F∗is truncated to w bits, where 1/2n−w is negligible.
6.13 (Non-adaptive attacks on CBC and cascade). This exercise examines whether variable
length CBC and cascade are secure PRFs against non-adaptive adversaries, i.e., adversaries that
make their queries all at once (see Exercise 4.6).
(a) Show that CBC is a secure PRF against non-adaptive adversaries, assuming the underlying
function F is a PRF.
Hint: Adapt the proof of Theorem 6.3.
(b) Give a non-adaptive attack that breaks the security of cascade as a PRF, regardless of the
choice of F.
6.14 (Generalized CMAC).
(a) Show that the CMAC rpf (Section 6.10) is a randomized 2 −n-preﬁx-free encoding.
(b) Use the CMAC rpf to convert cascade into a bit-wise secure PRF.
249
6.15 (A simple randomized preﬁx-free encoding). Show that appending a random message
block gives a randomized preﬁx-free encoding. That is, the following function
rpf (k,m) = m∥k
is a randomized 1 /|X|-preﬁx-free encoding. Here, m∈X≤ℓ and k∈X.
6.16 (An insecure variant of CMAC). Show that CMAC is insecure as a PRF if the sub-key
generation algorithm outputs k0 and k2 as in the current algorithm, but sets k1 ←L.
6.17 (Domain extension). This exercise explores some simple ideas for extending the domain
of a MAC system that do not work. Let I= (S,V ) be a deterministic MAC (see the deﬁnition
on page 217), deﬁned over ( K,M,{0,1}n). Each of the following are signing algorithms for de-
terministic MACs with message space M2. You are to show that each of the resulting MACs are
insecure.
(a) S1(k,(a1,a2)) = S(k,a1) ∥S(k,a2),
(b) S2(k,(a1,a2)) = S(k,a1) ⊕S(k,a2),
(c) S3((k1,k2),(a1,a2)) = S(k1,a1) ∥S(k2,a2),
(d) S4((k1,k2),(a1,a2)) = S(k1,a1) ⊕S(k2,a2).
6.18 (Integrity for database records). Let (S,V ) be a secure MAC deﬁned over ( K,M,T).
Consider a database containing records m1,...,m n ∈M. To provide integrity for the data the
data owner generates a random secret key k∈K and stores ti ←S(k,mi) alongside record mi for
every i= 1,...,n . This does not ensure integrity because an attacker can remove a record from the
database or duplicte a record without being detected. To prevent addition or removal of records
the data owner generates another secret key k′∈K and computes t←S
(
k′, (t1,...,t n)
)
(we are
assuming that Tn ⊆M). She stores ( k,k′,t) on her own machine, away from the database.
(a) Show that updating a single record in the database can be done eﬃciently. That is, explain
what needs to be done to recompute the tag t when a single record mj in the database is
replaced by an updated record m′
j.
(b) Does this approach ensure database integrity? Suppose the MAC ( S,V ) is built from a secure
PRF F deﬁned over (K,M,T) where |T| is super-poly. Show that the following PRF Fn is
a secure PRF on message space Mn
Fn
(
(k,k′),(m1,...,m n)
):= F
(
k′,
(
F(k,m1),...,F (k,mn)
))
.
6.19 (Timing attacks). Let (S,V ) be a deterministic MAC system where tags T are n-bytes
long. The veriﬁcation algorithm V(k,m,t ) is implemented as follows: it ﬁrst computes t′←S(k,m)
and then does:
for i←0 to n−1 do:
if t[i] ̸= t′[i] output reject and exit
output accept
(a) Show that this implementation is vulnerable to a timing attack. An attacker who can submit
arbitrary queries to algorithm V and accurately measure V’s response time can forge a valid
tag on every message m of its choice with at most 256 ·n queries to V.
250
(b) How would you implement V to prevent the timing attack from part (a)?
251
Chapter 7
Message integrity from universal
hashing
In the previous chapter we showed how to build secure MACs from secure PRFs. In particular,
we discussed the ECBC, NMAC, and PMAC constructions. We stated security theorems for these
MACs, but delayed their proofs to this chapter.
In this chapter we describe a general paradigm for constructing MACs using hash functions.
By a hash function we generally mean a function H that maps inputs in some large set Mto
short outputs in T. Elements in T are often called message digests or just digests. Keyed hash
functions, used throughout this chapter, also take as input a key k.
At a high level, MACs constructed from hash functions work in two steps. First, we use the
hash function to hash the message m to a short digest t. Second, we apply a PRF to the digest t,
as shown in Fig. 7.1.
As we will see, ECBC, NMAC, and PMAC 0 are instances of this “hash-then-PRF” paradigm.
For example, for ECBC (described in Fig. 6.5a), the CBC function acts as a hash function that
hashes long input messages into short digests. The ﬁnal application of the PRF using the key k2
is the ﬁnal PRF step. The hash-then-PRF paradigm will enable us to directly and quite easily
deduce the security of ECBC, NMAC, and PMAC 0.
The hash-then-PRF paradigm is very general and enables us to build new MACs out of a wide
variety of hash functions. Some of these hash functions are very fast, and yield MACs that are
more eﬃcient than those discussed in the previous chapter.
7.1 Universal hash functions (UHFs)
We begin our discussion by deﬁning akeyed hash function — a widely used tool in cryptography.
A keyed hash function H takes two inputs: a key k and a message m. It outputs a short digest
t := H(k,m). The key k can be thought of as a hash function selector: for every k we obtain a
speciﬁc function H(k,·) from messages to digests. More precisely, keyed hash functions are deﬁned
as follows:
Deﬁnition 7.1 (Keyed hash functions). A keyed hash function H is a deterministic algo-
rithm that takes two inputs, a key k and a message m; its output t:= H(k,x) is called a digest.
As usual, there are associated spaces: the keyspace K, in which k lies, a message space M, in
252
Hash PRFt
k2
tag
k1
m
Figure 7.1: The hash-then-PRF paradigm
which m lies, and the digest space T, in which t lies. We say that the hash function H is deﬁned
over (K,M,T).
We note that the output digest t∈T can be much shorter than the input message m. Typically
digests will have some ﬁxed size, say 128 or 256 bits, independent of the input message length. A
hash function H(k,·) can map gigabyte long messages into just 256-bit digests.
We say that two messages m0,m1 ∈M form a collision for H under key k∈K if
H(k, m0) = H(k, m1) and m0 ̸= m1.
Since the digest space T is typically much smaller than the message space M, many such collisions
exist. However, a general property we shall desire in a hash function is that it is hard to actually
ﬁnd a collision. As we shall eventually see, there are a number of ways to formulate this “collision
resistance” property. These formulations diﬀer in subtle ways in how much information about
the key an adversary gets in trying to ﬁnd a collision. In this chapter, we focus on the weakest
formulation of this collision resistance property, in which the adversary must ﬁnd a collision withno
information about the key at all. On the one hand, this property is weak enough that we can actually
build very eﬃcient hash functions that satisfy this property without making any assumptions at all
on the computational power of the adversary. On the other hand, this property is strong enough
to ensure that the hash-then-PRF paradigm yields a secure MAC.
Hash functions that satisfy this very weak collision resistance property are called universal
hash functions, or UHFs. Universal hash functions are used in various branches of computer
science, most notably for the construction of eﬃcient hash tables. UHFs are also widely used in
cryptography. Before we can analyze the security of the hash-then-PRF paradigm, we ﬁrst give a
more formal deﬁnition of UHFs. As usual, to make this intuitive notion more precise, we deﬁne an
attack game.
Attack Game 7.1 (universal hash function). For a keyed hash function H deﬁned over
(K,M,T), and a given adversary A, the attack game runs as follows.
• The challenger picks a random k←R Kand keeps k to itself.
• Aoutputs two distinct messages m0,m1 ∈M.
We say that Awins the above game if H(k,m0) = H(k,m1). We deﬁne A’s advantage with respect
to H, denoted UHFadv[A,H], as the probability that Awins the game. 2
We now deﬁne several diﬀerent notions of UHF, which depend on the power of the adversary
and its advantage in the above attack game.
253
Deﬁnition 7.2. Let H be a keyed hash function deﬁned over (K,M,T),
• We say that H is an ϵ-bounded universal hash function, or ϵ-UHF, if UHFadv[A,H] ≤ϵ
for all adversaries A(even ineﬃcient ones).
• We say that H is a statistical UHF if it is an ϵ-UHF for some negligible ϵ.
• We say that H is a computational UHF if UHFadv[A,H] is negligible for all eﬃcient
adversaries A.
Statistical UHFs are secure against all adversaries, eﬃcient or not: no adversary can win Attack
Game 7.1 against a statistical UHF with non-negligible advantage. The main reason that we
consider computationally unbounded adversaries is that we can: unlike most other security notions
we discuss in this text, good UHFs are something we know how to build without any computational
restrictions on the adversary. Note that every statistical UHF is also a computational UHF, but
the converse is not true.
If H is a keyed hash function deﬁned over ( K,M,T), an alternative characterization of the
ϵ-UHF property is the following (see Exercise 7.3):
for every pair of distinct messages m0,m1 ∈M we have Pr[H(k,m0) = H(k,m1)] ≤ϵ,
where the probability is over the random choice of k∈K. (7.1)
7.1.1 Multi-query UHFs
It will be convenient to consider a generalization of a computational UHF. Here the adversary wins
if he can output a list of distinct messages so that some pair of messages in the list is a collision
for H(k,·). The point is that although the adversary may not know exactly which pair of messages
in his list cause the collision, he still wins the game. In more detail, a multi-query UHF is deﬁned
using the following game:
Attack Game 7.2 (multi-query UHF). For a keyed hash function H over (K,M,T), and a
given adversary A, the attack game runs as follows.
• The challenger picks a random k←R Kand keeps k to itself.
• Aoutputs distinct messages m1,...,m s ∈M.
We say that Awins the above game if there are indices i̸= j such that H(k,mi) = H(k,mj). We
deﬁne A’s advantage with respect to H, denoted MUHFadv[A,H], as the probability that Awins
the game. We call Aa Q-query UHF adversary if it always outputs a list of size s≤Q. 2
Deﬁnition 7.3. We say that a hash function H over (K,M,T) is a multi-query UHF if for all
eﬃcient adversaries A, the quantity MUHFadv[A,H] is negligible.
Lemma 7.1 below shows that every UHF is also a multi-query UHF. However, for particular
constructions, we can sometimes get better security bounds.
Lemma 7.1. If H is a computational UHF, then it is also a multi-query UHF.
In particular, for every Q-query UHF adversary A, there exists a UHF adversary B, which is
an elementary wrapper around A, such that
MUHFadv[A,H] ≤(Q2/2) ·UHFadv[B,H]. (7.2)
254
Proof. The UHF adversary Bruns Aand obtains s≤Qdistinct messages m1,...,m s. It chooses a
random pair of distinct indices iand j from {1,...,s }, and outputs mi and mj. The list generated
by Acontains a collision for H(k,·) with probability MUHFadv[A,H] and Bwill choose a colliding
pair with probability at least 2 /Q2. Hence, UHF adv[B,H] is at least MUHF adv[A,H] ·(2/Q2), as
required. 2
7.1.2 Mathematical details
As usual, we give a more mathematically precise deﬁnition of a UHF using the terminology deﬁned
in Section 2.3.
Deﬁnition 7.4 (Keyed hash functions). A keyed hash function is an eﬃcient algorithm H,
along with three families of spaces with system parameterization P:
K = {Kλ,Λ}λ,Λ, M = {Mλ,Λ}λ,Λ, and T = {Tλ,Λ}λ,Λ,
such that
1. K, M, and T are eﬃciently recognizable.
2. K and T are eﬃciently sampleable.
3. Algorithm H is an eﬃcient deterministic algorithm that on input λ∈Z≥1, Λ ∈Supp(P(λ)),
k∈Kλ,Λ, and m∈Mλ,Λ, outputs an element of Tλ,Λ.
In deﬁning UHFs we parameterize Attack Game 7.1 by the security parameterλ. The advantage
UHFadv[A,H] is then a function of λ.
The information-theoretic property (7.1) is the more traditional approach in the literature
in deﬁning ϵ-UHFs for individual hash functions with no security or system parameters; in our
asymptotic setting, if property (7.1) holds for each setting of the security and system parameters,
then our deﬁnition of an ϵ-UHF will certainly be satisﬁed.
7.2 Constructing UHFs
The challenge in constructing good universal hash functions (UHFs) is to construct a function that
achieves a small collision probability using a short key. Preferably, the size of the key should not
depend on the length of the message being hashed. We give three constructions. The ﬁrst is an
elegant construction of a statistical UHF using modular arithmetic and polynomials. Our second
construction is based on the CBC and cascade functions deﬁned in Section 6.4. We show that both
are computational UHFs. The third construction is based on PMAC 0 from Section 6.11.
7.2.1 Construction 1: UHFs using polynomials
We start with a UHF construction using polynomials modulo a prime. Let ℓ be a (poly-bounded)
length parameter and let p be a prime. We deﬁne a hash function Hpoly that hashes a message
m∈Z≤ℓ
p to a single element t∈Zp. The key space is K:= Zp.
Let m be a message, so m = (a1,a2,...,a v) ∈Z≤ℓ
p for some 0 ≤v ≤ℓ. Let k ∈Zp be a key.
The hash function Hpoly(k,m) is deﬁned as follows:
Hpoly
(
k, (a1,...,a v)
):= kv + a1kv−1 + a2kv−2 + ··· + av−1k+ av ∈Zp (7.3)
255
That is, we use (1,a1,a2,...,a v) as the vector of coeﬃcients of a polynomial f(X) of degree v and
then evaluate f(X) at a secret point k.
A very useful feature of this hash function is that it can be evaluated without knowing the
length of the message ahead of time. One can feed message blocks into the hash as they become
available. When the message ends we obtain the ﬁnal hash. We do so using Horner’s method for
polynomial evaluation:
Input: m= (a1,a2,...,a v) ∈Z≤ℓ
p and key k∈Zp
Output: t:= Hpoly(k,m)
1. Set t←1
2. For i←1 to v:
3. t←t·k+ ai ∈Zp
4. Output t
It is not diﬃcult to show that this algorithm produces the same value as deﬁned in (7.3). Observe
that a long message can be processed one block at a time using little additional space. Every
iteration takes one multiplication and one addition.
On a machine that has several multiplication units, say four units, we can use a 4-way parallel
version of Horner’s method to utilize all the available units and speed up the evaluation of Hpoly.
Assuming the length of mis a multiple of 4, simply replace lines (2) and (3) above with the following
2. For i←1 to v incrementing i by 4 at every iteration:
3. t←t·k4 + ai ·k3 + ai+1 ·k2 + ai+2 ·k+ ai+3 ∈Zp
One can precompute the values k2,k3,k4 in Zp. Then at every iteration we process four blocks of
the message using four multiplications that can all be done in parallel.
Security as a UHF. Next we show that Hpoly is an ( ℓ/p)-UHF. If p is super-poly, this implies
that ℓ/p is negligible, which means that Hpoly is a statistical UHF.
Lemma 7.2. The function Hpoly over (Zp, (Zp)≤ℓ, Zp) deﬁned in (7.3) is an (ℓ/p)-UHF.
Proof. Consider two distinct messages m0 = (a1,...,a u) and m1 = (b1,...,b v) in (Zp)≤ℓ. We show
that Pr[ Hpoly(k,m0) = Hpoly(k,m1)] ≤ℓ/p, where the probability is over the random choice of
key k in Zp. Deﬁne the two polynomials:
f(X) := Xu + a1Xu−1 + a2Xu−2 + ··· + au−1X+ au
g(X) := Xv + b1Xv−1 + b2Xv−2 + ··· + bv−1X+ bv
(7.4)
in Zp[X]. Then, by deﬁnition of Hpoly we need to show that
Pr[f(k) = g(k)] ≤ℓ/p
where k is uniform in Zp. In other words, we need to bound the number of points k∈Zp for which
f(k)−g(k) = 0. Since the messages m0 and m1 are distinct we know that f(X)−g(X) is a nonzero
polynomial. Furthermore, its degree is at most ℓ and therefore it has at most ℓ roots in Zp. It
follows that there are at most ℓvalues of k∈Zp for which f(k) = g(k) and therefore, for a random
k∈Zp we have Pr[f(k) = g(k)] ≤ℓ/p as required. 2
256
Why the leading term kv in Hpoly(k,m)? The deﬁnition of Hpoly(k,m) in (7.3) includes a
leading term kv. This term ensures that the function is a statistical UHF for variable size inputs.
If instead we deﬁned Hfpoly(k,m) without this term, namely
Hfpoly
(
k, (a1,...,a v)
):= a1kv−1 + a2kv−2 + ··· + av−1k+ av ∈Zp, (7.5)
then the result would not be a UHF for variable size inputs. For example, the two messages
m0 = ( a1,a2) ∈Z2
p and m1 = (0 ,a1,a2) ∈Z3
p are a collision for Hfpoly under all keys k ∈Zp.
Nevertheless, in Exercise 7.16 we show that Hfpoly is a statistical UHF if we restrict its input space
to messages of ﬁxed length, i.e., M:= Zℓ
p for some ℓ. Speciﬁcally, Hfpoly is an ( ℓ−1)/p-UHF. In
contrast, the function Hpoly deﬁned in (7.3) is a statistical UHF for the input space Z≤ℓ
p containing
messages of varying lengths.
Remark 7.1. The function Hpoly takes inputs in Z≤ℓ
p and outputs values in Zp. This can be
diﬃcult to work with: we prefer to work with functions that operate on blocks of n-bits for some n.
We can adapt the deﬁnition of Hpoly in (7.3) so that instead of working in Zp, arithmetic is done
in the ﬁnite ﬁeld GF(2 n). This version of Hpoly is an ℓ/2n-UHF using the exact same analysis as
in Lemma 7.2. It outputs values in GF(2 n). In Exercise 7.1 we show that simply deﬁning Hpoly
modulo 2n (i.e., working in Z2n) is a completely insecure UHF. 2
Caution in using UHFs. UHFs can be brittle — an adversary who learns the value of the
function at a few points can completely recover the secret key. For example, the value of Hpoly(k,·)
at a single point completely exposes the secret key k∈Zp. Indeed, if m= (a1), since Hpoly(k,m) =
k+ a1 an adversary who has both m and Hpoly(k,m) immediately obtains k ∈Zp. Consequently,
in all of our applications of UHFs, we will always hide values of the UHF from the adversary, either
by encrypting them or by other means.
Mathematical details. The deﬁnition of Hpoly requires a prime p. So far we simply assumed
that p is a public value picked at the beginning of time and ﬁxed forever. In the formal UHF
framework (Section 7.1.2), the prime p is a system parameter denoted by Λ. It is generated by a
system parameter generation algorithm P that takes the security parameter λas input and outputs
some prime p.
More precisely, let L : Z → Z be some function that maps the security parameter to the
desired bit length of the prime. Then the formal description of Hpoly includes a description of an
algorithm P that takes the security parameter λ as input and outputs a prime p of length L(λ)
bits. Speciﬁcally, Λ := p and
Kλ,p = Zp, Mλ,p = Z≤ℓ(λ)
p , and Tλ,p = Zp,
where ℓ: Z →Z≥0 is poly-bounded. By Lemma 7.2 we know that
UHFadv[A,Hpoly](λ) ≤ℓ(λ)/2L(λ)
which is a negligible function of λ provided 2L(λ) is super-poly.
257
7.2.2 Construction 2: CBC and cascade are computational UHFs
Next we show that the CBC and cascade constructions deﬁned in Section 6.4 are computational
UHFs. More generally, we show that any preﬁx-free secure PRF that is also extendable is a
computational UHF. Recall that a PRF F over (K,X≤ℓ,Y) is extendable if for all k ∈K, x,y ∈
X≤ℓ−1, and a∈X we have:
if F(k,x) = F(k,y) then F(k, x∥a) = F(k, y∥a).
In the previous chapter we showed that both CBC and cascade are preﬁx-free secure PRFs and
that both are extendable.
Theorem 7.3. Let PF be an extendable and preﬁx-free secure PRF deﬁned over (K,X≤ℓ+1,Y)
where |Y|is super-poly and |X|>1. Then PF is a computational UHF deﬁned over (K,X≤ℓ,Y).
In particular, for every UHF adversary Athat plays Attack Game 7.1 with respect to PF , there
exists a preﬁx-free PRF adversary B, which is an elementary wrapper around A, such that
UHFadv[A,PF ] ≤PRFpfadv[B,PF ] + 1
|Y|. (7.6)
Moreover, Bmakes only two queries to PF .
Proof. Let Abe a UHF adversary attacking PF . We build a preﬁx-free PRF adversary Battack-
ing PF . Bplays the adversary in the PRF Attack Game 4.2. Its goal is to distinguish between
Experiment 0 where it queries a function f ←PF (k,·) for a random k ∈K, and Experiment 1
where it queries a random function f ←R Funs[X≤ℓ+1,Y].
We ﬁrst give some intuition as to how Bworks. Bstarts by running the UHF adversary Ato
obtain two distinct messages m0,m1 ∈X≤ℓ. By the deﬁnition of A, we know that in Experiment 0
we have
Pr
[
f(m0) = f(m1)
]
= UHFadv[A,PF ]
while in Experiment 1, since f is a random function and m0 ̸= m1, we have
Pr
[
f(m0) = f(m1)
]
= 1/|Y|.
Hence, if Bcould query f at m0 and m1 it could distinguish between the two experiments with
advantage
⏐⏐UHFadv[A,PF ] −1/|Y|
⏐⏐, which would prove the theorem.
Unfortunately, this design for Bdoes not quite work: m0 might be a proper preﬁx of m1, in
which case Bis not allowed to query f at both m0 and m1, because Bis supposed to be a preﬁx-
free adversary. However, the extendability property provides a simple solution: we extend both
m0 and m1 by a single block a ∈X such that m0 ∥a is no longer a proper preﬁx of m1 ∥a. If
m0 = (a1,...,a u) and m1 = (b1,...,b v), then any a ̸= bu+1 will do the trick. Moreover, by the
extension property we know that
PF (k, m0) = PF (k, m1) = ⇒ PF (k, m0 ∥a) = PF (k, m1 ∥a).
Since m0 ∥ais no longer a proper preﬁx of m1 ∥a, our Bis free to query f at both inputs. Bthen
obtains the desired advantage in distinguishing Experiment 0 from Experiment 1.
In more detail, adversary Bworks as follows:
258
run Ato obtain two distinct messages m0,m1 in X≤ℓ, where
m0 = (a1,...,a u) and m1 = (b1,...,b v)
assume u≤v (otherwise, swap the two messages)
if m0 is a proper preﬁx of m1
choose some a∈X such that a̸= bu+1
m′
0 ←m0 ∥a and m′
1 ←m1 ∥a
else
m′
0 ←m0 and m′
1 ←m1
/ / At this point we know that m′
0 is not a proper preﬁx of m′
1 nor vice versa.
query f at m′
0 and m′
1 and obtain t0 := f(m′
0) and t1 := f(m′
1)
if t0 = t1 output 1; otherwise output 0
Observe that Bis a preﬁx-free PRF adversary that only makes two queries to f, as required.
Now, for b= 0,1 let pb be the probability that Boutputs 1 in Experiment b. Then in Experiment 0,
we know that
p0 := Pr
[
f(m′
0) = f(m′
1)
]
≥Pr
[
f(m0) = f(m1)
]
= UHFadv[A,PF ]. (7.7)
In Experiment 1, we know that
p1 := Pr
[
f(m′
0) = f(m′
1)
]
= 1/|Y|. (7.8)
Therefore, by (7.7) and (7.8):
PRFpfadv[B,PF ] = |p0 −p1|≥ p0 −p1 ≥UHFadv[A,PF ] −1/|Y|,
from which (7.6) follows. 2
PF as a multi-query UHF. Lemma 7.1 shows that PF is also a multi-query UHF. However, a
direct proof of this fact gives a better security bound.
Theorem 7.4. Let PF be an extendable and preﬁx-free secure PRF deﬁned over (K,X≤ℓ+1,Y),
where |X| and |Y|are super-poly and ℓ is poly-bounded. Then PF is a multi-query UHF deﬁned
over (K,X≤ℓ,Y).
In particular, if |X| > ℓQ, then for every Q-query UHF adversary A, there exists a Q-query
preﬁx-free PRF adversary B, which is an elementary wrapper around A, such that
MUHFadv[A,PF ] ≤PRFpfadv[B,PF ] + Q2
2|Y|. (7.9)
Proof. The proof is similar to the proof of Theorem 7.3. Adversary Bbegins by running the Q-
query UHF adversary Ato obtain distinct messages m1,...,m s in X≤ℓ, where s ≤Q. Next, B
ﬁnds an a ∈X such that a is not equal to any of the message blocks in m1,...,m s. Since |X| is
super-poly, we may assume it is larger than ℓQ, and therefore this a must exist. Let m′
i := mi ∥a
for i = 1 ,...,s . Then, by deﬁnition of a, the set {m′
1,...,m ′
s}is a preﬁx-free set. The preﬁx-
free adversary Bnow queries the challenger at m′
1,...,m ′
s and obtains t1,...,t s in response. B
outputs 1 if there exist i̸= j such that ti = tj, and outputs 0 otherwise.
259
To analyze the advantage of B, we let pb be the probability that Boutputs 1 in PRF Experi-
ment b, for b= 0,1. As in (7.7), the extension property implies that
p0 ≥MUHFadv[A,PF ].
In Experiment 1 the union bound implies that
p1 ≤Q(Q−1)
2|Y| .
Therefore,
PRFpfadv[B,PF ] = |p0 −p1|≥ p0 −p1 ≥MUHFadv[A,PF ] − Q2
2|Y|
from which (7.9) follows. 2
Applications of Theorems 7.3 and 7.4. Applying Theorem 7.4 to CBC and cascade proves
that both are computational UHFs. We state the resulting error bounds in the following corol-
lary, which follows from the bounds in the CBC theorem (Theorem 6.3) and the cascade theorem
(Theorem 6.4).1
Corollary 7.5. Let F be a secure PRF deﬁned over (K,X,Y). Then the CBC construction FCBC
(assuming Y= X is super-poly size) and the cascade construction F∗ (assuming Y= K), which
take inputs in X≤ℓ for poly-bounded ℓ, are computational UHFs.
In particular, for every Q-query UHF adversary A, there exist preﬁx-free PRF adversaries
B1,B2, which are elementary wrappers around A, such that
MUHFadv[A,FCBC] ≤PRFpfadv[B1,F] + Q2(ℓ+ 1)2 + Q2
2|Y| and (7.10)
MUHFadv[A,F∗] ≤Q(ℓ+ 1) ·PRFpfadv[B2,F] + Q2
2|Y|. (7.11)
Setting Q:= 2 in (7.10)–(7.11) gives the error bounds on FCBC and F∗as UHFs.
7.2.3 Construction 3: a parallel UHF from a small PRF
The CBC and cascade constructions yield eﬃcient UHFs from small domain PRFs, but they are
inherently sequential: they cannot take advantage of hardware parallelism. Fortunately, construct-
ing a UHF from a small domain PRF that is suitable for a parallel architecture is not diﬃcult. An
example called XOR-hash, denoted F⊕, is shown in Fig. 7.2. XOR-hash is deﬁned over (K,X≤ℓ,Y),
where Y= {0,1}n, and is built from a PRF F deﬁned over (K,X×{ 1,...,ℓ },Y). The XOR-hash
works as follows:
input: k∈K and m= (a1,...,a v) ∈X≤ℓ for some 0 ≤v≤ℓ
output: a tag in Y
t←0n
for i= 1 to v do:
t←t⊕F(k, (ai,i) )
output t
1Note that Theorem 7.4 compels us to apply Theorems 6.3 and 6.4 using ℓ+ 1 in place of ℓ.
260
(a1, 1)
F(k,·)
(a2, 2)
F(k,·)
(a3, 3)
F(k,·)
(av, v)
F(k,·)
···
···
⨁
F⊕(k,m)
Figure 7.2: A parallel UHF from a small PRF
Evaluating F⊕ can easily be done in parallel. The following theorem shows that F⊕ is a compu-
tational UHF. Note that unlike our previous UHF constructions, security does not depend on the
length of the input message. In the next section we will use F⊕to construct a secure MAC suitable
for parallel architectures.
Theorem 7.6. Let F be a secure PRF and assume |Y|is super-poly. Then F⊕is a computational
UHF.
In particular, for every UHF adversary A, there exists a PRF adversary B, which is an elemen-
tary wrapper around A, such that
UHFadv[A,F⊕] ≤PRFadv[B,F] + 1
|Y|. (7.12)
Proof. The proof is a sequence of two games.
Game 0. The challenger in this game computes:
k←R K, f ←F(k,·)
The adversary Aoutputs two distinct messages U,V in X≤ℓ. Let u:= |U|and v:= |V|. We deﬁne
W0 to be the event that the condition
u−1⨁
i=0
f(U[i],i) =
v−1⨁
j=0
f(V[j],j) (7.13)
holds in Game 0. Clearly, we have
Pr[W0] = UHFadv[A,F⊕]. (7.14)
Game 1. We play the “PRF card” and replace the challenger’s computation by
f ←R Funs[X×{ 1,...,ℓ }, Y]
261
We deﬁne W1 to be the event that the condition (7.13) holds in Game 1.
As usual, there is a PRF adversary Bsuch that
⏐⏐Pr[W0] −Pr[W1]
⏐⏐≤PRFadv[B,F] (7.15)
The crux of the proof is in bounding Pr[ W1], namely bounding the probability that (7.13) holds for
the messages U,V . Assume u ≥v, swapping U and V if necessary. It is easy to see that since U
and V are distinct, there must be an index i∗such that the pair (U[i∗],i∗) on the left side of (7.13)
does not appear among the pairs ( V[j],j) on the right side of (7.13): if u>v then i∗= u−1 does
the job; otherwise, if u= v, then there must exist some i∗such that U[i∗] ̸= V[i∗], and this i∗does
the job.
We can re-write (7.13) as
f(U[i∗],i∗) =
⨁
i̸=i∗
f(U[i],i) ⊕
⨁
j
f(V[j],j). (7.16)
Since the left and right sides of (7.16) are independent, and the left side is uniformly distributed
over Y, equality holds with probability 1 /|Y|. It follows that
Pr[W1] = 1/|Y| (7.17)
The proof of the theorem follows from (7.14), (7.15), and (7.17). 2
In Exercise 7.28 we generalize Theorem 7.6 to derive bounds for F⊕as a multi-query UHF.
7.3 PRF(UHF) composition: constructing MACs using UHFs
We now proceed to show that the hash-then-PRF paradigm yields a secure PRF provided the
hash is a computational UHF. ECBC, NMAC, and PMAC 0 can all be viewed as instances of
this construction and their security follows quite easily from the security of the hash-then-PRF
paradigm.
Let H be a keyed hash function deﬁned over ( KH,M,X) and let F be a PRF deﬁned over
(KF,X,T). As usual, we assume Mcontains much longer messages than X, so that H hashes long
inputs to short digests. We build a new PRF, denoted F′, by composing the hash function H with
the PRF F, as shown in Fig. 7.3. More precisely, F′is deﬁned as follows:
F′(
(k1,k2), m
):= F(k2, H(k1, m) ) (7.18)
We refer to F′ as the composition of F and H. It takes inputs in Mand outputs values in
T using a key ( k1,k2) in KH ×KF. Thus, we obtain a PRF with the same output space as the
underlying F, but taking much longer inputs. The following theorem shows that F′ is a secure
PRF.
Theorem 7.7 (PRF(UHF) composition). Suppose H is a computational UHF and F is a
secure PRF. Then F′ deﬁned in (7.18) is a secure PRF.
In particular, suppose Ais a PRF adversary that plays Attack Game 4.2 with respect to F′and
issues at most Q queries. Then there exist a PRF adversary BF and a UHF adversary BH,
which are elementary wrappers around A, such that
PRFadv[A,F′] ≤PRFadv[BF,F] + (Q2/2) ·UHFadv[BH,H]. (7.19)
262
H(k1,·) F(k2,·)m t
Figure 7.3: PRF(UHF) composition: MAC signing
More generally, there exists a Q-query UHF adversary B′
H, which is an elementary wrapper
around Asuch that
PRFadv[A,F′] ≤PRFadv[BF,F] + MUHFadv[B′
H,H]. (7.20)
To understand why H needs to be a UHF let us suppose for a minute that it is not. In
particular, suppose it was easy to ﬁnd distinct m0,m1 ∈M such that H(k1,m0) = H(k1,m1),
without knowledge of k1. This collision on H implies that F′((k1,k2), m0) = F′((k1,k2), m1).
But then F′ is clearly not a secure PRF: the adversary could ask for t0 := F′((k1,k2), m0) and
t1 := F′((k1,k2), m1) and then output 1 only if t0 = t1. When interacting with F′ the adversary
would always output 1, but for a random function he would most often output 0. Thus, the
adversary successfully distinguishes F′ from a random function. This argument shows that for F′
to be a PRF it must be diﬃcult to ﬁnd collisions for H without knowledge of k1. In other words,
for F′to be a PRF the hash function H must be a UHF. Theorem 7.7 shows that this condition is
suﬃcient.
Remark 7.2. The bound in Theorem 7.7 is tight. Consider the UHF Hpoly discussed in Sec-
tion 7.2.1. For concreteness, let us assume that ℓ = 2, so the message space for Hpoly is Z2
p, the
output space is Zp, and the collision probability is ϵ = 1 /p. In Exercise 7.26, you are asked to
show that for any ﬁxed hash key k1, among √p random inputs to Hpoly(k1,·), the probability of a
collision is bounded from below by a constant; moreover, for any such collision, one can eﬃciently
recover the key k1. Now consider the MAC obtained from PRF(UHF) composition using Hpoly. If
the adversary ever ﬁnds two messages m0,m1 that cause an internal collision (i.e., a collision on
Hpoly) he can recover the secret Hpoly key and then break the MAC. This shows that the term
(Q2/2)ϵ that appears in (7.19) cannot be substantially improved upon. 2
Proof of Theorem 7.7. We now prove that the composition of F and H is a secure PRF.
Proof idea. Let Abe an eﬃcient PRF adversary that plays Attack Game 4.2 with respect to F′.
We derive an upper bound on PRFadv[A,F′]. That is, we bound A’s ability to distinguish F′from
a truly random function in Funs[ M,X]. As usual, we ﬁrst observe that replacing the underlying
secure PRF F with a truly random function f does not change A’s advantage much. Next, we will
show that, since f is a random function, the only way Acan distinguish F′:= f(H(k1,m)) from a
truly random function is if he can ﬁnd two inputs m0,m1 such that H(k1,m0) = H(k1,m1). But
since H is a computational UHF, Acannot ﬁnd collisions for H(k1,·). Consequently, F′cannot be
distinguished from a random function. 2
Proof. We prove the bound in (7.20). Equation (7.19) follows from (7.20) by Lemma 7.1. We let
263
Ainteract with closely related challengers in three games. For j = 0,1,2, we deﬁne Wj to be the
event that Aoutputs 1 at the end of Game j.
Game 0. The Game 0 challenger is identical to the challenger in Experiment 0 of the PRF Attack
Game 4.2 with respect to F′. Without loss of generality we assume that A’s queries to F′ are all
distinct. The challenger works as follows:
k1 ←R KH, k2 ←R KF
upon receiving the ith query mi ∈M (for i= 1,2,... ) do:
xi ←H(k1, mi)
ti ←F(k2, xi)
send ti to the adversary
Note that since Ais guaranteed to make distinct queries, all the mi values are distinct.
Game 1. Now we play the usual “PRF card,” replacing the function F(k2,·) by a truly random
function f in Funs[X,T], which we implement as a faithful gnome (as in Section 4.4.2). The Game 1
challenger works as follows:
k1 ←R KH, t′
1,...,t ′
Q ←R T
upon receiving the ith query mi ∈M (for i= 1,2,... ) do:
xi ←H(k1, mi)
ti ←t′
i
(∗) if xi = xj for some j <ithen ti ←tj
send ti to the adversary
For i= 1,...,Q , the value t′
i is chosen in advance to be the default, random value for ti = f(xi).
Although the messages are distinct, their hash values might not be. The line marked with a ( ∗)
ensures that the challenger emulates a function in Funs[ X,T] — if two hash values collide, the
challenger’s response to both queries is the same. As usual, one can easily show that there is a
PRF adversary BF whose running time is about the same as that of Asuch that:
⏐⏐Pr[W1] −Pr[W0]
⏐⏐= PRFadv[BF,F] (7.21)
Game 2. Next, we make our gnome forgetful, by removing the line marked ( ∗).
We show thatAcannot distinguish Games 1 and 2 using the fact thatAcannot ﬁnd collisions for
H. Formally, we analyze the quantity|Pr[W2]−Pr[W1]|using the Diﬀerence Lemma (Theorem 4.7).
Let Zbe the event that in Game 2 we havexi = xj for some i̸= j. Event Zis essentially the winning
condition in the multi-query UHF game (Attack Game 7.2) with respect to H. In particular, there
is a Q-query UHF adversary B′
H that wins Attack Game 7.2 with probability equal to Pr[ Z].
Adversary B′
H simply emulates the challenger in Game 2 until Aterminates and then outputs the
queries m1,m2,... from Aas its ﬁnal list. This works, because in Game 2, the challenger does not
really need the hash key k1: it simply responds to each query with a random element of T. Thus,
adversary B′
H can easily emulate the challenger in Game 2 without knowledge of k1. By deﬁnition
of Z, we have MUHFadv[B′
H,H] = Pr[Z].
Clearly, Games 1 and 2 proceed identically unless event Z occurs; in particular, W2 ∧¯Z occurs
if and only if W1 ∧¯Z occurs. Applying the Diﬀerence Lemma, we obtain
⏐⏐Pr[W2] −Pr[W1]
⏐⏐≤Pr[Z] = MUHFadv[B′
H,H]. (7.22)
264
Finishing the proof. The Game 2 challenger emulates for Aa random function in Funs[ M,T]
and is therefore identical to an Experiment 1 PRF challenger with respect to F′. We obtain
PRFadv[A,F′] =
⏐⏐Pr[W2] −Pr[W0]
⏐⏐≤⏐⏐Pr[W2] −Pr[W1]
⏐⏐+
⏐⏐Pr[W1] −Pr[W0]
⏐⏐≤
PRFadv[BF,F] + MUHFadv[B′
H,H]
which proves (7.20), as required. 2
7.3.1 Using PRF(UHF) composition: ECBC and NMAC security
Using Theorem 7.7 we can quickly prove security of many MAC constructions. It suﬃces to show
that the MAC signing algorithm can be described as the composition of a PRF with a UHF. We
begin by showing that ECBC and NMAC can be described this way and give more examples in the
next two sub-sections.
Security of ECBC and NMAC follows directly from PRF(UHF) composition. The proof for
both schemes runs as follows:
• First, we proved that CBC and cascade are preﬁx-free secure PRFs (Theorems 6.3 and 6.4).
We observed that both are extendable.
• Next, we showed that any extendable preﬁx-free secure PRF is also a computational UHF
(Theorem 7.3). In particular, CBC and cascade are computational UHFs.
• Finally, we proved that the composition of a computational UHF and a PRF is a secure PRF
(Theorem 7.7). Hence, ECBC and NMAC are secure PRFs.
More generally, the encrypted PRF construction (Theorem 6.5) is an instance of PRF(UHF) com-
position and hence its proof follows from Theorem 7.7. The concrete bounds in the ECBC and
NMAC theorems (Theorems 6.6 and 6.7) are obtained by plugging (7.10) and (7.11), respectively,
into (7.20).
One can simplify the proof of ECBC and NMAC security by directly proving that CBC and
cascade are computational UHFs. We proved that they are preﬁx-free secure PRFs, which is more
than we need. However, this stronger result enabled us to construct other secure MACs such as
CMAC (see Section 6.7).
7.3.2 Using PRF(UHF) composition with polynomial UHFs
Of course, one can use the PRF(UHF) construction with a polynomial-based UHF, such as Hpoly.
Depending on the underlying hardware, this construction can be much faster than either ECBC,
NMAC, or PMAC0 especially for very long messages.
Recall that Hpoly hashes messages in Z≤ℓ
p to digests in Zp, where p is a prime. For our PRF,
we may very well want to use a block cipher, like AES, that takes as input an n-bit block.
To make this work, we have to somehow make an adjustment so that the output space of the
hash is contained in the input space of the PRF. One way to do this is to choose the prime p so
that it is just a little bit smaller than 2 n, so that we can encode hash digests as inputs to the PRF.
This approach works; however, it has the drawback that we have to view the input to the hash
as a sequence of elements of Zp. So, for example, with n = 128 as in AES, we could choose a
265
128-bit prime, but then the input to the hash would have to be broken up into, say, 120-bit (i.e.,
15 byte) blocks. It would be even more convenient if we could also process the input to the hash
directly as a sequence of n-bit blocks. Part (d) of Exercise 7.23 shows how this can be done, using
a prime that is just a little bit bigger than 2n. Yet another approach is that instead of basing the
hash on arithmetic modulo a prime p, we instead base it on arithmetic in the ﬁnite ﬁeld GF(2 n),
as discussed in Remark 7.1.
7.3.3 Using PRF(UHF) composition: PMAC0 security
Next we show that the PMAC0 construction discussed in Section 6.11 is an instance of PRF(UHF)
composition. Recall that PMAC 0 is built out of two PRFs, F1, which is deﬁned over ( K1,Zp,Y),
and F2, which is deﬁned over ( K2,Y,Z), where Y:= {0,1}n.
The reader should review the PMAC0 construction, especially Fig. 6.9. One can see that PMAC0
is the composition of the PRF F2 with a certain keyed hash function ˆH, which is everything else
in Fig. 6.9.
The goal now is to show that ˆH is a computational UHF. To do this, we observe that ˆH can
be viewed as an instance of the XOR-hash construction in Section 7.2.3, applied to the PRF F′
deﬁned over (Zp ×K1,Zp ×{1,...,ℓ },Y) as follows:
F′((k,k1),(a,i)) := F1(k1,a + i·k).
So it suﬃces to show that F′ is a secure PRF. But it turns out we can view F′ itself as an
instance of PRF(UHF) composition. Namely, it is the composition of the PRF F1 with the keyed
hash function H deﬁned over ( Zp,Zp ×{1,...,ℓ },Zp) as H(k,(a,i)) := a+ i·k. However, H is
just a special case of Hfpoly (see Section 7.2.1). In particular, by the result of Exercise 7.16, H is a
1/p-UHF.
The security of PMAC 0 follows from the above observations. The concrete security bound
(6.28) in Theorem 6.11 follows from the concrete security bound (7.20) in Theorem 7.7 and the
more reﬁned analysis of XOR-hash in Exercise 7.28.
In the design of PMAC0, we assumed the input space ofF1 was equal to Zp. While this simpliﬁes
the analysis, it makes it harder to work with in practice. Just as in Section 7.3.2 above, we would
prefer to work with a PRF deﬁned in terms of a block cipher, like AES, which takes as input an
n-bit block. One can apply the same techniques discussed Section 7.3.2 to get a variant of PMAC 0
whose input space consists of sequences of n-bit blocks, rather than sequences of elements of Zp.
For example, see Exercise 7.25.
7.4 The Carter-Wegman MAC
In this section we present a diﬀerent paradigm for constructing secure MAC systems that oﬀers
diﬀerent tradeoﬀs compared to PRF(UHF) composition.
Recall that in PRF(UHF) composition the adversary’s advantage in breaking the MAC after
seeing Q signed messages grows as ϵ·Q2/2 when using an ϵ-UHF. Therefore to ensure security
when many messages need to be signed, the ϵ-UHF must have a suﬃciently small ϵso that ϵ·Q2/2
is small. This can hurt the performance of an ϵ-UHF like Hpoly where the smaller ϵ is, the slower
the hash function. As an example, suppose that after signing Q := 232 messages, the adversary’s
advantage in breaking the MAC should be no more than 2 −64. Then ϵ must be at most 1 /2127.
266
H(k1,·) F(k2,·)⨁m r←
R
R
v r
Figure 7.4: Carter-Wegman MAC signing algorithm
Our second MAC paradigm, called a Carter-Wegman MAC, maintains the same level of security
as PRF(UHF) composition, but does so with a much larger value of ϵ. With the parameters in the
example above, ϵneed only be 1/264 and this can improve the speed of the hash function, especially
for long messages. The downside is that the resulting tags are longer than those generated by
a PRF(UHF) composition MAC of comparable security. In Exercise 7.5 we explore a diﬀerent
randomized MAC construction that achieves the same security as Carter-Wegman with the same
ϵ, but with shorter tags.
The Carter-Wegman MAC is our ﬁrst example of a randomized MAC system. The signing
algorithm is randomized and there are many valid tags for every message.
To describe the Carter-Wegman MAC, ﬁrst ﬁx some large integer N and set T := ZN, the
group of size N where addition is deﬁned “modulo N.” We use a hash function H and a PRF F
that output values in ZN:
• H is a keyed hash function deﬁned over ( KH,M,T),
• F is a PRF deﬁned over ( KF,R,T).
The Carter-Wegman MAC, denoted ICW, takes inputs in Mand outputs tags in R×T . It uses
keys in KH ×KF. The Carter-Wegman MAC derived from F and H works as follows (see
also Fig. 7.4):
• For key (k1,k2) and message m we deﬁne
S
(
(k1,k2), m
):=
r←R R
v←H(k1,m) + F(k2,r) ∈ZN / / addition modulo N
output (r,v)
• For key (k1,k2), message m, and tag ( r,v) we deﬁne
V
(
(k1,k2), m, (r,v)
):=
v∗←H(k1,m) + F(k2,r) ∈ZN / / addition modulo N
if v= v∗output accept; otherwise output reject
The Carter-Wegman signing algorithm uses a randomizerr∈R. As we will see, the set Rneeds
to be suﬃciently large so that the probability that two tags use the same randomizer is negligible.
267
An encrypted UHF MAC. The Carter-Wegman MAC can be described as an encryption of
the output of a hash function. Indeed, let E= (E,D) be the cipher
E(k,m) :=
{
r←R R, output (r, m+ F
(
k,r)
)}
and D
(
k,(r,c)
):= c−F(k,r)
where F is a PRF deﬁned over ( KF,R,T). This cipher is CPA secure when F is a secure PRF as
shown in Example 5.2. Then the Carter-Wegman MAC can be written as:
S
(
(k1,k2), m
):= E(k2, H(k1,m)
)
V
(
(k1,k2), m, t
):=
{
accept if D(k2,t) = H(k1,m),
reject otherwise.
which we call the encrypted UHF MAC system derived from Eand H.
Why encrypt the output of a hash function? Recall that in the PRF(UHF) composition MAC,
if the adversary ﬁnds two messages m1,m2 that collide on the hash function (i.e., H(k1,m1) =
H(k1,m2)) then the MAC for m1 is the same as the MAC for m2. Therefore, by requesting the
tags for many messages the adversary can identify messages m1 and m2 that collide on the hash
function (assuming collisions on the PRF are unlikely). A collision m1,m2 on the UHF can reveal
information about the hash function key k1 that may completely break the MAC. To prevent
this we must use an ϵ-UHF with a suﬃciently small ϵ to ensure that with high probability the
adversary will never ﬁnd a hash function collision. In contrast, by encrypting the output of the
hash function with a CPA secure cipher, we prevent the adversary from learning when a hash
function collision occurred: the tags for m1 and m2 are diﬀerent, with high probability, even if
H(k1,m1) = H(k1,m2). This lets us maintain security with a much smaller ϵ.
The trouble is that the encrypted UHF MAC is not generally secure even when ( E,D) is
CPA secure and H is an ϵ-UHF. For example, we show in Remark 7.5 below that the Carter-
Wegman MAC is insecure when the hash function H is instantiated with Hpoly. To obtain a secure
Carter-Wegman MAC, we strengthen the hash function H and require that it satisfy a stronger
property called diﬀerence unpredictability deﬁned below. Exercise 9.16 explores other aspects of
the encrypted UHF MAC.
Security of the Carter-Wegman MAC. To prove security of ICW we need the hash function
H to satisfy a stronger property than universality (UHF). We refer to this stronger property as
diﬀerence unpredictability. Roughly speaking, it means that for any two distinct messages, it
is hard to predict the diﬀerence (in ZN) of their hashes. As usual, a game:
Attack Game 7.3 (diﬀerence unpredictability). For a keyed hash function H deﬁned over
(K,M,T), where T = ZN, and a given adversary A, the attack game runs as follows.
• The challenger picks a random k←R Kand keeps k to itself.
• Aoutputs two distinct messages m0,m1 ∈M and a value δ∈T .
We say that Awins the game if H(k,m1) −H(k,m0) = δ. We deﬁne A’s advantage with respect
to H, denoted DUFadv[A,H], as the probability that Awins the game. 2
Deﬁnition 7.5. Let H be a keyed hash function deﬁned over (K,M,T),
• We say that H is an ϵ-bounded diﬀerence unpredictable function , or ϵ-DUF, if
DUFadv[A,H] ≤ϵ for all adversaries A(even ineﬃcient ones).
268
• We say that H is a statistical DUF if it is an ϵ-DUF for some negligible ϵ.
• We say that H is a computational DUF if DUFadv[A,H] is negligible for all eﬃcient
adversaries A.
Remark 7.3. Note that as we have deﬁned a DUF, the digest space T must be of the form ZN
for some integer N. We did this to keep things simple. More generally, one can deﬁne a notion
of diﬀerence unpredictability for a keyed hash function whose digest space comes equipped with
an appropriate diﬀerence operator (in the language of abstract algebra, T should be an abelian
group). Besides ZN, another popular digest space is the set of all n-bit strings, {0,1}n, with the
XOR used as the diﬀerence operator. In this setting, we use the terms ϵ-XOR-DUF and statis-
tical/computational XOR-DUF to correspond to the terms ϵ-DUF and statistical/computational
DUF. 2
When H is a keyed hash function deﬁned over (K,M,T), an alternative characterization of the
ϵ-DUF property is the following:
for every pair of distinct messages m0,m1 ∈M, and every δ ∈T , the following inequality
holds: Pr[H(k,m1) −H(k,m0) = δ] ≤ϵ. Here, the probability is over the random choice of
k∈K.
Clearly if H is an ϵ-DUF then H is also an ϵ-UHF: a UHF adversary can be converted into a
DUF adversary that wins with the same probability (just set δ= 0).
We give a simple example of a statistical DUF that is very similar to the hash function Hpoly
deﬁned in equation (7.3). Recall that Hpoly is a UHF deﬁned over ( Zp, (Zp)≤ℓ, Zp). It is clearly
not a DUF: for a∈Zp set m0 := (a) and m1 := (a+ 1) so that both m0 and m1 are tuples over Zp
of length 1. Then for every key k, we have
Hpoly(k,m1) −Hpoly(k,m0) = (k+ a+ 1) −(k+ a) = 1
which lets the attacker win the DUF game.
A simple modiﬁcation to Hpoly yields a good DUF. For a message m = (a1,a2,...,a v) ∈Z≤ℓ
p
and key k∈Zp deﬁne a new hash function Hxpoly(k,m) as:
Hxpoly(k,m) := k·Hpoly(k,m) = kv+1 + a1kv + a2kv−1 + ··· + avk∈Zp. (7.23)
Lemma 7.8. The function Hxpoly over (Zp, (Zp)≤ℓ, Zp) deﬁned in (7.23) is an (ℓ+ 1)/p-DUF.
Proof. Consider two distinct messages m0 = (a1,...,a u) and m1 = (b1,...,b v) in ( Zp)≤ℓ and an
arbitrary value δ ∈Zp. We want to show that Pr[ Hxpoly(k,m1) −Hxpoly(k,m0) = δ] ≤(ℓ+ 1)/p,
where the probability is over the random choice of key k in Zp. Just as in the proof of Lemma 7.2,
the inputs m0 and m1 deﬁne two polynomials f(X) and g(X) in Zp[X], as in (7.4). However,
Hxpoly(k,m1)−Hxpoly(k,m0) = δholds if and only ifkis root of the polynomial X(g(X)−f(X))−δ,
which is a nonzero polynomial of degree at most ℓ+ 1, and so has at most ℓ+ 1 roots in Zp. Thus,
the chances of choosing such a k is at most ( ℓ+ 1)/p. 2
Remark 7.4. We can modify Hxpoly to operate on n-bit blocks by doing all arithmetic in the ﬁnite
ﬁeld GF(2n) instead of Zp. The exact same analysis as in Lemma 7.8 shows that the resulting hash
function is an ( ℓ+ 1)/2n-XOR-DUF. 2
269
We now turn to the security analysis of the Carter-Wegman construction.
Theorem 7.9 (Carter-Wegman security). Let F be a secure PRF deﬁned over (KF,R,T)
where |R| is super-poly. Let H be a computational DUF deﬁned over (KH,M,T). Then the
Carter-Wegman MAC ICW derived from F and H is a secure MAC.
In particular, for every MAC adversary Athat attacks ICW as in Attack Game 6.1, there exist
a PRF adversary BF and a DUF adversary BH, which are elementary wrappers around A, such
that
MACadv[A,ICW] ≤PRFadv[BF,F] + DUFadv[BH,H] + Q2
2|R|+ 1
|T|. (7.24)
Remark 7.5. To understand why H needs to be a DUF, let us suppose for a minute that it
is not. In particular, suppose it was easy to ﬁnd distinct m0,m1 ∈ Mand δ ∈ Tsuch that
H(k1,m1) = H(k1,m0) +δ, without knowledge of k1. The adversary could then ask for the tag on
the message m0 and obtain (r,v) where v= H(k1,m0) + F(k2,r). Since
v= H(k1,m0) + F(k2,r) = ⇒ v+ δ= H(k1,m1) + F(k2,r),
the tag ( r,v + δ) is a valid tag for m1. Therefore,
(
m1, (r,v + δ)
)
is an existential forgery on
ICW. This shows that the Carter-Wegman MAC is easily broken when the hash function H is
instantiated with Hpoly. 2
Remark 7.6. We also note that the termQ2/2|R|in (7.24) corresponds to the probability that two
signing queries generate the same randomizer. In fact, if such a collision occurs, Carter-Wegman
may be completely broken for certain DUFs (including Hxpoly) — see Exercises 7.13 and 7.14. 2
Proof idea. Let Abe an eﬃcient MAC adversary that plays Attack Game 6.1 with respect to
ICW. We derive an upper bound on MAC adv[A,ICW]. As usual, we ﬁrst replace the underlying
secure PRF F with a truly random function f ∈Funs[R,T] and argue that this doesn’t change
the adversary’s advantage much. We then show that only three things can happen that enable the
adversary to generate a forged message-tag pair and that the probability for each of those is small:
1. The challenger might get unlucky and choose the same randomizer r∈R to respond to two
separate signing queries. This happens with probability at most Q2/(2|R|).
2. The adversary might output a MAC forgery
(
m,(r,v)
)
where r ∈R is a fresh randomizer
that was never used to respond to A’s signing queries. Then f(r) is independent of A’s view
and therefore the equality v= H(k1,m) + f(r) will hold with probability at most 1 /|T|.
3. Finally, the adversary could output a MAC forgery
(
m,(r,v)
)
where r= rj for some uniquely
determined signed message-tag pair ( mj,(rj,vj)). But then
vj = H(k1,mj) + f(rj) and v= H(k1,m) + f(rj).
By subtracting the right equality from the left, the f(rj) term cancels, and we obtain
vj −v= H(k1,mj) −H(k1,m).
But since H is an computational DUF, the adversary can ﬁnd such a relation with only
negligible probability. 2
270
Proof. We make the intuitive argument above rigorous by considering A’s behavior in three closely
related games. For j = 0,1,2, we deﬁne Wj to be the event that Awins Game j. Game 0 will be
identical to the original MAC attack game with respect to I. We then slightly modify each game in
turn and argue that the attacker will not detect these modiﬁcations. Finally, we argue that Pr[ W3]
is negligible, which will prove that Pr[ W0] is negligible, as required.
Game 0. We begin by describing in detail the challenger in the MAC Attack Game 6.1 with
respect to ICW. In this description, we assume that the actual number of signing queries made by
the adversary in a particular execution of the attack game is s, which is at most Q.
Initialization:
k1 ←R KH, k2 ←R KF
r1,...,r Q ←R R / / prepare randomizers needed for the game
upon receiving the ith signing query mi ∈M (for i= 1,...,s ) do:
vi ←H(k1,mi) + F(k2,ri) ∈T
send (ri,vi) to the adversary
upon receiving a forgery attempt ( m,(r,v)) /∈{(m1,(r1,v1)),..., (ms,(rs,vs))}do:
if v= H(k1,m) + F(k2,r)
then output “win”
else output “lose”
Then, by construction
MACadv[A,ICW] = Pr[W0]. (7.25)
Game 1. We next play the usual “PRF card,” replacing the function F(k2,·) by a truly random
function f in Funs[R,T], which we implement as a faithful gnome (as in Section 4.4.2). Our
challenger in Game 1 thus works as follows:
Initialization:
k1 ←R KH
r1,...,r Q ←R R / / prepare randomizers needed for the game
u′
0,u′
1,...,u ′
Q ←R T / / prepare default f outputs
upon receiving the ith signing query mi ∈M (for i= 1,...,s ) do:
ui ←u′
i
(1) if ri = rj for some j <ithen ui ←uj
vi ←H(k1,mi) + ui ∈T
send (ri,vi) to the adversary
upon receiving a forgery attempt ( m,(r,v)) /∈{(m1,(r1,v1)),..., (ms,(rs,vs))}do:
(2) if r= rj for some j = 1,...,s
then u←uj
else u←u′
0
if v= H(k1,m) + u
then output “win”
else output “lose”
For i= 1,...,Q , the value u′
i is chosen in advance to be the default, random value for ui = f(ri).
The tests at the lines marked (1) and (2) ensure that our gnome is faithful, i.e., that we emulate a
271
function in Funs[R,T]. At (2), if the value u= f(r) has already been deﬁned, we use that value;
otherwise, we use the fresh random value u′
0 for u.
As usual, one can show that there is a PRF adversary BF, just as eﬃcient as A, such that:
⏐⏐Pr[W1] −Pr[W0]
⏐⏐= PRFadv[BF,F] (7.26)
Game 2. We make our gnome forgetful. We do this by deleting the line marked (1) in the
challenger. In addition, we insert the following special test before the line marked (2):
if ri = rj for some 1 ≤i<j ≤s then output “lose” (and stop)
Let Z be the event that ri = rj for some 1 ≤i < j≤Q. By the union bound we know that
Pr[Z] ≤Q2/(2|R|). Moreover, if Z does not happen, then Games 1 and 2 proceed identically.
Therefore, by the Diﬀerence Lemma (Theorem 4.7), we obtain
⏐⏐Pr[W2] −Pr[W1]
⏐⏐≤Pr[Z] ≤Q2/(2|R|) (7.27)
To bound Pr[W2], we decompose W2 into two events:
• W′
2: Awins in Game 2 and r= rj for some j = 1,...,s ;
• W′′
2 : Awins in Game 2 and r̸= rj for all j = 1,...,s .
Then W2 = W′
2 ∪W′′
2 , and since these events are disjoint we know that
Pr[W2] = Pr[W′
2] + Pr[W′′
2 ]. (7.28)
We analyze these two events separately. Consider W′′
2 ﬁrst. If this happens, then u = u′
0 and
v= u+ H(k1,m); that is, u′
0 = v−H(k1,m). But since u′
0 and v−H(k1,m) are independent, this
happens with probability 1 /|T|. So we have
Pr[W′′
2 ] ≤1/|T|. (7.29)
Next, consider W′
2. Our goal here is to show that
Pr[W′
2] ≤DUFadv[BH,H] (7.30)
for a DUF adversary BH that is just as eﬃcient as A. To this end, consider what happens if Awins
in Game 2 and r= rj for some j = 1,...,s . Since Awins, and because of the special test that we
added above the line marked (2), the values r1,...,r s are distinct, and so there can be only one
such index j, and u= uj. Therefore, we have the following two equalities:
vj = H(k1,mj) + uj and v= H(k1,m) + uj;
subtracting, we obtain
vj −v= H(k1,mj) −H(k1,m). (7.31)
We claim that m̸= mj. Indeed, if m= mj, then (7.31) would imply v= vj, which would imply
(m,(r,v)) = (mj,(rj,vj)); however, this is impossible, since we require that Adoes not submit a
previously signed pair as a forgery attempt.
272
So, if W′
2 occurs, we have m̸= mj and the equality (7.31) holds. But observe that in Game 2,
the challenger’s responses are completely independent of k1, and so we can easily convert Ainto a
DUF adversary BH that succeeds with probability at least Pr[ W′
2] in Attack Game 7.3. Adversary
BH works as follows: it interacts with A, simulating the challenger in Game 2 by simply responding
to each signing query with a random pair ( ri,vi) ∈R×T ; when Aoutputs its forgery attempt
(m,(r,v)), our BH determines if r = rj and m ̸= mj for some j = 1,...,s ; if so, BH outputs the
triple (mj,m,v j−v). Now (7.31) shows that BH succeeds in attacking H as a DUF whenever event
W′
2 happens. The bound in (7.30) now follows.
The theorem follows from (7.25)–(7.30). 2
7.4.1 Using Carter-Wegman with polynomial UHFs
If we want to use the Carter-Wegman construction with a polynomial-based DUF, such as Hxpoly,
then we have to make an adjustment so that the digest space of the hash function is equal to the
output space of the PRF. Again, the issue is that our example Hxpoly has outputs in Zp, while for
typical implementations, the PRF will have outputs that are n-bit blocks.
Similarly to what we did in Section 7.3.2, we can choose p to be a prime that is just a little
bit bigger than 2 n. This also allows us to view the inputs to the hash as n-bit blocks. Part (b) of
Exercise 7.23 shows how this can be done. One can also use a prime p that is a bit smaller than
2n (see part (a) of Exercise 7.22), although this is less convenient, because inputs to the hash will
have to be broken up into blocks of size less than n. Alternatively, we can use a variant of Hxpoly
where all arithmetic is done in the ﬁnite ﬁeld GF(2 n), as discussed in Remark 7.4.
7.5 Nonce-based MACs
In the Carter-Wegman construction (Section 7.4), the only essential property we need for the
randomizers is that they are distinct. This motivates the study of nonce-based MACs, which are
the analogue of nonce-based encryption (Section 5.5). Not only can this approach reduce the size
of the tag, it can also improve security.
A nonce-based MAC is similar to an ordinary MAC and consists of a pair of deterministic
algorithms S and V for signing and verifying tags. However, these algorithms take an additional
input N called a nonce that lies in a nonce-space N . Algorithms S and V work as follows:
• S takes as input a key k∈K, a message m∈M, and a nonce N ∈N . It outputs a tag t∈T .
• V takes as input four values k,m,t, N , where k is a key, m is a message, t is a tag, and N is
a nonce. It outputs either accept or reject.
We say that the nonce-based MAC is deﬁned over ( K,M,T,N ). As usual, we require that tags
generated by S are always accepted by V, as long as both are given the same nonce . The MAC
must satisfy the following correctness property: for all keys k, all messages m, and all nonces
N ∈N :
V
(
k, m, S(k, m, N ), N
)
= accept.
Just as in Section 5.5, in order to guarantee security, the sender should avoid using the same
nonce twice (on the same key). If the sender can maintain state then a nonce can be implemented
using a simple counter. Alternatively, nonces can be chosen at random, so long as the nonce space
is large enough to ensure that the probability of generating the same nonce twice is negligible.
273
7.5.1 Secure nonce-based MACs
Nonce-based MACs must be existentially unforgeable under a chosen message attack when the
adversary chooses the nonces. The adversary, however, must never request a tag using a previously
used nonce. This captures the idea that nonces can be chosen arbitrarily, as long as they are never
reused. Nonce-based MAC security is deﬁned using the following game.
Attack Game 7.4 (nonce-based MAC security). For a given nonce-based MAC system I=
(S,V ), deﬁned over (K,M,T,N ), and a given adversary A, the attack game runs as follows:
• The challenger picks a random k←R K.
• Aqueries the challenger several times. For i = 1 ,2,..., the ith signing query consists of
a pair ( mi,N i) where mi ∈M and N i ∈N . We require that N i ̸= N j for all j < i. The
challenger computes ti ←R S(k,mi,N i), and gives ti to A.
• Eventually Asends outputs a candidate forgery triple ( m,t, N ) ∈M×T ×N , where
(m,t, N ) /∈{(m1,t1,N 1),(m2,t1,N 2),... }.
We say that Awins the game if V(k,m,t, N ) = accept. We deﬁne A’s advantage with respect to I,
denoted nMACadv[A,I], as the probability that Awins the game. 2
Deﬁnition 7.6. We say that a nonce-based MAC system I is secure if for all eﬃcient adver-
saries A, the value nMACadv[A,I] is negligible.
Nonce-based Carter-Wegman MAC. The Carter-Wegman MAC (Section 7.4) can be recast
as a nonce-based MAC: We simply view the randomizer r∈R as a nonce, supplied as an input to
the signing algorithm, rather than a randomly generated value that is a part of the tag. Using the
notation of Section 7.4, the MAC system is then
S
(
(k1,k2), m, N
):=H(k1,m) + F(k2,N )
V
(
(k1,k2), m, t,N
):=
{
accept if t= S
(
(k1,k2), m, N
)
reject otherwise
We obtain the following security theorem, which is the nonce-based analogue of Theorem 7.9. The
proof is essentially the same as the proof of Theorem 7.9.
Theorem 7.10. With the notation of Theorem 7.9 we obtain the following bounds
nMACadv[A,ICW] ≤PRFadv[BF,F] + DUFadv[BH,H] + 1
|T|.
This bound is much tighter than (7.24): the Q2-term is gone. Of course, it is gone because
we insist that the same nonce is never used twice. If nonces are, in fact, generated by the signer
at random, then the Q2-term returns; however, if the signer implements the nonce as a counter,
then we avoid the Q2-term — the only requirement is that the signer does not sign more than |R|
values. See also Exercise 7.12 for a subtle point regarding the implementation of F.
Analogous to the discussion in Remark 7.6, when using nonce-based Carter-Wegman it is vital
that the nonce is never re-used for diﬀerent messages. If this happens, Carter-Wegman may be
completely broken — see Exercises 7.13 and 7.14.
274
7.6 Unconditionally secure one-time MACs
In Chapter 2 we saw that the one-time pad gives unconditional security as long as the key is only
used to encrypt a single message. Even algorithms that run in exponential time cannot break the
semantic security of the one-time pad. Unfortunately, security is lost entirely if the key is used
more than once.
In this section we ask the analogous question for MACs: can we build a “one-time MAC” that
is unconditionally secure if the key is only used to provide integrity for a single message?
We can model one-time MACs using the standard MAC Attack Game 6.1 used to deﬁne MAC
security. To capture the one-time nature of the MAC we allow the adversary to issue only one
signing query. We denote the adversary’s advantage in this restricted game by MAC 1adv[A,I].
This game captures the fact that the adversary sees only one message-tag pair and then tries to
create an existential forgery using this pair.
Unconditional security means that MAC1adv[A,I] is negligible for all adversaries A, even com-
putationally unbounded ones. In this section, we show how to implement eﬃcient and uncondi-
tionally secure one-time MACs using hash functions.
7.6.1 Pairwise unpredictable functions
Let H be a keyed hash function deﬁned over ( K,M,T). Intuitively, H is a pairwise unpre-
dictable function if the following holds for a randomly chosen key k ∈ K: given the value
H(k,m0), it is hard to predict H(k,m1) for any m1 ̸= m0. As usual, we make this deﬁnition
rigorous using an attack game.
Attack Game 7.5 (pairwise unpredicability). For a keyed hash function H deﬁned over
(K,M,T), and a given adversary A, the attack game runs as follows.
• The challenger picks a random k←R Kand keeps k to itself.
• Asends a message m0 ∈M to the challenger, who responds with t0 = H(k,m0).
• Aoutputs (m1,t1) ∈M×T , where m1 ̸= m0.
We say thatAwins the game if t1 = H(k,m1). We deﬁne A’s advantage with respect toH, denoted
PUFadv[A,H], as the probability that Awins the game. 2
Deﬁnition 7.7. We say that H is an ϵ-bounded pairwise unpredictable function, or ϵ-PUF
for short, if PUFadv[A,H] ≤ϵ for all adversaries A(even ineﬃcient ones).
It should be clear that if H is an ϵ-PUF, then H is also an ϵ-UHF; if, in addition, T is of the
form ZN (or is an abelian group as in Remark 7.3), then H is an ϵ-DUF.
7.6.2 Building unpredictable functions
So far we know that any ϵ-PUF is also an ϵ-DUF. The converse is not true (see Exercise 7.29).
Nevertheless, we show that any ϵ-DUF can be tweaked so that it becomes an ϵ-PUF. This tweak
increases the key size.
Let H be a keyed hash function deﬁned over (K,M,T), where T = ZN for some N. We build a
new hash function H′derived from H with the same input and output space as H. The key space,
275
however, is K×T . The function H′is deﬁned as follows:
H′(
(k1,k2), m
)
= H(k1,m) + k2 ∈T (7.32)
Lemma 7.11. If H is an ϵ-DUF, then H′ is an ϵ-PUF.
Proof. Let A attack H′ as a PUF. In response to its query m0, adversary A receives t0 :=
H(k1,m0) + k2. Observe that t0 is uniformly distributed over T, and is independent of k1. More-
over, if A’s prediction t1 of H(k1,m1) + k2 is correct, then t1 −t0 correctly predicts the diﬀerence
H(k1,m1) −H(k1,m0).
So we can deﬁne a DUF adversary Bas follows: it runs A, and when Asubmits its query m0,
Bresponds with a random t0 ∈T ; when Aoutputs (m1,t1), adversary Boutputs (m0,m1,t1 −t0).
It is clear that
PUFadv[A,H′] ≤DUFadv[B,H] ≤ϵ. 2
Lemma 7.11 shows how to convert the function Hxpoly, deﬁned in (7.23), into an (ℓ+1)/p-PUF.
We obtain the following keyed hash function deﬁned over ( Z2
p,Z≤ℓ
p ,Zp):
H′
xpoly
(
(k1,k2),(a1,...,a v)
):= kv+1
1 + a1kv
1 + ··· + avk1 + k2 (7.33)
for 0 ≤v≤ℓ. This function is an ( ℓ+ 1)/p-PUF.
7.6.3 From PUFs to unconditionally secure one-time MACs
We now return to the problem of building unconditionally secure one-time MACs. In fact, PUFs
are just the right tool for the job.
Let H be a keyed hash function deﬁned over ( K,M,T). We can use H to deﬁne the MAC
system I= (S,V ) derived from H:
S(k, m) := H(k, m);
V(k, m, t) :=
{
accept if H(k,m) = t,
reject otherwise.
The following theorem shows that PUFs are the MAC analogue of the one-time pad, since both
provide unconditional security for one time use. The proof is immediate from the deﬁnitions.
Theorem 7.12. Let H be an ϵ-PUF and let Ibe the MAC system derived from H. Then for all
adversaries A(even ineﬃcient ones), we have MAC1adv[A,I] ≤ϵ.
The PUF construction in Section 7.6.2 is very similar to the Carter-Wegman MAC. The only
diﬀerence is that the PRF is replaced by a truly random pad k2. Hence, Theorem 7.12 shows that
the Carter-Wegman MAC with a truly random pad is an unconditionally secure one-time MAC.
7.7 A fun application: timing attacks
To be written.
276
H(k1,·)
F(k2,·)
m
r←
R
R
v
r
Figure 7.5: Randomized PRF(UHF) composition: MAC signing
7.8 Notes
Citations to the literature to be added.
7.9 Exercises
7.1 (Using Hpoly with power-of-2 modulus). We can adapt the deﬁnition of Hpoly in (7.3)
so that instead of working in Zp we work in Z2n (i.e., work modulo 2 n). Show that this version
of Hpoly is not a good UHF, and in particular an attacker can ﬁnd two messages m0,m1 each of
length two blocks that are guaranteed to collide.
7.2 (Non-adaptively secure PRFs are computational UHFs). Show that if F is a secure
PRF against non-adaptive adversaries (see Exercise 4.6), and the size of the output space of F is
super-poly, then F is a computational UHF.
Note: Using the result of Exercise 6.13, this gives another proof that CBC is a computational
UHF.
7.3 (On the alternative characterization of the ϵ-UHF property). Let H be a keyed hash
function deﬁned over ( K,M,T). Suppose that for some pair of distinct messages m0 and m1, we
have Pr[H(k,m0) = H(k,m1)] >ϵ, where the probability is over the random choice of k∈K. Give
an adversary Athat wins Attack Game 7.1 with probability greater than ϵ. Your adversary is not
allowed to just have the values m0 and m1 “hardwired” into its code, but it may be very ineﬃcient.
7.4 (MAC(UHF) composition is insecure). The PRF(UHF) composition shows that a UHF
can extend the input domain of a speciﬁc type of MAC, namely a MAC that is itself a PRF. Show
that this construction cannot be extended to arbitrary MACs. That is, exhibit a secure MAC
I = ( S,V ) and a computational UHF H for which the MAC(UHF) composition I′ = ( S′,V ′)
where S′((k1,k2),m) = S(k2,H(k1,m)) is insecure. In your design, you may assume the existence
of a secure PRF deﬁned over any convenient spaces. Then show how to “sabotage” this PRF so
that it remains a secure MAC, but the MAC(UHF) composition becomes insecure.
7.5 (Randomized PRF(UHF) composition). In this exercise we develop a randomized variant
of PRF(UHF) composition that provides better security with little impact on the running time. Let
H be a keyed hash function deﬁned over ( KH,M,X) and let F be a PRF deﬁned over ( KF, R×
277
X, T). Deﬁne the randomized PRF(UHF) system I= (S,V ) as follows: for key ( k1,k2) and
message m∈M deﬁne
S
(
(k1,k2), m
):=
{
r←R R, x←H(k1,m), v←F
(
k2,(r,x)
)
, output (r,v)
}
(see Fig. 7.5)
V
(
(k1,k2), m, (r,v)
):=
{
accept if x←H(k1,m), v= F
(
k2,(r,x)
)
reject otherwise.
This MAC is deﬁned over (KF×KH, M, R×T). The tag size is a little larger than in deterministic
PRF(UHF) composition, but signing and veriﬁcation time is about the same.
(a) Suppose Ais a MAC adversary that plays Attack Game 6.1 with respect to Iand issues
at most Q queries. Show that there exists a PRF adversary BF and UHF adversaries BH
and B′
H, which are elementary wrappers around A, such that
MACadv[A,I] ≤PRFadv[BF,F] + UHFadv[BH,H] + Q2
2|R|UHFadv[B′
H,H]
+ Q2
2|R||T| + 1
|T|.
(7.34)
Discussion: When H is an ϵ-UHF let us set ϵ = 1/|T| and |R|= Q2/2 so that the right
most four terms in (7.34) are all equal. Then (7.34) becomes simply
MACadv[A,I] ≤PRFadv[BF,F] + 4ϵ. (7.35)
Comparing to deterministic PRF(UHF) composition, the error term ϵ·Q2/2 in (7.19) is
far worse than in (7.35). This means that for the same parameters, randomized PRF(UHF)
composition security is preserved for far many more queries than for deterministic PRF(UHF)
composition.
In the Carter-Wegman MAC to get an error bound as in (7.35) we must set |R|to |Q|2/ϵ in
(7.24). In randomized PRF(UHF) composition we only need |R|= |Q|2 and therefore tags
in randomized PRF(UHF) are shorter than in Carter-Wegman for the same security and the
same ϵ.
(b) Rephrase the MAC system Ias a nonce-based MAC system (as in Section 7.5). What are
the concrete security bounds for this system?
Observe that if the nonce is accidentally re-used, or even always set to the same value, then the
MAC system Istill provides some security: security degrades to the security of deterministic
PRF(UHF) composition. We refer to this as nonce re-use resistance.
7.6 (One-key PRF(UHF) composition). This exercise analyzes a one-key variant of the
PRF(UHF) construction. Let F be a PRF deﬁned over ( K,X,Y) and let H be a keyed hash
function deﬁned over ( Y,M,X); in particular, the output space of F is equal to the key space of
H, and the output space of H is equal to the input space of F. Let x0 ∈X be a public constant.
Consider the PRF F′deﬁned over (K,M,Y) as follows:
F′(k,m) := F(k,H(k0,m)), where k0 := F(k,x0).
This is the same as the usual PRF(UHF) composition, except that we use a single key k and use
F to derive the key k0 for H.
278
(a) Show that F′ is a secure PRF assuming that F is a PRF, that H is a computational UHF,
and that H satisﬁes a certain preimage resistance property, deﬁned by the following game.
In this game, the adversary computes a messageM and the challenger (independently) chooses
a random hash key k0 ∈K. The adversary wins the game if H(k0,M) = x0, where x0 ∈X
is a constant, as above. We say that H is preimage resistant if every eﬃcient adversary wins
this game with only negligible probability.
Hint: Modify the proof of Theorem 7.7.
(b) Show that the cascade construction is preimage resistant, assuming the underlying PRF is a
secure PRF.
Hint: This follows almost immediately from the fact that the cascade is a preﬁx-free PRF.
7.7 (XOR-DUFs). In Remark 7.3 we adapted the deﬁnition of DUF to a hash function whose
digest space T is the set of all n-bit strings, {0,1}n, with the XOR used as the diﬀerence operator.
(a) Show that the XOR-hash F⊕deﬁned in Section 7.2.3 is a computational XOR-DUF.
(b) Show that the CBC construction FCBC deﬁned in Section 6.4.1 is a computational XOR-DUF.
Hint: Use the fact that FCBC is a preﬁx-free secure PRF (or, alternatively, the result of
Exercise 6.13).
7.8 (Luby-Rackoﬀ with an XOR-DUF). Show that the Luby-Rackoﬀ construction (see Sec-
tion 4.5) remains secure if the ﬁrst round function F(k1,·) is replaced by a computational XOR-
DUF.
7.9 (Nonce-based CBC cipher with an XOR-DUF). Show that in the nonce-based CBC
cipher (Section 5.5.3) the PRF that is applied to the nonce can be replaced by an XOR-DUF.
7.10 (Tweakable block ciphers). Continuing with Exercise 4.11, show that in the construc-
tion from part (c) the PRF can be replaced by an XOR-DUF. That is, prove that the following
construction is a strongly secure tweakable block cipher:
E′(
(k0,k1),m,t
):=
{
p←h(k0,t); output p⊕E(k1, m⊕p)
}
D′(
(k0,k1),c,t
):=
{
p←h(k0,t); output p⊕D(k1, c⊕p)
}
Here (E,D) is a strongly secure block cipher deﬁned over ( K0,X) and h is an XOR-DUF deﬁned
over (K1,T,X) where X:= {0,1}n.
Discussion: XTS mode, used in disk encryption systems, is based on this tweakable block cipher.
The tweak in XTS is a combination of i, the disk sector number, and j, the position of the block
within the sector. The XOR-DUF used in XTS is deﬁned as h
(
k0,(i,j)
):= E(k0,i) ·αj ∈GF(2n)
where α is a ﬁxed primitive element of GF(2 n). XTS uses ciphertext stealing (Exercise 5.16) to
handle sectors whose bit length is not a multiple of n.
7.11 (Carter-Wegman with veriﬁcation queries: concrete security). Consider the security
of the Carter-Wegman construction (Section 7.4) in an attack with veriﬁcation queries (Section 6.2).
Show that following concrete security result: for every MAC adversary Athat attacks ICW as in
Attack Game 6.2, and which makes at most Qv veriﬁcation queries and at most Qs signing queries,
279
there exist a PRF adversary BF and a DUF adversary BH, which are elementary wrappers around
A, such that
MACvqadv[A,ICW] ≤PRFadv[BF,F] + Qv ·DUFadv[BH,H] + Q2
s
2|R|+ Qv
|T|.
7.12 (Nonce-based Carter-Wegman: improved security bounds).In Section 7.5, we studied
a nonce-based version of the Carter-Wegman MAC. In particular, in Theorem 7.10, we derived the
security bound
nMACadv[A,ICW] ≤PRFadv[BF,F] + DUFadv[BH,H] + 1
|T|,
and rejoiced in the fact that there were no Q2-terms in this bound, where Q is a bound on the
number of signing queries. Unfortunately, a common implementation of F is to use the encryption
function of a block cipher E deﬁned over ( K,X), so R = X = T = ZN. A straightforward
application of the PRF switching lemma (see Theorem 4.4) gives us the security bound
nMACadv[A,ICW] ≤BCadv[BE,E] + Q2
2N + DUFadv[BH,H] + 1
N,
and a Q2-term has returned! In particular, when Q2 ≈N, this bound is entirely useless. However,
one can obtain a better bound. Using the result of Exercise 4.25, show that assuming Q2 <N , we
have the following security bound:
nMACadv[A,ICW] ≤BCadv[BE,E] + 2·
(
DUFadv[BH,H] + 1
N
)
.
7.13 (Carter-Wegman MAC falls apart under nonce re-use). Suppose that when using a
nonce-based MAC, an implementation error causes the system to re-use a nonce more than once.
Let us show that the nonce-based Carter-Wegman MAC falls apart if this ever happens.
(a) Consider the nonce-based Carter-Wegman MAC built from the hash function Hxpoly. Show
that if the adversary obtains the tag on some one-block messagem1 using nonce N and the tag
on a diﬀerent one-block message m2 using the same nonce N , then the MAC system becomes
insecure: the adversary can forge the MAC on any message of its choice with non-negligible
probability.
(b) Consider the nonce-based Carter-Wegman MAC with an arbitrary hash function. Suppose
that an adversary is free to re-use nonces at will. Show how to create an existential forgery.
Note: These attacks also apply to the randomized version of Carter-Wegman, if the signer is
unlucky enough to generate the same randomizer r ∈R more than once. Also, the attack in part
(a) can be extended to work even if the messages are not single-block messages by using eﬃcient
algorithms for ﬁnding roots of polynomials over ﬁnite ﬁelds.
7.14 (Encrypted Carter-Wegman). Continuing with the previous exercise, we show how to
make Carter-Wegman resistant to nonce re-use by encrypting the tag. To make things more con-
crete, suppose that H is an ϵ-DUF deﬁned over ( KH,M,X), where X= ZN, and E= (E,D) is a
secure block cipher deﬁned over (KE,X). The encrypted Carter-Wegman nonce-based MAC system
I= (S,V ) has key space KH ×K2
E, message space M, tag space X, nonce space X, and is deﬁned
as follows:
280
• For key (k1,k2,k3), message m, and nonce N , we deﬁne
S((k1,k2,k3),m, N ) := E(k3, H(k1,m) + E(k2,N ) )
• For key (k1,k2,k3), message m, tag v, and nonce N , we deﬁne
V((k1,k2,k3),m,v, N ) :=
v∗←E(k3, H(k1,m) + E(k2,N ) )
if v= v∗output accept; otherwise output reject
(a) Show that assuming no nonces get re-used, this scheme is just as secure as Carter-Wegman.
In particular, using the result of Exercise 7.12, show that for every adversary Athat makes
at most Q signing queries, where Q2 < N, the probability that Aproduces an existential
forgery is at most BC adv[B,E] + 2(ϵ+ 1/N), where Bis an elementary wrapper around A.
(b) Now suppose an adversary can re-use nonces at will. Show that for every such adversary
Athat makes at most Q signing queries, where Q2 < N, the probability that Aproduces
an existential forgery is at most BC adv[B,E] + (Q+ 1)2ϵ+ 2/N, where Bis an elementary
wrapper around A. Thus, while nonce re-use degrades security, it is not catastrophic.
Hint: Theorem 7.7 and Exercises 4.25 and 7.21 may be helpful.
7.15 (Composing UHFs). Let H1 be a keyed hash function deﬁned over ( K1,X,Y). Let H2
be a keyed hash function deﬁned over ( K2,Y,Z). Let H be the keyed hash function deﬁned over
(K1 ×K2,X,Z) as H((k1,k2),x) := H2(k2,H(k1,x)).
(a) Show that if H1 is an ϵ1-UHF and H2 is an ϵ2-UHF, then H is an (ϵ1 + ϵ2)-UHF.
(b) Show that if H1 is an ϵ1-UHF and H2 is an ϵ2-DUF, then H is an (ϵ1 + ϵ2)-DUF.
7.16 (Variations on Hpoly). Show that if p is prime and the input space is Zℓ
p for some ﬁxed
(poly-bounded) value ℓ, then
(a) the function Hfpoly deﬁned in (7.5) is an ( ℓ−1)/p-UHF.
(b) the function Hfxpoly deﬁned as
Hfxpoly(k,(a1,...,a ℓ)) := k·Hfpoly(k,(a1,...,a ℓ)) = a1kℓ + a2kv−1 + ··· + aℓk∈Zp
is an (ℓ/p)-DUF.
7.17 (A DUF from an ideal permutation). Let π : X →Xbe an permutation where X:=
{0,1}n. Deﬁne H : X×X ≤ℓ →X as the following keyed hash function:
H(k,(a1,...,a v)) := h←k
for i←1 to v do: h←π(ai ⊕h)
output h
Assuming 2n is super-poly, show that H is a computational XOR-DUF (see Remark 7.3) in the
ideal permutation model, where we model π as a random permutation Π (see Section 4.7).
We outline here one possible proof approach. The ﬁrst idea is to use the same strategy that was used
in the analysis of CBC in the proof of Theorem 6.3; indeed, one can see that the two constructions
281
process message blocks in a very similar way. The second idea is to use the Domain Separation
Lemma (Theorem 4.15) to streamline the proof.
Consider two games:
0. The original attack game: adversary makes a series of ideal permutation queries, which
evaluate Π and Π −1 on points of the adversary’s choice. Then the adversary submits two
distinct messages m0,m1 to the challenger, along with a value δ, and hopes that H(k,m0) ⊕
H(k,m1) = δ.
1. Use the Domain Separation Lemma to split Π into many independent permutations. One
is Πip, which is used to evaluate the ideal permutation queries. The others are of the form
Πstd,α for α ∈X ≤ℓ
>0 . These are used to perform the evaluations H(k,m0), H(k,m1): in the
evaluation of H(k,(a1,...,a s)), in the ith loop iteration in the hash algorithm, we use the
permutation Πstd,α, where α = (a1,...,a i). Now one just has to analyze the probability of
separation failure.
Note that His certainly not a secure PRF, even if we restrict ourselves to non-adaptive or preﬁx-free
adversaries: given H(k,m) for any message m, we can eﬃciently compute the key k.
7.18 (Optimal collision probability with shorter hash keys). For positive integer d, let
Id := {0,...,d −1}and I∗
d := {1,...,d −1}.
(a) Let p be a prime, and let N < pbe a positive integer. Consider the keyed hash function H
deﬁned over (Ip ×I∗
p, Ip, IN) as follows: H((k0,k1),a) := ((k0 + ak1) mod p) mod N. Show
that H is a 1/N-UHF.
(b) While the construction in part (a) gives a UHF with “optimal” collision probability, the key
space is unfortunately larger than the message space. Using the result of part (a), along with
part (a) of Exercise 7.15, and the result of Exercise 7.16, you are to design a hash function
with “nearly optimal” collision probability, but with much smaller keys.
In particular, let N and ℓ be positive integers. Let α be a number with 0 <α< 1. Design a
(1 + α)/N-UHF with message space {0,1}ℓ and output space IN, where keys are bit strings
of length O(log(Nℓ/α)).
7.19 (Inner product hash). Let p be a prime.
(a) Consider the keyed hash function H deﬁned over (Zℓ
p,Zℓ
p,Zp) as follows:
H((k1,...,k ℓ),(a1,...,a ℓ)) := a1k1 + ··· + aℓkℓ.
Show that H is a 1/p-DUF.
(b) Since multiplications can be much more expensive than additions, the following variant of the
hash function in part (a) is sometimes preferable. Assume ℓ is even, and consider the keyed
hash function H′deﬁned over (Zℓ
p,Zℓ
p,Zp) as follows:
H′((k1,...,k ℓ),(a1,...,a ℓ)) :=
ℓ/2∑
i=1
(a2i−1 + k2i−1)(a2i + k2i).
Show that H′is also a 1 /p-DUF.
282
(c) Although both H and H′are ϵ-DUFs with “optimal” ϵvalues, the keys are unfortunately very
large. Using a similar approach to part (b) of the previous exercise, design a (1 + α)/p-DUF
with message space {0,1}ℓ and output space Zp, where keys bit strings of lengthO(log(pℓ/α)).
7.20 (Division-free hash). This exercise develops a hash function that does not require any
division or mod operations, which can be expensive. It can be implemented just using shifts and
adds. For positive integer d, let Id := {0,...,d −1}. Let n be a positive integer and set N := 2n.
(a) Consider the keyed hash function H deﬁned over (Iℓ
N2,Iℓ
N,ZN) as follows:
H((k1,...,k ℓ),(a1,...,a ℓ)) := [t]N ∈ZN, where t:=
⌊((∑
i
aiki
)
mod N2 )/
N
⌋
.
Show that H is a 2 /N-DUF. Below in Exercise 7.31 we will see a minor variant of H that
satisﬁes a stronger property, and in particular, is a 1 /N-DUF.
(b) Analogous to part (b) in the previous exercise, assume ℓis even, and consider the keyed hash
function H deﬁned over (Iℓ
N2,Iℓ
N,ZN) as follows:
H′((k1,...,k ℓ),(a1,...,a ℓ)) := [t]N ∈ZN,
where
t:=
⌊(( ℓ/2∑
i=1
(a2i−1 + k2i−1)(a2i + k2i)
)
mod N2 )/
N
⌋
.
Show that H′is a 2/N-DUF.
7.21 (DUF to UHF conversion). Let H be a keyed hash function deﬁned over (K,M,ZN). We
construct a new keyed hash function H′, deﬁned over (K,M×ZN,ZN) as follows: H′(k,(m,x)) :=
H(k,m) + x. Show that if H is an ϵ-DUF, then H′is an ϵ-UHF.
7.22 (DUF modulus switching). We will be working with DUFs with digest spaces Zm for
various m, and so to make things clearer, we will work with digest spaces that are plain old sets of
integers, and state explicitly the modulus m, as in “an ϵ-DUF modulo m”. For positive integer d,
let Id := {0,...,d −1}.
Let p and N be integers greater than 1. Let H be a keyed hash function deﬁned over ( K,M,Ip).
Let H′be the keyed hash function deﬁned over (K,M,IN) as follows: H′(k,m) := H(k,m) mod N.
(a) Show that if p≤N/2 and H is an ϵ-DUF modulo p, then H′is an ϵ-DUF modulo N.
(b) Suppose that p ≥N and H is an ϵ-DUF modulo p. Show that H′ is an ϵ′-DUF modulo N
for ϵ′= 2(p/N+ 1)ϵ. In particular, if ϵ= α/p, we can take ϵ′= 4α/N.
7.23 (More ﬂexible output spaces). As in the previous exercise, we work with DUFs whose
digest spaces are plain old sets of integers, but we explicitly state the modulus m. Again, for
positive integer d, we let Id := {0,...,d −1}.
Let 1 <N ≤p, where p is prime.
(a) H∗
fxpoly is the keyed hash function deﬁned over ( Ip,Iℓ
N,IN) as follows:
H∗
fxpoly(k,(a1,...,a ℓ)) :=
(
(a1kℓ + ··· + aℓk
)
mod p
)
mod N.
283
Show that H∗
fxpoly is a 4ℓ/N-DUF modulo N.
(b) H∗
xpoly is the keyed hash function deﬁned over ( Ip,I≤ℓ
N ,IN) as follows:
H∗
xpoly(k,(a1,...,a v)) :=
(
(kv+1 + a1kv + ··· + avk
)
mod p
)
mod N.
Show that H∗
xpoly is a 4(ℓ+ 1)/N-DUF modulo N.
(c) H∗
fpoly is the keyed hash function deﬁned over ( Ip,Iℓ
N,IN) as follows:
H∗
fpoly(k,(a1,...,a ℓ)) :=
((
(a1kℓ−1 + ··· + aℓ−1k
)
mod p
)
+ aℓ
)
mod N.
Show that H∗
fpoly is a 4(ℓ−1)/N-UHF.
(d) H∗
poly is the keyed hash function is deﬁned over ( Ip,I≤ℓ
N ,IN) as follows:
H∗
poly(k,(a1,...,a v)) :=
((
(kv + a1kv−1 + ··· + av−1k
)
mod p
)
+ av
)
mod N.
for v >0, and for zero-length messages, it is deﬁned to be the constant 0. Show that H∗
poly
is a 4ℓ/N-UHF.
Hint: All of these results follow easily from the previous two exercises, except that the analysis in
part (d) requires that zero-length messages are treated separately.
7.24 (Be careful: reducing at the wrong time can be dangerous). With notation as in
the previous exercise, show that if (3 /2)N ≤p <2N, the keyed hash function H deﬁned over
(Ip,I2
N,IN) as
H(k,(a,b)) := ((ak+ b) mod p) mod N
is not a (1/3)-UHF. Contrast this function with that in part (c) of the previous exercise with ℓ= 2.
7.25 (A PMAC0 alternative). Again, for positive integer d, let Id := {0,...,d −1}. Let N = 2n
and let p be a prime with N/4 < p < N/2. Let H be the hash function deﬁned over ( IN/4,IN ×
IN/4,IN) as follows:
H(k,(a,i)) := (((i·k) mod p) + a) mod N.
(a) Show that H is a 4/N-UHF.
Hint: Use Exercise 7.21 and part (a) of Exercise 7.22.
(b) Show how to use H to modify PMAC0 so that the message space is Y≤ℓ (where Y= {0,1}n
and ℓ < N/4), and the PRF F1 is deﬁned over ( K1,Y,Y). Analyze the security of your
construction, giving a concrete security bound.
7.26 (Collision lower-bounds for Hpoly). Consider the function Hpoly(k,m) deﬁned in (7.3)
using a prime p and assume ℓ= 2.
(a) Show that for all suﬃciently large p, the following holds: for any ﬁxed k ∈ Zp, among
⌊√p⌋random inputs to Hpoly(k,·), the probability of a collision is bounded from below by a
constant.
Hint: Use the birthday paradox (Appendix B.1).
284
(b) Show that given any collision for Hpoly under key k, we can eﬃciently compute k. That is,
give an eﬃcient algorithm that takes two inputs m,m′∈Z2
p, and that outputs ˆk ∈Zp, and
satisﬁes the following property: for every k∈Zp, if H(k,m) = H(k,m′), then ˆk= k.
7.27 ( Hpoly key collisions). Consider the function Hpoly(k,m) deﬁned in (7.3), where k ∈Zp
and m ∈Zℓ
p. Show that there is an eﬃcient algorithm that takes t,k1,...,k ℓ ∈Zp as input, and
outputs a message m ∈Zℓ
p such that t = E(k1,m) = ··· = E(kℓ,m). This message maps to the
same output t under ℓ diﬀerent keys. This property of Hpoly has been used in some attacks [102].
7.28 (XOR-hash analysis). Generalize Theorem 7.6 to show that for every Q-query UHF ad-
versary A, there exists a PRF adversary B, which is an elementary wrapper around A, such that
MUHFadv[A,F⊕] ≤PRFadv[B,F] + Q2
2|Y|.
Moreover, Bmakes at most Qℓ queries to F.
7.29 ( Hxpoly is not a good PUF). Show that Hxpoly deﬁned in (7.23) is not a good PUF by
exhibiting an adversary that wins Attack Game 7.5 with probability 1.
7.30 (Converting a one-time MAC to a MAC). Suppose I= (S,V ) is a (possibly random-
ized) MAC deﬁned over ( K1,M,T), where T = {0,1}n, that is one-time secure (see Section 7.6).
Further suppose that F is a secure PRF deﬁned over (K2,R,T), where |R|is super-poly. Consider
the MAC I′= (S′,V ′) deﬁned over (K1 ×K2,M,R×T ) as follows:
S′((k1,k2),m) :=
{
r←R R; t←R S(k1,m); t′←F(k2,r) ⊕t; output ( r,t′)
}
V′((k1,k2),m, (r,t′)) :=
{
t←F(k2,r) ⊕t′; output V(k1,m,t )
}
Show that I′is a secure (many time) MAC.
7.31 (Pairwise independent functions). In this exercise, we develop the notion of a PRF that
is unconditionally secure, provided the adversary can make at most two queries. We say that a
PRF F deﬁned over (K,X,Y) is an ϵ-almost pairwise independent function, or ϵ-APIF, if the
following holds: for all adversaries A(even ineﬃcient ones) that make at most 2 queries in Attack
Game 4.2, we have PRFadv[A,F] ≤ϵ. If ϵ= 0, we call F a pairwise independent function, or
PIF.
(a) Show that any ϵ-APIF is an ( ϵ+ 1/|Y|)-PUF.
Discussion: This shows that every PIF is also a PUF. The converse is false: there is an
ϵ-PUF that is not an ϵ′-APIF even for ϵ′= 1/2.
(b) Suppose that |X|>1 and that for all x0,x1 ∈X with x0 ̸= x1, and all y0,y1 ∈Y, we have
Pr
[
F(k,x0) = y0 and F(k,x1) = y1
]
= 1
|Y|2 ,
where the probability is over the random choice of k∈K. Show that F is a PIF.
(c) Consider the function H′built from H in (7.32). Show that if H is a (1/N)-DUF, then H′is
a PIF.
285
(d) For positive integer d, let Id := {0,...,d −1}. Let n be a positive integer and set N := 2n.
Consider the keyed hash function H deﬁned over (Iℓ+1
N2 ,Iℓ
N,IN) as follows:
H((k0,k1,...,k ℓ),(a1,...,a ℓ)) :=
⌊((
k0 +
∑
i
aiki
)
mod N2 )/
N
⌋
.
Show that H is a PIF. Note: on a typical computer, if n is not too large, this can be
implemented very easily with just integer multiplications, additions, and shifts.
(e) Show that in the PRF(UHF) composition, if H is an ϵ1-UHF and F is an ϵ2-APIF, then the
composition F′is an (ϵ1 + ϵ2)-APIF.
7.32 (Unconditionally secure two time encryption). Let F be a PRF deﬁned over (K,X,Y)
where X= {0,1}n and |X|is super-poly. Suppose that F is an ϵ-APIF for a negligible ϵ, as deﬁned
in the previous exercise. Consider the following cipher E= (E,D) deﬁned over (K,X,Y) where
E(k,m) :=
{
r←R X, output c←
(
r, F(k,r) ⊕m
)}
D
(
k,(c0,c1)
):= c1 ⊕F(k,c0)
Show that this cipher is unconditionally CPA secure provided the adversary can make at most
two queries in Attack Game 5.2. Speciﬁcally, CPA adv[A,E] ≤2(ϵ+ (1/|X|)) for all 2-query adver-
saries A, even ineﬃcient ones.
Discussion: Recall that the one time pad cipher is unconditionally secure against a single query
CPA adversary. The construction here is a generalization of the one time pad that is unconditionally
secure against a two query CPA adversary. One can further generalize this: for every d≥1, there
is a cipher Ed = (Ed,Dd) that is unconditionally secure against a d-query CPA adversary. The size
of the key used by Ed grows linearly with d.
286
Chapter 8
Message integrity from collision
resistant hashing
In the previous chapter we discussed universal hash functions (UHFs) and showed how they can be
used to construct MACs. Recall that UHFs are keyed hash functions for which ﬁnding collisions is
diﬃcult, as long as the key is kept secret.
In this chapter we studykeyless hash functions for which ﬁnding collisions is diﬃcult. Informally,
a keyless function is an eﬃciently computable function whose description is fully public. There are
no secret keys and anyone can evaluate the function. Let H be a keyless hash function from some
large message space Minto a small digest space T. As in the previous chapter, we say that two
messages m0,m1 ∈M are a collision for the function H if
H(m0) = H(m1) and m0 ̸= m1.
Informally, we say that the function H is collision resistant if ﬁnding a collision for H is diﬃcult.
Since the digest space T is much smaller than M, we know that many such collisions exist. Nev-
ertheless, if H is collision resistant, actually ﬁnding a pair m0,m1 that collide should be diﬃcult.
We give a precise deﬁnition in the next section.
In this chapter we will construct collision resistant functions and present several applications.
To give an example of a collision resistant function we mention a US federal standard called the
Secure Hash Algorithm Standard or SHA for short. The SHA standard describes a number of hash
functions that oﬀer varying degrees of collision resistance. For example, SHA256 is a function
that hashes long messages into 256-bit digests. It is believed that ﬁnding collisions for SHA256 is
diﬃcult.
Collision resistant hash functions have many applications. We brieﬂy mention two such appli-
cations here and give the details later on in the chapter. Many other applications are described
throughout the book.
Extending the domain of cryptographic primitives. An important application for collision
resistance is its ability to extend primitives built for short inputs to primitives for much longer
inputs. We give a MAC construction as an example. Suppose we are given a MAC systemI= (S,V )
that only authenticates short messages, say messages that are 256 bits long. We want to extend
the domain of the MAC so that it can authenticate much longer inputs. Collision resistant hashing
gives a very simple solution. To compute a MAC for some long message m we ﬁrst hash m and
287
H Sm
k
t
Figure 8.1: Hash-then-MAC construction
then apply S to the resulting short digest, as described in Fig. 8.1. In other words, we deﬁne a new
MAC system I= (S′,V ′) where S′(k, m) := S(k, H(m)). MAC veriﬁcation works analogously by
ﬁrst hashing the message and then verifying the tag of the digest.
Clearly this hash-then-MAC construction would be insecure if it were easy to ﬁnd collisions
for H. If an adversary could ﬁnd two long messages m0 and m1 such that H(m0) = H(m1) then
he could forge tags using a chosen message attack. Suppose m0 is an innocuous message while m1
is evil, say a virus infected program. The adversary would ask for the tag on the message m0 and
obtain a tag t in response. Then the pair ( m0,t) is a valid message-tag pair, but so is the pair
(m1,t). Hence, the adversary is able to forge a tag for m1, which breaks the MAC. Even worse,
the valid tag may fool a user into running the virus. This argument shows that collision resistance
is necessary for this hash-then-MAC construction to be secure. Later on in the chapter we prove
that collision resistance is, in fact, suﬃcient to prove security.
The hash-then-MAC construction looks similar to the PRF(UHF) composition discussed in the
previous chapter (Section 7.3). These two methods build similar looking MACs from very diﬀerent
building blocks. The main diﬀerence is that a collision resistant hash can extend the input domain
of any MAC. On the other hand, a UHF can only extend the domain of a very speciﬁc type of MAC,
namely a PRF. This is illustrated further in Exercise 7.4. Another diﬀerence is that the secret key
in the hash-then-MAC method is exactly the same as in the underlying MAC. The PRF(UHF)
method, in contrast, extends the secret key of the underlying PRF by adding a UHF secret key.
The hash-then-MAC construction performs better than PRF(UHF) when we wish to compute
the tag for a single messagemunder multiple keys k1,...,k n. That is, we wish to compute S′(ki,m)
for all i= 1,...,n . This comes up, for example, when providing integrity for ﬁles on a disk that is
readable by multiple users. Each ﬁle header contains one integrity tag per user, so that each user
can verify integrity using its own MAC key. With the hash-then-MAC construction it suﬃces to
compute H(m) once and then quickly derive the n tags from this single hash. With a PRF(UHF)
MAC, the UHF depends on the key ki and consequently we will need to rehash the entire ﬁle n
times, once for each user. See also Exercise 6.4 for more on this problem.
File integrity. Collision resistance is frequently used for ﬁle integrity. Consider a set of ncritical
ﬁles that change infrequently, such as a set of executables on disk. We want a method to verify
that these ﬁles have not been modiﬁed by some malicious code or malware. To do so we need a
small amount of read-only memory, namely memory that the malware can read, but cannot modify.
Read-only memory can be implemented, for example, using a small USB disk that has a physical
switch ﬂipped to the “read-only” position. We place a hash of each of the n critical ﬁles in the
288
H(F1)
H(F2)
H(F3)
H(FH)
File F1
File F2
File F3
hash ﬁle FH
Disk Read-only memory
Figure 8.2: File integrity using small read-only memory
read-only memory so that this storage area only contains n short hashes. Now, we can check
integrity of a ﬁle F by hashing F and comparing the resulting hash to the one stored in read-only
memory. If a mismatch is found, the system declares that ﬁle F is corrupt. Operating systems
use this mechanism to ensure integrity of operating system ﬁles during the boot process. Package
managers also use this mechanism: a user who downloads a package from a remote server can verify
that the package is authentic by hashing it, and verifying that the hash value is equal to the value
stored on a trusted read-only site.
What property should the hash function H satisfy for this integrity mechanism to be secure?
Let F be a ﬁle protected by this system. Since the malware cannot alter the contents of the read-
only storage, its only avenue for modifying F without being detected is to ﬁnd another ﬁle F′such
that H(F) = H(F′). Replacing F by F′ would not be caught by this hashing system. However,
ﬁnding such an F′will be diﬃcult if H is collision resistant. Collision resistance, thus, implies that
the malware cannot change F without being detected by the hash.
This system stores all ﬁle hashes in read-only memory. The amount of read-only memory needed
could become quite large when there are many ﬁles to protect. We can reduce the size of read-only
memory by treating the entire list of hashes as just another ﬁle stored on disk and denoted FH.
We store the hash of FH in read-only memory, as described in Fig. 8.2, so that now read-only
memory contains just a single hash value. To verify the integrity of some ﬁle F, we ﬁrst verify
integrity of the ﬁle FH by hashing the contents of FH and comparing the result to the value in
read-only memory. Then we verify integrity of F by hashing F and comparing the result with the
corresponding hash stored in FH. In Section 8.9 we will see a more eﬃcient solution using a data
structure called a Merkle tree.
In the introduction to Chapter 6 we proposed a MAC-based ﬁle integrity system. The system
stored a tag of every ﬁle along with the ﬁle. We also needed a small amount ofsecret storage to store
the user’s secret MAC key. This key was used every time ﬁle integrity was veriﬁed. In comparison,
when using collision resistant hashing there are no secrets and there is no need for secret storage.
Instead, we need a small amount of read-only storage for storing ﬁle hashes. Generally speaking,
read-only storage is much easier to build than secret storage. Hence, collision resistance seems more
appropriate for this particular application. In Chapter 13 we will develop an even better solution to
this problem, using digital signatures, that does not need read-only storage or online secret storage.
289
Security without collision resistance. By extending the input to the hash function with a few
random bits we can prove security for both applications above using a weaker notion of collision
resistance called target collision resistance or TCR for short. We show in Section 8.11.2 how to
use TCR for both ﬁle integrity and for extending cryptographic primitives. The downside is that the
resulting tags are longer than the ones obtained from collision resistant hashing. Hence, although
in principle it is often possible to avoid relying on collision resistance, the resulting systems are not
as eﬃcient.
8.1 Deﬁnition of collision resistant hashing
A (keyless) hash function H : M→T is an eﬃciently computable function from some (large)
message space Minto a (small) digest space T. We say that H is deﬁned over (M,T). We deﬁne
collision resistance of H using the following (degenerate) game:
Attack Game 8.1 (Collision Resistance). For a given hash function H deﬁned over (M,T)
and adversary A, the adversary takes no input and outputs two messages m0 and m1 in M.
We say that Awins the game if the pair m0,m1 is a collision for H, namely m0 ̸= m1 and
H(m0) = H(m1). We deﬁne A’s advantage with respect to H, denoted CR adv[A,H], as the
probability that Awins the game. Adversary Ais called a collision ﬁnder. 2
Deﬁnition 8.1. We say that a hash function H over (M,T) is collision resistant if for all
eﬃcient adversaries A, the quantity CRadv[A,H] is negligible.
At ﬁrst glance, it may seem that collision resistant functions cannot exist. The problem is this:
since |M|> |T| there must exist inputs m0 and m1 in Mthat collide, namely H(m0) = H(m1).
An adversary Athat simply prints m0 and m1 and exits is an eﬃcient adversary that breaks the
collision resistance of H. We may not be able to write the explicit program code for A(since we do
not know m0,m1), but this Acertainly exists. Consequently, for any hash function H deﬁned over
(M,T) there exists some eﬃcient adversary AH that breaks the collision resistance of H. Hence,
it appears that no function H can satisfy Deﬁnition 8.1.
The way out of this is that, formally speaking, our hash functions are parameterized by a
system parameter: each choice of a system parameter describes a diﬀerent function H, and so we
cannot simply “hardwire” a ﬁxed collision into an adversary: an eﬀective adversary must be able
to eﬃciently compute a collision as a function of the system parameter . This is discussed in more
depth in the Mathematical details section below. 1
8.1.1 Mathematical details
As usual, we give a more mathematically precise deﬁnition of a collision resistant hash function
using the terminology deﬁned in Section 2.3.
Deﬁnition 8.2 (Keyless hash functions). A (keyless) hash function is an eﬃcient algorithm
H, along with two families of spaces with system parameterization P:
M = {Mλ,Λ}λ,Λ, and T = {Tλ,Λ}λ,Λ,
1Some authors deal with this issue by haveH take as input a randomly chosen keyk, and giving kto the adversary
at the beginning of this attack game. By viewing k as a system parameter, this approach is really the same as ours.
290
λ λ
CRHF Challenger
Adversary AΛ ←
R
P(λ)
Λ
m0,m1
Figure 8.3: Asymptotic version of Attack Game 8.1
such that
1. M, and T are eﬃciently recognizable.
2. Algorithm H is an eﬃcient deterministic algorithm that on input λ∈Z≥1, Λ ∈Supp(P(λ)),
and m∈Mλ,Λ, outputs an element of Tλ,Λ.
In deﬁning collision resistance, we parameterize Attack Game 8.1 by the security parameter λ.
The asymptotic game is shown in Fig. 8.3. The advantage CR adv[A,H] is then a function of λ.
Deﬁnition 8.1 should be read as saying that CR adv[A,H](λ) is a negligible function.
It should be noted that the security and system parameters are artifacts of the formal framework
that are needed to make sense of Deﬁnition 8.1. In the real world, however, these parameters are
picked when the hash function is designed, and are ignored from that point onward. SHA256, for
example, does not take either a security parameter or a system parameter as input.
8.2 Building a MAC for large messages
To exercise the deﬁnition of collision resistance, we begin with an easy application described in
the introduction — extending the message space of a MAC. Suppose we are given a secure MAC
I= (S,V ) for short messages. Our goal is to build a new secure MAC I′for much longer messages.
We do so using a collision resistant hash function: I′computes a tag for a long message mby ﬁrst
hashing m to a short digest and then applying Ito the digest, as shown in Fig. 8.1.
More precisely, let H be a hash function that hashes long messages in Mto short digests in TH.
Suppose Iis deﬁned over (K,TH,T). Deﬁne I′= (S′,V ′) for long messages as follows:
S′(k,m) := S
(
k, H(m)
)
and V′(k,m,t ) := V
(
k, H(m), t
)
(8.1)
Then I′ authenticates long messages in M. The following easy theorem shows that I′ is secure,
assuming H is collision resistant.
Theorem 8.1. Suppose the MAC system Iis a secure MAC and the hash function H is collision
resistant. Then the derived MAC system I′= (S′,V ′) deﬁned in (8.1) is a secure MAC.
291
In particular, suppose Ais a MAC adversary attacking I′(as in Attack Game 6.1). Then there
exist a MAC adversary BI and an eﬃcient collision ﬁnder BH, which are elementary wrappers
around A, such that
MACadv[A,I′] ≤MACadv[BI,I] + CRadv[BH,H].
It is clear that collision resistance of H is essential for the security of I′. Indeed, if an adversary
can ﬁnd a collisionm0,m1 on H, then he can win the MAC attack game as follows: submitm0 to the
MAC challenger for signing, obtaining a tag t0 := S(k,H(m0)), and then output the message-tag
pair (m1,t0). Since H(m0) = H(m1), the tag t0 must be a valid tag on the message m1.
Proof idea. Our goal is to show that no eﬃcient adversary can win the MAC Attack Game 6.1 for
our new MAC system I′. An adversary Ain this game asks the challenger to MAC a few long
messages m1,m2,... ∈M and then tries to invent a new valid message-MAC pair ( m,t). If Ais
able to produce a valid forgery ( m,t) then one of two things must happen:
1. either m collides with some query mi from A, so that H(m) = H(mi) and m̸= mi;
2. or m does not collide under H with any of A’s queries m1,m2,... ∈M.
It should be intuitively clear that if Aproduces forgeries of the ﬁrst type then Acan be used to
break the collision resistance of H since mand mi are a valid collision for H. On the other hand, if
Aproduces forgeries of the second type then Acan be used to break the MAC system I: the pair
(H(m), t) is a valid MAC forgery for I. Thus, if Awins the MAC attack game for I′ we break
one of our assumptions. 2
Proof. We make this intuition rigorous. Let m1,m2,... ∈M be A’s queries during the MAC attack
game and let (m,t) ∈M×T be the adversary’s output, which we assume is not among the signed
pairs. We deﬁne three events:
• Let X be the event that adversary Awins the MAC Attack Game 6.1 with respect to I′.
• Let Y denote the event that some mi collides with m under H, that is, for some i we have
H(m) = H(mi) and m̸= mi.
• Let Z denote the event that Awins Attack Game 6.1 on I′and event Y did not occur.
Using events Y and Z we can rewrite A’s advantage in winning Attack Game 6.1 as follows:
MACadv[A,I′] = Pr[ X] ≤ Pr[X∧¬Y] + Pr[Y] = Pr[ Z] + Pr[Y] (8.2)
To prove the theorem we construct a collision ﬁnder BH and a MAC adversary BI such that
Pr[Y] = CRadv[BH,H] and Pr[ Z] = MACadv[BI,I].
Both adversaries are straight-forward.
Adversary BH plays the role of challenger to Ain the MAC attack game, as follows:
292
MAC Challenger
MAC Adversary BI attacking I
Adversary A
mi ∈Mhi ←H(mi)hi
ti ∈T ti ∈T
(m,t)(H(m), t)
Figure 8.4: Adversary BI in the proof of Theorem 8.1
Initialization:
k←R K
Upon receiving a signing query mi ∈M from Ado:
ti ←R S(k, H(mi) )
Send ti to A
Upon receiving the ﬁnal message-tag pair ( m,t) from Ado:
if H(m) = H(mi) and m̸= mi for some i
then output the pair ( m,mi)
Algorithm BH responds to A’s signature queries exactly as in a real MAC attack game. Therefore,
event Y happens during the interaction with BH with the same probability that it happens in a
real MAC attack game. Clearly when event Y happens, AH succeeds in ﬁnding a collision for H.
Hence, CRadv[BH,H] = Pr[Y] as required.
MAC adversary BI is just as simple and is shown in Fig. 8.4. When Aoutputs the ﬁnal
message-tag pair ( m,t), adversary BI outputs (H(m),t). When event Z happens we know that
V′(k,m,t ) outputs accept and the pair (m,t) is not equal to any of (m1,t1), (m2,t2),... ∈M×T .
Furthermore, since event Y does not happen, we know that ( H(m),t) is not equal to any of
(H(m1),t1), (H(m2),t2),... ∈TH ×T. It follows that ( H(m), t) is a valid existential forgery for
I. Hence, BI succeeds in creating an existential forgery with the same probability that event Z
happens. In other words, MAC adv[BI,I] = Pr[Z], as required. The proof now follows from (8.2).
2
8.3 Birthday attacks on collision resistant hash functions
Cryptographic hash functions are most useful when the output digest size is small. The challenge
is to design hash functions whose output is as short as possible and yet ﬁnding collisions is diﬃcult.
It should be intuitively clear that the shorter the digest, the easier it is for an attacker to ﬁnd
collisions. To illustrate this, consider a hash function H that outputs ℓ-bit digests for some small ℓ.
Clearly, by hashing 2 ℓ + 1 distinct messages the attacker will ﬁnd two messages that hash to the
293
same digest and will thus break collision resistance of H. This brute-force attack will break the
collision resistance of any hash function. Hence, for instance, hash functions that output 16-bit
digests cannot be collision resistant — a collision can always be found using only 2 16 + 1 = 65537
evaluations of the hash.
Birthday attacks. A far more devastating attack can be built using the birthday paradox dis-
cussed in Section B.1 in the appendix. Let H be a hash function deﬁned over ( M,T) and set
N := |T|. For standard hash functions N is quite large, for example N = 2 256 for SHA256.
Throughout this section we will assume that the size of Mis at least 100 N. This basically means
that messages being hashed are slightly longer than the output digest. We describe a general colli-
sion ﬁnder that ﬁnds collisions for H after an expected O(
√
N) evaluations of H. For comparison,
the brute-force attack above took O(N) evaluations. This more eﬃcient collision ﬁnder forces us
to use much larger digests.
The birthday collision ﬁnder forHworks as follows: it chooses s≈
√
N random and independent
messages, m1,...,m s ←R M, and looks for a collision among these s messages. We will show that
the birthday paradox implies that a collision is likely to exist among these messages. More precisely,
the birthday collision ﬁnder works as follows:
Algorithm BirthdayAttack:
1. Set s←⌈2
√
N ⌉+ 1
2. Generate s uniform random messages m1,...,m s in M
3. Compute xi ←H(mi) for all i= 1,...,s
4. Look for distinct i,j ∈{1,...,s }such that H(mi) = H(mj)
5. If such i,j exist and mi ̸= mj then
6. output the pair ( mi,mj)
We argue that when the adversary picks s :=
⌈
2
√
N
⌉
+ 1 random messages in M, then with
probability at least 1/2, there will exist distinct i,j such that H(mi) = H(mj) and mi ̸= mj. This
means that the algorithm will output a collision with probability at least 1 /2.
Lemma 8.2. Let m1,...,m s be the random messages sampled in Step 2. Assume |M|≥ 100N.
Then with probability at least 1/2 there exists i,j in {1,...,s }such that H(mi) = H(mj) and
mi ̸= mj.
Proof. For i = 1 ,...,s let xi := H(mi). First, we argue that two of the xi values will collide
with probability at least 3 /4. If the xi were uniformly distributed in T then this would follow
immediately from part (i) of Theorem B.1. Indeed, if the xi were independent and uniform in T a
collision among the xi will occur with probability at least 1 −e−s(s−1)/2N ≥1 −e−2 ≥3/4.
However, in reality, the function H(·) might bias the output distribution. Even though the mi
are sampled uniformly from M, the resulting xi may not be uniform in T. As a simple example,
consider a hash function H(·) that only outputs digests in a certain small subset ofT. The resulting
xi would certainly not be uniform inT. Fortunately (for the attacker) Corollary B.2 shows that non-
uniform xi only increase the probability of collision. Since the xi are independent and identically
distributed the corollary implies that a collision among the xi will occur with probability at least
1 −e−s(s−1)/2N ≥3/4 as required.
Next, we argue that a collision among thexi is very likely to lead to a collision onH(·). Suppose
xi = xj for some distinct i,j in {1,...,s }. Since xi = H(mi) and xj = H(mj), the pair mi,mj is a
294
candidate for a collision on H(·). We just need to argue that mi ̸= mj. We do so by arguing that
all the m1,...,m s are distinct with probability at least 4 /5. This follows directly from part (ii) of
Theorem B.1. Recall that Mis greater than 100N. Since m1,m2,... are uniform and independent
in M, and s <|M|/2, part (ii) of Theorem B.1 implies that the probability of collision among
these mi is at most 1 −e−s(s−1)/100N ≤1/5. Therefore, the probability that no collision occurs is
at least 4/5.
In summary, for the algorithm to discover a collision for H(·) it is suﬃcient that both a collision
occurs on the xi values and no collision occurs on the mi values. This happens with probability at
least 3/4 −1/5 >1/2, as required. 2
Variations. Algorithm BirthdayAttack requires O(
√
N) memory space, which can be quite
large: larger than the size of commercially available disk farms. However, a modiﬁed birthday
collision ﬁnder, described in Exercise 8.8, will ﬁnd a collision with an expected 4
√
N evaluations of
the hash function and constant memory space.
The birthday attack is likely to fail if one makes fewer than
√
N queries to H(·). Suppose we
only make s = ϵ
√
N queries to H(·), for some small ϵ ∈[0,1]. For simplicity we assume that
H(·) outputs digests distributed uniformly in T. Then part (ii) of Theorem B.1 shows that the
probability of ﬁnding a collision degrades exponentially to approximately 1 −e−(ϵ2) ≈ϵ2.
Put diﬀerently, if after evaluating the hash function s times an adversary should obtain a
collision with probability at most δ, then we need the digest space T to satisfy |T|≥ s2/δ. For
example, if after 280 evaluations of H a collision should be found with probability at most 2−80 then
the digest size must be at least 240 bits. Cryptographic hash functions such as SHA256 output a
256-bit digest. Other hash functions, such as SHA384 and SHA512, output even longer digests,
namely, 384 and 512 bits respectively.
8.4 The Merkle-Damg ˚ ard paradigm
We now turn to constructing collision resistant hash functions. Many practical constructions follow
the Merkle-Damg˚ ard paradigm: start from a collision resistant hash function that hashes short
messages and build from it a collision resistant hash function that hashes much longer messages.
This paradigm reduces the problem of constructing collision resistant hashing to the problem of
constructing collision resistance for short messages, which we address in the next section.
Let h: X×Y→X be a hash function. We shall assume that Yis of the form {0,1}ℓ for some
ℓ. While it is not necessary, typically Xis of the form {0,1}n for some n. The Merkle-Damg ˚ ard
function derived from h, denoted HMD and shown in Fig. 8.5, is a hash function deﬁned over
({0,1}≤L, X) that works as follows (the pad PB is deﬁned below):
295
m1
ht0 := IV
m2
ht1
ms
h ts := H(M)
PB···
ts−1t2
Figure 8.5: The Merkle-Damg˚ ard iterated hash function
input: M ∈{0,1}≤L
output: a tag in X
ˆM ←M ∥PB / / pad with PB to ensure that the length of M is a multiple of ℓ bits
partition ˆM into consecutive ℓ-bit blocks so that
ˆM = m1 ∥m2 ∥···∥ ms where m1,...,m s ∈{0,1}ℓ
t0 ←IV ∈X
for i= 1 to s do:
ti ←h(ti−1,mi)
output ts
The function SHA256 is a Merkle-Damg˚ ard function whereℓ= 512 and n= 256.
Before proving collision resistance ofHMD, let us ﬁrst introduce some terminology for the various
elements in Fig. 8.5:
• The hash function h is called the compression function of H.
• The constant IV is called theinitial value and is ﬁxed to some pre-speciﬁed value. One could
take IV = 0 n, but usually the IV is set to some complicated string. For example, SHA256
uses a 256-bit IV whose value in hex is
IV := 6A09E667 BB67AE85 3C6EF372 A54FF53A 510E527F 9B05688C 1F83D9AB 5BE0CD19 .
• The variables m1,...,m s are called message blocks.
• The variables t0,t1,...,t s ∈X are called chaining variables.
• The string PB is called the padding block. It is appended to the message to ensure that
the message length is a multiple of ℓ bits.
The padding block PB must contain an encoding of the input message length. We will use this
in the proof of security below. A standard format for PB is as follows:
PB := 100 ... 00 ∥⟨s⟩
where ⟨s⟩is a ﬁxed-length bit string that encodes, in binary, the number of ℓ-bit blocks in M.
Typically this ﬁeld is 64-bits which means that messages to be hashed are less than 2 64 blocks
296
long. The ‘100 ... 00’ string is a variable length pad used to ensure that the total message length,
including PB, is a multiple of ℓ. The variable length string ‘100 ... 00’ starts with a ‘1’ to identify
the position where the pad ends and the message begins. If the message length is such that there
is no space for PB in the last block (for example, if the message length happens to be a multiple
of ℓ), then an additional block is added just for the padding block.
Security of Merkle-Damg ˚ ard.Next we prove that the Merkle-Damg˚ ard function is collision
resistant, assuming the compression function is.
Theorem 8.3 (Merkle-Damg ˚ ard).Let L be a poly-bounded length parameter and let h be a
collision resistant hash function deﬁned over (X×Y, X). Then the Merkle-Damg ˚ ard hash function
HMD derived from h, deﬁned over ({0,1}≤L,X), is collision resistant.
In particular, for every collision ﬁnder Aattacking HMD (as in Attack Game 8.1) there exists a
collision ﬁnder Battacking h, where Bis an elementary wrapper around A, such that
CRadv[A,HMD] = CRadv[B,h].
Proof. The collision ﬁnder Bfor ﬁnding h-collisions works as follows: it ﬁrst runs Ato obtain two
distinct messages M and M′in {0,1}≤L such that HMD(M) = HMD(M′). We show that Bcan use
M and M′ to ﬁnd an h-collision. To do so, Bscans M and M′ starting from the last block and
works its way backwards. To simplify the notation, we assume that M and M′already contain the
appropriate padding block PB in their last block.
Let M = m1m2 ...m u be the u blocks of M and let M′= m′
1m′
2 ...m ′
v be the v blocks of M′.
We let t0,t1,...,t u ∈X be the chaining values for M and t′
0,t′
1,...,t ′
v ∈X be the chaining values
for M′. The very last application of hgives the ﬁnal output digest and since HMD(M) = HMD(M′)
we know that
h(tu−1,mu) = h(t′
v−1,m′
v).
If either tu−1 ̸= t′
v−1 or mu ̸= m′
v then the pair of inputs (tu−1,mu) and (t′
v−1,m′
v) is an h-collision.
Boutputs this collision and terminates.
Otherwise, tu−1 = t′
v−1 and mu = m′
v. Recall that the padding blocks are contained in mu and
m′
v and these padding blocks contain an encoding of uand v. Therefore, since mu = m′
v we deduce
that u= v so that M and M′must contain the same number of blocks.
At this point we know that u = v, mu = m′
u, and tu−1 = t′
u−1. We now consider the second-
to-last block. Since tu−1 = t′
u−1 we know that
h(tu−2,mu−1) = h(t′
u−2,m′
u−1).
As before, if either tu−2 ̸= t′
u−2 or mu−1 ̸= m′
u−1 then Bjust found an h-collision. It outputs this
collision and terminates.
Otherwise, we know that tu−2 = t′
u−2 and mu−1 = m′
u−1 and mu = m′
u. We now consider the
third block from the end. As before, we either ﬁnd an h-collision or deduce that mu−2 = m′
u−2
and tu−3 = t′
u−3. We keep iterating this process moving from right to left one block at a time. At
the ith block one of two things happens. Either the pair of messages ( ti−1,mi) and (t′
i−1,m′
i) is an
h-collision, in which case Boutputs this collision and terminates. Or we deduce that ti−1 = t′
i−1
and mj = m′
j for all j = i,i + 1,...,u .
297
Suppose this process continues all the way to the ﬁrst block and we still did not ﬁnd an h-
collision. Then at this point we know that mi = m′
i for i= 1,...,u . But this implies that M = M′
contradicting the fact that M and M′were a collision for HMD. Hence, since M ̸= M′, the process
of scanning blocks of M and M′ from right to left must produce an h-collision. We conclude that
Bbreaks the collision resistance of h as required.
In summary, we showed that whenever Aoutputs an HMD-collision, Boutputs an h-collision.
Hence, CRadv[A,HMD] = CRadv[B,h] as required. 2
Variations. Note that the Merkle-Damg˚ ard construction is inherently sequential — theith block
cannot be hashed before hashing all previous blocks. This makes it diﬃcult to take advantage of
hardware parallelism when available. In Exercise 8.9 we investigate a diﬀerent hash construction
that is better suited for a multi-processor machine.
The Merkle-Damg˚ ard theorem (Theorem 8.3) shows that collision resistance of the compression
function is suﬃcient to ensure collision resistance of the iterated function. This condition, however,
is not necessary. Black, Rogaway, and Shrimpton [24] give several examples of compression functions
that are clearly not collision resistant, and yet the resulting iterated Merkle-Damg˚ ard functions are
collision resistant.
8.4.1 Joux’s attack
We brieﬂy describe a cute attack that applies speciﬁcally to Merkle-Damg˚ ard hash functions. Let
H1 and H2 be Merkle-Damg˚ ard hash functions that output tags inX:= {0,1}n. Deﬁne H12(M) :=
H1(M) ∥H2(M) ∈{0,1}2n. One would expect that ﬁnding a collision for H12 should take time at
least Ω(2n). Indeed, this would be the case if H1 and H2 were independent random functions.
We show that when H1 and H2 are Merkle-Damg˚ ard functions we can ﬁnd collisions forH in
time approximately n2n/2 which is far less than 2n. This attack illustrates that our intuition about
random functions may lead to incorrect conclusions when applied to a Merkle-Damg˚ ard function.
We say that an s-collision for a hash function H is a set of messages M1,...,M s ∈M such that
H(M1) = ... = H(Ms). Joux showed how to ﬁnd an s-collision for a Merkle-Damg˚ ard function in
time O((log2 s)|X|1/2). Using Joux’s method we can ﬁnd a 2 n/2-collision M1,...,M 2n/2 for H1 in
time O(n2n/2). Then, by the birthday paradox it is likely that two of these messages, say Mi,Mj,
are also a collision for H2. This pair Mi,Mj is a collision for both H1 and H2 and therefore a
collision for H12. It was found in time O(n2n/2), as promised.
Finding s-collisions. To ﬁnd an s-collision, let H be a Merkle-Damg˚ ard function over (M,X)
built from a compression function h. We ﬁnd an s-collision M1,...,M s ∈M where each message
Mi contains log2 sblocks. For simplicity, assume that sis a power of 2 so that log 2 sis an integer.
As usual, we let t0 denote the Initial Value (IV) used in the Merkle-Damg˚ ard construction.
The plan is to use the birthday attack log 2 s times on the compression function h. We ﬁrst
spend time 2n/2 to ﬁnd two distinct blocks m0,m′
0 such that (t0,m0) and (t0,m′
0) collide under h.
Let t1 := h(t0,m0). Next we spend another 2 n/2 time to ﬁnd two distinct blocks m1,m′
1 such that
(t1,m1) and ( t1,m′
1) collide under h. Again, we let t2 := h(t1,m1) and repeat. We iterate this
process b:= log2 s times until we have b pairs of blocks:
(mi,m′
i) for i= 0,1,...b −1 that satisfy h(ti,mi) = h(ti,m′
i).
298
Now, consider the message M = m0m1 ...m b−1. The main point is that replacing any block mi in
this message by m′
i will not change the chaining value ti+1 and therefore the value of H(M) will
not change. Consequently, we can replace any subset of m0,...,m b−1 by the corresponding blocks
in m′
0,...,m ′
b−1 without changing H(M). As a result we obtain s= 2b messages
m0m1 ...m b−1
m′
0m1 ...m b−1
m0m′
1 ...m b−1
m′
0m′
1 ...m b−1
...
m′
0m′
1 ...m ′
b−1
that all hash to the same value under H. In summary, we found a 2 b-collision in time O(b2n/2). As
explained above, this lets us ﬁnd collisions for H(M) := H1(M) ∥H2(M) in time O(n2n/2).
8.5 Building Compression Functions
The Merkle-Damg˚ ard paradigm shows that to construct a collision resistant hash function for long
messages, it suﬃces to construct a collision resistant compression function h for short blocks. In
this section we describe a few candidate compression functions. These constructions fall into two
categories:
• Compression functions built from a block cipher. The most widely used method is called
Davies-Meyer. The SHA family of cryptographic hash functions all use Davies-Meyer.
• Compression functions using number theoretic primitives. These are elegant constructions
with clean proofs of security. Unfortunately, they are generally far less eﬃcient than the ﬁrst
method.
8.5.1 A simple but ineﬃcient compression function
We start with a compression function built using modular arithmetic. Let p be a large prime such
that q := ( p−1)/2 is also prime. Let x and y be suitably chosen integers in the range [1 ,q].
Consider the following simple compression function that takes as input two integers in [1 ,q] and
outputs an integer in [1 ,q]:
H(a,b) = abs(xayb mod p), where abs( z) :=
{
z if z≤q,
p−z if z >q. (8.3)
We will show later in Exercise 10.22 that this function is collision resistant assuming a certain
standard number theoretic problem is hard. Applying the Merkle-Damg˚ ard paradigm to this func-
tion gives a collision resistant hash function for arbitrary size inputs. Although this is an elegant
collision resistant hash with a clean security proof, it is far less eﬃcient than functions derived from
the Davies-Meyer construction and, as a result, is hardly ever used in practice.
299
E
⨁
y:= mi ∈K
x:= ti−1
ti := E(mi, ti−1) ⊕ti−1 ∈X
Figure 8.6: The Davies-Meyer compression function
8.5.2 Davies-Meyer compression functions
In this section we show how to construct a fast compression function from a secure block cipher
E= (E,D). One method, called Davies-Meyer, enables us to do just that. To prove security of the
resulting compression function we need to model the block cipher Eas an ideal cipher.
Let E= (E,D) be a block cipher over ( K,X) where X= {0,1}n. The Davies-Meyer com-
pression function derived from E maps inputs in X×K to outputs in X. The function is
deﬁned as follows:
hDM(x,y) := E(y, x) ⊕x
and is illustrated in Fig. 8.6. In symbols, hDM is deﬁned over (X×K , X).
When plugging this compression function into the Merkle-Damg˚ ard paradigm, the inputs are a
chaining variable x:= ti−1 ∈X and a message block y:= mi ∈K. The output is the next chaining
variable ti := E(mi, ti−1) ⊕ti−1 ∈X. Note that the message block is used as the block cipher key
which seems a bit odd since the adversary has full control over the message. Nevertheless, we will
show that hDM is collision resistant and therefore the resulting Merkle-Damg˚ ard function is collision
resistant.
When using hDM in Merkle-Damg˚ ard the block cipher key (mi) changes from one message block
to the next, which is an unusual way of using a block cipher. Common block ciphers are optimized
to encrypt long messages with a ﬁxed key; changing the block cipher key on every block can slow
down the cipher. Consequently, using Davies-Meyer with an oﬀ-the-shelf block cipher such as AES
will result in a relatively slow hash function. Instead, one uses a custom block cipher speciﬁcally
designed for rapid key changes.
Another reason to not use an oﬀ-the-shelf block cipher in Davies-Meyer is that the block size
may be too short, for example 128 bits for AES. An AES-based compression function would produce
a 128-bit output which is much too short for collision resistance: a collision could be found with
only 2 64 evaluations of the function. In addition, oﬀ-the-shelf block ciphers use relatively short
keys, say 128 bits long. This would result in Merkle-Damg˚ ard processing only 128 message bits per
round. Typical ciphers used in Merkle-Damg˚ ard hash functions use longer keys (typically, 512-bits
or even 1024-bits long) so that many more message bits are processed in every round.
Davies-Meyer variants. The Davies-Meyer construction is not unique. Many other similar
methods can convert a block cipher into a collision resistant compression function. For example,
300
Matyas-Meyer-Oseas Miyaguchi-Preneel
Eg ⨁
y:= mi ∈X
x:= ti−1
ti ∈X Eg ⨁
y:= mi ∈X
x:= ti−1
ti ∈X
Figure 8.7: Other block cipher compression functions
one could use
Matyas-Meyer-Oseas: h1(x,y) := E(x, y) ⊕y
Miyaguchi-Preneel: h2(x,y) := E(x, y) ⊕y⊕x
Or even: h3(x,y) := E(x⊕y, y) ⊕y
or many other such variants. Preneel et al. [132] give twelve diﬀerent variants that can be shown
to be collision resistant.
The Matyas-Meyer-Oseas function h1 is similar to Davies-Meyer, but reverses the roles of the
chaining variable and the message block — in h1 the chaining variable is used as the block cipher
key. The function h1 maps elements in ( K×X ) to X. Therefore, to use h1 in Merkle-Damg˚ ard
we need an auxiliary encoding function g : X →Kthat maps the chaining variable ti−1 ∈X to
an element in K, as shown in Fig. 8.7. The same is true for the Miyaguchi-Preneel function h2.
The Davies-Meyer function does not need such an encoding function. We note that the Miyaguchi-
Preneel function has a minor security advantage over Davies-Meyer, as discussed in Exercise 8.15.
Many other natural variants of Davies-Meyer are totally insecure. For example, for the following
functions
h4(x,y) := E(y, x) ⊕y
h5(x,y) := E(x, x⊕y) ⊕x
we can ﬁnd collisions in constant time (see Exercise 8.11).
8.5.3 Collision resistance of Davies-Meyer
We cannot prove that Davies-Meyer is collision resistant by assuming a standard complexity as-
sumption about the block cipher. Simply assuming that E = ( E,D) is a secure block cipher is
insuﬃcient for proving that hDM is collision resistant. Instead, we have to model the block cipher
as an ideal cipher.
We introduced the ideal cipher model back in Section 4.7. Recall that this is a heuristic technique
in which we treat the block cipher as if it were a family of random permutations. If E= (E,D) is
a block cipher with key space Kand data block space X, then the family of random permutations
301
is {Πk }k ∈K, where each Π k is a truly random permutation on X, and the Π k ’s collectively are
mutually independent.
Attack Game 8.1 can be adapted to the ideal cipher model, so that before the adversary outputs
a collision, it may make a series of Π-queries and Π −1-queries to its challenger.
• For a Π-query, the adversary submits a pair (k ,a) ∈K×X , to which the challenger responds
with b := Πk (a).
• For a Π−1-query, the adversary submits a pair (k ,b) ∈K×X, to which the challenger responds
with a := Π−1
k (b).
After making these queries, the adversary attempts to output a collision, which in the case of
Davies-Meyer, means (x,y) ̸= (x′,y′) such that
Πy(x) ⊕x= Πy′(x′) ⊕x′.
The adversary A’s advantage in ﬁnding a collision for hDM in the ideal cipher model is denoted
CRicadv[A,hDM], and security in the ideal cipher model means that this advantage is negligible for
all eﬃcient adversaries A.
Theorem 8.4 (Davies-Meyer). Let hDM be the Davies-Meyer hash function derived from a block
cipher E= (E,D) deﬁned over (K,X), where |X| is large. Then hDM is collision resistant in the
ideal cipher model.
In particular, every collision ﬁnding adversary Athat issues at most q ideal-cipher queries will
satisfy
CRicadv[A,hDM] ≤(q+ 1)(q+ 2)/|X|.
The theorem shows that Davies-Meyer is an optimal compression function: the adversary must
issue q= Ω(
√
|X|) queries (and hence must run for at least that amount of time) if he is to ﬁnd a
collision for hDM with constant probability. No compression function can have higher security due
to the birthday attack.
Proof. Let Abe a collision ﬁnder for hDM that makes at most a total of q ideal cipher queries.
We shall assume that A is “reasonable” meaning that before A outputs its collision attempt
(x,y),(x′,y′), it makes corresponding ideal cipher queries: for ( x,y), either a Π-query on ( y,x)
or a Π−1-query on (y,·) that yields x, and similarly for ( x′,y′). If Ais not already reasonable, we
can make it so by increasing total number of queries to at most q′ := q+ 2. So from now on we
will assume Ais reasonable and makes at most q′ideal cipher queries.
For i= 1,...,q ′, the ith ideal cipher query deﬁnes a triple ( k i,ai,bi): for a Π-query ( k i,ai), we
set bi := Πk i(ai), and for a Π −1-query (k i,bi), we set ai := Π−1
k i
(bi). We assume that Amakes no
extraneous queries, so that no triples repeat.
If the adversary outputs a collision, then by our reasonableness assumption, for some distinct
pair of indices i,j = 1,...,q ′, we have ai ⊕bi = aj ⊕bj. Let us call this event Z. So we have
CRicadv[A,hDM] ≤Pr[Z].
Our goal is to show
Pr[Z] ≤q′(q′−1)
2n , (8.4)
302
where |X|= 2n.
Consider any ﬁxed indices i<j . Conditioned on any ﬁxed values of the adversary’s coins and
the ﬁrst j−1 triples, one of aj and bj is completely ﬁxed, while the other is uniformly distributed
over a set of size at least |X|− j+ 1. Therefore,
Pr[ai ⊕bi = aj ⊕bj] ≤ 1
2n −j+ 1.
So by the union bound, we have
Pr[Z] ≤
q′
∑
j=1
j−1∑
i=1
Pr[ai ⊕bi = aj ⊕bj] ≤
q′
∑
j=1
j−1
2n −j+ 1 ≤
q′
∑
j=1
j−1
2n −q′ = q′(q′−1)
2(2n −q′). (8.5)
For q′≤2n−1 this bound simpliﬁes to Pr[Z] ≤q′(q′−1)/2n. For q′>2n−1 the bound holds trivially.
Therefore, (8.4) holds for all q′. 2
8.6 Case study: SHA256
The Secure Hash Algorithm (SHA) was published by NIST in 1993 [FIPS 180] as part of the design
speciﬁcation of the Digital Signature Standard (DSS). This hash function, often called SHA0,
outputs 160-bit digests. Two years later, in 1995, NIST updated the standard [FIPS 180-1] by
adding one extra instruction to the compression function. The resulting function is called SHA1.
NIST gave no explanation for this change, but it was later found that this extra instruction is
crucial for collision resistance. SHA1 became the de-facto standard for collision resistant hashing
and was widely deployed.
The birthday attack can ﬁnd collisions for SHA1 using an expected 2 80 evaluations of the
function. In 2002 NIST added [FIPS 180-2] two new hash functions to the SHA family: SHA256
and SHA512. They output larger digests (256 and 512-bit digests respectively) and therefore
provide better protection against the birthday attack. NIST also approved SHA224 and SHA384
which are obtained from SHA256 and SHA512 respectively by truncating the output to 224 and
384 bits. These and a few other proposed hash functions are summarized in Table 8.1.
The years 2004–5 were bad years for collision resistant hash functions. A number of new attacks
showed how to ﬁnd collisions for several hash functions. In particular, Wang, Yao, and Yao [155]
presented a collision ﬁnder for SHA1 that uses 2 63 evaluations of the function — far less than the
birthday attack. The ﬁrst collision for SHA1, using an improved algorithm, was found in 2017.
As a result SHA1 is no longer considered collision resistant, and should not be used. The current
recommended practice is to use SHA256 which we describe here.
The SHA256 function. SHA256 is a Merkle-Damg˚ ard hash function using a Davies-Meyer
compression function h. This h takes as input a 256-bit chaining variable t and a 512-bit message
block m. It outputs a 256-bit chaining variable.
We ﬁrst describe the SHA256 Merkle-Damg˚ ard chain. Recall that the padding block PB in our
description of Merkle-Damg˚ ard contained a 64-bit encoding of the number ofblocks in the message
being hashed. The same is true for SHA256 with the minor diﬀerence that PB encodes the number
2Performance numbers were provided by Wei Dai using the Crypto++ 5.6.0 benchmarks running on a 1.83 GhZ
Intel Core 2 processor. Higher numbers are better.
303
digest message Speed 2 best known
Name year size block size MB/sec attack time
SHA0 1993 160 512 2 39
SHA1 1995 160 512 153 2 63
SHA224 2004 224 512
SHA256 2002 256 512 111
SHA384 2002 384 1024
SHA512 2002 512 1024 99
MD4 1990 128 512 2 1
MD5 1992 128 512 255 2 16
Whirlpool 2000 512 512 57
Table 8.1: Merkle-Damg˚ ard collision resistant hash functions
of bits in the message. Hence, SHA256 can hash messages that are at most 2 64 −1 bits long. The
Merkle-Damg˚ ard Initial Value (IV) in SHA256 is set to:
IV := 6A09E667 BB67AE85 3C6EF372 A54FF53A 510E527F 9B05688C 1F83D9AB 5BE0CD19 ∈{0,1}256
written in base 16.
Clearly the output of SHA256 can be truncated to obtain shorter digests at the cost of reduced
security. This is, in fact, how the SHA224 hash function works — it is identical to SHA256 with
two exceptions: (1) SHA224 uses a diﬀerent initialization vector IV, and (2) SHA224 truncates the
output of SHA256 to its left most 224 bits.
Next, we describe the SHA256 Davies-Meyer compression function h. It is built from a block
cipher which we denote by ESHA256. However, instead of using XOR as in Davies-Meyer, SHA256
uses addition modulo 2 32. That is, let
x0,x1,...,x 7 ∈{0,1}32 and y0,y1,...,y 7 ∈{0,1}32
and set
x:= x0 ∥···∥ x7 ∈{0,1}256 and y:= y0 ∥···∥ y7 ∈{0,1}256.
Deﬁne: x⊞y := (x0 + y0) ∥···∥ (x7 + y7) ∈{0,1}256 where all additions are modulo 2 32.
Then the SHA256 compression function h is deﬁned as:
h(t,m) := ESHA256(m,t) ⊞ t ∈{0,1}256.
Our ideal cipher analysis of Davies-Meyer (Theorem 8.4) applies equally well to this modiﬁed
function.
The SHA256 block cipher. To complete the description of SHA256 it remains to describe the
block cipher ESHA256. The algorithm makes use of a few auxiliary functions deﬁned in Table 8.2.
Here, SHR and ROTR denote the standard shift-right and rotate-right functions.
304
For x,y,z in {0,1}32 deﬁne:
SHRn(x) := ( x>>n ) (Shift Right)
ROTRn(x) := ( x>>n ) ∨(x<< 32 −n) (Rotate Right)
Ch(x,y,z ) := ( x∧y) ⊕(¬x∧z)
Maj(x,y,z ) := ( x∧y) ⊕(x∧z) ⊕(y∧z)
Σ0(x) := ROTR 2(x) ⊕ROTR13(x) ⊕ROTR22(x)
Σ1(x) := ROTR 6(x) ⊕ROTR11(x) ⊕ROTR25(x)
σ0(x) := ROTR 7(x) ⊕ROTR18(x) ⊕SHR3(x)
σ1(x) := ROTR 17(x) ⊕ROTR19(x) ⊕SHR10(x)
Table 8.2: Functions used in the SHA256 block cipher
The cipher ESHA256 takes as input a 512-bit key k and a 256-bit message t. We ﬁrst break both
the key and the message into 32-bit words. That is, write:
k:= k0 ∥k1 ∥···∥ k15 ∈{0,1}512
t:= t0 ∥t1 ∥···∥ t7 ∈{0,1}256
where each ki and ti is in {0,1}32.
The code for ESHA256 is shown in Table 8.3. It iterates the same round function 64 times. In
each round the cipher uses a round key Wi ∈{0,1}32 deﬁned recursively during the key setup step.
One cipher round, shown in Fig. 8.8, looks like two adjoined Feistel rounds. The cipher uses 64
ﬁxed constants K0,K1,...,K 63 ∈{0,1}32 whose values are speciﬁed in the SHA256 standard. For
example, K0 := 428A2F98 and K1 := 71374491, written base 16.
Interestingly, NIST never gave the block cipher ESHA256 an oﬃcial name. The cipher was given
the unoﬃcial name SHACAL-2 by Handschuh and Naccache (submission to NESSIE, 2000).
Similarly, the block cipher underlying SHA1 is called SHACAL-1. The SHACAL-2 block cipher is
identical to ESHA256 with the only diﬀerence that it can encrypt using keys shorter than 512 bits.
Given a key k ∈{0,1}≤512 the SHACAL-2 cipher appends zeros to the key to get a 512-bit key.
It then applies ESHA256 to the given 256-bit message block. Decryption in SHACAL-2 is similar to
encryption. This cipher is well suited for applications where SHA256 is already implemented, thus
reducing the overall size of the crypto code.
8.6.1 Other Merkle-Damg ˚ ard hash functions
MD4 and MD5. Both cryptographic hash functions were designed by Ron Rivest in 1990–1 [134,
135]. Both are Merkle-Damg˚ ard hash functions that output a 128-bit digest. They are quite similar,
although MD5 uses a stronger compression function than MD4. Collisions for both hash functions
can be found eﬃciently as described in Table 8.1. Consequently, these hash functions are no longer
used.
Whirlpool. Whirlpool was designed by Barreto and Rijmen in 2000 and was adopted as an
ISO/IEC standard in 2004. Whirlpool is a Merkle-Damg˚ ard hash function. Its compression function
305
Input: plaintext t= t0 ∥···∥ t7 ∈{0,1}256 and
key k= k0 ∥k1 ∥···∥ k15 ∈{0,1}512
Output: ciphertext in {0,1}256.
/ / Here all additions are modulo 232.
/ / The algorithm uses constants K0,K1,...,K 63 ∈{0,1}32
Key setup: Construct 64 round keys W0,...,W 63 ∈{0,1}32:
{
for i= 0,1,..., 15 set Wi ←ki,
for i= 16,17,..., 63 set Wi ←σ1(Wi−2) + Wi−7 + σ0(Wi−15) + Wi−16
64 Rounds:(
a0, b0, c0, d0, e0, f0, g0, h0
)
←
(
t0, t1, t2, t3, t4, t5, t6, t7
)
for i= 0 to 63 do:
T1 ←hi + Σ1(ei) + Ch(ei,fi,gi) + Ki + Wi
T2 ←Σ0(ai) + Maj(ai,bi,ci)(
ai+1, bi+1, ci+1, di+1, ei+1, fi+1, gi+1, hi+1
)
←(
T1 + T2, ai, bi, ci, di + T1, ei, fi, gi
)
Output: a64 ∥b64 ∥c64 ∥d64 ∥e64 ∥f64 ∥g64 ∥h64 ∈{0,1}256
Table 8.3: The SHA256 block cipher
ai bi ci di eifigihi
ai+1 bi+1 ci+1 di+1 ei+1fi+1gi+1hi+1
⨁ ⨁F1(ai,bi,ci,ei,fi,gi) F2(ei,fi,gi,hi)
F1(a,b,c,e,f,g ) := Σ1(e) + Ch(e,f,g ) + Σ0(a) + Maj(a,b,c ) + Ki + Wi
F2(e,f,g,h ) := h+ Σ1(e) + Ch(e,f,g ) + Ki + Wi
Figure 8.8: One round of the SHA256 block cipher
306
uses the Miyaguchi-Preneel method (Fig. 8.7) with a block cipher called W. This block cipher is
very similar to AES, but has a 512-bit block size. The resulting hash output is 512-bits.
Others. Many other Merkle-Damg˚ ard hash functions were proposed in the literature. Two ex-
amples are Tiger/192 [20] and RIPEMD-160.
8.7 Case study: HMAC
In this section, we return to our problem of building a secure MAC that works on long messages.
Merkle-Damg˚ ard hash functions such as SHA256 are widely deployed. Most Crypto libraries include
an implementation of multiple Merkle-Damg˚ ard functions. Furthermore, these implementations are
very fast: one can typically hash a very long message with SHA256 much faster than one can apply,
say, CBC-MAC with AES to the same message.
Of course, one might use the hash-then-MAC construction analyzed in Section 8.2. Recall that
in this construction, we combine a secure MAC system I= (S,V ) and a collision resistant hash
function H, so that the resulting signing algorithm signs a message m by ﬁrst hashing m using H
to get a short digest H(m), and then signs H(m) using S to obtain the MAC tag t= S(k,H(m)).
As we saw in Theorem 8.1 the resulting construction is secure. However, this construction is not
very widely deployed. Why?
First of all, as discussed after the statement of Theorem 8.1, if one can ﬁnd collisions in H,
then the hash-then-MAC construction is completely broken. A collision-ﬁnding attack, such as a
birthday attack (Section 8.3), or a more sophisticated attack, can be carried out entirely oﬄine,
that is, without the need to interact with any users of the system. In contrast, online attacks
require many interactions between the adversary and honest users of the system. In general, oﬄine
attacks are considered especially dangerous since an adversary can invest huge computing resources
over an extended period of time: in an attack on hash-then-MAC, an attacker could spend months
quietly computing on many machines to ﬁnd a collision on H, without arousing any suspicions.
Another reason not to use the hash-then-MAC construction directly is that we need both a hash
function H and a MAC system I. So an implementation might need software and/or hardware to
execute both, say, SHA256 for the hash and CBC-MAC with AES for the MAC. All other things
being equal, it would be nice to simply use one algorithm as the basis for a MAC.
This leads us to the following problem: how to take a keyless Merkle-Damg˚ ard hash function,
such as SHA256, and use it somehow to implement a keyed function that is a secure MAC, or even
better, a secure PRF. Moreover, we would like to be able to prove the security of this construction
under an assumption that is (qualitatively, at least) weaker than collision resistance; in particular,
the construction should not be susceptible to an oﬄine collision-ﬁnding attack on the underlying
compression function.
Assume that H is a Merkle-Damg˚ ard hash built from a compression function h : {0,1}n ×
{0,1}ℓ →{0,1}n. A few simple approaches come to mind.
Prepend the key: Fpre(k,M) := H(k∥M). This is completely insecure, because of the following
extension attack : given Fpre(k,M), one can easily compute Fpre(k,M ∥PB ∥M′) for any
M′. Here, PB is the Merkle-Damg˚ ard padding block for the messagek∥M. Aside from this
extension attack, the construction is secure, under reasonable assumptions (see Exercise 8.18).
307
Append the key: Fpost(k,M) := H(M ∥k). This is somewhat similar to the hash-then-MAC
construction, and relies on the collision resistance of h. Indeed, it is vulnerable to an oﬄine
collision-ﬁnding attack: assuming we ﬁnd two distinct ℓ-bit strings M0 and M1 such that
h(IV,M0) = h(IV,M1), then we have Fpost(k,M0) = Fpost(k,M1). For these reasons, this
construction does not solve our problem. However, under the right assumptions (including
the collision resistance of h, of course), we can still get a security proof (see Exercise 8.19).
Envelope method: Fenv(k,M) := H(k ∥M ∥k). Under reasonable pseudorandomness assump-
tions on h, and certain formatting assumptions (that k is an ℓ-bit string and M is padded
out to a bit string whose length is a multiple of ℓ), this can be proven to be a secure PRF.
See Exercise 8.17.
Two-key nest: Fnest((k1,k2),M) := H(k2 ∥H(k1 ∥M)). Under reasonable pseudorandomness
assumptions on h, and certain formatting assumptions (that k1 and k2 are ℓ-bit strings), this
can also be proven to be a secure PRF.
The two-key nest is very closely related to a classic MAC construction known as HMAC.
HMAC is the most widely deployed MAC on the Internet. It is used in SSL, TLS, IPsec, SSH, and
a host of other security protocols. TLS and IPsec also use HMAC as a means for deriving session
keys during session setup. We will give a security analysis of the two-key nest, and then discuss its
relation to HMAC.
8.7.1 Security of two-key nest
We will now show that the two-key nest is indeed a secure PRF, under appropriate pseudorandom-
ness assumptions on h. Let us start by “opening up” the deﬁnition of Fnest((k1,k2),M), using the
fact that H is a Merkle-Damg˚ ard hash built fromh. See Fig. 8.9. The reader should study this
ﬁgure carefully. We are assuming that the keys k1 and k2 are ℓ-bit strings, so they each occupy one
full message block. The input to the inner evaluation of H is the padded string k1 ∥M ∥PBin,
which is broken into ℓ-bit blocks as shown. The output of the inner evaluation of H is the n-bit
string t. The input to the outer evaluation of H is the padded string k2 ∥t ∥PBout. We shall
assume that n is signiﬁcantly smaller than ℓ, so that t∥PBout is a single ℓ-bit block, as shown in
the ﬁgure.
We now state the pseudorandomness assumptions we need. We deﬁne the following two PRFs
hbot and htop derived from h:
hbot(k,m) := h(k,m) and htop(k,m) := h(m,k). (8.6)
For the PRF hbot, the PRF key k is viewed as the ﬁrst input to h, i.e., the n-bit chaining variable
input, which is the bottom input to the h-boxes in Fig. 8.9. For the PRF htop, the PRF key k
is viewed as the second input to h, i.e., the ℓ-bit message block input, which is the top input to
the h-boxes in the ﬁgure. To make the ﬁgure easier to understand, we have decorated the h-box
inputs with a > symbol, which indicates which input is to be viewed as a PRF key. Indeed, the
reader will observe that we will treat the two evaluations of hthat appear within the dotted boxes
as evaluations of the PRF htop, so that the values labeled k′
1 and k′
2 in the ﬁgure are computed
as k′
1 ←htop(k1,IV) and k′
2 ←htop(k2,IV). All of the other evaluations of h in the ﬁgure will be
treated as evaluations of hbot.
308
k1
hIV k′
1
m1
h
ms ∥PBin
h t
···
k2
hIV k′
2
t ∥PBout
h
Figure 8.9: The two-key nest
Our assumption will be that hbot and htop are both secure PRFs. Later, we will use the ideal ci-
pher model to justify this assumption for the Davies-Meyer compression function (see Section 8.7.3).
We will now sketch a proof of the following result:
If hbot and htop are secure PRFs, then so is the two-key nest.
The ﬁrst observation is that the keys k1 and k2 are only used to derive k′
1 and k′
2 as k′
1 =
htop(k1,IV) and k′
2 = htop(k2,IV). The assumption that htop is a secure PRF means that in the
PRF attack game, we can eﬀectively replace k′
1 and k′
2 by truly random n-bit strings. The resulting
construction is drawn in Fig. 8.10. All we have done here is to throw away all of the elements in
Fig. 8.9 that are within the dotted boxes. The function in this new construction takes as input
the two keys k′
1 and k′
2 and a message M. By the above observations, it suﬃces to prove that the
construction in Fig. 8.10 is a secure PRF.
Hopefully (without reading the caption), the reader will recognize the construction in Fig. 8.10
as none other than NMAC applied to hbot, which we introduced in Section 6.5.1 (in particular,
take a look at Fig. 6.5b). Actually, the construction in Fig. 8.10 is a bit-wise version of NMAC,
obtained from the block-wise version via padding (as discussed in Section 6.8). Thus, security for
the two-key nest now follows directly from the NMAC security theorem (Theorem 6.7) and the
assumption that hbot is a secure PRF.
8.7.2 The HMAC standard
The HMAC standard is exactly the same as the two-key nest (Fig. 8.9), but with one important
diﬀerence: the keys k1 and k2 are not independent, but rather, are derived in a somewhat ad hoc
way from a single key k.
To describe this in more detail, we ﬁrst observe that HMAC itself is somewhat byte oriented, so
all strings are byte strings. Message blocks for the underlying Merkle-Damg˚ ard hash are assumed
to be B bytes (rather than ℓ bits). A key k for HMAC is a byte string of arbitrary length. To
309
m1
hk′
1
ms ∥PBin
h t
···
t ∥PBout
hk′
2
Figure 8.10: A bit-wise version of NMAC
derive the keys k1 and k2, which are byte strings of length B, we ﬁrst make k exactly B bytes long:
if the length of k is less than or equal to B, we pad it out with zero bytes; otherwise, we replace it
with H(k) padded with zero bytes. Then we compute
k1 ←k⊕ipad and k2 ←k⊕opad,
where ipad and opad (“i” and “o” stand for “inner” and “outer”) are B-byte constant strings,
deﬁned as follows:
ipad = the byte 0x36 repeated B times
opad = the byte 0x5C repeated B times
HMAC implemented using a hash function H is denoted HMAC-H. The most common HMACs
used in practice are HMAC-SHA1 and HMAC-SHA256. The HMAC standard also allows the output
of HMAC to be truncated. For example, when truncating the output of SHA1 to 80 bits, the HMAC
function is denoted HMAC-SHA1-80. Implementations of TLS 1.0, for example, are required to
support HMAC-SHA1-96.
Security of HMAC. Since the keys k′
1,k′
2 are related — their XOR is equal to opad ⊕ipad —
the security proof we gave for the two-key nest no longer applies: under the stated assumptions,
we cannot justify the claim that the derived keys k′
1,k′
2 are indistinguishable from random. One
solution is to make a stronger assumption about the compression function h– one needs to assume
that htop remains a PRF under a related key attack (as deﬁned by Bellare and Kohno [12]). If his
itself a Davies-Meyer compression function, then this stronger assumption can be justiﬁed in the
ideal cipher model.
8.7.3 Davies-Meyer is a secure PRF in the ideal cipher model
It remains to justify our assumption that the PRFs hbot and htop derived from hin (8.6) are secure.
Suppose the compression function h is a Davies-Meyer function, that is h(x,y) := E(y, x) ⊕x for
some block cipher E= (E,D). Then
310
• hbot(k,m) := h(k,m) = E(m,k) ⊕k is a PRF deﬁned over( X,K,X), and
• htop(k,m) := h(m,k) = E(k,m) ⊕m is a PRF deﬁned over( K,X,X)
When E is a secure block cipher, the fact that htop is a secure PRF is trivial (see Exercise 4.1
part (c)). The fact that hbot is a secure PRF is a bit surprising — the message m given as input
to hbot is used as the key for E. But m is chosen by the adversary and hence E is evaluated with
a key that is completely under the control of the adversary. As a result, even though E is a secure
block cipher, there is no security guarantee for hbot. Nevertheless, we can prove that hbot is a
secure PRF, but this requires the ideal cipher model. Just assuming that Eis a secure block cipher
is insuﬃcient.
If necessary, the reader should review the basic concepts regarding the ideal cipher model,
which was introduced in Section 4.7. We also used the ideal cipher model earlier in this chapter
(see Section 8.5.3).
In the ideal cipher model, we heuristically model a block cipher E= (E,D) deﬁned over (K,X)
as a family of random permutations {Πk }k ∈K. We adapt the PRF Attack Game 4.2 to work in
the ideal cipher model. The challenger, in addition to answering standard queries, also answers Π-
queries and Π−1-queries: a Π-query is a pair (k ,a) to which the challenger responds withb := Πk (a);
a Π−1-query is a pair ( k ,b) to which is the challenger responds with a := Π−1
k (b). For a standard
query m, the challenger responds with v:= f(m): in Experiment 0 of the attack game, f is F(k,·),
where F is a PRF and k is a randomly chosen key; in Experiment 1, f is a truly random function.
Moreover, in Experiment 0, F is evaluated using the random permutations in the role of E and D
used in the construction of F. For our PRF hbot(k,m) = E(m,k) ⊕k= Πm(k) ⊕k.
For an adversary A, we deﬁne PRFicadv[A,F] to be the advantage in the modiﬁed PRF attack
game, and security in the ideal cipher model means that this advantage is negligible for all eﬃcient
adversaries.
Theorem 8.5 (Security of hbot). Let E= (E,D) be a block cipher over (K,X), where |X| is
large. Then hbot(k,m) := E(m,k) ⊕k is a secure PRF in the ideal cipher model.
In particular, for every PRF adversary Aattacking hbot and making at most a total of Qic ideal
cipher queries, we have
PRFicadv[A,hbot] ≤2Qic
|X| .
The bound in the theorem is fairly tight, as brute-force key search gets very close to this bound.
Proof. The proof will mirror the analysis of the Evan-Mansour/EX constructions (see Theorem 4.14
in Section 4.7.4), and in particular, will make use of the Domain Separation Lemma (see Theo-
rem 4.15, also in Section 4.7.4).
Let Abe an adversary as in the statement of the theorem. Let pb be the probability that A
outputs 1 in Experiment b of Attack Game 4.2, for b= 0,1. So by deﬁnition we have
PRFicadv[A,hbot] = |p0 −p1|. (8.7)
We shall prove the theorem using a sequence of two games, applying the Domain Separation
Lemma.
Game 0. The game will correspond to Experiment 0 of the PRF attack game in the ideal cipher
model. We can write the logic of the challenger as follows:
311
Initialize:
for each k ∈K, set Πk ←R Perms[X]
k←R X
standard hbot-query m:
1. c←Πm(k)
2. v←c⊕k
3. return v
The challenger in Game 0 processes ideal cipher queries exactly as in Game 0 of the proof of
Theorem 4.14:
ideal cipher Π-query k ,a:
1. b ←Πk (a)
2. return b
ideal cipher Π−1-query k ,b:
1. a ←Π−1
k (b)
2. return a
Let W0 be the event that Aoutputs 1 at the end of Game 0. It should be clear from construction
that
Pr[W0] = p0. (8.8)
Game 1. Just as in the proof of Theorem 4.14, we declare “by ﬁat” that standard queries and
ideal cipher queries are processed using independent random permutations. In detail (changes from
Game 0 are highlighted):
Initialize:
for each k ∈K, set Πstd,k ←R Perms[X] and Πic,k ←R Perms[X]
k←R X
standard hbot-query m:
1. c←Πstd,m(k) / / add k to sampled domain of Πstd,m, add c to sampled range of Πstd,m
2. v←c⊕k
3. return v
The challenger in Game 1 processes ideal cipher queries exactly as in Game 1 of the proof of
Theorem 4.14:
ideal cipher Π-query k ,a:
1. b ←Πic,k (a) / / add a to sampled domain of Πic,k , add b to sampled range of Πic,k
2. return b
312
ideal cipher Π−1-query k ,b:
1. a ←Π−1
ic,k (b) / / add a to sampled domain of Πic,k , add b to sampled range of Πic,k
2. return a
Let W1 be the event that Aoutputs 1 at the end of Game 1. Consider an input/output pair
(m,v) for a standard query in Game 1. Observe that k is the only item ever added to the sampled
domain of Πstd,m(k), and c= v⊕k is the only item ever added to the sampled range of Π std,m(k).
In particular, c is generated at random and k remains perfectly hidden (i.e., is independent of the
adversary’s view).
Thus, from the adversary’s point of view, the standard queries behave identically to a random
function, and the ideal cipher queries behave like ideal cipher queries for an independent ideal
cipher. In particular, we have
Pr[W1] = p1. (8.9)
Finally, we use the Domain Separation Lemma to analyze |Pr[W0] −Pr[W1]|. The domain
separation failure event Z is the event that in Game 1, the sampled domain of one of the Π std,m’s
overlaps with the sampled domain of one of the Π ic,k ’s, or the sampled range of one of the Π std,m’s
overlaps with the sampled range of one of the Π ic,k ’s. The Domain Separation Lemma tells us that
|Pr[W0] −Pr[W1]|≤ Pr[Z]. (8.10)
If Z occurs, then for some input/output triple ( k ,a,b) corresponding to an ideal cipher query,
k = m was the input to a standard query with output v, and either
(i) a = k, or
(ii) b = v⊕k.
For any ﬁxed triple ( k ,a,b), by the independence of k, conditions (i) and (ii) each hold with
probability 1/|X|, and so by the union bound
Pr[Z] ≤2Qic
|X|. (8.11)
The theorem now follows from (8.7)–(8.11). 2
8.8 The Sponge Construction and SHA3
For many years, essentially all collision resistant hash functions were based on the Merkle-Damg˚ ard
paradigm. Recently, however, an alternative paradigm has emerged, called the sponge construc-
tion. Like Merkle-Damg˚ ard, it is a simple iterative construction built from a more primitive
function; however, instead of a compression function h : {0,1}n+ℓ → {0,1}n, a permutation
π : {0,1}n →{0,1}n is used. We stress that unlike a block cipher, the function π has no key.
There are two other high-level diﬀerences between the sponge and Merkle-Damg˚ ard that we should
point out:
• On the negative side, it is not known how to reduce the collision resistance of the sponge
to a concrete security property of π. The only known analysis of the sponge is in the ideal
permutation model, where we (heuristically) model π as a truly random permutation Π.
313
• On the positive side, the sponge is designed to be used ﬂexibly and securely in a variety of
applications where collision resistance is not the main property we need. For example, in
Section 8.7, we looked at several possible ways to convert a hash function H into a PRF
F. We saw, in particular, that the intuitive idea of simply prepending the key, deﬁning
Fpre(k,M) := H(k ∥M), does not work when H is instantiated with a Merkle-Damg˚ ard
hash. The sponge avoids these problems: it allows one to hash variable length inputs to
variable length outputs, and if we model πas a random permutation, then one can argue that
for all intents and purposes, the sponge is a random function (we will discuss this in more
detail in Section 8.10). In particular, the construction Fpre is secure when H is instantiated
with a sponge hash.
A new hash standard, called SHA3, is based on the sponge construction. After giving a de-
scription and analysis of the general sponge construction, we discuss some of the particulars of
SHA3.
8.8.1 The sponge construction
We now describe the sponge construction. In addition to specifying a permutation π : {0,1}n →
{0,1}n, we need to specify two positive integers r and c such that n = r+ c. The number r is
called the rate of the sponge: larger rate values lead to faster evaluation. The number c is called
the capacity of the sponge: larger capacity values lead to better security bounds. Thus, diﬀerent
choices of r and c lead to diﬀerent speed/security trade-oﬀs.
The sponge allows variable length inputs. To hash a long messageM ∈{0,1}≤L, we ﬁrst append
a padding string to M to make its length a multiple of r, and then break the padded M into a
sequence of r-bit blocks m1,...,m s. The requirements of the padding procedure are minimal: it
just needs to be injective. Just adding a string of the form 10 ∗suﬃces, although in SHA3 a pad of
the form 10∗1 is used: this latter padding has the eﬀect of encoding the rate in the last block and
helps to analyze security in applications that use the same sponge with diﬀerent rates; however, we
will not explore these use cases here. Note that an entire dummy block may need to be added if
the length of M is already at or near a multiple of r.
The sponge allows variable length outputs. So in addition to a message M ∈{0,1}≤L as above,
it takes as input a positive integer v, which speciﬁes the number of output bits.
Here is how the sponge works:
314
Absorbing phase Squeezing phase
0
0
m1
c bits
r bits
f
m2
f
m3
f
m4
f
z1
f
z2
f
z3
Figure 8.11: The sponge construction
Input: message M ∈{0,1}≤L and desired output length v >0
Output: a tag h∈{0,1}v
/ / Absorbing stage
pad M and break into r-bit blocks m1,...,m s ∈{0,1}r
h←0n
for i←1 to s do
m′
i ←mi ∥0c ∈{0,1}n
h←π(h⊕m′
i)
/ / Squeezing stage
z1 ←h[0 . .r−1]
for i←2 to ⌈v/r⌉do
h←π(h)
zi ←h[0 . .r−1]
output
(
z1 ∥···∥ z⌈v/r⌉
)
[0 . .v−1]
The diagram in Fig. 8.11 may help to clarify the algorithm. The sponge runs in two stages:
the “absorbing stage” where the message blocks get “mixed in” to a chaining variable h, and a
“squeezing stage” where the output is “pulled out” of the chaining variable. Note that input blocks
and output blocks are r-bit strings, so that the remaining cbits of the chaining variable cannot be
directly tampered with or seen by an attacker. This is what gives the sponge its security, and is
the reason why cmust be large. Indeed, if the sponge has small capacity, it is easy to ﬁnd collisions
(see Exercise 8.21).
In the SHA3 standard, the sponge construction is intended to be used as a collision resistant
hash, and the output length is ﬁxed to a value v≤r, and so the squeezing stage simply outputs the
ﬁrst vbits of the output hof the absorbing stage. We will now prove that this version of the sponge
is collision resistant in the ideal permutation model, assuming 2 c and 2v are both super-poly.
Theorem 8.6. Let H be the hash function obtained from a permutation π: {0,1}n →{0,1}n, with
capacity c, rate r (so n= r+ c), and output length v ≤r. In the ideal permutation model, where
π is modeled as a random permutation Π, the hash function H is collision resistant, assuming 2v
and 2c are super-poly.
315
In particular, for every collision ﬁnding adversary A, if the number of ideal-permutation queries
plus the number of r-bit blocks in the output messages of Ais bounded by q, then
CRicadv[A,H] ≤q(q−1)
2v + q(q+ 1)
2c .
Proof. As in the proof of Theorem 8.4, we assume our collision-ﬁnding adversary is “reasonable”,
in the sense that it makes ideal permutation queries corresponding to its output. We can easily
convert an arbitrary adversary into a reasonable one by forcing the adversary to evaluate the hash
function on its output messages if it has not done so already. As we have deﬁned it, q will be an
upper bound on the total number of ideal permutation queries made by our reasonable adversary.
So from now on, we assume a reasonable adversary Athat makes at most q queries, and we bound
the probability that suchAﬁnds anything during its queries that can be “assembled” into a collision
(we make this more precise below).
We also assume that no queries are redundant . This means that if the adversary makes a Π-
query on a yielding b = Π(a), then the adversary never makes a Π −1-query on b, and never makes
another Π-query on a; similarly, if the adversary makes a Π−1-query on b yielding a = Π−1(b), then
the adversary never makes a Π-query on a, and never makes another Π −1-query on b. Of course,
there is no need for the adversary to make such redundant queries, which is why we exclude them;
moreover, doing so greatly simpliﬁes the “bookkeeping” in the proof.
It helps to visualize the adversary’s attack as building up a directed graph G. The nodes in G
consist of the set of all 2 n bit strings of length n. The graph Gstarts out with no edges, and every
query that Amakes adds an edge to the graph: an edge a →b is added if Amakes a Π-query
on a that yields b or a Π −1-query on b that yields a. Notice that if we have an edge a →b, then
Π(a) = b, regardless of whether that edge was added via a Π-query or a Π −1-query. We say that
an edge added via a Π-query is a forward edge, and one added via a Π −1-query is a back edge.
Note that the assumption that the adversary makes no redundant queries means that an edge
gets added only once to the graph, and its classiﬁcation is uniquely determined by the type of query
that added the edge.
We next deﬁne a special type of path in the graph that corresponds to sponge evaluation. For
an n-bit string z, let R(z) be the ﬁrst r bits of z and C(z) be the last cbits of z. We refer to R(z)
as the R-part of z and C(z) as the C-part of z. For s≥1, a C-path of length s is a sequence
of 2s nodes
a0,b1,a1,b2,a2,..., bs−1,as−1,bs,
where
• C(a0) = 0c and for i= 1,...,s −1, we have C(bi) = C(ai), and
• G contains edges ai−1 →bi for i= 1,...,s .
For such a path p, the message of p is deﬁned as ( m0,...,m s−1), where
m0 := R(a0) and mi := R(bi) ⊕R(ai) for i= 1,...,s −1.
and the result of p is deﬁned to be ms := R(bs). Such a C-path p corresponds to evaluating the
sponge at the message ( m0,...,m s−1) and obtaining the (untruncated) output ms. Let us write
such a path as
m0|a0 −→b1|m1|a1 −→···−→ bs−2|ms−2|as−2 −→bs−1|ms−1|as−1 −→bs|ms. (8.12)
The following diagram illustrates a C-path of length 3.
316
a0 −−−−−−→b1
m0 = R(a0) a1 −−−−−−→b2
0c = C(a0) m1 = R(b1) ⊕R(a1) a2 −−−−−−→b3
C(b1) = C(a1) m2 = R(b2) ⊕R(a2) m3 = R(b3)
C(b2) = C(a2)
The path has message (m0,m1,m2) and result m3. Using the notation in (8.12), we write this path
as
m0|a0 −→b1|m1|a1 −→b2|m2|a2 −→b3|m3.
We can now state what a collision looks like in terms of the graph G. It is a pair of C-paths
on diﬀerent messages but whose results agree on their ﬁrst v bits (recall v≤r). Let us call such a
pair of paths colliding.
To analyze the probability of ﬁnding a pair of colliding paths, it will be convenient to deﬁne
another notion. Let pand p′be two C-paths on diﬀerent messages whose ﬁnal edges are as−1 →bs
and a′
t−1 →b′
t. Let us call such a pair of paths problematic if
(i) as−1 = a′
t−1, or
(ii) one of the edges in p or p′are back edges.
Let W be the event that Aﬁnds a pair of colliding paths. Let Z be the event that Aﬁnds a
pair of problematic paths. Then we have
Pr[W] ≤Pr[Z] + Pr[W and not Z]. (8.13)
First, we bound Pr[ W and not Z]. For an n-bit string z, let V(z) be the ﬁrst v bits of z, and
we refer to V(z) as the V-part of z. Suppose Ais able to ﬁnd a pair of colliding paths that is not
problematic. By deﬁnition, the ﬁnal edges on these two paths correspond to Π-queries on distinct
inputs that yield outputs whose V-parts agree. That is, if W and not Z occurs, then it must be
the case that at some point Aissued two Π-queries on distinct inputs a and a′, yielding outputs b
and b′such that V(b) = V(b′). We can use the union bound: for each pair of indices i<j , let Xij
be the event that the ith query is a Π-query on some value, say a, yielding b = Π(a), and the j-th
query is also a Π-query on some other value a′̸= a, yielding b′= Π(a′) such that V(b) = V(b′). If
we ﬁx i and j, ﬁx the coins of A, and ﬁx the outputs of all queries made prior to the jth query,
then the values a, b, and a′are all ﬁxed, but the value b′is uniformly distributed over a set of size
at least 2 n −j+ 1. To get V(b) = V(b′), the value of b′ must be equal to one of the 2 n−v strings
whose ﬁrst v bits agree with that of b, and so we have
Pr[Xij] ≤ 2n−v
2n −j+ 1.
A simple calculation like that done in (8.5) in the proof of Theorem 8.4 yields
Pr[W and not Z] ≤q(q−1)
2v . (8.14)
Second, we bound Pr[Z], the probability that Aﬁnds a pair of problematic paths. The technical
heart of the analysis is the following:
317
Main Claim: If Z occurs, then one of the following occurs:
(E1) some query yields an output whose C-part is 0c, or
(E2) two diﬀerent queries yield outputs whose C-parts are equal.
Just to be clear, (E1) means Amade a query of the form:
(i) a Π −1-query on some value b such that C(Π−1(b)) = 0c, or (ii) a Π-query on some
value a such that C(Π(a)) = 0c,
and (E2) means Amade pair of queries of the form:
(i) a Π-query on some value a and a Π−1-query on some value b, such that C(Π(a)) =
C(Π−1(b)), or (ii) Π-queries on two distinct values a and a′ such that C(Π(a)) =
C(Π(a′)).
First, suppose Ais able to ﬁnd a problematic pair of paths, and one of the paths contain a back
edge. So at the end of the execution, there exists a C-path containing one or more back edges. Let
pbe such a path of shortest length, and write it as in (8.12). We observe that the last edge in pis
a back edge, and all other edges (if any) in pare forward edges. Indeed, if this is not the case, then
we can delete this edge from p, obtaining a shorter C-path containing a back edge, contradicting
the assumption that p is a shortest path of this type. From this observation, we see that either:
• s= 1 and (E1) occurs with the Π −1-query on b1, or
• s> 1 and (E2) occurs with the Π −1-query on bs and the Π-query on as−2.
Second, suppose Ais able to ﬁnd a problematic pair of paths, neither of which contains any
back edges. Let us call these paths p and p′. The argument in this case somewhat resembles the
“backwards walk” in the Merkle-Damg˚ ard analysis. Writep as in (8.12) and write p′as
m′
0|a′
0 −→b′
1|m′
1|a′
1 −→···−→ b′
t−2|m′
t−2|a′
t−2 −→b′
t−1|m′
t−1|a′
t−1 −→b′
t|m′
t.
We are assuming that ( m0,...,m s−1) ̸= (m′
0,...,m ′
t−1) but as−1 = a′
t−1, and that none of these
edges are back edges. Let us also assume that we choose the paths so that they are shortest, in the
sense that s+tis minimal among all C-paths of this type. Also, let us assume that s≤t(swapping
if necessary). There are a few cases:
1. s= 1 and t= 1. This case is impossible, since in this case the paths are just m0|a0 →b1|m1
and m′
0|a′
0 →b′
1|m′
1, and we cannot have both m0 ̸= m′
0 and a0 = a′
0.
2. s= 1 and t≥2. In this case, we have a0 = b′
t−1, and so (E1) occurs on the Π-query on a′
t−2.
3. s≥2 and t≥2. Consider the penultimate edges, which are forward edges:
as−2 →bs−1|ms−1|as−1
and
a′
t−2 →b′
t−1|m′
t−1|a′
t−1.
We are assuming as−1 = a′
t−1. Therefore, the C-parts of bs−1 and b′
t−1 are equal and their
R-parts diﬀer by ms−1 ⊕m′
t−1. There are two subcases:
318
(a) ms−1 = m′
t−1. We argue that this case is impossible. Indeed, in this case, we have
bs−1 = b′
t−1, and therefore as−2 = a′
t−2, while the truncated messages ( m0,...,m s−2)
and (m′
0,...,m ′
t−2) diﬀer. Thus, we can simply throw away the last edge in each of the
two paths, obtaining a shorter pair of paths that contradicts the minimality of s+ t.
(b) ms−1 ̸= m′
t−1. In this case, we know: the C-parts of bs−1 and b′
t−1 are the same, but
their R-parts diﬀer, and therefore, as−2 ̸= a′
t−2. Thus, (E2) occurs on the Π-queries on
as−2 and a′
t−2.
That proves the Main Claim. We can now turn to the problem of bounding the probability
that either (E1) or (E2) occurs. This is really just the same type of calculation we did at least
twice already, once above in obtaining (8.14), and earlier in the proof of Theorem 8.4. The only
diﬀerence from (8.14) is that we are now counting collisions on the C-parts, and we have a new
type of “collision” to count, namely, “hitting 0 c” as in (E1). We leave it to the reader to verify:
Pr[Z] ≤q(q+ 1)
2c . (8.15)
The theorem now follows from (8.13)–(8.15). 2
8.8.2 Case study: SHA3, SHAKE128, and SHAKE256
The NIST standard for SHA3 speciﬁes a family of sponge-based hash functions. At the heart
of these hash functions is a permutation called Keccak, which maps 1600-bit strings to 1600-bit
strings. We denote by Keccak[ c] the sponge derived from Keccak with capacity c, and using the
10∗1 padding rule. This is a function that takes two inputs: a message m and output length v.
Here, the input m is an arbitrary bit string and the output of Keccak[ c](m,v) is a v-bit string.
We will not describe the internal workings of the Keccak permutation; they can be found in
the SHA3 standard. We just describe the diﬀerent parameter choices that are standardized. The
standard speciﬁes four hash functions whose output lengths are ﬁxed, and two hash functions with
variable length outputs.
Here are the four ﬁxed-length output hash functions:
• SHA3-224(m) = Keccak[448](m∥01,224);
• SHA3-256(m) = Keccak[512](m∥01,256);
• SHA3-384(m) = Keccak[768](m∥01,384);
• SHA3-512(m) = Keccak[1024](m∥01,512).
Note the two extra padding bits that are appended to the message. Note that in each case, the
capacity c is equal to twice the output length v. Thus, as the output length grows, the security
provided by the capacity grows as well, and the rate — and, therefore, the hashing speed —
decreases.
Here are the two variable-length output hash functions:
• SHAKE128(m,v) = Keccak[256](m∥1111,v);
• SHAKE256(m,v) = Keccak[512](m∥1111,v).
Note the four extra padding bits that are appended to the message. The only diﬀerence between
these two is the capacity size, which aﬀects the speed and security. The various padding bits and
the 10∗1 padding rule ensure that these six functions behave independently.
319
8.9 Merkle trees: proving properties of a hashed list
Now that we understand how to construct collision resistant functions, let’s see more of their
applications to data integrity. Consider a large executable ﬁle, stored on disk as a list of short ℓ-bit
blocks x1,...,x n. Before the operating system loads and runs this executable, it needs to verify
that its contents have not been altered. At the beginning of the chapter we discussed how one can
store a short hash of the entire ﬁle in read-only storage 3. Every time the ﬁle is run, the system
ﬁrst recomputes the ﬁle hash, and veriﬁes that it matches the value in storage. We explained that
a collision resistant hash ensures that the adversary cannot tamper with the ﬁle without being
detected. The problem is that for a large ﬁle, computing the hash of the entire ﬁle can take quite
a while, and this will greatly increase the time to launch the executable.
Can we do better? To start running the executable, the system only needs to verify the ﬁrst
block x1. When execution moves to some other block, the system only needs to verify that block,
and so on. In other words, instead of verifying the entire ﬁle all at once, it would be much better if
the system could verify each block independently, just before that block is loaded. One option is to
compute the hash of every block x1,...,x n, and store the resulting n hashes in read-only storage.
This makes it easy to verify every block by itself, but also takes up a lot of read-only space to store
the n hashes. Fortunately, there is a much better solution.
Merkle trees. To restate the problem, we have a vector of n items T := (x1,...,x n) ∈Xn, and
we wish to compute a short hash y of this vector. Later, we are presented with the hash y and a
pair (i,x) where 1 ≤i≤n. We need to validate that x is the item at position i in T.
A solution to this problem makes use of a clever data structure called a Merkle tree, shown
in Fig. 8.12. The resulting hash function H is called a Merkle tree hash.
The Merkle tree hash uses a collision resistant hash function h, such as SHA256, that outputs
values in a set Y. The input to h is either a single element in X, or a pair of elements in Y. The
Merkle tree hash H, derived from h, is deﬁned over ( Xn,Y). For simplicity, let’s assume that n
is a power of two (if not, one can pad with dummy elements to the closest power of two). The
Merkle tree hash works as in Fig. 8.12: to hash ( x1,...,x n) ∈Xn, ﬁrst apply h to each of the n
input elements to get ( y1,...,y n) ∈Yn. Then build a hash tree from these elements, as shown in
the ﬁgure. More precisely, the hash function H is deﬁned as follows:
input: x1,...,x n ∈X, where n is a power of 2
output: y∈Y
for i= 1 to n: yi ←h(xi) / / initialize y1,...,y n
for i= 1 to n−1: yi+n ←h
(
y2i−1, y2i
)
/ / compute tree nodes yn+1,...,y 2n−1
output y2n−1 ∈Y
In Exercise 8.9 we show that a closely related hash function, designed for variable length inputs,
is collision resistant, assuming h is collision resistant.
Proving membership. The remarkable thing about the Merkle tree hash is that given a hash
value y := H(x1,...,x n), it is quite easy to prove that an x ∈X is the element at a particular
3Recall that read-only storage can be read, but not modiﬁed, by an adversary. It can be implemented as a separate
system that provides the data to anyone who asks for it. Or, more simply, it can be implemented by signing the data
using a digital signature scheme, as discussed in Chapter 13, and storing the signing key oﬄine.
320
x1
h
x2
h
x3
h
x4
h
x5
h
x6
h
x7
h
x8
h
h
y1 y2
h
y3 y4
h
y5 y6
h
y7 y8
h
y9 y10
h
y11 y12
h
y13 y14
y15
x3
Figure 8.12: A Merkle tree with eight leaves. The values y4,y9,y14 prove authenticity of x3.
position in T := (x1,...,x n). For example, to prove that x = x3 in Fig. 8.12, one provides the
intermediate hashes π:= (y4, y9, y14), shaded in the ﬁgure. The veriﬁer can then compute
ˆy3 ←h(x), ˆy10 ←h(ˆy3,y4), ˆy13 ←h(y9,ˆy10), ˆy15 ←h(ˆy13,y14), (8.16)
and accept that x= x3 if y= ˆy15. This π is called a Merkle proof that x is in position 3 of T.
More generally, to prove that an element x is the element in position i of T := (x1,...,x n),
one outputs as the proof π all the intermediate hashes that are the siblings of nodes on the path
from the leaf number i to the root of the tree. This proof π contains exactly log2 n elements in Y.
The veriﬁer can use the quantities provided in π to re-derive the Merkle hash of T. It does so by
computing hashes, starting at leaf number i, and working its way up to the root, as in (8.16). It
accepts that x as authentic (i.e., that x= xi) if the ﬁnal computed Merkle hash matches the hash
value y stored in read-only memory.
We will show in Theorem 8.8 below that, if his collision resistant, an adversary cannot exhibit
an x and an i, along with a proof π′, that incorrectly convinces the veriﬁer that x is in position i
of T.
Consider again our executable stored on disk as a sequence of blocks x1,...,x n, and suppose
that the system has y:= H(x1,...,x n) in read-only storage. We can store the 2 n−1 hash values
in the Merkle tree, denoted y1,...,y 2n−1, along with the executable. Then, to validate a block,
the system will quickly locate the log 2 nhash values that make up the Merkle proof for that block,
compute the Merkle hash by computing log2 nhashes, and compare the result to the stored value y.
In practice, suppose blocks are 4KB each. Then even for an executable of 2 16 blocks, we are adding
at most two hash values per block (2 n−1 hash values in total), which is only 64 bytes per block.
Validating a block is done by computing 16 hashes.
There are other solutions to this problem. For example, the system could store a MAC tag
next to every block, and verify the tag before executing the block. However, this would require the
system to manage the secret MAC key, and ensure that it is never read by the adversary. While
this may be reasonable in some settings, the Merkle tree approach provides an eﬃcient solution
that requires no online secret keys.
321
Proving membership of multiple elements. Suppose again that y:= H(x1,...,x n) is stored
in read-only storage, and let T := ( x1,...,x n). Let L ⊆X be a set of elements. We wish to
convince the veriﬁer that all the elements in Lare in T. We could provide a Merkle proof for every
element in L, giving a total proof size of |L|log2 n elements in Y. However, many of these Merkle
proofs overlap, and we can shrink the overall proof by removing repeated elements. The following
theorem bounds the worst-case proof size. We write L⊆T to denote the fact that all the elements
in L are contained in T.
Theorem 8.7. Let T ⊆X be a set of size n, where n is a power of two. For every 1 ≤r ≤n,
and a set L ⊆T of size r, the Merkle proof that all the elements of L are in T contains at most
r·log2(n/r) elements in Y.
Proof. The theorem is a direct corollary of Theorem 5.8. Let S := T \L, so that |S|= n−r. It
is not diﬃcult to see that the set of hash values in the Merkle proof for L are precisely those that
correspond to nodes in cover( S). The bound on |cover(S)|provided in Theorem 5.8 proves the
theorem. 2
Proving non-membership. Let’s look at another application for Merkle trees. Consider a credit
card processing center that maintains a list T of revoked credit card numbers T := (x1,...,x n) ∈
Xn. The list T is sent to untrusted cache servers all over the world, and every merchant is sent the
short Merkle tree hash y:= H(x1,...,x n). This hash yis assumed to be computed correctly by the
center. When a merchant needs to process a customer’s credit card x, it sends xto the closest cache
server to test if x is revoked (i.e., test if x is in T). If so, the cache server responds with a Merkle
proof that x is in T, and this convinces the merchant to reject the transaction. Security of the
Merkle tree scheme implies that a malicious cache server cannot fool the merchant into believing
that an active credit card is revoked. More generally, Merkle trees let us replicate a data set T
across untrusted cache servers, so that no cache server can lie about membership in the set.
For the credit card application, proving membership in T is not enough. The cache server
must also be able to convince the merchant that a credit card x is not in T (i.e., not revoked).
Surprisingly, a Merkle tree can also be used to prove set non-membership, but to do so we must
ﬁrst slightly modify the Merkle tree construction.
Suppose that the elements in T are integers, so that X⊆ Z. In the modiﬁed tree hash we ﬁrst
sort the leaves of the tree, so that x1 < x2 < ··· < xn, as shown in Fig. 8.13. We then compute
the tree hash y:= H(x1,...,x n) as before. We call this the sorted Merkle tree hash .
Now, given some x ̸∈T, we wish to produce a proof that x is not in T. The veriﬁer only has
the sorted tree hash y. To produce the proof, the prover ﬁrst locates the two adjacent leaves xi and
xi+1 in T that bracket x, namely xi <x<x i+1. For simplicity, let’s assume that x1 <x<x n, so
that the required xi and xi+1 always exist. Next, the prover provides a Merkle proof that xi is in
position iin T, and that xi+1 is in position i+ 1 in T. The veriﬁer can check that these two leaves
are adjacent, and that xi <x<x i+1, and this proves that x is not in T. Indeed, if x were in T, it
must occupy a leaf between xi and xi+1, but because xi and xi+1 are adjacent leaves, this is not
possible.
Fig. 8.13 gives an example proof that a value xin the interval (x4,x5) is not in T. The proof is
the set of hashes ( y3, y6, y9, y12) along with the data items x4,x5. The veriﬁes checks the Merkle
proofs to convince itself that x4 and x5 are in T, and that they are adjacent in the tree. It then
checks that x4 <x<x 5, and this proves that xis not in the tree. We see that in the worst case, a
322
x1
h
x2<
h
x3<
h
x4<
h
x5< x <
h
x6<
h
x7<
h
x8<
h
h
y1 y2
h
y3 y4
h
y5 y6
h
y7 y8
h
y9 y10
h
y11 y12
h
y13 y14
y15
x4 x5
Figure 8.13: The sorted tree hash. The shaded elements prove non-membership of x.
proof of non-membership contains 2 log2(n/2) elements in Y, plus two data items in X.
Security of the scheme is discussed in the next section. It shows that, when the underlying hash
function h is collision resistant, an adversary cannot convince the veriﬁer that an x ∈T is not a
member of T. In our example application, a malicious cache server cannot convince a merchant
that a revoked credit card is active.
8.9.1 Authenticated data structures
A Merkle tree is an example of a more abstract concept called an authenticated data structure. An
authenticated data structure is used to compute a short hash of a sequenceT := (x1,...,x n), so that
later one can prove properties of T with respect to this hash. Merkle trees let us prove membership
and non-membership. Other authenticated data structures support additional operations, such as
eﬃcient insertions and deletions, as discussed below.
We begin by deﬁning an authenticated data structure for set membership, and its security
property.
Deﬁnition 8.3. An authenticated data structure scheme D= (H,P,V ) deﬁned over (Xn,Y)
is a tuple of three eﬃcient deterministic algorithms:
• H is an algorithm that is invoked as y←H(T), where T := (x1,...,x n) ∈Xn and y∈Y.
• P is an algorithm that is invoked as π ←P(i,x,T ), where x ∈X and 1 ≤i ≤n. The
algorithm outputs a proof π that x= xi, where T := (x1,...,x n).
• V is an algorithm that is invoked as V(i,x,y,π ) and outputs accept or reject.
• We require that for all T := (x1,...,x n) ∈Xn, and all 1 ≤i≤n, we have that
V
(
i, xi, H(T), P(i,xi,T)
)
= accept
The Merkle tree scheme from the previous section can be easily cast as these three algorithms
(H,P,V ).
323
We next deﬁne security. We say that an adversary defeats the scheme if it can output a hash
value y∈Y and then fool the veriﬁer into accepting two diﬀerent elements x and x′in Xat some
position i.
Attack Game 8.2 (authenticated data structure security). For an authenticated data struc-
ture scheme D= (H,P,V ) deﬁned over ( Xn,Y), and a given adversary A, the attack game runs
as follows:
The adversary Aoutputs a y ∈Y, a position i ∈{1,...,n }, and two pairs ( x,π) and
(x′,π′) where x,x′∈X.
We say that Awins the game if x̸= x′and V
(
i, x, y, π
)
= V
(
i, x′, y, π′)
= accept. Deﬁne A’s
advantage with respect to D, denoted ADSadv[A,D], as the probability that Awins the game. 2
Deﬁnition 8.4. We say that an authenticated data structure scheme Dis secure if for all eﬃcient
adversaries A, the value ADSadv[A,D] is negligible.
Theorem 8.8. The Merkle hash tree scheme is a secure authenticated data structure scheme,
assuming the underlying hash function h is collision resistant.
Proof. The proof is essentially the same as the proof of Exercise 8.9. 2
One can similarly formulate a security deﬁnition for proving non-membership in a hashed data
set. We leave it as an instructive exercise to state the security deﬁnition, and prove that the sorted
Merkle tree is a secure scheme for proving non-membership, assuming that the underlying hash
function is collision resistant.
Updatable Merkle data structures. Let T be a data set of size n. One downside of sorted
Merkle hash trees is that if even a single element in the data set T is changed, that element may
need to move to a diﬀerent leaf, and the entire hash tree will need to be recomputed from scratch.
This can take O(n) hash computations. Other data structures provide the same functionality as
Merkle trees, but also support an eﬃcient update, requiring at most O(log n) hash calculations to
update an element. One example is a scheme based on a 2-3 tree [126], and another is a scheme
based on skip lists [76]. A common authenticated data structure, used in certain crypto currency
systems, is a hash tree based on the Patricia tree data structure.
8.10 Key derivation and the random oracle model
Although hash functions like SHA256 were initially designed to provide collision resistance, we have
already seen in Section 8.7 that practitioners are often tempted to use them to solve other problems.
Intuitively, hash functions like SHA256 are designed to “thoroughly scramble” their inputs, and
so this approach seems to make some sense. Indeed, in Section 8.7, we looked at the problem of
taking an unkeyed hash function and turning it into a keyed function that is a secure PRF, and
found that it was indeed possible to give a security analysis under reasonable assumptions.
In this section, we study another problem, called key derivation. Roughly speaking, the
problem is this: we start with some secret data, and we want to convert it into an n-bit string that
we can use as the key to some cryptographic primitive, like AES. Now, the secret data may be
random in some sense — at the very least, somewhat hard to guess — but it may not look anything
324
at all like a uniformly distributed, random, n-bit string. So how do we get from such a secret s
to a cryptographic key t? Hashing, of course. In practice, one takes a hash function H, such as
SHA256 (or, as we will ultimately recommend, some function built out of SHA256), and computes
t←H(s).
Along the way, we will also introduce the random oracle model, which is a heuristic tool that is
useful not only for analyzing the key derivation problem, but a host of other problems as well.
8.10.1 The key derivation problem
Let us look at the key derivation problem in more detail. Again, at a high level, the problem is to
convert some discreet data that is hard to guess into an n-bit string we can use directly as a key
to some standard cryptographic primitive, such as AES. The solution in all cases will be to hash
the secret to obtain the key. We begin with some motivating examples.
• The secret might be a password. While such a password might be somewhat hard to guess, it
could be dangerous to use such a password directly as an AES key. Even if the password were
uniformly distributed over a large dictionary (already a suspect assumption), the distribution
of its encoding as a bit string is certainly not. It could very well be that a signiﬁcant fraction
of passwords correspond to “weak keys” for AES that make it vulnerable to attack. Recall
that AES was designed to be used with a random bit string as the key, so how it behaves on
passwords is another matter entirely.
• The secret could be the log of various types of system events on a running computer (e.g., the
time of various interrupts such as those caused by key presses or mouse movements). Again,
it might be diﬃcult for an attacker who is outside the computer system to accurately predict
the contents of such a log. However, using the log directly as an AES key is problematic: it
is likely far too long, and far from uniformly distributed.
• The secret could be a cryptographic key which has been partially compromised. Imagine that
a user has a 128-bit key, but that 64 of the bits have been leaked to the adversary. The key
is still fairly diﬃcult to guess, but it is still not uniformly distributed from the adversary’s
point of view, and so should not be used directly as an AES key.
• Later, we will see examples of number-theoretic transformations that are widely used in
public-key cryptography. Looking ahead a bit, we will see that for a large, composite modulus
N, if x is chosen at random modulo N, and an adversary is given y := x3 mod N, it is
hard to compute x. We can view x as the secret, and similarly to the previous example,
we can view y as information that is leaked to the adversary. Even though the value of y
completely determines x in an information-theoretic sense, it is still widely believed to be
hard to compute. Therefore, we might want to treat x as secret data in exactly the same
way as in the previous examples. Many of the same issues arise here, not the least of which
is that x is typically much longer (typically, thousands of bits long) than an AES key.
As already mentioned, the solution that is adopted in practice is simply to hash the secret s
using a hash function H to obtain the key t←H(s).
Let us now give a formal deﬁnition of the security property we are after.
We assume the secret s is sampled according to some ﬁxed (and publicly known) probability
distribution P. We assume any such secret data can be encoded as an element of some ﬁnite set S.
325
Further, we model the fact that some partial information about s could be leaked by introducing
a function I, so that an adversary trying to guess s knows the side information I(s).
Attack Game 8.3 (Guessing advantage). Let P be a probability distribution deﬁned on a
ﬁnite set Sand let I be a function deﬁned in S. For a given adversary A, the attack game runs as
follows:
• the challenger chooses s at random according to P and sends I(s) to A;
• the adversary outputs a guess ˆs for s, and wins the game if ˆs= s.
The probability that A wins this game is called its guessing advantage , and is denoted
Guessadv[A,P,I ]. 2
In the ﬁrst example above, we might simplistically model s as being a password that is uni-
formly distributed over (the encodings of) some dictionary D of words. In this case, there is no
side information given to the adversary, and the guessing advantage is 1 /|D|, regardless of the
computational power of the adversary.
In the second example above, it seems very hard to give a meaningful and reliable estimate of
the guessing advantage.
In the third example above, s is uniformly distributed over {0,1}128, and I(s) is (say) the ﬁrst
64-bits of s. Clearly, any adversary, no matter how powerful, has guessing advantage no greater
than 2−64.
In the fourth example above, s is the number x and I(s) is the number y. Since y completely
determines x, it is possible to recover s from I(s) by brute-force search. There are smarter and
faster algorithms as well, but there is no known eﬃcient algorithm to do this. So for all eﬃcient
adversaries, the guessing advantage appears to be negligible.
Now suppose we use a hash function H : S→T to derive the key t from s. Intuitively, we
want t to “look random”. To formalize this intuitive notion, we use the concept of computational
indistinguishability from Section 3.11. So formally, the property that we want is that ifsis sampled
according to P and tis chosen at random fromT, the two distributions (I(s),H(s)) and (I(s),t) are
computationally indistinguishable. For an adversary A, let Dist adv[A,P,I,H ] be the adversary’s
advantage in Attack Game 3.3 for these two distributions.
The type of theorem we would like to be able to prove would say, roughly speaking, if H
satisﬁes some speciﬁc property, and perhaps some constraints are placed on P and I, then for every
adversary A, there exists an adversary B(which is an elementary wrapper around A) such that
Distadv[A,P,I,H ] is not too much larger than Guess adv[B,P,I ]. In fact, in certain situations it is
possible to prove such a theorem. We will discuss this result later, in Section 8.10.4 — for now, we
will simply say that this rigorous approach is not widely used in practice, for a number of reasons.
Instead, we will examine in greater detail the heuristic approach of using an “oﬀ the shelf” hash
function like SHA256 to derive keys.
Sub-key derivation. Before moving on, we consider the following, related problem: what to do
with the key t derived from s. In some applications, we might use t directly as, say, an AES key.
In other applications, however, we might need several keys: for example, an encryption key and
a MAC key, or two diﬀerent encryption keys for bi-directional secure communications (so Alice
has one key for sending encrypted messages to Bob, and Bob uses a diﬀerent key for sending
encrypted messages to Alice). So once we have derived a single key t that “for all intents and
326
purposes” behaves like a random bit string, we wish to derive several sub-keys. We call this the
sub-key derivation problem to distinguish it from the key derivation problem. For the sub-key
derivation problem, we assume that we start with a truly random key t — it is not, but when t is
computationally indistinguishable from a truly random key, this assumption is justiﬁed.
Fortunately, for sub-key derivation, we already have all the tools we need at our disposal.
Indeed, we can derive sub-keys from t using either a PRG or a PRF. For example, in the above
example, if Alice and Bob have a shared key t, derived from a secret s, they can use a PRF F as
follows:
• derive a MAC key kmac ←R F(t,"MAC-KEY");
• derive an Alice-to-Bob encryption key kAB ←R F(t,"AB-KEY");
• derive a Bob-to-Alice encryption key kBA ←R F(t,"BA-KEY").
Assuming F is a secure PRF, then the keyskmac, kAB, and kBA behave, for all intents and purposes,
as independent random keys. To implement F, we can even use a hash-based PRF, like HMAC, so
we can do everything we need — key derivation and sub-key derivation — using a single “oﬀ the
shelf” hash function like SHA256.
So once we have solved the key derivation problem, we can use well-established tools to solve
the sub-key derivation problem. Unfortunately, the practice of using “oﬀ the shelf” hash functions
for key derivation is not very well understood or analyzed. Nevertheless, there are some useful
heuristic models to explore.
8.10.2 Random oracles: a useful heuristic
We now introduce a heuristic that we can use to model the use of hash functions in a variety of
applications, including key derivation. As we will see later in the text, this has become a popular
heuristic that is used to justify numerous cryptographic constructions.
The idea is that we simply model a hash function H as if it were a truly random function
O. If H maps Mto T, then Ois chosen uniformly at random from the set Funs[ M,T]. We
can translate any attack game into its random oracle version: the challenger uses Oin place of
H for all its computations, and in addition, the adversary is allowed to obtain the value of Oat
arbitrary input points of his choosing. The function Ois called a random oracle and security in
this setting is said to hold in the random oracle model . The function Ois too large to write
down and cannot be used in a real construction. Instead, we only use Oas a means for carrying
out a heuristic security analysis of the proposed system that actually uses H.
This approach to analyzing constructions using hash function is analogous to the ideal cipher
model introduced in Section 4.7, where we replace a block cipher E= (E,D) deﬁned over ( K,X)
by a family of random permutations {Πk }k ∈K.
As we said, the random oracle model is used quite a bit in modern cryptography, and it would
be nice to be able to use an “oﬀ the shelf” hash function H, and model it as a random oracle.
However, if we want a truly general purpose tool, we have to be a bit careful, especially if we want
to model H as a random oracle taking variable length inputs . The basic rule of thumb is that
Merkle-Damg˚ ard hashes should not be useddirectly as general purpose random oracles. We will
discuss in Section 8.10.3 how to safely (but again, heuristically) use Merkle-Damg˚ ard hashes as
general purpose random oracles, and we will also see that the sponge construction (see Section 8.8)
can be used directly “as is”.
327
We stress that even though security results in the random oracle are rigorous, mathematical
theorems, they are still only heuristic results that do not guarantee any security for systems built
with any speciﬁc hash function. They do, however, rule out “generic attacks” on systems thatwould
work if the hash function were a random oracle. So, while such results do not rule out all attacks,
they do rule out generic attacks, which is better than saying nothing at all about the security of
the system. Indeed, in the real world, given a choice between two systems, S1 and S2, where S1
comes with a security proof in the random oracle model, and S2 comes with a real security proof
but is twice as slow as S1, most practitioners would (quite reasonably) choose S1 over S2.
Deﬁning security in the random oracle model. Suppose we have some type of cryptographic
scheme S whose implementation makes use of a subroutine for computing a hash function H
deﬁned over ( M,T). The scheme Sevaluates H at arbitrary points of its choice, but does not
look at the internal implementation of H. We say that Suses H as an oracle . For example,
Fpre(k,x) := H(k ∥x), which we brieﬂy considered in Section 8.7, is a PRF that uses the hash
function H as an oracle.
We wish to analyze the security of S. Let us assume that whatever security property we are
interested in, say “property X,” is modeled (as usual) as a game between a challenger (speciﬁc
to property X) and an arbitrary adversary A. Presumably, in responding to certain queries, the
challenger computes various functions associated with the scheme S, and these functions may in
turn require the evaluation of H at certain points. This game deﬁnes an advantage Xadv[A,S], and
security with respect to property X means that this advantage should be negligible for all eﬃcient
adversaries A.
If we wish to analyze Sin the random oracle model, then the attack game deﬁning security is
modiﬁed so that H is eﬀectively replaced by a random function O∈ Funs[M,T], to which both the
adversary and the challenger have oracle access. More precisely, the game is modiﬁed as follows.
• At the beginning of the game, the challenger chooses O∈ Funs[M,T] at random.
• In addition to its standard queries, the adversary Amay submit random oracle queries : it
gives m∈M to the challenger, who responds with t= O(m). The adversary may make any
number of random oracle queries, arbitrarily interleaved with standard queries.
• In processing standard queries, the challenger performs its computations using Oin place of
H.
The adversary’s advantage is deﬁned using the same rule as before, but is denoted X roadv[A,S] to
emphasize that this is an advantage in the random oracle model . Security in the random oracle
model means that Xroadv[A,S] should be negligible for all eﬃcient adversaries A.
A simple example: PRFs in the random oracle model. We illustrate how to apply the
random oracle framework to construct secure PRFs. In particular, we will show that Fpre is a
secure PRF in the random oracle model. We ﬁrst adapt the standard PRF security game to obtain
a PRF security game in the random oracle model. To make things a bit clearer, if we have a PRF
F that uses a hash function H as an oracle, we denote by FO the function that uses the random
oracle Oin place of H.
328
Attack Game 8.4 (PRF in the random oracle model). Let F be a PRF deﬁned over (K,X,Y)
that uses a hash function H deﬁned over (M,T) as an oracle. For a given adversary A, we deﬁne
two experiments, Experiment 0 and Experiment 1. For b= 0,1, we deﬁne:
Experiment b:
• O←R Funs[M,T].
• The challenger selects f ∈Funs[X,Y] as follows:
if b= 0: k←R K, f ←FO(k,·);
if b= 1: f ←R Funs[X,Y].
• The adversary submits a sequence of queries to the challenger.
– F-query: respond to a query x∈X with y= f(x) ∈Y.
– O-query: respond to a query m∈M with t= O(m) ∈T .
• The adversary computes and outputs a bit ˆb∈{0,1}.
For b = 0,1, let Wb be the event that Aoutputs 1 in Experiment b. We deﬁne A’s advantage
with respect to F as
PRFroadv[A,F] :=
⏐⏐⏐Pr[W0] −Pr[W1]
⏐⏐⏐. 2
Deﬁnition 8.5. We say that a PRF F is secure in the random oracle model if for all eﬃcient
adversaries A, the value PRFroadv[A,F] is negligible.
Consider again the PRF Fpre(k,x) := H(k ∥ x). Let us assume that Fpre is deﬁned over
(K,X,T), where K= {0,1}κ and X = {0,1}≤L, and that H is deﬁned over ( M,T), where M
includes all bit strings of length at most κ+ L.
We will show that this is a secure PRF in the random oracle model. But wait! We already argued
in Section 8.7 that Fpre is completely insecure when H is a Merkle-Damg˚ ard hash. This seems to be
a contradiction. The problem is that, as already mentioned, it is not safe to use a Merkle-Damg˚ ard
hash directly as a random oracle. We will see how to ﬁx this problem in Section 8.10.3.
Theorem 8.9. If Kis large then Fpre is a secure PRF when H is modeled as a random oracle.
In particular, if Ais a random oracle PRF adversary, as in Attack Game 8.4, that makes at
most Qro oracle queries, then
PRFroadv[A,Fpre] ≤Qro/|K|
Note that Theorem 8.9 is unconditional, in the sense that the only constraint on Ais on the
number of oracle queries: it does not depend on any complexity assumptions.
Proof idea. Once H is replaced with O, the adversary has to distinguish O(k ∥·) from a random
function in Funs[X,T], without the key k. Since O(k∥·) is a random function in Funs[ X,T], the
only hope the adversary has is to somehow use the information returned from queries to O. We
say that an O-query k′∥x′ is relevant if k′= k. It should be clear that queries to Othat are not
relevant cannot help distinguish O(k ∥·) from random since the returned values are independent
329
of the function O(k ∥·). Moreover, the probability that after Qro queries the adversary succeeds
in issuing a relevant query is at most Qro/|K|. 2
Proof. To make this proof idea rigorous we let Ainteract with two PRF challengers. For j = 0,1,
let Wj to be the event that Aoutputs 1 in Game j.
Game 0. We write the challenger in Game 0 so that it is equivalent to Experiment 0 of Attack
Game 8.4, but will be more convenient for us to analyze. We assume the adversary never makes the
same Fpre-query twice. Also, we use an associative array Map : M→T to build up the random
oracle on the ﬂy, using the “faithful gnome” idea we have used so often. Here is our challenger:
Initialization:
initialize the empty associative array Map : M→T
k←R K
Upon receiving an Fpre-query on x∈{0,1}≤L do:
t←R T
(1) if ( k∥x) ∈Domain(Map) then t←Map[k∥x]
(2) Map[k∥x] ←t
send t to A
Upon receiving an O-query m∈M do:
t←R T
if m∈Domain(Map) then t←Map[m]
Map[m] ←t
send t to A
It should be clear that this challenger is equivalent to that in Experiment 0 of Attack Game 8.4. In
Game 0, whenever the challenger needs to sample the random oracle at some input (in processing
either an Fpre-query or an O-query), it generates a random “default output”, overriding that default
if it turns out the oracle has already been sampled at that input; in either case, the associative
array records the input/output pair.
Game 1. We make our gnome “forgetful”: we modify Game 0 by deleting the lines marked (1) and
(2) in that game. Observe now that in Game 1, the challenger does not use Map or kin responding
to Fpre-queries: it just returns a random value. So it is clear (by the assumption that Anever
makes the same Fpre-query twice) that Game 1 is equivalent to Experiment 1 of Attack Game 8.4,
and hence
PRFroadv[A,Fpre] = |Pr[W1] −Pr[W0]|.
Let Z be the event that in Game 1 , the adversary makes an O-query at a point m= (k∥ˆx). It is
clear that both games result in the same outcome unless Z occurs, so by the by Diﬀerence Lemma,
we have
|Pr[W1] −Pr[W0]|≤ Pr[Z].
Since the key k is completely independent of A’s view in Game 1, each O-query hits the key with
probability 1/|K|, and so a simple application of the union bound yields
Pr[Z] ≤Qro/|K|.
That completes the proof. 2
330
Key derivation in the random oracle model. Let us now return to the key derivation problem
introduced in Section 8.10.1. Again, we have a secret s sampled from some distribution P, and
information I(s) is leaked to the adversary. We want to argue that if H is modeled as a random
oracle, then the adversary’s advantage in distinguishing (I(s),H(s)) from (I(s),t), where tis truly
random, is not too much more than the adversary’s advantage in guessing the secret s with only
I(s) (and not H(s)).
To model H as a random oracle O, we convert the computational indistinguishability At-
tack Game 3.3 to the random oracle model, so that the attacker is now trying to distinguish
(I(s),O(s)) from ( I(s),t), given oracle access to O. The corresponding advantage is denoted
Distroadv[A,P,I,H ].
Before stating our security theorem, it is convenient to generalize Attack Game 8.3 to allow
the adversary to output a list of guesses ˆ s1,..., ˆsQ, where the adversary is said to win the game
if ˆsi = s for some i= 1,...,Q . An adversary A’s probability of winning in this game is called his
list guessing advantage, denoted ListGuessadv[A,P,I ].
Clearly, if an adversary Acan win the above list guessing game with probability ϵ, we can
convert him into an adversary that wins the singleton guessing game with probability ϵ/Q: we
simply run Ato obtain a list ˆs1,..., ˆsQ, choose i= 1,...,Q at random, and output ˆsi. However,
sometimes we can do better than this: using the partial information I(s) may allow us to rule out
some of the ˆsi’s, and in some situations, we may be able to identify the correct ˆ si uniquely. This
depends on the application.
Theorem 8.10. If H is modeled as a random oracle, then for every distinguishing adversary A
that makes at most Qro random oracle queries, there exists a list guessing adversary B, which is an
elementary wrapper around A, such that
Distroadv[A,P,I,H ] ≤ListGuessadv[B,P,I ]
and Boutputs a list of size at most Qro. In particular, there exists a guessing adversary B′, which
is an elementary wrapper around A, such that
Distroadv[A,P,I,H ] ≤Qro ·Guessadv[B′,P,I ].
Proof. The proof is almost identical to that of Theorem 8.9. We deﬁne two games, and for j = 0,1,
let Wj to be the event that Aoutputs 1 in Game j.
Game 0. We write the challenger in Game 0 so that it is equivalent to Experiment 0 of the
(I(s),H(s)) vs ( H(s),t) distinguishing game. We build up the random oracle on the ﬂy with an
associative array Map : S→T . Here is our challenger:
Initialization:
initialize the empty associative array Map : S→T
generate s according to P
t←R T
(∗) Map[s] ←t
send (I(s),t) to A
Upon receiving an O-query ˆs∈S do:
ˆt←R T
if ˆs∈Domain(Map) then ˆt←Map[ˆs]
Map[ˆs] ←ˆt
send ˆt to A
331
Game 1. We delete the line marked ( ∗). This game is equivalent to Experiment 1 of this dis-
tinguishing game, as the value t is now truly independent of the random oracle. Moreover, both
games result in the same outcome unless the adversary Ain Game 1 makes an O-query at the
point s. So our list guessing adversary Bsimply takes the value I(s) that it receives from its own
challenger, and plays the role of challenger to Aas in Game 1. At the end of the game, Bsimply
outputs Domain(Map) — the list of points at which Amade O-queries. The essential points are:
our Bcan play this role with no knowledge of s besides I(s), and it records all of the O-queries
made by A. So by the Diﬀerence Lemma, we have
Distroadv[A] = |Pr[W0] −Pr[W1]|≤ ListGuessadv[B]. 2
8.10.3 Random oracles: safe modes of operation
We have already seen that Fpre(k,x) := H(k ∥x) is secure in the random oracle model, and yet
we know that it is completely insecure if H is a Merkle-Damg˚ ard hash. The problem is that a
Merkle-Damg˚ ard construction has a very simple, iterative structure which exposes it to “extension
attacks”. While this structure is not a problem from the point of view of collision resistance, it
shows that grabbing a hash function “oﬀ the shelf” and using it as if it were a random oracle is a
dangerous move.
In this section, we discuss how to safely use a Merkle-Damg˚ ard hash as a random oracle. We
will also see that the sponge construction (see Section 8.8) is already safe to use “as is”; in fact, the
sponge was designed exactly for this purpose: to provide a variable-length input and variable-length
output hash function that could be used directly as a random oracle.
Suppose H is a Merkle-Damg˚ ard hash built from a compression functionh: {0,1}n×{0,1}ℓ →
{0,1}n. One recommended mode of operation is to use HMAC with a zero key:
HMAC0(m) := HMAC(0ℓ,m) = H(opad ∥H(ipad ∥m)).
While this construction foils the obvious extension attacks, why should we have any conﬁdence that
HMAC0 is safe to use as a general purpose random oracle? We can only give heuristic evidence.
Essentially, what we want to argue is that there are no inherent structural weaknesses in HMAC 0
that give rise to a generic attack that treats the underlying compression function itself as a random
oracle — or perhaps, more realistically, as a Davies-Meyer construction based on an ideal cipher.
So basically, we want to show that using certain modes of operation, we can build a “big”
random oracle out of a “small” random oracle — or out of an ideal cipher or even out of an ideal
permutation.
The mathematical tool used to carry out such a task is called indiﬀerentiability. We shall
present a somewhat simpliﬁed version of this notion here. Suppose we are trying to build a “big”
random oracle Oout of a smaller primitive ρ, where ρcould be a random oracle on a small domain,
or an ideal cipher, or an ideal permutation. Let us denote by F[ρ] a particular construction for a
random oracle based on the ideal primitive ρ.
Now consider a generic attack game deﬁned by some challenger C and adversary A. Let us
write the interaction between C and Aas ⟨C,A⟩. We assume that the interaction results in an
output bit. All of our security deﬁnitions are modeled in terms of games of this form.
In the random oracle version of the attack game, with the big random oracle O, we would
give both the challenger and adversary oracle access to the random function O, and we denote the
interaction ⟨CO,AO⟩. However, if we are using the construction F[ρ] to implement the big random
332
oracle, then while the challenger accesses ρ only via the construction F, the adversary is allowed
to directly query ρ. We denote this interaction as ⟨CF[ρ],Aρ⟩.
For example, in the HMAC0 construction, the compression function h is modeled as a random
oracle ρ, or if h itself is built via Davies-Meyer, then the underlying block cipher is modeled as
an ideal cipher ρ. In either case, F[ρ] corresponds to the HMAC 0 construction itself. Note the
asymmetry: in any attack game, the challenger only accesses ρindirectly via F[ρ] (HMAC0 in this
case), while the adversary can access ρ itself (the compression function h or the underlying block
cipher).
We say that F[ρ] is indiﬀerentiable from Oif the following holds:
for every eﬃcient challenger C and eﬃcient adversary A, there exists an eﬃcient ad-
versary B, which is an elementary wrapper around A, such that
⏐⏐Pr[⟨CF[ρ],Aρ⟩outputs 1] −Pr[⟨CO,BO⟩outputs 1]
⏐⏐
is negligible.
It should be clear from the deﬁnition that if we prove security of any cryptographic scheme in
the random oracle model for the big random oracle O, the scheme remains secure if we implement
Ousing F[ρ]: if an adversary Acould break the scheme with F[ρ], then the adversary Babove
would break the scheme with O.
Some safe modes. The HMAC0 construction can be proven to be indiﬀerentiable from a random
oracle on variable length inputs, if we either model the compression function h itself as a random
oracle, or if his built via Davies-Meyer and we model the underlying block cipher as an ideal cipher.
One problem with using HMAC0 as a random oracle is that itsoutput is fairly short. Fortunately,
it is fairly easy to use HMAC 0 to get a random oracle with longer outputs. Here is how. Suppose
HMAC0 has an n-bit output, and we need a random oracle with, say, N > nbits of output. Set
q := ⌈N/n⌉. Let e0,e1,...,e q be ﬁxed-length encodings of the integers 0 ,1,...,q . Our new hash
function H′works as follows. On input m, we compute t←HMAC0(e0 ∥m). Then, for i= 1,...,q ,
we compute ti ←HMAC0(ei ∥t). Finally, we output the ﬁrst N bits of t1 ∥t2 ∥···∥ tq. One
can show that H′is indiﬀerentiable from a random oracle with N-bit outputs. This result holds if
we replace HMAC0 with any hash function that is itself indiﬀerentiable from a random oracle with
n-bit outputs. Also note that when applied to long inputs, H′ is quite eﬃcient: it only needs to
evaluate HMAC0 once on a long input.
The sponge construction has been proven to be indiﬀerentiable from a random oracle on vari-
able length inputs, if we model the underlying permutation as an ideal permutation (assuming
2c is super-poly, where c is the capacity). This includes the standardized implementations SHA3
(for ﬁxed length outputs) and the SHAKE variants (for variable length outputs), discussed in Sec-
tion 8.8.2. The special padding rules used in the SHA3 and SHAKE speciﬁcations ensure that all
of the variants act as independent random oracles.
Sometimes, we need random oracles whose output should be uniformly distributed over some
specialized set. For example, we may want the output to be uniformly distributed over the set
S = {0,...,d −1}for some positive integer d. To realize this, we can use a hash function H
with an n-bit output, which we can view as an n-bit binary encoding of a number, and deﬁne
H′(m) := H(m) mod d. If H is indiﬀerentiable from a random oracle with n-bit outputs, and 2n/d
is super-poly, then the hash function H′is indiﬀerentiable from a random oracle with outputs in S.
333
8.10.4 The leftover hash lemma
We now return to the key derivation problem. Under the right circumstances, we can solve the key
derivation problem with no heuristics and no computational assumptions whatsoever. Moreover,
the solution is a surprising and elegant application of universal hash functions (see Section 7.1).
The result, known as the leftover hash lemma , says that if we use an ϵ-UHF to hash a secret
that can be guessed with probability at most γ, then provided ϵ and γ are suﬃciently small, the
output of the hash is statistically indistinguishable from a truly random value. Recall that a UHF
has a key, which we normally think of as a secret key; however, in this result, the key may be made
public — indeed, it could be viewed as a public, system parameter that is generated once and for
all, and used over and over again.
Our goal here is to simply state the result, and to indicate when and where it can (and cannot)
be used. To state the result, we will need to use the notion of the statistical distance between two
random variables, which we introduced in Section 3.11. Also, if s is a random variable taking values
in a set S, we deﬁne the guessing probability of s to be maxx∈SPr[s = x].
Theorem 8.11 (Leftover Hash Lemma). Let H be a keyed hash function deﬁned over (K,S,T).
Assume that H is a (1 + α)/N-UHF, where N := |T|. Let k,s1,..., sm be mutually independent
random variables, where k is uniformly distributed over K, and each si has guessing probability at
most γ. Let δ be the statistical diﬀerence between
(k,H(k,s1),...,H (k,sm))
and the uniform distribution on K×T m. Then we have
δ≤1
2m
√
Nγ + α.
Let us look at what the lemma says when m = 1. We have a secret s that can be guessed
with probability at most γ, given whatever side information I(s) is known about s. To apply the
lemma, the bound γon the guessing probability must hold for all adversaries, even computationally
unbounded ones. We then hash susing a random hash key k. It is essential that s(given I(s)) and
k are independent — although we have not discussed the possibility here, there are potential use
cases where the distribution of sor the function I can be somehow biased by an adversary in a way
that depends on k, which is assumed public and known to the adversary. Therefore, to apply the
lemma, we must ensure that s (given I(s)) and k are truly independent. If all of these conditions
are met, then the lemma says that for any adversary A, even a computationally unbounded one,
its advantage in distinguishing (k,I(s),H(k,s)) from (k,I(s),t), where tis a truly random element
of T, is bounded by δ, as in the lemma.
Now let us plug in some realistic numbers. If we want the output to be used as an AES key, we
need N = 2128. We know how to build (1 /N)-UHFs, so we can take α = 0 (see Exercise 7.18 —
with α non-zero, but still quite small, one can get by with signiﬁcantly shorter hash keys). If we
want δ≤2−64, we will need the guessing probability γ to be about 2 −256.
So in addition to all the conditions listed above, we really need an extremely small guessing
probability for the lemma to be applicable. None of the examples discussed in Section 8.10.1
meet these requirements: the guessing probabilities are either not small enough, or do not hold
unconditionally against unbounded adversaries, or can only be heuristically estimated. So the
practical applicability to the Leftover Hash Lemma is limited — but when it does apply, it can
334
be a very powerful tool. Also, we remark that by using the lemma with m >1, under the right
conditions, we can model the situation where the same hash key is used to derive many keys from
many independent secrets with small guessing probability. The distinguishing probability grows
linearly with the number of derivations, which is not surprising.
Because of these practical limitations, it is more typical to use cryptographic hash functions,
modeled as random oracles, for key derivation, rather than UHFs. Indeed, if one uses a UHF
and any of the assumptions discussed above turns out to be wrong, this could easily lead to a
catastrophic security breach. Cryptographic hash functions, while only heuristically secure for key
derivation, are also more forgiving.
8.10.5 Case study: HKDF
HKDF is a key derivation function speciﬁed in RFC 5869, and is deployed in many standards.
HKDF is speciﬁed in terms of the HMAC construction (see Section 8.7). So it uses the function
HMAC(k,m), where k and mare variable length byte strings, which itself is implemented in terms
of a Merkle-Damg˚ ard hashH, such as SHA256.
The input to HKDF consists of a secret s, an optional salt value salt (discussed below), an
optional info ﬁeld (also discussed below), and an output length parameter L. The parameters s,
salt, and info are variable length byte strings.
The execution of HKDF consists of two stages, called extract (which corresponds to what we
called key derivation), and expand (which corresponds to what we called sub-key derivation).
In the extract stage, HKDF uses salt and s to compute
t←HMAC(salt,s).
Using the intermediate key t, along with info, the expand (or sub-key derivation) stage computes
L bytes of output data, as follows:
q←⌈L/HashLen⌉ / / HashLen is the output length (in bytes) of H
initialize z0 to the empty string
for i←1 to q do:
zi ←HMAC(t,zi−1 ∥info ∥Octet(i)) / / Octet (i) is a single byte whose value is i
output the ﬁrst L octets of z1 ∥... ∥zq
When salt is empty, the extract stage of HKDF is the same as what we called HMAC 0 in
Section 8.10.3. As discussed there, HMAC 0 can heuristically be viewed as a random oracle, and so
we can use the analysis in Section 8.10.2 to show that this is a secure key derivation procedure in
the random oracle model. Thus, if s is hard to guess, then t is indistinguishable from random.
Users of HKDF have the option of providing a non-zero salt. The salt plays a role akin to the
random hash key used in the Leftover Hash Lemma (see Section 8.10.4); in particular, it need not
be secret, and may be reused. However, it is important that the salt value is independent of the
secret s and cannot be manipulated by an adversary. The idea is that under these circumstances,
the output of the extract stage of HKDF seems more likely to be indistinguishable from random,
without relying on the full power of the random oracle model. Unfortunately, the known security
proofs apply to limited settings, so in the general case, this is still somewhat heuristic.
The expand stage is just a simple application of HMAC as a PRF to derive sub-keys, as we
discussed at the end of Section 8.10.1. The info parameter may be used to “name” the derived
335
sub-keys, ensuring the independence of keys used for diﬀerent purposes. Since the output length of
the underlying hash is ﬁxed, a simple iterative scheme is used to generate longer outputs. This stage
can be analyzed rigorously under the assumption that the intermediate key t is indistinguishable
from random, and that HMAC is a secure PRF — and we already know that HMAC is a secure
PRF, under reasonable assumptions about the compression function of H.
8.11 Security without collision resistance
Theorem 8.1 shows how to extend the domain of a MAC using a collision resistant hash. It is
natural to ask whether MAC domain extension is possible without relying on collision resistant
functions. In this section we show that a weaker property called second preimage resistance is
suﬃcient.
8.11.1 Second preimage resistance
We start by deﬁning two classic security properties for non-keyed hash functions: one-wayness and
2nd-preimage resistance. Let H be a hash function deﬁned over ( M,T).
• We say that H is one-way if given t:= H(m) as input, for a random m∈M, it is diﬃcult
to ﬁnd an m′∈M such that H(m′) = t. Such an m′is called an inverse of t. In other words,
H is one-way if it is easy to compute but diﬃcult to invert.
• We say that H is 2nd-preimage resistant if given a random m∈M as input, it is diﬃcult
to ﬁnd a diﬀerent m′∈M such that H(m) = H(m′). In other words, it is diﬃcult to ﬁnd an
m′that collides with a given m.
• For completeness, recall that a hash function is collision resistant if it is diﬃcult to ﬁnd two
distinct messages m,m′∈M such that H(m) = H(m′).
The following deﬁnes these concepts more precisely.
Deﬁnition 8.6. Let H be a hash function deﬁned over (M,T).
• We deﬁne the advantage OWadv[A,H] of an adversary Ain defeating the one-wayness of H
as the probability of winning the following game:
– the challenger chooses m∈M at random and sends t:= H(m) to A;
– the adversary Aoutputs m′∈M, and wins if H(m′) = t.
H is one-way if OWadv[A,H] is negligible for every eﬃcient adversary A.
• We deﬁne the advantage SPRadv[A,H] of an adversary Ain defeating the 2nd-preimage
resistance of H as the probability of winning the following game:
– the challenger chooses m∈M at random and sends m to A;
– the adversary Aoutputs m′∈M, and wins if H(m′) = H(m) and m′̸= m.
H is 2nd-preimage resistant if SPRadv[A,H] is negligible for every eﬃcient adversary A.
336
Let’s ﬁrst look at some easy relations between these security notions. SupposeH is compressing,
meaning that the domain of H is bigger than its range. Speciﬁcally, suppose |M|≥ s·|T| for some
compression factor s> 1. If s is super-poly, then we have the following implications:
H is collision resistant ⇒ H is 2nd-preimage resistant ⇒ H is one-way, (8.17)
as shown in Exercise 8.23. In fact, the implication on the left holds with no restriction on s.
The converse of (8.17) is not true. A hash function can be 2nd-preimage resistant, but not
collision resistant. For example, SHA1 is believed to be 2nd-preimage resistant even though SHA1
is not collision resistant. Similarly, a hash function can be one-way, but not be 2nd-preimage
resistant. For example, let h be a one-way function deﬁned over ( M,T). Let h′ be a function
over (M×{ 0,1}, T) deﬁned as h′(m,b) := h(m). Then clearly h′ is one-way, but is not 2nd-
preimage resistant: given ( m,b) ∈M×{ 0,1}, the input ( m, 1 −b) is a second preimage because
h′(m,b) = h′(m, 1 −b).
Our goal for this section is to show that 2nd-preimage resistance is suﬃcient for extending the
domain of a MAC and for providing ﬁle integrity. To give some intuition, consider the ﬁle integrity
problem (which we discussed at the very beginning of this chapter). Our goal is to ensure that
malware cannot modify a ﬁle without being detected. Recall that we hash all critical ﬁles on disk
using a hash function H and store the resulting hashes in read-only memory. For a ﬁle F it should
be diﬃcult for the malware to ﬁnd an F′ such that H(F′) = H(F). Clearly, if H is collision
resistant then ﬁnding such an F′is diﬃcult. It would seem, however, that 2nd-preimage resistance
of H is suﬃcient. To see why, consider malware trying to modify a speciﬁc ﬁle F without being
detected. The malware is given F as input and must come up with a 2nd-preimage of F, namely
an F′ such that H(F′) = H(F). If H is 2nd-preimage resistant the malware cannot ﬁnd such an
F′ and it would seem that 2nd-preimage resistance is suﬃcient for ﬁle integrity. Unfortunately,
this argument doesn’t quite work. Our deﬁnition of 2nd-preimage resistance says that ﬁnding a
2nd-preimage for a random F in Mis diﬃcult. But ﬁles on disk are not random bit strings —
it may be diﬃcult to ﬁnd a 2nd-preimage for a random ﬁle, but it may be quite easy to ﬁnd a
2nd-preimage for a speciﬁc ﬁle on disk.
The solution is to randomize the data before hashing it. To do so we ﬁrst convert the hash
function to a keyed hash function. We then require that the resulting keyed function satisfy a
property called target collision resistance which we now deﬁne.
8.11.2 Randomized hash functions: target collision resistance
At the beginning of the chapter we mentioned two applications for collision resistance: extending
the domain of a MAC and protecting ﬁle integrity. In this section we describe solutions to these
problems that rely on a weaker security property than collision resistance. The resulting systems,
although more likely to be secure, are not as eﬃcient as the ones obtained from collision resistance.
Target collision resistance. Let H be a keyed hash function. We deﬁne what it means for H
to be target collision resistant, or TCR for short, using the following attack game, also shown
in Fig. 8.14.
Attack Game 8.5 (Target collision resistance). For a given keyed hash function H over
(K,M,T) and adversary A, the attack game runs as follows:
337
TCR Challenger
Adversary A
k←
R
K
m0
k
m1
Figure 8.14: TCR Attack Game
• Asends a message m0 ∈M to the challenger.
• The challenger picks a random k←R Kand sends k to A.
• Asends a second message m1 ∈M to the challenger.
The adversary is said to win the game if m0 ̸= m1 and H(k,m0) = H(k,m1). We deﬁne A’s
advantage with respect to H, denoted TCR adv[A,H], as the probability that Awins the game.
2
Deﬁnition 8.7. We say that a keyed hash function H over (K,M,T) is target collision resistant
if TCRadv[A,H] is negligible.
Casting the deﬁnition in our formal mathematical framework is done exactly as for universal
hash functions (Section 7.1.2).
We note that one can view a collision resistant hash H over (M,T) as a TCR function with
an empty key. More precisely, let Kbe a set of size one containing only the empty word. We can
deﬁne a keyed hash function H′over (K,M,T) as H′(k,m) := H(m). It is not diﬃcult to see that
if H is collision resistant then H′is TCR. Thus, a collision resistant function can be viewed as the
ultimate TCR hash — its key is the shortest possible.
8.11.3 TCR from 2nd-preimage resistance
We show how to build a keyed TCR hash function from a keyless 2nd-preimage resistant function
such as SHA1. Let H, deﬁned over (M,T), be a 2nd-preimage resistant function. We construct a
keyed TCR function Htcr deﬁned over (M,M,T) as follows:
Htcr(k,m) = H(k⊕m) (8.18)
Note that the length of the key k is equal to the length of the message being hashed. This is a
problem for the applications we have in mind. As a result, we will only use this construction as a
TCR hash for short messages. First we prove that the construction is secure.
Theorem 8.12. Suppose H is 2nd-preimage resistant then Htcr is TCR.
In particular, for every TCR adversary Aattacking Htcr as in Attack Game 8.5, there exists a
2nd-preimage ﬁnder B, which is an elementary wrapper around A, such that
TCRadv[A,Htcr] ≤SPRadv[B,H].
338
Proof. The proof is a simple direct reduction. Adversary Bemulates the challenger in Attack
Game 8.5 and works as follows:
Input: Random m∈M
Output: m′∈M such that m̸= m′and H(m) = H(m′)
1. Run Aand obtain an m0 ∈M from A
2. k←m⊕m0
3. Send k as the hash key to A
4. Aresponds with an m1 ∈M
5. Output m′:= m1 ⊕k
We show that SPRadv[B,H] = TCRadv[A,Htcr]. First, denote by W the event that in step (4) the
messages m0,m1 output by Aare distinct and Htcr(k,m0) = Htcr(k,m1).
The input m given to Bis uniformly distributed in M. Therefore, the key k given to Ain
step (2) is uniformly distributed in Mand independent of A’s current view, as required in Attack
Game 8.5. It follows that Bperfectly emulates the challenger in Attack Game 8.5 and consequently
Pr[W] = TCRadv[A,Htcr].
By deﬁnition of Htcr, we also have the following:
Htcr(k,m0) = H((m⊕m0) ⊕m0) = H(m) (8.19)
Htcr(k,m1) = H(m1 ⊕k) = H(m′)
Now, suppose event W happens. Then Htcr(k,m0) = Htcr(k,m1) and therefore, by (8.19), we
know that H(m) = H(m′). Second, we deduce that m ̸= m′ which follows since m0 ̸= m1 and
m′= m⊕(m1 ⊕m0). Hence, when event W occurs, Boutputs a 2nd-preimage of m. It now follows
that:
SPRadv[B,H] ≥Pr[W] = TCRadv[A,Htcr]
as required. 2
Target collision resistance for long inputs. The function Htcr in (8.18) shows that a 2nd-
preimage resistant function directly gives a TCR function. If we assume that the SHA256 compres-
sion function h is 2nd-preimage resistant (a weaker assumption than assuming that h is collision
resistant) then, by Theorem 8.12 we obtain a TCR hash for inputs of length 512 + 256 = 768 bits.
The length of the required key is also 768 bits.
We will often need TCR functions for much longer inputs. Using the SHA256 compression
function we already know how to build a TCR hash for short inputs using a short key. Thus, let
us assume that we have a TCR function h deﬁned over ( K, T ×M, T) where M:= {0,1}ℓ for
some small ℓ, say ℓ = 512. We build a new TCR hash for much larger inputs. Let L ∈Z>0 be a
power of 2. We build a derived TCR hash H that hashes messages in {0,1}≤ℓL using keys in
(K×T 1+log2 L). Note that the length of the keys is logarithmic in the length of the message, which
is much better than (8.18).
To describe the function H we need an auxiliary function ν : Z>0 →Z>0 deﬁned as:
ν(x) := largest n∈Z>0 such that 2n divides x.
Thus, ν(x) counts the number of least signiﬁcant bits of x that are zero. For example, ν(x) = 0 if
x is odd and ν(x) = n if x= 2n. Note that ν(x) ≤7 for more than 99% of the integers.
339
m1
h
k1
⨁IV
k2[ν(1)]
m2
h
k1
⨁
k2[ν(2)]
ms ∥PB
h
k1
t⨁
k2[ν(3)]
⨁
k2[ν(s)]
···
Figure 8.15: Extending the domain of a TCR hash
The derived TCR hash H is similar to Merkle-Damg˚ ard. It uses the same padding block PB
as in Merkle-Damg˚ ard and a ﬁxed initial value IV. The derived TCR hashH is deﬁned as follows
(see Fig. 8.15):
Input: Message M ∈{0,1}≤ℓL and key (k1,k2) ∈K×T 1+log2 L
Output: t∈T
M ←M ∥PB
Break M into consecutive ℓ-bit blocks so that
M = m1 ∥m2 ∥···∥ ms where m1,...,m s ∈{0,1}ℓ
t0 ←IV
for i= 1 to s do:
u←k2[ν(i)] ⊕ti−1 ∈T
ti ←h(k1, (u,mi) ) ∈T
Output ts
We note that directly using Merkle-Damg˚ ard to extend the domain of a TCR hash does not
work. Plugging h(k1,·) directly into Merkle-Damg˚ ard can fail to give a TCR hash.
Security of the derived hash. The following theorem shows that the derived hash H is TCR
assuming the underlying hash h is TCR. We refer to [146, 114] for the proof of this theorem.
Theorem 8.13. Suppose h is a TCR hash function that hashes messages in (T ×{0,1}ℓ). Then,
for any bounded L, the derived function H is a TCR hash for messages in {0,1}≤ℓL.
In particular, suppose Ais a TCR adversary attacking H (as in Attack Game 8.5). Then there
exists a TCR adversary B(whose running times are about the same as that of A) such that
TCRadv[A,H] ≤L·TCRadv[B,h].
As in Merkle-Damg˚ ard this construction is inherently sequential. A tree-based construction
similar to Exercise 8.9 gives a TCR hash using logarithmic size keys that is more suitable for a
parallel machine. We refer to [14] for the details.
340
8.11.4 Using target collision resistance
We now know how to build a TCR function for large inputs from a small 2nd-preimage resistant
function. We show how to use such TCR functions to extend the domain for a MAC and to ensure
ﬁle integrity. We start with ﬁle integrity.
8.11.4.1 File integrity
Let H be a TCR hash deﬁned over (K,M,T). We use H to protect integrity of ﬁles F1,F2,... ∈M
using a small amount of read-only memory. The idea is to pick a random key ri in Kfor every ﬁle
Fi and then store the pair ( ri, H(ri,Fi) ) in read-only memory. Note that we are using a little
more read-only memory than in the system based on collision resistance. To verify integrity of ﬁle
Fi we simply recompute H(ri,Fi) and compare to the hash stored in read-only memory.
Why is this mechanism secure? Consider malware targeting a speciﬁc ﬁle F. We store in read-
only memory the key r and t:= H(r,F). To modify F without being detected the malware must
come up with a new ﬁle F′ such that t= H(r,F′). In other words, the malware is given as input
the ﬁle F along with a random key r∈K and must produce a new F′such that H(r,F) = H(r,F′).
The adversary (the malware writer in this case) chooses which ﬁle F to attack. But this is precisely
the TCR Attack Game 8.5 — the adversary chooses an F, gets a random key r, and must output
a new F′that collides with F under r. Hence, if H is TCR the malware cannot modify F without
being detected.
In summary, we can provide ﬁle integrity using a small amount of read-only memory and by
relying only on 2nd-preimage resistance. The cost, in comparison to the system based on collision
resistance, is that we need a little more read-only memory to store the keyr. In particular, using the
TCR construction from the previous section, the amount of additional read-only memory needed is
logarithmic in the size of the ﬁles being protected. Using a recursive construction (see Exercise 8.25)
we can reduce the additional read-only memory used to a small constant, but still non-zero.
8.11.4.2 Extending the domain of a MAC
Let H be a TCR hash deﬁned over (KH,M,T). Let I= (S,V ) be a MAC for authenticating short
messages in KH ×T using keys in K. We assume that Mis much larger than T. We build a new
MAC I′= (S′,V ′) for authenticating messages in Musing keys in Kas follows:
S′(k, m) := V′(
k, m,(t,r)
):=
r←R KH h←H(r,m) (8.20)
h←H(r,m) Output V(k, (r,h), t)
t←S
(
k, (r,h)
)
Output (t,r)
Note the MAC signing is randomized — we pick a random TCR key r, include r in the input to
the signing algorithm S, and output r as part of the ﬁnal tag. As a result, tags produced by this
MAC are longer than tags produced from extending MACs using a collision resistance hash (as in
Section 8.2). Using the construction from the previous section, the length of r is logarithmic in the
size of the message being authenticated. This extra logarithmic size key is included in every tag.
On the plus side, this construction only relies on H being TCR which is a much weaker property
than collision resistance and hence much more likely to hold for H.
341
The following theorem proves security of the construction in (8.20) above. The theorem is the
analog of Theorem 8.1 and its proof is similar. Note however, that the error bounds are not as
tight as the bounds in Theorem 8.1.
Theorem 8.14. Suppose the MAC system Iis a secure MAC and the hash function H is TCR.
Then the derived MAC system I′= (S′,V ′) deﬁned in (8.20) is a secure MAC.
In particular, for every MAC adversary Aattacking I′ (as in Attack Game 6.1) that issues
at most Q signing queries, there exist an eﬃcient MAC adversary BI and an eﬃcient TCR
adversary BH, which are elementary wrappers around A, such that
MACadv[A,I′] ≤MACadv[BI,I] + Q·TCRadv[BH,H].
Proof idea. Our goal is to show that no eﬃcient MAC adversary can successfully attack I′. Such
an adversary Aasks the challenger to sign a few long messages m1,m2,... ∈M and gets back tags
(ti,ri) for i = 1,2,.... It then tries to invent a new valid message-MAC pair ( m, (t,r)). If Ais
able to produce a valid forgery ( m, (t,r)) then one of two things must happen:
1. either ( r,H(r,m)) is equal to ( ri,H(ri,mi)) for some i;
2. or not.
It is not diﬃcult to see that forgeries of the second type can be used to attack the underlying
MAC I. We show that forgeries of the ﬁrst type can be used to break the target collision resistance
of H. Indeed, if ( r,H(r,m)) = (ri,H(ri,mi)) then r= ri and therefore H(r,m) = H(r,mi). Thus
mi and m collide under the random key r. We will show that this lets us build an adversary BH
that wins the TCR game when attacking H. Unfortunately, BH must guess ahead of time which
of A’s queries to use as mi. Since there are Qqueries to choose from, BH will guess correctly with
probability 1/Q. This is the reason for the extra factor of Q in the error term. 2
Proof. Let X be the event that adversary Awins the MAC Attack Game 6.1 with respect to I′.
Let m1,m2,... ∈M be A’s queries during the game and let ( t1,r1), (t2,r2),... be the challenger’s
responses. Furthermore, let ( m, (t,r)) be the adversary’s ﬁnal output. We deﬁne two additional
events:
• Let Y denote the event that for some i= 1,2,... we have that ( r,H(r,m)) = (ri,H(r,mi))
and m̸= mi.
• Let Z denote the event that Awins Attack Game 6.1 on I′and event Y did not occur.
Then
MACadv[A,I′] = Pr[ X] ≤ Pr[X∧¬Y] + Pr[Y] = Pr[ Z] + Pr[Y] (8.21)
To prove the theorem we construct a TCR adversary BH and a MAC adversary BI such that
Pr[Y] ≤Q·TCRadv[BH,H] and Pr[ Z] = MACadv[BI,I].
Adversary BI is essentially the same as in the proof of Theorem 8.1. Here we only describe the
TCR adversary BH, which emulates a MAC challenger for Aas follows:
342
k←R K
u←R {1,2,...,Q }
Run algorithm A
Upon receiving the ith signing query mi ∈M from Ado:
If i̸= u then
ri ←R KH
Else / / i= u: for query number u get ri from the TCR challenger
BH sends ˆm0 := mi to its TCR challenger
BH receives a random key ˆr∈K from its challenger
ri ←ˆr
h←H(ri,mi)
t←S(k, (ri,h) )
Send (t,r) to A
Upon receiving the ﬁnal message-tag pair ( m, (t,r) ) from Ado:
BH sends ˆm1 := m to its challenger
Algorithm BH responds to A’s signature queries exactly as in a real MAC attack game. Therefore,
event Y happens during the interaction with BH with the same probability that it happens in
a real MAC attack game. Now, when event Y happens there exists a j ∈{1,2,... }such that
(r,H(r,m)) = ( rj,H(rj,mj)) and m ̸= mj. Suppose that furthermore j = u. Then r = rj = ˆr
and therefore H(ˆr,m) = H(ˆr,mu). Hence, if event Y happens and j = u then BH wins the TCR
attack game. In symbols,
TCRadv[BH,H] = Pr[Y ∧(j = u)].
Notice that u is independent of A’s view — it is only used for choosing which random key ri is
from BH’s challenger, but no matter what u is, the key ri given to Ais always uniformly random.
Hence, event Y is independent of the event j = u. For the same reason, if the adversary makes a
total of w queries then Pr[j = u] = 1/w≥1/Q. In summary,
TCRadv[BH,H] = Pr[Y ∧(j = u)] = Pr[Y] ·Pr[j = u] ≥Pr[Y]/Q
as required. 2
8.12 A fun application: commitments and auctions
Alice and Bob decide to participate in an auction for a rare porcelain vase, along with other bidders.
The auction house that runs the auction uses aVickrey sealed bid auction, a commonly used auction
mechanism, that works as follows: each participant submits a secret sealed bid for the vase. Once
all the bids are in, the party who submitted the highest bid gets the vase, and the price paid is the
second-highest bid. This auction mechanism can be shown to have good game theoretic properties,
assuming the participants do not collude, and do not know each other’s bids until the auction is
ﬁnished. Hence, the need for a sealed bid auction.
When all the participants are in the same room, the auction house can implement a Vickrey
auction by having each participant submit their bid in a sealed envelope. After collecting all the
envelopes, the auctioneer opens them and announces the results. The participants can inspect the
343
envelopes and the papers in them to verify that the auction was administered correctly. There is
no need to trust the auction house.
Let’s see how to implement a sealed bid auction when the participants are remote and commu-
nicate with the auction house over the Internet. We do so using a cryptographic commitment, an
important cryptographic primitive that has many applications. We have previously encountered
commitments in Section 3.12, where we used them for a coin-ﬂipping protocol between two parties.
Recall that a commitment scheme Clets one party, Alice, commit to a message m ∈M by
publishing a commitment string c. Later, Alice can open the commitment and convince some other
party, Bob, that the committed message was m. More precisely, a commitment scheme for a
ﬁnite message space M, is a pair of eﬃcient algorithms C= (C,V ) where:
• Algorithm C is invoked as ( c,o) ←R C(m), where m∈M is the message to be committed, c
is the commitment string, and o is an opening string.
• Algorithm V is a deterministic algorithm invoked as V(m,c,o ) and outputs accept or reject.
• Correctness property: for all m∈M, if we compute ( c,o) ←R C(m), then V(m,c,o ) = accept
with probability 1.
Alice commits to a message m∈M by computing (c,o) ←R C(m). She sends the commitment cto
Bob, and keeps o to herself. Later, when Alice wants to open the commitment, she sends m and o
to Bob, and Bob veriﬁes that the commitment was opened correctly by running V(m,c,o ).
A commitment scheme Cis intended to be the digital analogue of a sealed envelope. As such,
it needs to satisfy two properties:
• Binding: Once a commitment c is generated, Alice can only open it to a single message.
In particular, for every eﬃcient adversary Athat outputs a 5-tuple ( c,m1,o1,m2,o2) the
advantage
BINDadv[A,C] := Pr
[
m1 ̸= m2 and V(m1,c,o 1) = V(m2,c,o 2) = accept
]
is negligible.
• Hiding: The commitment stringcshould reveal no information about the committed message
m ∈M. We capture this using a semantic security deﬁnition. Speciﬁcally, we deﬁne two
experiments, Experiment 0 and Experiment 1, between an adversary Aand a challenger. For
b = 0 ,1, in Experiment b adversary Abegins by sending m0,m1 ∈M to the challenger,
who computes ( c,o) ←R C(mb) and sends c to A; ﬁnally, Aoutputs a guess ˆb ∈{0,1}. For
b = 0 ,1, let Wb be the event that Aoutputs 1 in Experiment b. We require that for all
eﬃcient adversaries Athe advantage
HIDadv[A,C] :=
⏐⏐⏐Pr[W0] −Pr[W1]
⏐⏐⏐
is negligible.
Deﬁnition 8.8. A commitment scheme C= (C,V ) is secure if it is both hiding and binding.
Remark 8.1 (Encryption can be non-binding). One might be tempted to implement a com-
mitment scheme using encryption. Let ( E,D) be a semantically secure cipher with key space K
344
and message space M. The derived commitment scheme ( C,V ) works as follows: C(m) chooses
a random key k ←R K, computes c←R E(k,m), and outputs ( c,k) as the commitment and opening
strings. Algorithm V(m,c,k ) accepts if D(k,c) = m.
At ﬁrst glance, this may seem like a ﬁne commitment scheme. However, this construction may
be completely insecure, and this is a common source of real-world implementation errors. Let’s see
why. The commitment scheme is clearly hiding, because the cipher is semantically secure, so this is
not where the problem lies. However, the scheme may not be binding, and this breaks security. The
problem is that it may be feasible to ﬁnd a commitment string cand two keys k1 and k2 such that
D(k1,c) ̸= D(k2,c) and both decryptions are in M. This lets an attacker open the commitment
c to two diﬀerent messages, which breaks the binding property. This shows that encryption and
commitments, while related, are quite diﬀerent objects.
For example, suppose M= K= {0,1}n and (E,D) is the one-time pad. Then the commitment
to a message m∈M is simply c= m⊕k, where k←R K. Now, it is quite easy for the committer to
open this commitment c as any messages m′ in Mof its choice. Simply compute k′= c⊕m′ and
send (m′,k′) to the veriﬁer, who will incorrectly accept this opening. We say that the one-time pad
is a non-binding encryption scheme because the derived commitment scheme is non-binding.
Many other encryption schemes can also be shown to be non-binding. 2
A simple auction. Coming back to the auction question, let’s see how to use a secure commit-
ment scheme C= (C,V ) to implement a simple veriﬁable sealed bid auction that does not require
trusting the auction house. Every participant posts a commitment to his or her bid on a pub-
lic bulletin board (say, hosted at the auction house web site). The commitment hiding property
ensures that nothing is revealed about the participant’s bid. The commitment binding property
ensures that the participant can no longer change the bid. Once all the bids are posted, the auction
house asks all the participants to open their commitments, and the winner is determined. All the
openings are posted on the bulletin board, so that the participants can audit the auction. If a
participant does not open his or her commitment by a certain deadline, then their bid is discarded.
Of course, the bulletin board needs to be authenticated, so that everyone can tell that the com-
mitment came from the participant, and not from someone who is masquerading as the participant.
This can be done using digital signatures, which is the topic of Chapter 13. We also need to ensure
that once a message is posted on the bulletin board it cannot be removed, so that bids cannot be
maliciously deleted.
One downside of this auction scheme is that it forces everyone, even non-winners, to publicly
reveal their bids. This can be ﬁxed by using a private sealed bid auction scheme, where bids remain
secret even after the auction is ﬁnished. We will see how to construct such a scheme in Chapter 23,
after we develop a few more tools.
As it turns out, the simple auction scheme described above may be insecure, even if we use a
secure commitment scheme. Let’s ﬁrst use collision resistance to construct two secure commitment
schemes, and then take another look at this auction at the end of the section.
A commitment scheme from collision resistance. In Section 3.12 we constructed an elegant
commitment scheme from a pseudorandom generator. However, that commitment scheme is diﬃcult
to use in this and other applications, because it utilizes an interactive commitment protocol. We
can do much better using a collision resistant hash function.
Let H be a hash function deﬁned over (X,Y) where X= M×R. Here Mis the message space
345
for the commitment scheme, and Ris a ﬁnite nonce space that will be used for the hiding property.
For m∈M, the derived commitment scheme CH = (C,V ) is deﬁned as:
C(m) :=
{
o←R R, c ←H(m,o), output(c,o)
}
,
V(m,c,o ) :=
{
output accept if c= H(m,o)
}
.
To argue that this is a secure commitment scheme we need the hash function H to satisfy two
properties.
• First, to ensure that CH is binding, we require that H is collision resistant. It is easy to
see that that collision resistance is suﬃcient, as an eﬃcient adversary Athat defeats the
binding property immediately gives a collision for H. Indeed, suppose Aoutputs two pairs
(m1,o1) and ( m2,o2), where m1 ̸= m2, but V(m1,c,o 1) = V(m2,c,o 2) = accept, for some
commitment string c. Then H(m1,o1) = c= H(m2,o2) is a collision for H.
Because the binding property depends on a computational assumption, we say that CH is
computationally binding. We note that the commitment scheme presented in Section 3.12
is unconditionally binding, meaning that the binding property holds without any compu-
tational assumptions.
• Second, to ensure that CH is binding, we require that H to satisﬁes a certain technical
property, called input hiding, which means that for adversarially chosen m1,m2 ∈M, and
for a random o←R R, the distribution {H(m1,o)}is computationally indistinguishable from
the distribution {H(m2,o)}(as in Deﬁnition 3.4). The input hiding property for H clearly
implies that the commitment scheme CH is hiding.
Standard collision resistant hash functions are believed to be input hiding in a stronger sense,
provided the set Ris suﬃciently large relative to the size of the output space Yof H (for
example, taking R= {0,1}512 should be suﬃcient for SHA256). Indeed, it is believed that
such hash functions are statistically input hiding , meaning that for all m1,m2 ∈ M
the distribution {H(m1,o)}is statistically indistinguishable from the distribution {H(m2,o)}
(see Deﬁnition 3.6). In this case, we say that the commitment scheme is unconditionally
hiding, meaning that the committed message remains well hidden without any computational
assumptions.
We note that the commitment scheme presented in Section 3.12 iscomputationally hiding,
meaning that the hiding property holds only under a computational assumption.
A weakly homomorphic commitment scheme. Let’s construct another commitment scheme,
this time with an additional property. The message space for this commitment will be Zq, for some
integer q > 1. We will need two hash functions: H1 deﬁned over ( R,Y) and H2 deﬁned over
(R,Zq). For m∈Zq, the derived commitment scheme CH1,H2 = (C,V ) is deﬁned as:
C(m) :=
{
o←R R, c1 ←H1(o), c2 ←m+ H2(o), output
(
(c1,c2),o
)}
V
(
m,(c1,c2),o
):=
{
output accept if c1 = H(o) and c2 = m+ H2(o)
}
As before, to argue that this is a secure commitment scheme we need the hash functions H1 and
H2 to satisfy two properties.
346
• First, to ensure thatCH1,H2 is binding, we require thatH1 is collision resistant. To see why this
is suﬃcient, suppose that an eﬃcient adversary Adefeats the binding property. Then Amust
output two pairs ( m1,o1) and ( m2,o2), where m1 ̸= m2, but V(m1,c,o 1) = V(m2,c,o 2) =
accept, for some commitment string c= (c1,c2). But then H1(o1) = c1 = H1(o2). For this to
be a collision, we need to argue that o1 ̸= o2, but this must be true because m1 + H2(o1) =
c2 = m2 + H2(o2). Hence, if o1 = o2 then m1 = m2, which is not the case.
• Second, for the hiding property we again need ( H1,H2) to satisfy a speciﬁc property. We
say that (H1,H2) are compatible if for o←R Rand r←R Zq, the distribution
(
H1(o),H2(o)
)
is computationally indistinguishable from the distribution
(
H1(o),r
)
. When ( H1,H2) are
compatible, then the message m∈Zq is eﬀectively hidden using a one-time pad, and therefore
the scheme is hiding.
When q <2128 and R= {0,1}768, it is believed that the functions H1(x) := SHA256(1 ∥x)
and H2(x) := SHA256(2 ∥x) mod q are unconditionally compatible, meaning that the
compatibility property holds without any computatitional assumption.
This new secure commitment scheme has an interesting property: let c= (c1,c2) be a commitment
for a message m ∈Zq, and let δ ∈Zq. Then c′ := (c1,c2 + δ) is a commitment for the message
(m+δ) ∈Zq. In other words, a commitment formcan be transformed into a commitment for (m+δ)
by anyone and without any knowledge ofm. A commitment scheme that has such a property is said
to be a weakly homomorphic commitment scheme or a malleable commitment scheme.
What is this good for? For some applications, a homomorphic property is a blessing, for others
it is a curse. For now, let’s see a negative application.
Consider again the auction scheme from the beginning of the section. In that scheme, the
homomorphic property leads to a complete break of the system, even when the commitment scheme
is otherwise secure. Let’s see the attack.
Suppose Alice submits a commitment c for her bid b ∈Zq for the vase. Bob really wants the
vase, and is willing to outspend Alice by a small amount. Unfortunately for Bob, he does not know
what Alice bid. But Bob has a way to cheat. Once Alice’s commitment c is posted on the public
bulletin board, Bob can use the homomorphic property to create a new commitment c′ for the
value b+ 1 ∈Zq. This ensures that Bob will beat Alice in the auction (unless b = q−1, but we
can assume that q is suﬃciently large so that this will not happen). Once Alice posts the opening
for her commitment c, Bob has what he needs to post the opening for his commitment c′.
This attack is possible even though the commitment scheme CH1,H2 is secure according to
Deﬁnition 8.8. A potential defense is to require that no two openings have the same opening
string o, but the auction house needs to be aware of this attack to know to check for this.
To avoid this problem altogether, the auction house needs a commitment scheme with a stronger
security property. The required property is called non-malleability, and is deﬁned and studied
in [56]. Interestingly, the ﬁrst commitment scheme CH, can be shown to be non-malleable, when H
is modeled as a random oracle, and its range Yis suﬃciently large.
8.13 Notes
Citations to the literature to be added.
347
8.14 Exercises
8.1 (Truncating a CRHF is dangerous). Let H be a collision resistant hash function deﬁned
over (M,{0,1}n). Use H to construct a hash function H′ over (M,{0,1}n) that is also collision
resistant, but if one truncates the output of H′ by one bit then H′ is no longer collision resistant.
That is, H′is collision resistant, but H′′(x) := H′(x)[0 . .n−2] is not.
8.2 (CRHF combiners). We want to build a CRHF H using two CRHFs H1 and H2, so that if
at some future time one of H1 or H2 is broken (but not both) then H is still secure.
(a) Suppose H1 and H2 are deﬁned over ( M,T). Let H(m) :=
(
H1(m),H2(m)
)
. Show that H
is a secure CRHF if either H1 or H2 is secure.
(b) Show that H′(x) = H1(H2(x)) need not be a secure CRHF even if one of H1 or H2 is secure.
8.3 (Broken collision resistance). For i = 1,...,n let Hi be a function deﬁned over ( M,T)
where T := {0,1}b. Deﬁne H(n) as the hash function
H(n)(m1,...,m n) := H1(m1) ⊕···⊕ Hn(mn)
deﬁned over (Mn,T). Suppose that H1,...,H n are independent random oracles.
(a) Show that H(2) is collision resistant if 2b is super-poly. In particular, show that CRadv[A,H(2)]
is at most Q4/2b for every adversary Athat makes at most Q queries to each of H1 and H2.
(b) Show that H(n) is not collision resistant whenever n≥b/2. In particular, there is an eﬃcient
adversary Asuch that CRadv[A,H(n)] ≥1/4, where Amakes 4 queries to each of the functions
H1,...,H n.
Hint: use the fact that a random 2 n×2n matrix over Z2 is full rank with probability at
least 1/4.
(c) Show that H(n) is not one-way whenever n≥b.
Discussion: In Chapter 17 we will see how to make this construction collision resistant even when
n≥b. We will also explain the reason for the interest in this construction.
8.4 (Extending the domain of a PRF with a CRHF). Suppose F is a secure PRF deﬁned over
(K,X,Y) and His a collision resistant hash deﬁned over (M,X). Show that F′(k,m) = F(k,H(m))
is a secure PRF. This shows that H can be used to extend the domain of a PRF.
8.5 (Hash-then-encrypt MAC). Let H be a collision resistant hash deﬁned over ( M,X) and
let E= (E,D) be a secure block cipher deﬁned over ( K,X). Show that the encrypted-hash MAC
system (S,V ) deﬁned by S(k,m) := E(k,H(m)) is a secure MAC.
Hint: Use Theorem 8.1.
8.6 (Finding many collisions). Let H be a hash function deﬁned over ( M,T) where N := |T|
and |M| ≫N. We showed that O(
√
N) evaluations of H are suﬃcient to ﬁnd a collision for
H with probability 1 /2. Show that O
(√
sN
)
evaluations of H are suﬃcient to ﬁnd s collisions
(x(1)
0 ,x(1)
1 ),..., (x(s)
0 ,x(s)
1 ) for H with probability at least 1/2. Therefore, ﬁnding a million collisions
is only about a thousand times harder than ﬁnding a single collision.
348
8.7 (Finding multi-collisions). Continuing with Exercise 8.6, we say that an s-collision for H
is a set of s distinct points x1,...,x s in Msuch that H(x1) = ··· = H(xs). Show that for each
constant value of s, O
(
N(s−1)/s)
evaluations of H are suﬃcient to ﬁnd an s-collision for H, with
probability at least 1/2.
8.8 (Collision ﬁnding in constant space). Let H be a hash function deﬁned over ( M,T)
where N := |M|. In Section 8.3 we developed a method to ﬁnd an H collision with constant
probability using O(
√
N) evaluations of H. However, the method required O(
√
N) memory space.
In this exercise we develop a constant-memory collision ﬁnding method that runs in about the
same time. More precisely, the method only needs memory to store two hash values in T. You may
assume that H : M→T is a random function chosen uniformly from Funs[ M,T] and T ⊆M. A
collision should be produced with probability at least 1 /2.
(a) Let x0 ←R Mand deﬁne H(i)(x0) to be the ith iterate of H starting at x0. For example,
H(3)(x0) = H(H(H(x0))).
(i) Let i be the smallest positive integer satisfying H(i)(x0) = H(2i)(x0).
(ii) Let jbe the smallest positive integer satisfyingH(j)(x0) = H(j+i)(x0). Notice that j ≤i.
Show that H(j−1)(x0) and H(j+i−1)(x0) are an H collision with probability at least 3 /4.
(b) Show that i from part (a) satisﬁes i= O(
√
N) with probability at least 3 /4 and that it can
be found using O(
√
N) evaluations of H. Once i is found, ﬁnding j takes another O(
√
N)
evaluations, as required. The entire process only needs to store two elements in T at any
given time.
8.9 (A parallel Merkle-Damg ˚ ard).The Merkle-Damg˚ ard construction in Section 8.4 gives a
sequential method for extending the domain of a secure CRHF. The tree construction in Fig. 8.16 is
a parallelizable approach: all the hash functions hwithin a single level can be computed in parallel.
Prove that the resulting hash function deﬁned over ( X≤L, X) is collision resistant, assuming h
is collision resistant. Here h is a compression function h : X2 →X , and we assume the message
length can be encoded as an element of X. More precisely, the hash function is deﬁned as follows:
input: m1 ...m s ∈Xs for some 1 ≤s≤L
output: y∈X
let t∈Z be the smallest power of two such that t≥s (i.e., t:= 2⌈log2 s⌉)
for i= s+ 1 to t: mi ←⊥
for i= t+ 1 to 2t−1:
ℓ←2(i−t) −1, r ←ℓ+ 1 / / indices of left and right children
if mℓ = ⊥and mr = ⊥: mi ←⊥ / / if node has no children, set node to null
else if mr = ⊥: mi ←mℓ / / if one child, propagate child as is
else mi ←h(mℓ,mr) / / if two children, hash with h
output y←h
(
m2t−1, s
)
/ / hash ﬁnal output and message length
8.10 (Secure variants of Davies-Meyer). Prove that the h1,h2, and h3 variants of Davies-
Meyer deﬁned on page 301 are collision resistant in the ideal cipher model.
8.11 (Insecure variants of Davies-Meyer). Show that the h4 and h5 variants of Davies-Meyer
deﬁned on page 301 are not collision resistant.
349
m1 m2 m3 m4 m5 m6 m7 m8 m9 m10 m11
h h h h h
h h h
h
h 11 (msg-len)
h
output
Figure 8.16: Tree-based Merkle-Damg˚ ard for a message of lengths= 11 blocks
8.12 (An insecure instantiation of Davies-Meyer). Let’s show that Davies-Meyer may not
be collision resistant when instantiated with a real-world block cipher. Let (E,D) be a block cipher
deﬁned over (K,X) where K= X= {0,1}n. For y∈X let y denote the bit-wise complement of y.
(a) Suppose that E(k,x) = E(k,x) for all keys k∈K and all x∈X. The DES block cipher has
precisely this property. Show that the Davies-Meyer construction, h(k,x) := E(k,x) ⊕x, is
not collision resistant when instantiated with algorithm E.
(b) Suppose ( E,D) is an Even-Mansour cipher, E(k,x) := π(x⊕k) ⊕k, where π : X →X
is a ﬁxed public permutation. Show that the Davies-Meyer construction instantiated with
algorithm E is not collision resistant.
Hint: Show that this Even-Mansour cipher satisﬁes the property from part (a).
8.13 (Merkle-Damg ˚ ard without length encoding).Suppose that in the Merkle-Damg˚ ard
construction, we drop the requirement that the padding block encodes the message length. Let h
be the compression function, let H be the resulting hash function, and let IV be the prescribed
initial value.
(a) Show that H is collision resistant, assuming his collision resistant and that it is hard to ﬁnd
a preimage of IV under h.
(b) Show that if h is a Davies-Meyer compression function, and we model the underlying block
cipher as an ideal cipher, then for any ﬁxed IV, it is hard to ﬁnd a preimage of IV under h.
8.14 (2nd-preimage resistance of Merkle-Damg ˚ ard).Let H be a Merkle-Damg˚ ard hash
built out of a Davies-Meyer compression function h : {0,1}n ×{0,1}ℓ →{0,1}n. Consider the
attack game characterizing 2nd-preimage resistance in Deﬁnition 8.6. Let us assume that the
initial, random message in that attack game consists of s blocks. We shall model the underlying
block cipher used in the Davies-Meyer construction as an ideal cipher, and adapt the attack game to
work in the ideal cipher model. Show that for every adversary Athat makes at most Qideal-cipher
queries, we have
SPRicadv[A,H] ≤(Q+ s)s
2n−1 .
Discussion: This bound for ﬁnding second preimages is signiﬁcantly better than the bound for
ﬁnding arbitrary collisions. Unfortunately, we have to resort to the ideal cipher model to prove it.
350
8.15 (Fixed points). We consider the Davies-Meyer and Miyaguchi-Preneel compression functions
deﬁned in Section 8.5.2.
(a) Show that for a Davies-Meyer compression function it is easy to ﬁnd a pair ( t,m) such that
hDM(t,m) = t. Such a pair is called a ﬁxed point for hDM.
(b) Show that in the ideal cipher model it is diﬃcult to ﬁnd ﬁxed points for the Miyaguchi-Preneel
compression function.
The next exercise gives an application for ﬁxed points.
8.16 (Finding second preimages in Merkle-Damg ˚ ard).In this exercise, we develop a second
preimage attack on Merkle-Damg˚ ard that roughly matches the security bounds in Exercise 8.14.
Let HMD be a Merkle-Damg˚ ard hash built out of a Davies-Meyer compression functionh: {0,1}n×
{0,1}ℓ →{0,1}n. Recall that HMD pads a given message with a padding block that encodes the
message length. We will also consider the hash function H, which is the same as HMD, but which
uses a padding block that does not encode the message length. Throughout this exercise, we model
the underlying block cipher in the Davies-Meyer construction as an ideal cipher. For concreteness,
assume ℓ= 2n.
(a) Let s≈2n/2. You are given a message M that consists of s random ℓ-bit blocks. Show that
by making O(s) ideal cipher queries, with probability 1 /2 you can ﬁnd a message M′ ̸= M
such that H(M′) = H(M). Here, the probability is over the random choice of M, the random
permutations deﬁning the ideal cipher, and the random choices made by your attack.
Hint: Repeatedly choose random blocks x in {0,1}ℓ until h(IV,x) is the same as one of
the schaining variables obtained when computing H(M). Use this xto construct the second
preimage M′.
(b) Repeat part (a) for HMD.
Hint: The attack in part (a) will likely ﬁnd a second preimage M′ that is shorter than M;
because of length encoding, this will not be a second preimage under HMD; nevertheless, show
how to use ﬁxed points (see previous exercise) to modify M′ so that it has the same length
as M.
Discussion: Let H be a hash function with an n-bit output. If H is a random function then break-
ing 2nd-preimage resistance takes about 2 n time. This exercise shows that for Merkle-Damg˚ ard
functions, breaking 2nd-preimage resistance can be done much faster, taking only about 2 n/2 time.
8.17 (The envelope method is a secure PRF). Consider the envelope method for building a
PRF from a hash function discussed in Section 8.7: Fenv(k,M) := H(k∥M ∥k). Here, we assume
that H is a Merkle-Damg˚ ard hash built from a compression functionh: {0,1}n×{0,1}ℓ →{0,1}n.
Assume that the keys for Fenv are ℓ-bit strings. Furthermore, assume that the message M is a bit
string whose length is an even multiple of ℓ (we can always pad the message, if necessary). Under
the assumption that both htop and hbot are secure PRFs, show that Fenv is a secure PRF.
Hint: Use the result of Exercise 7.6; also, ﬁrst consider a simpliﬁed setting where H does not
append the usual Merkle-Damg˚ ard padding block to the inputsk∥M ∥k(this padding block does
not really help in this setting, but it does not hurt either — it just complicates the analysis).
351
8.18 (The key-prepending method revisited). Consider the key-prepending method for build-
ing a PRF from a hash function discussed in Section 8.7:Fpre(k,M) := H(k∥M). Here, we assume
that H is a Merkle-Damg˚ ard hash built from a compression functionh: {0,1}n×{0,1}ℓ →{0,1}n.
Assume that the keys for Fpre are ℓ-bit strings. Under the assumption that both htop and hbot are
secure PRFs, show that Fpre is a preﬁx-free secure PRF.
8.19 (The key-appending method revisited). Consider the following variant of the key-
appending method for building a PRF from a hash function discussed in Section 8.7: F′
post(k,M) :=
H(M ∥PB ∥k). Here, we assume that H is a Merkle-Damg˚ ard hash built from a compression
function h : {0,1}n ×{0,1}ℓ →{0,1}n. Also, PB is the standard Merkle-Damg˚ ard padding for
M, which encodes the length of M. Assume that the keys for F′
post are ℓ-bit strings. Under the
assumption that his collision resistant and htop is a secure PRF, show that F′
post is a secure PRF.
8.20 (Dual PRFs). The security analysis of HMAC assumes that the underlying compression
function is a secure PRF when either input is used as the key. A PRF with this property is said to
be a dual PRF. Let F be a secure PRF deﬁned over ( K,X,Y) where Y= {0,1}n for some n. We
wish to build a new PRF ˆF that is a dual PRF. This ˆF can be used as a building block for HMAC.
(a) Suppose K= X. Show that the most natural construction ˆF(x,y) := F(x,y) ⊕F(y,x) is
insecure: there exists a secure PRF F for which ˆF is not a dual PRF.
Hint: Start from a secure PRF F′and then “sabotage” it to get the required F.
(b) Let G be a PRG deﬁned over ( S, K×X ). Let G0 : S→K be the left output of G and let
G1 : S→X be the right output of G. Let ˆF be the following PRF deﬁned over ( S,S,Y):
ˆF(x,y) := F
(
G0(x), G1(y)
)
⊕ F
(
G0(y), G1(x)
)
.
Prove that ˆF is a dual PRF assuming G is a secure PRG and that G1 is collision resistant.
8.21 (Sponge with low capacity is insecure). Let H be a sponge hash with rate r and
capacity c, built from a permutation π : {0,1}n →{0,1}n, where n = r+ c (see Section 8.8).
Assume r ≥2c. Show how to ﬁnd a collision for H with probability at least 1 /2 in time O(2c/2).
The colliding messages can be 2 r bits each.
8.22 (Sponge as a PRF). Let H be a sponge hash with rate r and capacity c, built from a
permutation π: {0,1}n →{0,1}n, where n= r+ c(see Section 8.8). Consider again the PRF built
from H by pre-pending the key: Fpre(k,M) := H(k ∥M). Assume that the key is r bits and the
output of Fpre is also r bits. Prove that in the ideal permutation model, where π is replaced by a
random permutation Π, this construction yields a secure PRF, assuming 2 r and 2c are super-poly.
Note: This follows immediately from the fact that H is indiﬀerentiable from a random oracle (see
Section 8.10.3) and Theorem 8.9. However, you are to give a direct proof of this fact.
Hint: Use the same domain splitting strategy as outlined in Exercise 7.17.
8.23 (Relations among hash function properties). In this exercise we explore the relation
between 2nd-preimage resistance and one-wayness. Let H be a hash function deﬁned over ( M,T)
where |M|≥ s·|T| for some s> 1.
(a) We say that m ∈ Mhas a second preimage if there exists an m′ ̸= m in Msuch that
H(m′) = H(m). Show that at most 1 /sof the elements in Mdo not have a second preimage.
352
(b) Suppose s is super-poly. Show that if H is 2nd-preimage resistant then it must also be one-
way. In particular, for every one-way adversary Athere is a 2nd-preimage adversary B, which
is an elementary wrapper around A, such that
OWadv[A,H] ≤2 ·SPRadv[B,H] + (1/s). (8.22)
Hint: Bis given a random m∈M and asks Ato invert H(m). Use part (a) to argue that
this Bsatisﬁes (8.22).
(c) Show that part (b) may be false when s >1 is poly-bounded, say when s = 2. Let H be
a hash function deﬁned over ( M,T) where M:= T ×{1,2,..., 2s}. Suppose that H is
2nd-preimage resistant. Let T′:= T ×{0,1}. Construct an H′ deﬁned over (M,T′) that is
2nd-preimage resistant, but not one-way. This H′ is a counter-example to part (b) when s
is poly-bounded. Note that |M|= s·|T ′|, so that H′ has compression ratio s, as required.
First construct H′when s= 2 and then generalize.
(d) Show that a collision resistant hash must be 2nd-preimage resistant, with no restriction on s.
8.24 (From TCR to 2nd-preimage resistance). Let H be a TCR hash deﬁned over (K,M,T).
Choose a random r∈M. Prove that fr(x) := H(r,x) is 2nd-preimage resistant, where r is treated
as a system parameter.
8.25 (File integrity: reducing read-only memory). The ﬁle integrity construction in Sec-
tion 8.11.4 uses additional read-only memory proportional to log |F|where |F|is the size of the ﬁle
F being protected.
(a) By ﬁrst hashing the ﬁle F and then hashing the key r, show how to reduce the amount of ad-
ditional read-only memory used to O(log log|F|). This requires storing additional O(log |F|)
bits on disk.
(b) Generalize your solution from part (a) to show how to reduce read-only overhead to constant
size independent of |F|. The extra information stored on disk is still of size O(log |F|).
8.26 (Strong 2nd-preimage resistance). Let H be a hash function deﬁned over ( X×Y , T)
where X := {0,1}n. We say that H is strong 2nd-preimage resistant , or simply strong
SPR, if no eﬃcient adversary, given a random x in X as input, can output y,x′,y′ such that
H(x,y) = H(x′,y′) with non-negligible probability.
(a) Show that HTCR(k,(x,y)) := H(k⊕x, y) is a TCR hash function assuming H is a strong
SPR hash function. If Xis relatively small and Yis much larger, we obtain a TCR for long
messages, and with short keys, that is a lot simpler than the construtions in Section 8.11.3.
(b) Let H be a strong SPR. Use H to construct a collision resistant hash function H′ deﬁned
over (Y, T).
Discussion: This result shows that when Yis much bigger than T, the range T of a strong
SPR must be as big as the range of a collision resistant hash function. This was not the
case for an SPR, whose range can be smaller than that of a collision resistant function, while
providing the same level of security.
353
(c) Let us show that a function H can be a strong SPR, but not collision resistant. For example,
consider the hash function:
H′′(0,0) := H′′(0,1) := 0 and H′′(x,y) := H(x,y) for all other inputs.
Prove that if |X|is super-poly and H is a strong SPR then so is H′′. However, H′′is clearly
not collision resistant.
8.27 (Enhanced TCR). Let H be a keyed hash function deﬁned over (K,M,T). We say that H
is an enhanced TCR if no eﬃcient adversary Acan win the following game with non-negligible
advantage: the adversary outputs m∈M, is given random k ∈K, and outputs ( k′,m′) such that
H(k,m) = H(k′,m′), where (k,m) ̸= (k′,m′). As usual, let eTCR adv[A,H] denote A’s advantage
against H.
(a) Show how to use an enhanced TCR to extend the domain of a MAC. Let H be a enhanced
TCR deﬁned over (KH,M,X) and let (S,V ) be a secure MAC deﬁned over ( K,X,T). Show
that the following is a secure MAC for messages in M:
S′(k,m) :=
{
r←R KH, h←H(r,m), t←S(k,h), output (r,t)
}
V′(
k,m, (r,t)
):= V
(
k,H(r,m),t
)
Discussion: The small domain MAC ( S,V ) in this construction is only given h as the
input message, where as when using a TCR, the small domain MAC was given ( r,h) as the
message. Hence, the message space of the small domain MAC can be much smaller when
using an enhanced TCR.
(b) Let H be a hash function deﬁned over ( K×M , T). Show that modeling H as a random
oracle makes H an enhanced TCR deﬁned over ( K,M,T), assuming |K|and |T| are super-
poly. Speciﬁcally, for every adversary Athat makes at most Qro queries to H, we have
eTCR(ro)adv[A,H] ≤ Q2
ro
2|T|·|K| + Qro
|T|.
Discussion: When |K| = |T| this bound is less than 2 Qro/|T|. This shows that there
is no generic birthday attack on an enhanced TCR. Consequently, the small domain MAC
(S,V ) can operate on shorter messages than needed in the MAC extension construction from
collision resistance, discussed in Section 8.2. This fact will be quite useful in Chapter 14.
(c) Let H be a strong SPR hash function over ( X×Y ,T), as deﬁned in Exercise 8.26, where
X:= {0,1}n. Show that H′(k,(x,y)) := H(k⊕x, y) is an enhanced TCR function.
Discussion: Other constructions for enhanced TCR functions can be found in [82].
(d) Let H be a TCR deﬁned over (K,M,T). Show that H′(k,m) := (H(k,m), k) is an enhanced
TCR deﬁned over (K, M, T ×K).
8.28 (Even-Mansour is not an enhanced TCR). Let X:= {0,1}n and let π : X→X be a
random permutation.
• Consider the Even-Mansour function H1(k,m) := π(m⊕k) ⊕k deﬁned over (X,X,X). This
function is trivially a TCR because for every ﬁxed k ∈X it deﬁnes a permutation on X.
Show that it is not an enhanced TCR as deﬁned in Exercise 8.27.
354
• Show that the function H2(k,m) := π(k⊕m) ⊕m is a TCR deﬁned over ( X,X,X).
8.29 (Weak collision resistance). Let H be a keyed hash function deﬁned over (K,M,T). We
say that H is a weak collision resistant (WCR) if no eﬃcient adversary can win the following
game with non-negligible advantage: the challenger chooses a random key k ∈ Kand lets the
adversary query the function H(k,·) at any input of its choice. The adversary wins if it outputs a
collision m0,m1 for H(k,·).
(a) Show that WCR is a weaker notion than a secure MAC: (1) show that every deterministic
secure MAC is WCR, (2) give an example of a secure WCR that is not a secure MAC.
(b) MAC domain extension with a WCR: let ( S,V ) be a secure MAC and let H be a WCR. Show
that the MAC system ( S′,V ′) deﬁned by S′(
(k0,k1),m
):= S
(
k1,H(k0,m)
)
is secure.
(c) Show that Merkle-Damg˚ ard expands a compressing ﬁxed-input length WCR to a variable
input length WCR. In particular, let h be a WCR deﬁned over ( K,X×Y ,X), where X:=
{0,1}n and Y:= {0,1}ℓ. Deﬁne H as a keyed hash function over (K, {0,1}≤L, X) as follows:
H
(
(k1,k2),M
):=



pad and break M into ℓ-bit blocks: m1,...,m s
t0 ←0n ∈X
for i= 1 to s do:
ti ←h
(
k1, (ti−1,mi)
)
encode s as a block b∈Y
ts+1 ←h
(
k2, (ts,b)
)
output ts+1



Show that H is a WCR if h is.
8.30 (A simple random oracle PRF). Let K= X = {0,1}n and let H be a hash function
deﬁned over (X,Y) for some Y. Let Fxor be the PRF Fxor(k,x) := H(k⊕x) deﬁned over (K,X,Y).
(a) Show that if Kis large then Fxor(k,x) is a secure PRF when H is modeled as a random
oracle. In particular, if Amakes at most QF PRF queries and Qro random oracle queries,
then PRFroadv[A,Fxor] ≤Qro ·QF/|K|.
(b) Show an attack on Fxor that has advantage at least 1/2 and makes at most 2n/2 PRF queries
and 2n/2 random oracle queries. You may also assume that |Y|≫ 2n.
8.31 (The trouble with random oracles). Let H be a hash function deﬁned over ( K×X , Y).
We showed that H(k,x) is a secure PRF when H is modeled as a random oracle. In this exercise
we show that this PRF can be tweaked into a new PRF F that uses H as a black-box, and that is
a secure PRF when H is modeled as a random model. However, for every concrete instantiation of
the hash function H, the PRF F becomes insecure.
For simplicity, assume that Kand Yconsist of bit strings of length n and that X consists of bit
strings of length at most L for some poly-bounded n and L. Assume also that the program for H
parses its input as a bit string of the form k∥x, where k∈K and x∈X.
Consider a program Exec(P,v,t ) that takes as input three bit strings P,v,t . When Exec(P,v,t )
runs, it attempts to interpret P as a program written in some programming language (take your
pick); it runs P on input v, but stops the execution after |t|steps (if necessary), where |t|is the
355
bit-length of t. The output of Exec(P,v,t ) is whatever P outputs on input v, or some special default
value if the time bound is exceeded. For simplicity, assume that Exec(P,v,t ) always outputs an n-
bit string (padding or truncating as necessary). Even though P on input v may run in exponential
time (or even fall into an inﬁnite loop), Exec(P,v,t ) always runs in time bounded by a polynomial
in its input length.
Finally, let T be some arbitrary polynomial, and deﬁne
F(k,x) := H(k,x) ⊕Exec(x, k∥x, 0T(|k|+|x|)).
(a) Show that if H is any hash function that can be implemented by a program PH whose length
is at most Land whose running time on input k∥xis at most T(|k|+ |x|), then the concrete
instantiation of F using this H runs in polynomial time and is not a secure PRF.
Hint: Find a value of x that makes the PRF output 0 n, for all keys k∈K.
(b) Show that F is a secure PRF if H is modeled as a random oracle.
Discussion: Although this is a contrived example, it shakes our conﬁdence in the random oracle
model. Nevertheless, the reason why the random oracle model has been so successful in practice is
that typically real-world attacks treat the hash function as a black box. The attack on F clearly
does not. See also the discussion in [40], which removes the strict time bound restriction on H.
356
Chapter 9
Authenticated Encryption
This chapter is the culmination of our symmetric encryption story. Here we construct systems that
ensure both data secrecy (conﬁdentiality) and data integrity, even against very aggressive attackers
that can interact maliciously with both the sender and the receiver. Such systems are said to
provide authenticated encryption or are simply said to be AE-secure. This chapter concludes
our discussion of symmetric encryption, and shows how to correctly do secure encryption in the
real-world.
Recall that in our discussion of CPA security in Chapter 5 we stressed that CPA security does
not provide any integrity. An attacker can tamper with the output of a CPA-secure cipher without
being detected by the decryptor. We will present many real-world settings where undetected
ciphertext tampering compromises both message secrecy and message integrity. Consequently,
CPA security by itself is insuﬃcient for almost all applications. Instead, applications should almost
always use authenticated encryption to ensure both message secrecy and integrity. We stress that
even if secrecy is the only requirement, CPA security is insuﬃcient.
In this chapter we develop the notion of authenticated encryption and construct several AE
systems. There are two general paradigms for constructing AE systems. The ﬁrst, called generic
composition, is to combine a CPA-secure cipher with a secure MAC. There are many ways to
combine these two primitives and not all combinations are secure. We brieﬂy consider two examples.
Let (E,D) be a cipher and ( S,V ) be a MAC. Let kenc be a cipher key and kmac be a MAC key.
Two options for combining encryption and integrity immediately come to mind, which are shown
in Fig. 9.1 and work as follows:
Encrypt-then-MAC Encrypt the message, c ←R E(kenc,m), then MAC the ciphertext, tag ←R
S(kmac,c); the result is the ciphertext-tag pair ( c,tag). This method is supported in the
TLS 1.2 protocol and later versions as well as in the IPsec protocol and in a widely-used
NIST standard called GCM (see Section 9.7).
MAC-then-encrypt MAC the message, tag ←R S(kmac,m), then encrypt the message-tag pair,
c ←R E
(
kenc, (m,t)
)
; the result is the ciphertext c. This method is used in older versions
of TLS (e.g., SSL 3.0 and its successor called TLS 1.0) and in the 802.11i WiFi encryption
protocol.
As it turns out, only the ﬁrst method is secure for every combination of CPA-secure cipher and
secure MAC. The intuition is that the MAC on the ciphertext prevents any tampering with the
ciphertext. We will show that the second method can be insecure — the MAC and cipher can
357
m
c←E(kenc,m )
c tag
c←E(kenc,m )
c
tag ←S(kmac,c)
m
m tag
c←E(kenc, (m,tag) )c←E(kenc, (m,tag) )
tag ←S(kmac,m)
encrypt-then-mac mac-then-encrypt
Figure 9.1: Two methods to combine encryption and MAC
interact badly and cause the resulting system to not be AE-secure. This has led to many attacks
on widely deployed systems.
The second paradigm for constructing authenticated encryption is to build it directly from a
block cipher or a PRF, without ﬁrst constructing a standalone cipher or a MAC. These are called
integrated schemes. The OCB encryption mode is the primary example in this category (see
Exercise 9.17). Other examples include IAPM, XCBC, and CCFB.
Authenticated encryption standards. Cryptographic libraries such as OpenSSL often provide
an interface for CPA-secure encryption (such as counter mode with a random IV) and a separate
interface for computing MACs on messages. In the past, it was up to developers to correctly
combine these two primitives to provide authenticated encryption. Every system did it diﬀerently
and not all incarnations used in practice were secure.
More recently, several standards have emerged for secure authenticated encryption. A popular
method called Galois Counter Mode (GCM) uses encrypt-then-MAC to combine random counter
mode encryption with a Carter-Wegman MAC (see Section 9.7). We will examine the details
of this construction and its security later on in the chapter. Developers are encouraged to use
an authenticated encryption mode provided by the underlying cryptographic library and to not
implement it themselves.
9.1 Authenticated encryption: deﬁnitions
We start by deﬁning what it means for a cipher Eto provide authenticated encryption. It must
satisfy two properties. First, Emust be CPA-secure. Second, Emust provide ciphertext integrity,
as deﬁned below. Ciphertext integrity is a new property that captures the fact that E should
have properties similar to a MAC. Let E= (E,D) be a cipher deﬁned over ( K,M,C). We deﬁne
ciphertext integrity using the following attack game, shown in Fig. 9.2. The game is analogous to
the MAC Attack Game 6.1.
Attack Game 9.1 (ciphertext integrity). For a given cipherE= (E,D) deﬁned over (K,M,C),
and a given adversary A, the attack game runs as follows:
• The challenger chooses a random k←R K.
358
Challenger Adversary A
k←
R
K mi
ci ←E(k,mi)
c
Figure 9.2: Ciphertext integrity game (Attack Game 9.1)
• Aqueries the challenger several times. For i= 1,2,..., the ith query consists of a
message mi ∈M. The challenger computes ci ←R E(k,mi), and gives ci to A.
• Eventually Aoutputs a candidate ciphertext c∈C that is not among the cipher-
texts it was given, i.e.,
c̸∈{c1,c2,... }.
We say that Awins the game if cis a valid ciphertext under k, that is, D(k,c) ̸= reject. We deﬁne
A’s advantage with respect to E, denoted CI adv[A,E], as the probability that Awins the game.
Finally, we say that Ais a Q-query adversary if Aissues at most Q encryption queries. 2
Deﬁnition 9.1. We say that a E= (E,D) provides ciphertext integrity, or CI for short, if for
every eﬃcient adversary A, the value CIadv[A,E] is negligible.
CPA security and ciphertext integrity are the properties needed for authenticated encryption.
This is captured in the following deﬁnition.
Deﬁnition 9.2. We say that a cipher E = ( E,D) provides authenticated encryption, or is
simply AE-secure, if Eis (1) semantically secure under a chosen plaintext attack, and (2) provides
ciphertext integrity.
Why is Deﬁnition 9.2 the right deﬁnition? In particular, why are we requiring ciphertext in-
tegrity, rather than some notion of plaintext integrity (which might seem more natural)? In Sec-
tion 9.2, we will describe a very insidious class of attacks called chosen ciphertext attacks , and we
will see that our deﬁnition of AE-security is suﬃcient (and, indeed, necessary) to prevent such
attacks. In Section 9.3, we give a more high-level justiﬁcation for the deﬁnition.
9.1.1 One-time authenticated encryption
In practice, one often uses a symmetric key to encrypt a single message. The key is never used
again. For example, when sending encrypted email one often picks an ephemeral key and encrypts
the email body under this ephemeral key. The ephemeral key is then encrypted and transmitted
in the email header. A new ephemeral key is generated for every email.
In these settings one can use a one-time encryption scheme such as a stream cipher. The
cipher must be semantically secure, but need not be CPA-secure. Similarly, it suﬃces that the
359
cipher provides one-time ciphertext integrity, which is a weaker notion than ciphertext-integrity.
In particular, we change Attack Game 9.1 so that the adversary can only obtain the encryption of
a single message m.
Deﬁnition 9.3. We say that E= (E,D) provides one-time ciphertext integrity if for every
eﬃcient single-query adversary A, the value CIadv[A,E] is negligible.
Deﬁnition 9.4. We say that E= (E,D) provides one-time authenticated encryption , or is
1AE-secure for short, if Eis semantically secure and provides one-time ciphertext integrity.
In applications that only use a symmetric key once, 1AE-security suﬃces. We will show that
the encrypt-then-MAC construction of Fig. 9.1 using a semantically secure cipher and a one-time
MAC, provides one-time authenticated encryption. Replacing the MAC by a one-time MAC can
lead to eﬃciency improvements.
9.2 Implications of authenticated encryption
Before constructing AE-secure systems, let us ﬁrst play with Deﬁnition 9.1 a bit to see what it
implies. Consider a sender, Alice, and a receiver, Bob, who have a shared secret key k. Alice sends
a sequence of messages to Bob over a public network. Each message is encrypted with an AE-secure
cipher E= (E,D) using the key k.
For starters, consider an eavesdropping adversary A. Since Eis CPA-secure this does not help
Alearn any new information about messages sent from Alice to Bob.
Now consider a more aggressive adversary Athat attempts to make Bob receive a message that
was not sent by Alice. We claim this cannot happen. To see why, consider the following single-
message example: Alice encrypts to Bob a message mand the resulting ciphertext cis intercepted
by A. The adversary’s goal is to create some ˆ c such that ˆm := D(k,ˆc) ̸= reject and ˆm ̸= m.
This ˆc would fool Bob into thinking that Alice sent ˆm rather than m. But then Acould also win
Attack Game 9.1 with respect to E, contradicting E’s ciphertext integrity. Consequently, Acannot
modify c without being detected. More generally, applying the argument to multiple messages
shows that Acannot cause Bob to receive any messages that were not sent by Alice. The more
general conclusion here is that ciphertext integrity implies message integrity.
9.2.1 Chosen ciphertext attacks: a motivating example
We now consider an even more aggressive type of attack, called a chosen ciphertext attack for
short. As we will see, an AE-secure cipher provides message secrecy and message integrity even
against such a powerful attack.
To motivate chosen ciphertext attacks suppose Alice sends an email message to Bob. For
simplicity let us assume that every email starts with the letters To: followed by the recipient’s
email address. So, an email to Bob starts with To:bob@mail.com and an email to Mel begins with
To:mel@mail.com. The mail server decrypts every incoming email and writes it into the recipient’s
inbox: emails that start with To:bob@mail.com are written to Bob’s inbox and emails that start
with To:mel@mail.com are written to Mel’s inbox.
Mel, the attacker in this story, wants to read the email that Alice sent to Bob. Unfortunately
for Mel, Alice was careful and encrypted the email using a key known only to Alice and to the mail
360
server. When the ciphertext c is received at the mail server it will be decrypted and the resulting
message is placed into Bob’s inbox. Mel will be unable to read it.
Nevertheless, let us show that if Alice encrypts the email with a CPA-secure cipher such as
randomized counter mode or randomized CBC mode then Mel can quite easily obtain the email
contents. Here is how: Mel will intercept the ciphertext cen-route to the mail server and modify it
to obtain a ciphertext ˆcso that the decryption of ˆcstarts with To:mel@mail.com, but is otherwise
the same as the original message. Mel then forwards ˆ c to the mail server. When the mail server
receives ˆc it will decrypt it and (incorrectly) place the plaintext into Mel’s inbox where Mel can
easily read it.
To successfully carry out this attack, Mel must ﬁrst solve the following problem: given an encryp-
tion cof some message (u∥m) where uis a ﬁxed known preﬁx (in our caseu:= To:bob@mail.com),
compute a ciphertext ˆc that will decrypt to the message ( v ∥m), where v is some other preﬁx (in
our case v:= To:mel@mail.com).
Let us show that Mel can easily solve this problem, assuming the encryption scheme is either
randomized counter mode or randomized CBC. For simplicity, we also assume that u and v are
binary strings whose length is the same as the block size of the underlying block cipher. As usual
c[0] and c[1] are the ﬁrst and second blocks of c where c[0] is the random IV. Mel constructs ˆc as
follows:
• randomized counter mode: deﬁne ˆc to be the same as c except that ˆc[1] := c[1] ⊕u⊕v.
• randomized CBC mode: deﬁne ˆc to be the same as c except that ˆc[0] := c[0] ⊕u⊕v.
It is not diﬃcult to see that in either case the decryption of ˆ c starts with the preﬁx v (see Sec-
tion 3.3.2). Mel is now able to obtain the decryption of ˆ c and read the secret message m in the
clear.
What just happened? We proved that both encryption modes are CPA secure, and yet we just
showed how to break them. This attack is an example of a chosen ciphertext attack — by querying
for the decryption of ˆc, Mel was able to deduce the decryption of c. This attack is also another
demonstration of how attackers can exploit the malleability of a cipher — we saw another attack
based on malleability back in Section 3.3.2.
As we just saw, a CPA-secure system can become completely insecure when an attacker can
decrypt certain ciphertexts, even if he cannot directly decrypt a ciphertext that interests him. Put
another way, the lack of ciphertext integrity can completely compromise secrecy — even if plaintext
integrity is not an explicit security requirement.
We informally argue that if Alice used an AE-secure cipher E = ( E,D) then it would be
impossible to mount the attack we just described. Suppose Mel intercepts a ciphertextc:= E(k,m).
He tries to create another ciphertext ˆ c such that (1) ˆm := D(k,ˆc) starts with preﬁx v, and (2)
the adversary can recover m from ˆm, in particular ˆm̸= reject. Ciphertext integrity, and therefore
AE-security, implies that the attacker cannot create this ˆc. In fact, the attacker cannot create any
new valid ciphertexts and therefore an AE-secure cipher foils the attack.
In the next section, we formally deﬁne the notion of a chosen ciphertext attack, and show that
if a cipher is AE-secure then it is secure even against this type of attack.
361
9.2.2 Chosen ciphertext attacks: deﬁnition
In this section, we formally deﬁne the notion of a chosen ciphertext attack . In such an attack,
the adversary has all the power of an attacker in a chosen plaintext attack, but in addition, the
adversary may obtain decryptions of ciphertexts of its choosing — subject to a restriction. Recall
that in a chosen plaintext attack, the adversary obtains a number of ciphertexts from its challenger,
in response to encryption queries. The restriction we impose is that the adversary may not ask
for the decryptions of any of these ciphertexts. While such a restriction is necessary to make the
attack game at all meaningful, it may also seem a bit unintuitive: if the adversary can decrypt
ciphertexts of its choosing, why would it not decrypt the most important ones? We will explain
later (in Section 9.3) more of the intuition behind this deﬁnition. We will show in Section 9.2.3
that if a cipher is AE-secure then it is secure against a chosen ciphertext attack.
Here is the formal attack game:
Attack Game 9.2 (CCA security). For a given cipher E= (E,D) deﬁned over (K,M,C), and
for a given adversary A, we deﬁne two experiments. For b= 0,1, we deﬁne
Experiment b:
• The challenger selects k←R K.
• Athen makes a series of queries to the challenger. Each query can be one of two types:
– Encryption query: for i= 1,2,..., the ith encryption query consists of a pair of messages
(mi0,mi1) ∈M2. The challenger computes ci ←R E(k,mib) and sends ci to A.
– Decryption query: for j = 1 ,2,..., the jth decryption query consists of a ciphertext
ˆcj ∈C that is not among the responses to the previous encryption queries, i.e.,
ˆcj /∈{c1,c2,... }.
The challenger computes ˆmj ←D(k,ˆcj), and sends ˆmj to A.
• At the end of the game, the adversary outputs a bit ˆb∈{0,1}.
Let Wb be the event that Aoutputs 1 in Experiment band deﬁne A’s advantage with respect
to Eas
CCAadv[A,E] :=
⏐⏐Pr[W0] −Pr[W1]
⏐⏐. 2
We stress that in the above attack game, the encryption and decryption queries may be arbi-
trarily interleaved with one another.
Deﬁnition 9.5 (CCA security). A cipher Eis called semantically secure against chosen ci-
phertext attack, or simply CCA-secure, if for all eﬃcient adversaries A, the value CCAadv[A,E]
is negligible.
In some settings, a new key is generated for every message so that a particular key k is only
used to encrypt a single message. The system needs to be secure against chosen ciphertext attacks
where the attacker fools the user into decrypting multiple ciphertexts using k. For these settings
we deﬁne security against an adversary that can only issue a single encryption query, but many
decryption queries.
362
Deﬁnition 9.6 (1CCA security). In Attack Game 9.2, if the adversary Ais restricted to making
a single encryption query, we denote its advantage by 1CCAadv[A,E]. A cipher E is one-time
semantically secure against chosen ciphertext attack , or simply, 1CCA-secure, if for all
eﬃcient adversaries A, the value 1CCAadv[A,E] is negligible.
As discussed in Section 2.2.5, Attack Game 9.2 can be recast as a “bit guessing” game, where
instead of having two separate experiments, the challenger chooses b∈{0,1}at random, and then
runs Experiment b against the adversary A. In this game, we measure A’s bit-guessing advantage
CCAadv∗[A,E] (and 1CCA adv∗[A,E]) as |Pr[ˆb = b] −1/2|. The general result of Section 2.2.5
(namely, (2.11)) applies here as well:
CCAadv[A,E] = 2 ·CCAadv∗[A,E]. (9.1)
And similarly, for adversaries restricted to a single encryption query, we have:
1CCAadv[A,E] = 2 ·1CCAadv∗[A,E]. (9.2)
9.2.3 Authenticated encryption implies chosen ciphertext security
We now show that every AE-secure system is also CCA-secure. Similarly, every 1AE-secure system
is 1CCA-secure.
Theorem 9.1. Let E = ( E,D) be a cipher. If E is AE-secure, then it is CCA-secure. If E is
1AE-secure, then it is 1CCA-secure.
In particular, suppose Ais a CCA-adversary for Ethat makes at most Qe encryption queries
and Qd decryption queries. Then there exist a CPA-adversary Bcpa and a CI-adversary Bci,
where Bcpa and Bci are elementary wrappers around A, such that
CCAadv[A,E] ≤CPAadv[Bcpa,E] + 2Qd ·CIadv[Bci,E]. (9.3)
Moreover, Bcpa and Bci both make at most Qe encryption queries.
Before proving this theorem, we point out a converse of sorts: if a cipher is CCA-secure and
provides plaintext integrity, then it must be AE-secure. You are asked to prove this in Exercise 9.15.
These two results together provide strong support for the claim that AE-security is the right notion
of security for general purpose communication over an insecure network. We also note that it is
possible to build a CCA-secure cipher that does not provide ciphertext (or plaintext) integrity —
see Exercise 9.12 for an example.
Proof idea. A CCA-adversary Aissues encryption and allowed decryption queries. We ﬁrst argue
that the response to all these decryption queries must be reject. To see why, observe that if the
adversary ever issues a valid decryption query ci whose decryption is not reject, then this ci can be
used to win the ciphertext integrity game. Hence, since all of A’s decryption queries are rejected,
the adversary learns nothing by issuing decryption queries and they may as well be discarded. After
removing decryption queries we end up with a standard CPA game. The adversary cannot win this
game because Eis CPA-secure. We conclude that Ahas negligible advantage in winning the CCA
game. 2
Proof. Let Abe an eﬃcient CCA-adversary attacking Eas in Attack Game 9.2, and which makes
at most Qe encryption queries and Qd decryption queries. We want to show that CCA adv[A,E]
363
is negligible, assuming that Eis AE-secure. We will use the bit-guessing versions of the CCA and
CPA attack games, and show that
CCAadv∗[A,E] ≤CPAadv∗[Bcpa,E] + Qd ·CIadv[Bci,E]. (9.4)
for eﬃcient adversaries Bcpa and Bci. Then (9.3) follows from (9.4), along with (9.1) and (5.4).
Moreover, as we shall see, the adversary Bcpa makes at most Qe encryption queries; therefore, if E
is 1AE-secure, it is also 1CCA-secure.
Let us deﬁne Game 0 to be the bit-guessing version of the CCA Attack Game 9.2. The challenger
in this game, called Game 0, works as follows:
b←R {0,1} / / Awill try to guess b
k←R K
upon receiving the ith encryption query ( mi0,mi1) from Ado:
send ci ←R E(k,mib) to A
upon receiving the jth decryption query ˆcj from Ado:
(1) send D(k,ˆcj) to A
Eventually the adversary outputs a guess ˆb∈{0,1}. We say that Awins the game if b= ˆband we
denote this event by W0. By deﬁnition, the bit-guessing advantage is
CCAadv∗[A,E] =
⏐⏐Pr[W0] −1/2
⏐⏐. (9.5)
Game 1. We now modify line (1) in the challenger as follows:
(1) send reject to A
We argue that Acannot distinguish this challenger from the original. Let Z be the event that in
Game 1, Aissues a decryption query ˆcj such that D(k,ˆcj) ̸= reject. Clearly, Games 0 and 1 proceed
identically as long as Z does not happen. Hence, by the Diﬀerence Lemma (i.e., Theorem 4.7) it
follows that
⏐⏐Pr[W0] −Pr[W1]
⏐⏐≤Pr[Z].
Using a “guessing strategy” similar to that used in the proof of Theorem 6.1, we can use Ato
build a CI-adversary Bci that wins the CI attack game with probability at least Pr[ Z]/Qd. Note
that in Game 1, the decryption algorithm is not used at all. Adversary Bci’s strategy is simply to
guess a random number ω ∈{1,...,Q d}, and choose a random bit b in {0,1}. It then plays the
role of challenger to A:
• when Amakes an encryption query (m0,m1), our Bci forwards mb to its own challenger, and
returns the response to A;
• when Amakes a decryption query ˆcj, Bci simply sends reject to A. However, when j = ω,
our Bci outputs ˆcj and halts.
It is not hard to see that CI adv[Bci,E] ≥Pr[Z]/Qd, and so
⏐⏐⏐Pr[W0] −Pr[W1]
⏐⏐⏐≤Pr[Z] ≤Qd ·CIadv[Bci,E]. (9.6)
Final reduction. Since all decryption queries are rejected in Game 1, this is essentially a CPA
attack game. More precisely, we can construct a CPA adversaryBcpa that plays the role of challenger
to Aas follows:
364
• when Amakes an encryption query ( m0,m1), our Bcpa forwards this to its own CPA chal-
lenger, and returns the response to A;
• when Amakes a decryption query, Bcpa simply sends reject to A.
At the end of the game, Bcpa simply outputs the bit ˆb that Aoutputs. Clearly,
|Pr[W1] −1/2|= CPAadv∗[Bcpa,E] (9.7)
Putting equations (9.5)–(9.7) together gives us (9.4), which proves the theorem. 2
9.3 Encryption as an abstract interface
To further motivate the deﬁnition of authenticated encryption we show that it precisely captures
an intuitive notion of secure encryption as an abstract interface. AE-security implies that the real
implementation of this interface may be replaced by an idealized implementation in which messages
literally jump from sender to receiver, without going over the network at all (even in encrypted
form). We now develop this idea more fully.
Suppose a sender S and receiver R are using some arbitrary Internet-based system (e.g, gam-
bling, auctions, banking — whatever). Also, we assume that S and R have already established
a shared, random encryption key k. During the protocol, S will send encryptions of messages
m1,m2,... to R. The messages mi are determined by the logic of the protocol S is using, whatever
that happens to be. We can imagine S placing a message mi in his “out-box”, the precise details
of how the out-box works being of no concern to S. Of course, inside S’s out-box, we know what
happens: an encryption ci of mi under k is computed, and this is sent out over the wire to R.
On the receiving end, when a ciphertext ˆ c is received at R’s end of the wire, it is decrypted
using k, and if the decryption is a message ˆ m ̸= reject, the message ˆm is placed in R’s “in-box”.
Whenever a message appears in his in-box, Rcan retrieve it and processes it according to the logic
of his protocol, without worrying about how the message got there.
An attacker may try to subvert communication between S and R in a number of ways.
• First, the attacker may drop, re-order, or duplicate the ciphertexts sent by S.
• Second, the attacker may modify ciphertexts sent by S, or inject ciphertexts created out of
“whole cloth”.
• Third, the attacker may have partial knowledge of some of the messages sent by S, or may
even be able to inﬂuence the choice of some of these messages.
• Fourth, by observing R’s behavior, the attacker may be able to glean partial knowledge of
some of the messages processed by R. Even the knowledge of whether or not a ciphertext
delivered to R was rejected could be useful.
Having described an abstract encryption interface and its implementation, we now describe an
ideal implementation of this interface that captures in an intuitive way the guarantees ensured by
authenticated encryption. When S drops mi in its out-box, instead of encrypting mi, the ideal
implementation creates a ciphertext ci by encrypting a dummy message dummyi, that has nothing
to do with mi (except that it should be of the same length). Thus, ci serves as a “handle” for mi,
365
but does not contain any information about mi (other than its length). When ci arrives at R, the
corresponding message mi is magically copied from S’s out-box to R’s in-box. If a ciphertext arrives
at R that is not among the previously generated ci’s, the ideal implementation simply discards it.
This ideal implementation is just a thought experiment. It obviously cannot be physically
realized in any eﬃcient way (without ﬁrst inventing teleportation). As we shall argue, however, if
the underlying cipher Eprovides authenticated encryption, the ideal implementation is — for all
practical purposes — equivalent to the real implementation. Therefore, a protocol designer need
not worry about any of the details of the real implementation or the nuances of cryptographic
deﬁnitions: he can simply pretend he is using the abstract encryption interface with its ideal
implementation, in which ciphertexts are just handles and messages magically jump from S to R.
Hopefully, analyzing the security properties of the higher-level protocol will be much easier in this
setting.
Note that even in the ideal implementation, the attacker may still drop, re-order, or duplicate
ciphertexts, and these will cause the corresponding messages to be dropped, re-ordered, or dupli-
cated. Using sequence numbers and buﬀers, it is not hard to deal with these possibilities, but that
is left to the higher-level protocol.
We now argue informally that when Eprovides authenticated encryption, the real world im-
plementation is indistinguishable from the ideal implementation. The argument proceeds in three
steps. We start with the real implementation, and in each step, we make a slight modiﬁcation.
• First, we modify the real implementation of R’s in-box, as follows. When a ciphertext ˆ c
arrives on R’s end, the list of ciphertexts c1,c2,... previously generated by S is scanned, and
if ˆc = ci, then the corresponding message mi is magically copied from S’s out-box into R’s
in-box, without actually running the decryption algorithm.
The correctness property of Eensures that this modiﬁcation behaves exactly the same as the
real implementation.
• Second, we modify the implementation on R’s in-box again, so that if a ciphertext ˆc arrives
on R’s end that is not among the ciphertexts generated by S, the implementation simply
discards ˆc.
The only way the adversary could distinguish this modiﬁcation from the ﬁrst is if it could
create a ciphertext that would not be rejected and was not generated by S. But this is not
possible, since Ehas ciphertext integrity.
• Third, we modify the implementation of S’s out-box, replacing the encryption of mi with
the encryption of dummyi. The implementation of R’s in-box remains as in the second
modiﬁcation. Note that the decryption algorithm is never used in either the second or third
modiﬁcations. Therefore, an adversary who can distinguish this modiﬁcation from the second
can be used to directly break the CPA-security of E. Hence, since Eis CPA-secure, the two
modiﬁcations are indistinguishable.
Since the third modiﬁcation is identical to the ideal implementation, we see that the real and ideal
implementations are indistinguishable from the adversary’s point of view.
A technical point we have not considered is the possibility that the ci’s generated by S are not
unique. Certainly, if we are going to view theci’s as handles in the ideal implementation, uniqueness
366
would seem to be an essential property. In fact, CPA-security implies that the ci’s generated in the
ideal implementation are unique with overwhelming probability — see Exercise 5.12.
9.4 Authenticated encryption ciphers from generic composition
We now turn to constructing authenticated encryption by combining a CPA-secure cipher and a
secure MAC. We show that encrypt-then-MAC is always AE-secure, but MAC-then-encrypt is not.
9.4.1 Encrypt-then-MAC
Let E = (E,D) be a cipher deﬁned over ( Ke,M,C) and let I= (S,V ) be a MAC deﬁned over
(Km,C,T). The encrypt-then-MAC system EEtM = (EEtM,DEtM), or EtM for short, is deﬁned
as follows:
EEtM( (ke,km), m) := c←R E(ke,m), t ←R S(km,c)
Output (c,t)
DEtM((ke,km), (c,t) ) := if V(km,c,t ) = reject then output reject
otherwise, output D(ke,c)
The EtM system is deﬁned over ( Ke ×Km, M, C×T ). The following theorem shows that EEtM
provides authenticated encryption.
Theorem 9.2. Let E= (E,D) be a cipher and let I= (S,V ) be a MAC system. Then EEtM is
AE-secure assuming E is CPA-secure and Iis a secure MAC system. Also, EEtM is 1AE-secure
assuming Eis semantically secure and Iis a one-time secure MAC system.
In particular, for every ciphertext integrity adversary Aci that attacks EEtM as in Attack
Game 9.1 there exists a MAC adversary Bmac that attacks I as in Attack Game 6.1, where
Bmac is an elementary wrapper around Aci, and which makes no more signing queries than Aci
makes encryption queries, such that
CIadv[Aci,EEtM] = MACadv[Bmac,I].
For every CPA adversary Acpa that attacks EEtM as in Attack Game 5.2 there exists a CPA
adversary Bcpa that attacks E as in Attack Game 5.2, where Bcpa is an elementary wrapper
around Acpa, and which makes no more encryption queries than does Acpa, such that
CPAadv[Acpa,EEtM] = CPAadv[Bcpa,E].
Proof. Let us ﬁrst show thatEEtM provides ciphertext integrity. Suppose Aci is a ciphertext integrity
adversary attacking EEtM. We construct a MAC adversary Bmac attacking I.
Adversary Bmac plays the role of adversary in a MAC attack game for I. It interacts with
a MAC challenger Cmac that starts by picking a random km ←R Km. Adversary Bmac works by
emulating a EEtM ciphertext integrity challenger for Aci, as follows:
367
ke ←R Ke
upon receiving a query mi ∈M from Aci do:
ci ←R E(ke,mi)
Query Cmac on ci and obtain ti ←R S(km,ci) in response
Send (ci,ti) to Aci / / then (ci,ti) = EEtM( (ke,km), mi)
eventually Aci outputs a ciphertext ( c,t) ∈C×T
output the message-tag pair ( c,t)
It should be clear that Bmac responds to Aci’s queries as in a real ciphertext integrity attack game.
Therefore, with probability CI adv[Aci,EEtM] adversary Aci outputs a ciphertext ( c,t) that makes
it win Attack Game 9.1 so that ( c,t) ̸∈{(c1,t1),... }and V(km,c,t ) = accept. It follows that ( c,t)
is a message-tag pair that lets Bmac win the MAC attack game and therefore CI adv[Aci,EEtM] =
MACadv[Bmac,I], as required.
It remains to show that ifEis CPA-secure then so isEEtM. This simply says that the tag included
in the ciphertext, which is computed using the key km (and does not involve the encryption key ke
at all), does not help the attacker break CPA security of EEtM. This is straightforward and is left
as an easy exercise (see Exercise 5.20). 2
Recall that our deﬁnition of a secure MAC from Chapter 6 requires that given a message-tag
pair (c,t) the attacker cannot come up with a new tag t′̸= tsuch that (c,t′) is a valid message-tag
pair. At the time it seemed odd to require this: if the attacker already has a valid tag for c, why
do we care if he ﬁnds another tag for c? Here we see that if the attacker could come with a new
valid tag t′ for c then he could break ciphertext integrity for EtM. From an EtM ciphertext ( c,t)
the attacker could construct a new valid ciphertext ( c,t′) and win the ciphertext integrity game.
Our deﬁnition of secure MAC ensures that the attacker cannot modify an EtM ciphertext without
being detected.
9.4.1.1 Common mistakes in implementing encrypt-then-MAC
A common mistake when implementing encrypt-then-MAC is to use the same key for the cipher and
the MAC, i.e., setting ke = km. The resulting system need not provide authenticated encryption
and can be insecure, as shown in Exercise 9.8. In the proof of Theorem 9.2 we relied on the fact
that the two keys ke and km are chosen independently.
Another common mistake is to apply the MAC signing algorithm to only part of the ciphertext.
We look at an example. Suppose the underlying CPA-secure cipher E= (E,D) is randomized CBC
mode (Section 5.4.3) so that the encryption of a message mis (r,c) ←R E(k,m) where ris a random
IV. When implementing encrypt-then-MAC EEtM = ( EEtM,DEtM) the encryption algorithm is
incorrectly deﬁned as
EEtM
(
(ke,km), m
):=
{
(r,c) ←R E(ke,m), t←R S(km,c), output (r,c,t)
}
.
Here, E(ke,m) outputs the ciphertext ( r,c), but the MAC signing algorithm is only applied to
c; the IV is not protected by the MAC. This mistake completely destroys ciphertext integrity:
given a ciphertext ( r,c,t) an attacker can create a new valid ciphertext ( r′,c,t ) for some r′ ̸= r.
The decryption algorithm will not detect this modiﬁcation of the IV and will not output reject.
Instead, the decryption algorithm will output D
(
ke, (r′,c)
)
. Since ( r′,c,t ) is a valid ciphertext
the adversary wins the ciphertext integrity game. Even worse, if ( r,c,t) is the encryption of a
368
message m then changing ( r,c,t) to ( r⊕∆,c,t ) for any ∆ causes the CBC decryption algorithm
to output a message m′where m′[0] = m[0] ⊕∆. This means that the attacker can change header
information in the ﬁrst block of m to any value of the attacker’s choosing. An early edition of the
ISO 19772 standard for authenticated encryption made precisely this mistake [119]. Similarly, in
2013 it was discovered that the RNCryptor facility in Apple’s iOS, built for data encryption, used
a faulty encrypt-then-MAC where the HMAC was not applied to the encryption IV [123].
Another pitfall to watch out for in an implementation is that no plaintext data should be output
before the integrity tag over the entire message is veriﬁed. See Section 9.9 for an example of this.
9.4.2 MAC-then-encrypt is not generally secure: padding oracle attacks on SSL
Next, we consider the MAC-then-encrypt generic composition of a CPA secure cipher and a secure
MAC. We show that this construction need not be AE-secure, and can lead to many real world
problems.
To deﬁne MAC-then-encrypt precisely, let I= (S,V ) be a MAC deﬁned over ( Km,M,T) and
let E = ( E,D) be a cipher deﬁned over ( Ke, M×T , C). The MAC-then-encrypt system
EMtE = (EMtE,DMtE), or MtE for short, is deﬁned as follows:
EMtE( (ke,km), m) := t←R S(km,m), c ←R E(ke, (m,t) )
Output c
DMtE((ke,km), c) := ( m,t) ←D(ke,c)
if V(km,m,t ) = reject then output reject
otherwise, output m
The MtE system is deﬁned over ( Ke ×Km, M, C).
A badly broken MtE cipher. We show that MtE is not guaranteed to be AE-secure even if E
is a CPA-secure cipher and Iis a secure MAC. In fact, MtE can fail to be secure for widely-used
ciphers and MACs and this has lead to many signiﬁcant attacks on deployed systems.
Consider the SSL 3.0 protocol used to protect WWW traﬃc for over two decades (the protocol
is disabled in modern browsers). SSL 3.0 uses MtE to combine randomized CBC mode encryption
and a secure MAC. We showed in Chapter 5 that randomized CBC mode encryption is CPA-secure,
yet this combination is badly broken: an attacker can eﬀectively decrypt all traﬃc using a chosen
ciphertext attack. This leads to a devastating attack on SSL 3.0 called POODLE [25].
Let us assume that the underlying block cipher used in CBC operates on 16 byte blocks, as
in AES. Recall that CBC mode encryption pads its input to a multiple of the block length and
SSL 3.0 does so as follows: if a pad of length p> 0 bytes is needed, the scheme pads the message
with p−1 arbitrary bytes and adds one additional byte whose value is set to (p−1). If the message
length is already a multiple of the block length (16 bytes) then SSL 3.0 adds a dummy block of 16
bytes where the last byte is set to 15 and the ﬁrst 15 bytes are arbitrary. During decryption the
pad is removed by reading the last byte and removing that many more bytes.
Concretely, the cipher EMtE = (EMtE,DMtE) obtained from applying MtE to randomized CBC
mode encryption and a secure MAC works as follows:
• EMtE( ( ke,km), m): First use the MAC signing algorithm to compute a ﬁxed-length tag
t ←R S(km,m) for m. Next, encrypt m ∥ t with randomized CBC encryption: pad the
369
message and then encrypt in CBC mode using key ke and a random IV. Thus, the following
data is encrypted to generate the ciphertext c:
message m tag t pad p (9.8)
Notice that the tag t does not protect the integrity of the pad. We will exploit this to break
CPA security using a chosen ciphertext attack.
• DMtE( (ke,km), c): Run CBC decryption to obtain the plaintext data in (9.8). Next, remove
the pad pby reading the last byte in (9.8) and removing that many more bytes from the data
(i.e., if the last byte is 3 then that byte is removed plus 3 additional bytes). Next, verify the
MAC tag and if valid return the remaining bytes as the message. Otherwise, output reject.
Both SSL 3.0 and TLS 1.0 use a defective variant of randomized CBC encryption, discussed in
Exercise 5.13, but this is not relevant to our discussion here. Here we will assume that a correct
implementation of randomized CBC encryption is used.
The chosen ciphertext attack. We show a chosen ciphertext attack on the system EMtE that
lets the adversary decrypt any ciphertext of its choice. It follows that EMtE need not be AE-secure,
even though the underlying cipher is CPA-secure. Throughout this section we let ( E,D) denote
the block cipher used in CBC mode encryption. It operates on 16-byte blocks.
Suppose the adversary intercepts a valid ciphertext c:= EMtE( (ke,km), m) for some unknown
message m. The length of mis such that after a MAC tag tis appended to mthe length of (m∥t)
is a multiple of 16 bytes. This means that a full padding block of 16 bytes is appended during CBC
encryption and the last byte of this pad is 15. Then the ciphertext c looks as follows:
c = c[0]
  
IV
c[1] ···
  
encryption of m
c[ℓ−1]
  
encrypted tag
c[ℓ]
  
encrypted pad
Lets us ﬁrst show that the adversary can learn something about m[0] (the ﬁrst 16-byte block
of m). This will break semantic security of EMtE. The attacker prepares a chosen ciphertext query ˆc
by replacing the last block of c with c[1]. That is,
ˆc := c[0] c[1] ··· c[ℓ−1] c[1]
  
encrypted pad?
(9.9)
By deﬁnition of CBC decryption, decrypting the last block of ˆc yields the 16-byte plaintext block
v:= D
(
ke,c[1]
)
⊕c[ℓ−1] = m[0] ⊕c[0] ⊕c[ℓ−1].
If the last byte of v is 15 then during decryption the entire last block will be treated as a padding
block and removed. The remaining string is a valid message-tag pair and will decrypt properly. If
the last byte of v is not 15 then most likely the response to the decryption query will be reject.
Put another way, if the response to a decryption query for ˆ c is not reject then the attacker
learns that the last byte of m[0] is equal to the last byte of u:= 15 ⊕c[0] ⊕c[ℓ−1]. Otherwise, the
attacker learns that the last byte of m[0] is not equal to the last byte of u. This directly breaks
semantic security of the EMtE: the attacker learned something about the plaintext m.
370
We leave it as an instructive exercise to recast this attack in terms of an adversary in a chosen
ciphertext attack game (as in Attack Game 9.2). With a single plaintext query followed by a single
ciphertext query the adversary has advantage 1/256 in winning the game. This already proves that
EMtE is insecure.
Now, suppose the attacker obtains another encryption of m, call it c′, using a diﬀerent IV.
The attacker can use the ciphertexts c and c′to form four useful chosen ciphertext queries: it can
replace the last block of either c or c′ with either of c[1] or c′[1]. By issuing these four ciphertext
queries the attacker learns if the last byte of m[0] is equal to the last byte of one of
15 ⊕c[0] ⊕c[ℓ−1], 15 ⊕c[0] ⊕c′[ℓ−1], 15 ⊕c′[0] ⊕c[ℓ−1], 15 ⊕c′[0] ⊕c′[ℓ−1].
If these four values are distinct they give the attacker four chances to learn the last byte of m[0].
Repeating this multiple times with more fresh encryptions of the message mwill quickly reveal the
last byte of m[0]. Each chosen ciphertext query reveals that byte with probability 1/256. Therefore,
on average, with 256 chosen ciphertext queries the attacker learns the exact value of the last byte
of m[0]. So, not only can the attacker break semantic security, the attacker can actually recover one
byte of the plaintext. Next, suppose the adversary could request an encryption of m shifted one
byte to the right to obtain a ciphertext c1. Plugging c1[1] into the last block of the ciphertexts from
the previous phase (i.e., encryptions of the unshifted m) and issuing the resulting chosen ciphertext
queries reveals the second to last byte of m[0]. Repeating this for every byte of meventually reveals
all of m. We show next that this gives a real attack on SSL 3.0.
A complete break of SSL 3.0. Chosen ciphertext attacks may seem theoretical, but they
frequently translate to devastating real-world attacks. Consider a Web browser and a victim Web
server called bank.com. The two exchange information encrypted using SSL 3.0. The browser and
server have a shared secret called a cookie and the browser embeds this cookie in every request
that it sends to bank.com. That is, abstractly, requests from the browser to bank.com look like:
GET path cookie: cookie
where path identiﬁes the name of a resource being requested from bank.com. The browser only
inserts the cookie into requests that it sends to bank.com
The attacker’s goal is to recover the secret cookie. First it makes the browser visitattacker.com
where it sends a Javascript program to the browser. This Javascript program makes the browser
issue a request for resource “/AA” at bank.com. The reason for this particular path is to ensure
that the length of the message and MAC is a multiple of the block size (16 bytes), as needed for
the attack. Consequently, the browser sends the following request to bank.com
GET /AA cookie: cookie (9.10)
encrypted using SSL 3.0. The attacker can intercept this encrypted request c and mounts the
chosen ciphertext attack on MtE to learn one byte of the cookie. That is, the attacker prepares ˆ c
as in (9.9), sends ˆcto bank.com and looks to see if bank.com responds with an SSL error message.
If no error message is generated then the attacker learns one byte of the cookie. The Javascript can
cause the browser to repeatedly issue the request (9.10) giving the adversary the fresh encryptions
needed to eventually learn one byte of the cookie.
371
Once the adversary learns one byte of the cookie it can shift the cookie one byte to the right
by making the Javascript program issue a request to bank.com for
GET /AAA cookie: cookie
This gives the attacker a block of ciphertext, call it c1[2], where the cookie is shifted one byte to the
right. Resending the requests from the previous phase to the server, but now with the last block
replaced by c1[2], eventually reveals the second byte of the cookie. Iterating this process for every
byte of the cookie eventually reveals the entire cookie.
In eﬀect, Javascript in the browser provides the attacker with the means to mount the desired
chosen plaintext attack. Intercepting packets in the network, modifying them and observing the
server’s response, gives the attacker the means to mount the desired chosen ciphertext attack. The
combination of these two completely breaks MtE encryption in SSL 3.0.
One minor detail is that whenever bank.com responds with an SSL error message the SSL
session shuts down. This does not pose a problem: every request that the Javascript running in
the browser makes to bank.com initiates a new SSL session. Hence, every chosen ciphertext query
is encrypted under a diﬀerent session key, but that makes no diﬀerence to the attack: every query
tests if one byte of the cookie is equal to one known random byte. With enough queries the attacker
learns the entire cookie.
9.4.3 More padding oracle attacks.
TLS 1.0 is an updated version of SSL 3.0. It defends against the attack of the previous section by
adding structure to the pad as explained in Section 5.4.4: when padding with p bytes, all bytes
of the pad are set to p−1. Moreover, during decryption, the decryptor is required to check that
all padding bytes have the correct value and reject the ciphertext if not. This makes it harder to
mount the attack of the previous section. Of course our goal was merely to show that MtE is not
generally secure and SSL 3.0 made that abundantly clear.
A padding oracle timing attack. Despite the defenses in TLS 1.0 a naive implementation of
MtE decryption may still be vulnerable. Suppose the implementation works as follows: ﬁrst it
applies CBC decryption to the received ciphertext; next it checks that the pad structure is valid
and if not it rejects the ciphertext; if the pad is valid it checks the integrity tag and if valid it returns
the plaintext. In this implementation the integrity tag is checked only if the pad structure is valid.
This means that a ciphertext with an invalid pad structure is rejected faster than a ciphertext with
a valid pad structure, but an invalid tag. An attacker can measure the time that the server takes
to respond to a chosen ciphertext query and if a TLS error message is generated quickly it learns
that the pad structure was invalid. Otherwise, it learns that the pad structure was valid.
This timing channel is called a padding oracle side-channel. It is a good exercise to devise a
chosen ciphertext attack based on this behavior to completely decrypt a secret cookie, as we did for
SSL 3.0. To see how this might work, suppose an attacker intercepts an encrypted TLS 1.0 record
c. Let m be the decryption of c. Say the attacker wishes to test if the last byte of m[2] is equal
to some ﬁxed byte value b. Let B be an arbitrary 16-byte block whose last byte is b. The attacker
creates a new ciphertext block ˆc[1] := c[1] ⊕B and sends the 3-block record ˆc= (c[0],ˆc[1],c[2]) to
the server. After CBC decryption of ˆc, the last plaintext block will be
ˆm[2] := ˆc[1] ⊕D(k,c[2]) = m[2] ⊕B.
372
If the last byte of m[2] is equal to b then ˆm[2] ends in zero which is a valid pad. The server will
attempt to verify the integrity tag resulting in a slow response. If the last byte of m[2] is not equal
to b then ˆm[2] will not end in 0 and will likely end in an invalid pad, resulting in a fast response.
By measuring the response time the attacker learns if the last byte of m[2] is equal to b. Repeating
this with many chosen ciphertext queries, as we did for SSL 3.0, reveals the entire secret cookie.
An even more sophisticated padding oracle timing attack on MtE, as used in TLS 1.0, is called
Lucky13 [5]. It is quite challenging to implement TLS 1.0 decryption in a way that hides the timing
information exploited by the Lucky13 attack.
Informative error messages. To make matters worse, the TLS 1.0 speciﬁcation [52] states
that the server should send one type of error message (called bad record mac) when a received
ciphertext is rejected because of a MAC veriﬁcation error and another type of error message
(decryption failed) when the ciphertext is rejected because of an invalid padding block. In
principle, this tells the attacker if a ciphertext was rejected because of an invalid padding block or
because of a bad integrity tag. This could have enabled the chosen ciphertext attack of the previous
paragraph without needing to resort to timing measurements. Fortunately, the error messages are
encrypted and the attacker cannot see the error code.
Nevertheless, there is an important lesson to be learned here: when decryption fails, the system
should never explain why. A generic ‘decryption failed’ code should be sent without oﬀering any
other information. This issue was recognized and addressed in TLS 1.1. Moreover, upon decryption
failure, a correct implementation should always take the same amount of time to respond, no matter
the failure reason.
9.4.4 Secure instances of MAC-then-encrypt
Although MtE is not generally secure when applied to a CPA-secure cipher, it can be shown to
be secure for speciﬁc CPA ciphers discussed in Chapter 5. We show in Theorem 9.3 below that if
Ehappens to implement randomized counter mode, then MtE is secure. In Exercise 9.9 we show
that the same holds for randomized CBC, assuming there is no message padding.
Theorem 9.3 shows that MAC-then-encrypt with randomized counter mode is AE-secure even
if the MAC is only one-time secure. That is, it suﬃces to use a weak MAC that is only secure
against an adversary that makes a single chosen message query. Intuitively, the reason we can
prove security using such a weak MAC is that the MAC value is encrypted, and consequently it is
harder for the adversary to attack the MAC. Since one-time MACs are a little shorter and faster
than many-time MACs, MAC-then-encrypt with randomized counter mode has a small advantage
over encrypt-then-MAC. Nevertheless, the attacks on MAC-then-encrypt presented in the previous
section suggest that it is diﬃcult to implement correctly, and should not be used.
Our starting point is a randomized counter-mode cipher E = ( E,D), as discussed in Sec-
tion 5.4.2. We will assume that Ehas the general structure as presented in the case study on AES
counter mode at the end of Section 5.4.2 (page 194). Namely, we use a counter-mode variant where
the cipher Eis built from a secure PRF F deﬁned over (Ke, X×Zℓ, Y), where Y:= {0,1}n. More
373
precisely, for a message m∈Y≤ℓ algorithm E works as follows:
E(ke, m) :=



x←R X
for j = 0 to |m|−1:
u[j] ←F
(
ke, (x,j)
)
⊕m[j]
output c:= (x,u) ∈X×Y |m|



Algorithm D(ke,c) is deﬁned similarly. Let I= (S,V ) be a secure one-time MAC deﬁned over
(Km,M,T) where M:= Y≤ℓm and T := Yℓt, and where ℓm + ℓt <ℓ.
The MAC-then-encrypt cipher EMtE = (EMtE,DMtE), built from F and Iand taking messages
in M, is deﬁned as follows:
EMtE
(
(ke,km), m
):=
{
t←R S(km,m), c ←R E
(
ke, (m∥t)
)
, output c
}
DMtE
(
(ke,km), c
):=



(m∥t) ←D(ke,c)
if V(km,m,t ) = reject then output reject
otherwise, output m



(9.11)
As we discussed at the end of Section 9.4.1, and in Exercise 9.8, the two keys ke and km must be
chosen independently. Setting ke = km will invalidate the following security theorem.
Theorem 9.3. The cipher EMtE = ( EMtE,DMtE) in (9.11) built from the PRF F and MAC I
provides authenticated encryption assuming Iis a secure one-time MAC and F is a secure PRF
where 1/|X| is negligible.
In particular, for every Q-query ciphertext integrity adversary Aci that attacks EMtE as in Attack
Game 9.1 there exists two MAC adversaries Bmac and B′
mac that attack Ias in Attack Game 6.1,
and a PRF adversary Bprf that attacks F as in Attack Game 4.2, each of which is an elementary
wrapper around Aci, such that
CIadv[Aci,EMtE] ≤PRFadv[Bprf,F] +
Q·MAC1adv[Bmac,I] + MAC1adv[B′
mac,I] + Q2
2|X|.
(9.12)
For every CPA adversary Acpa that attacks EMtE as in Attack Game 5.2 there exists a CPA
adversary Bcpa that attacks Eas in Attack Game 5.2, which is an elementary wrapper around
Acpa, such that
CPAadv[Acpa,EMtE] = CPAadv[Bcpa,E]
Proof idea. CPA security of the system follows immediately from CPA security of randomized
counter mode. The challenge is to prove ciphertext integrity for EMtE. So let Aci be a ciphertext
integrity adversary. This adversary makes a series of queries, m1,...,m Q. For each mi, the CI
challenger gives to Aci a ciphertext ci = (xi,ui), where xi is a random IV, and ui is a one-time
pad encryption of the pair mi ∥ti using a pseudo-random pad ri derived from xi using the PRF
F. Here, ti is a MAC tag computed on mi. At the end of the attack game, adversary Aci outputs
a ciphertext c= (x,u), which is not among the ci’s, and wins if cis a valid ciphertext. This means
that u decrypts to m∥t using a pseudo-random pad r derived from x, and t is a valid tag on m.
Now, using the PRF security property and the fact that the xi’s are unlikely to repeat, we can
eﬀectively replace the pseudo-random ri’s (and r) with truly random pads, without aﬀecting Aci’s
374
advantage signiﬁcantly. This is where the terms PRFadv[Bprf,F] and Q2/2|X|in (9.12) come from.
Note that after making this modiﬁcation, the ti’s are perfectly hidden from the adversary.
We then consider two diﬀerent ways in which Aci can win in this modiﬁed attack game.
• In the ﬁrst way, the value x output by Aci is not among the xi’s. But in this case, the only
way for Aci to win is to hope that a random tag on a random message is valid. This is where
the term MAC1adv[B′
mac,I] in (9.12) comes from.
• In the second way, the value x is equal to xj for some j = 1,...,Q . In this case, to win, the
value u must decrypt under the pad rj to m∥t where t is a valid tag on m. Moreover, since
c ̸= cj, we have ( m,t) ̸= (mj,tj). To turn Aci into a one-time MAC adversary, we have to
guess the index j in advance: for all indices idiﬀerent from the guessed index, we can replace
the tag ti by a dummy tag. This guessing strategy is where the term Q·MAC1adv[Bmac,I]
in (9.12) comes from. 2
Proof. To prove ciphertext integrity, we letAci interact with a number of closely related challengers.
For j = 0,1,2,3,4 we deﬁne Wj to be the event that the adversary wins in Game j.
Game 0. As usual, we begin by letting Aci interact with the standard ciphertext integrity chal-
lenger in Attack Game 9.1 as it applies to EMtE, so that Pr[ W0] = CIadv[Aci,EMtE].
Game 1. Now, we replace the pseudo-random pads in the counter-mode cipher by truly indepen-
dent one-time pads. Since F is a secure PRF and 1 /|X|is negligible, the adversary will not notice
the diﬀerence. The resulting CI challenger for EMtE works as follows.
km ←R Km / / Choose random MAC key
ω←R {1,...,Q } / / this ω will be used in Game 3
upon receiving the ith query mi ∈Y≤ℓm for i= 1,2,... do:
(1) ti ←S(km,mi) ∈T / / compute the tag for mi
(2) xi ←R X / / Choose a random IV
ri ←R Y|mi|+ℓt / / Choose a suﬃciently long truly random one-time pad
ui ←(mi ∥ti) ⊕ri, c i ←(xi,ui) / / build ciphertext
send ci to the adversary
upon receiving c= (x,u) /∈{c1,c2,... }do:
/ / decrypt ciphertext c
(3) if x= xj for some j
then (m∥t) ←u⊕rj
(4) else r←R Y|u|and (m∥t) ←u⊕r
/ / check resulting message-tag pair
if V(km,m,t ) = accept
then output “win”
else output “lose”
Note that for speciﬁcity, in line (3) if there is more than one j for which x = xj, we can take
the smallest such j.
A standard argument shows that there exists an eﬃcient PRF adversary Bprf such that:
|Pr[W1] −Pr[W0]|≤ PRFadv[Bprf,F] + Q2
2|X|. (9.13)
375
Note that if we wanted to be a bit more careful, we would break this argument up into two steps.
In the ﬁrst step, we would play our “PRF card” to replace F(ke,·) be a truly random function f.
This introduces the term PRFadv[Bprf,F] in (9.13). In the second step, we would use the “forgetful
gnome” technique to make all the outputs of f independent. Using the Diﬀerence Lemma applied
to the event that all of the xi’s are distinct introduces the term Q2/2|X|in (9.13).
Game 2. Now we restrict the adversary’s winning condition to require that the IV used in the
ﬁnal ciphertext c is the same as one of the IVs given to Aci during the game. In particular, we
replace line (4) with
(4) else output “lose” (and stop)
Let Z2 be the event that in Game 2, the ﬁnal ciphertext c= (x,u) from Aci is valid despite using
a previously unused x ∈X . We know that the two games proceed identically, unless event Z2
happens. When event Z2 happens in Game 2 then the resulting pair ( m,t) is uniformly random
in Y|u|−ℓt ×Yℓt. Such a pair is unlikely to form a valid message-tag pair. Not only that, the
challenger in Game 2 eﬀectively encrypts all of the tags ti generated in line (1) with a one-time
pad, so these tags could be replaced by dummy tags, without aﬀecting the probability that Z2
occurs. Based on these observations, we can easily construct an eﬃcient MAC adversary B′
mac such
that Pr[Z2] ≤MAC1adv[B′
mac,I]. Adversary B′
mac runs as follows. It plays the role of challenger to
Aci as in Game 2, except that in line (1) above, it computes ti ←0ℓt. When Aci outputs c= (x,u),
adversary B′
mac outputs a random pair in Y|u|−ℓt ×Yℓt. Hence, by the diﬀerence lemma, we have
|Pr[W2] −Pr[W1]|≤ MAC1adv[B′
mac,I]. (9.14)
Game 3. We further constrain the adversary’s winning condition by requiring that the ciphertext
forgery use the IV from ciphertext numberωgiven to Aci. Here ωis a random number in {1,...,Q }
chosen by the challenger. The only change to the winning condition of Game 2 is that line (3) now
becomes:
(3) if x= xω then
Since ω is independent of Aci’s view, we know that
Pr[W3] ≥(1/Q) ·Pr[W2] (9.15)
Game 4. Finally, we change the challenger so that it only computes a valid tag for query numberω
issued by Aci. For all other queries the challenger just makes up an arbitrary (invalid) tag. Since
the tags are encrypted using one-time pads the adversary cannot tell that he is given encryptions
of invalid tags. In particular, the only diﬀerence from Game 3 is that we replace line (1) by the
following two lines:
(1) ti ←(0n)ℓt ∈T
if i= ω then ti ←S(km,mi) ∈T / / only compute correct tag for mω
Since the adversary’s view in this game is identical to its view in Game 3 we have
Pr[W4] = Pr[W3] (9.16)
Final reduction. We claim that there is an eﬃcient one-time MAC forger Bmac so that
Pr[W4] = MAC1adv[Bmac,I] (9.17)
Adversary Bmac interacts with a MAC challenger C and works as follows:
376
ω←R {1,...,Q }
upon receiving the ith query mi ∈{0,1}ℓm for i= 1,2,... do:
ti ←(0n)ℓt ∈T
if i= ω then query C for the tag on mi and let ti ∈T be the response
xi ←R X / / Choose a random IV
ri ←R Y|m|+ℓt / / Choose a suﬃciently long random one-time pad
ui ←(mi ∥ti) ⊕ri, c i ←(xi,ui)
send ci to the adversary
when Aci outputs c= (x,u) do:
if x= xω then
(m∥t) ←u⊕rω
output (m,t) as the message-tag forgery
Since c ̸= cω we know that ( m,t) ̸= (mω,tω). Hence, whenever Aci wins Game 4 we know that
Bmac does not abort, and outputs a pair ( m,t) that lets it win the one-time MAC attack game. It
follows that Pr[W4] = MAC1adv[Bmac,I] as required. In summary, putting equations (9.13)–(9.17)
together proves the theorem. 2
9.4.5 Encrypt-then-MAC or MAC-then-encrypt?
So far we proved the following facts about the MtE and EtM modes:
• EtM provides authenticated encryption whenever the cipher is CPA-secure and the MAC is
secure. The MAC on the ciphertext prevents any tampering with the ciphertext.
• MtE is not generally secure — there are examples of CPA-secure ciphers for which the MtE
system is not AE-secure. Moreover, MtE is diﬃcult to implement correctly due to a potential
timing side-channel that can lead to a chosen ciphertext attack. However, for speciﬁc ciphers,
such as randomized counter mode and randomized CBC, the MtE mode is AE-secure even if
the MAC is only one-time secure.
• A third mode, called encrypt-and-MAC (EaM), is discussed in Exercise 9.10. The exercise
shows that EaM is secure when using randomized counter-mode cipher as long as the MAC
is a secure PRF. EaM is inferior to EtM in every respect and should not be used.
These facts, and the example attacks on MtE, suggest that EtM is the better mode to use.
Of course, it is critically important that the underlying cipher be CPA-secure and the underlying
MAC be a secure MAC. Otherwise, EtM may provide no security at all.
Given all the past mistakes in implementing these modes it is advisable that developers not
implement EtM themselves. Instead, it is best to use an encryption standard, like GCM (see
Section 9.7), that uses EtM to provide authenticated encryption out of the box.
9.5 Nonce-based authenticated encryption with associated data
In this section we extend the syntax of authenticated encryption to match the way in which it is
commonly used. First, as we did for encryption and for MACs, we deﬁne nonce-based authenticated
377
encryption where we make the encryption and decryption algorithms deterministic, but let them
take as input a unique nonce. This approach can reduce ciphertext size and also improve security.
Second, we extend the encryption algorithm by giving it an additional input message, called
associated data, whose integrity is protected by the ciphertext, but its secrecy is not. The need
for associated data comes up in a number of settings. For example, when encrypting packets in
a networking protocol, authenticated encryption protects the packet body, but the header must
be transmitted in the clear so that the network can route the packet to its intended destination.
Nevertheless, we want to ensure header integrity. The header is provided as the associated data
input to the encryption algorithm.
A cipher that supports associated data is called an AD cipher. The syntax for a nonce-based
AD cipher E= (E,D) is as follows:
c= E(k,m,d, N ),
where c∈C is the ciphertext, k∈K is the key, m∈M is the message, d∈D is the associated data,
and N ∈N is the nonce. Moreover, the encryption algorithm E is required to be deterministic.
Likewise, the decryption syntax becomes
D(k,c,d, N )
which outputs a message m or reject. We say that the nonce-based AD cipher is deﬁned over
(K,M,D,C,N ). As usual, we require that ciphertexts generated by E are correctly decrypted
by D, as long as both are given the same nonce and associated data . That is, for all keys k, all
messages m, all associated data d, and all nonces N ∈N :
D
(
k, E(k, m, d,N ), d, N
)
= m.
If the message m given as input to the encryption algorithm is the empty message then cipher
(E,D) essentially becomes a MAC system for the associated data d.
CPA security. A nonce-based AD cipher is CPA-secure if it does not leak any useful information
to an eavesdropper assuming that no nonce is used more than once in the encryption process. CPA
security for a nonce-based AD cipher is deﬁned as CPA security for a standard nonce-based cipher
(Section 5.5). The only diﬀerence is in the encryption queries. Encryption queries in Experiment b,
for b= 0,1, are processed as follows:
The ith encryption query is a pair of messages, mi0,mi1 ∈M, of the same length,
associated data di ∈D, and a unique nonce N i ∈N \{N 1,..., N i−1}.
The challenger computes ci ←E(k,mib,di,N i), and sends ci to the adversary.
Nothing else changes from the deﬁnition in Section 5.5. Note that the associated data di is under
the adversary’s control, as are the nonces N i, subject to the nonces being unique. For b= 0,1, let
Wb be the event that Aoutputs 1 in Experiment b. We deﬁne A’s advantage with respect to Eas
nCPAadadv[A,E] := |Pr[W0] −Pr[W1]|. 2
Deﬁnition 9.7 (CPA security). A nonce-based AD cipher is called semantically secure
against chosen plaintext attack , or simply CPA-secure, if for all eﬃcient adversaries A,
the quantity nCPAadadv[A,E] is negligible.
378
Ciphertext integrity. A nonce-based AD cipher provides ciphertext integrity if an attacker who
can request encryptions under key k for messages, associated data, and nonces of his choice cannot
output a new triple (c,d, N ) that is accepted by the decryption algorithm. The adversary, however,
must never issue an encryption query using a previously used nonce.
More precisely, we modify the ciphertext integrity game (Attack Game 9.1) as follows:
Attack Game 9.3 (ciphertext integrity). For a given AD cipher E = ( E,D) deﬁned over
(K,M,D,C,N ), and a given adversary A, the attack game runs as follows:
• The challenger chooses a random k←R K.
• Aqueries the challenger several times. For i = 1 ,2,..., the ith query consists
of a message mi ∈M, associated data di ∈D, and a previously unused nonce
N i ∈N \{N 1,..., N i−1}. The challenger computes ci ←R E(k,mi,di,N i), and gives
ci to A.
• Eventually Aoutputs a candidate triple ( c,d, N ) where c∈C, d∈D, and N ∈N
that is not among the triples it was given, i.e.,
(c,d, N ) ̸∈{(c1,d1,N 1), (c2,d2,N 2), ...}.
We say that Awins the game if D(k,c,d, N ) ̸= reject. We deﬁne A’s advantage with respect to E,
denoted nCIadadv[A,E], as the probability that Awins the game. 2
Deﬁnition 9.8. We say that a nonce-based AD cipher E= (E,D) has ciphertext integrity if
for all eﬃcient adversaries A, the value nCIadadv[A,E] is negligible.
Authenticated encryption. We can now deﬁne nonce-based authenticated encryption for an
AD cipher. We refer to this notion as a nonce-based AEAD cipher which is shorthand for
authenticated encryption with associated data .
Deﬁnition 9.9. We say that a nonce-based AD cipher E= (E,D) provides authenticated encryp-
tion, or is simply a nonce-based AEAD cipher, if Eis CPA-secure and has ciphertext integrity.
Generic encrypt-then-MAC composition. We construct a nonce-based AEAD cipher E=
(EEtM,DEtM) by combining a nonce-based CPA-secure cipher ( E,D) (as in Section 5.5) with a
nonce-based secure MAC (S,V ) (as in Section 7.5) as follows:
EEtM( (ke,km), m,d,N ) := c←E(ke,m, N ), t ←S(km,(c,d),N )
Output (c,t)
DEtM((ke,km), (c,t), d,N ) := if V(km, (c,d), t, N ) = reject then output reject
otherwise, output D(ke,c,d, N )
The EtM system is deﬁned over ( Ke ×Km,M,D,C×T ,N ). The following theorem shows that
EEtM is a secure AEAD cipher.
Theorem 9.4. Let E= (E,D) be a nonce-based cipher and let I= (S,V ) be a nonce-based MAC
system. Then EEtM is a nonce-based AEAD cipher assuming E is CPA-secure and Iis a secure
MAC system.
The proof of Theorem 9.4 is essentially the same as the proof of Theorem 9.2.
379
9.6 One more variation: CCA-secure ciphers with associated data
In Section 9.5, we introduced two new features to our ciphers: nonces and associated data. There
are two variations we could consider: ciphers with nonces but without associated data, and ciphers
with associated data but without nonces. We could also consider all of these variations with respect
to other security notions, such as CCA security. Considering all of these variations in detail would
be quite tedious. However, we consider one variation that will be important later in the text,
namely CCA-secure ciphers with associated data (but without nonces).
To deﬁne this notion, we begin by deﬁning the syntax for a cipher with associated data, or
AD cipher, without nonces. For such a cipher E = ( E,D), the encryption algorithm may be
probabilistic and works as follows:
c←R E(k,m,d ),
where c∈C is the ciphertext, k∈K is the key, m∈M is the message, and d∈D is the associated
data. The decryption syntax is
D(k,c,d ),
which outputs a message m or reject. We say that the AD cipher is deﬁned over ( K,M,D,C). As
usual, we require that ciphertexts generated by E are correctly decrypted by D, as long as both
are given the same associated data. That is,
Pr
[
D
(
k, E(k, m, d), d
)
= m
]
= 1.
Deﬁnition 9.10 (CCA and 1CCA security with associated data). The deﬁnition of CCA
security for ordinary ciphers carries over naturally to AD ciphers. Attack Game 9.2 is modiﬁed
as follows. For encryption queries, in addition to a pair of messages (mi0,mi1), the adversary
also submits associated data di, and the challenger computes ci ←R E(k,mib,di). For decryption
queries, in addition to a ciphertext ˆcj, the adversary submits associated data ˆdj, and the challenger
computes ˆmj ←D(k,ˆcj, ˆdj). The restriction is that the pair (ˆcj, ˆdj) may not be among the pairs
(c1,d1),(c2,d2),... corresponding to previous encryption queries. An adversary A’s advantage in
this game is denoted CCAadadv[A,E], and the cipher is said to be CCA secure if this advantage is
negligible for all eﬃcient adversaries A. If we restrict the adversary to a single encryption query,
as in Deﬁnition 9.6, the advantage is denoted 1CCAadadv[A,E], and the cipher is said to be 1CCA
secure if this advantage is negligible for all eﬃcient adversaries A.
Generic encrypt-then-MAC composition. In later applications, the notion that we will use
is 1CCA security, so for simplicity we focus on that notion for now. We construct a 1CCA-secure
AD cipher E= (EEtM,DEtM) by combining a semantically secure cipher ( E,D) with a one-time
MAC (S,V ) as follows:
EEtM( (ke,km), m,d) := c←R E(ke,m), t ←R S(km,(c,d))
Output (c,t)
DEtM((ke,km), (c,t), d) := if V(km, (c,d), t) = reject then output reject
otherwise, output D(ke,c,d )
The EtM system is deﬁned over ( Ke ×Km,M,D,C×T ).
380
Theorem 9.5. Let E= (E,D) be a semantically secure cipher and let I= (S,V ) be a one-time
secure MAC system. Then EEtM is a 1CCA-secure AD cipher.
The proof of Theorem 9.5 is straightforward, and we leave it as an exercise to the reader.
We observe that in most common implementations of the semantically secure cipherE= (E,D),
the encryption algorithm E is deterministic. Likewise, in the most common implementations of the
one-time secure MAC I= (S,V ), the signing algorithm is deterministic. So for such implementa-
tions, the resulting 1CCA-secure AD cipher will have a deterministic encryption algorithm.
9.7 Case study: Galois counter mode (GCM)
Galois counter mode (GCM) is a popular nonce-based AEAD cipher standardized by NIST in 2007.
GCM is an encrypt-then-MAC cipher combining a CPA-secure cipher and a secure MAC. The
CPA secure cipher is nonce-based counter mode, usually using AES. The secure MAC is a Carter-
Wegman MAC built from a keyed hash function called GHASH, a variant of the function Hxpoly
from Section 7.4. When encrypting the empty message the cipher becomes a MAC system called
GMAC providing integrity for the associated data.
GCM uses an underlying block cipher E = ( E,D) such as AES deﬁned over ( K,X) where
X:= {0,1}128. The block cipher is used for both counter mode encryption and the Carter-Wegman
MAC. The GHASH function is deﬁned over ( X,X≤ℓ,X) for ℓ:= 232 −1.
GCM can take variable size nonces, but let us ﬁrst describe GCM using a 96-bit nonce N which
is the simplest case. The GCM encryption algorithm operates as follows:
input: key k∈K, message m, associated data d, and nonce N ∈{0,1}96
km ←E(k, 0128) / / ﬁrst, generate the key for GHASH (a variant of Hxpoly)
Compute the initial value of the counter in counter mode encryption:
x←(N ∥0311) ∈{0,1}128
x′←x+ 1 / / initial value of counter
c←{encrypt m using counter mode starting the counter at x′}
d′←{pad d with zeros to closest multiple of 128 bits }
c′←{pad c with zeros to closest multiple of 128 bits }
Compute the Carter-Wegman MAC:
(∗) h←GHASH
(
km,
(
d′∥c′∥length(d) ∥length(c)
))
∈{0,1}128
t←h⊕E(k,x) ∈{0,1}128
output (c,t) / / encrypt-then-MAC ciphertext
Each of the length ﬁelds on line ( ∗) is a 64-bit value indicating the length in bytes of the
respective ﬁeld. If the input nonce N is not 96-bits long, then N is padded to the closest multiple
of 128 bits, yielding the padded string N ′, and the initial counter value x is computed as x ←
GHASH
(
km, (N ′∥length(N ))
)
which is a value in {0,1}128.
As usual, the integrity tag t can be truncated to whatever length is desired. The shorter the
tag t the more vulnerable the system becomes to ciphertext integrity attacks.
Messages to be encrypted must be less than 2 32 blocks each (i.e., messages must be in Xv for
some v <232). Recommendations in the standard suggest that a single key k should not be used
to encrypt more than 2 32 messages.
381
The GCM decryption algorithm takes as input a keyk∈K, a ciphertext (c,t), associated data d
and a nonce N . It operates as in encrypt-then-MAC: it ﬁrst derives km ←E(k,0128) and checks the
Carter-Wegman integrity tag t. If valid it outputs the counter mode decryption of c. We emphasize
that decryption must be atomic: no plaintext data is output before the integrity tag is veriﬁed over
the entire message.
GHASH. It remains to describe the keyed hash function GHASH deﬁned over (X,X≤ℓ,X). This
hash function is used in a Carter-Wegman MAC and therefore, for security, must be a DUF. In
Section 7.4 we showed that the function Hxpoly is a DUF. Recall that Hxpoly(k,z) is deﬁned as the
evaluation of a polynomial derived from z at the point k. We described Hxpoly using arithmetic
modulo a prime p, so that the blocks of z and the output are elements in Zp.
The hash function GHASH is almost the same as Hxpoly, except that the input message blocks
and the output are elements of {0,1}128. Also, the DUF property holds with respect to the XOR
operator ⊕, rather than subtraction modulo some number. As discussed in Remark 7.4, to build
an XOR-DUF we use polynomials deﬁned over the ﬁnite ﬁeld GF(2 128). This is a ﬁeld of 2 128
elements called a Galois ﬁeld, which is where GCM gets its name. This ﬁeld is deﬁned by the
irreducible polynomial g(X) := X128 + X7 + X2 + X+ 1. Elements of GF(2 128) are polynomials
over GF(2) of degree less than 128, with arithmetic done modulo g(X). While that sounds fancy,
an element of GF(2 128) can be conveniently represented as a string of 128 bits (each bit encodes
one of the coeﬃcients of the polynomial). Addition in the ﬁeld is just XOR. Multiplication is done
by multiplying the two given polynomials and reducing the result modulo g. Modern processors
provide direct hardware support for this operation.
With this notation, for k∈GF(2128) and z∈
(
GF(2128)
)v the function GHASH(k,z) is simply
polynomial evaluation in GF(2128):
GHASH(k,z) := z[0] ·kv + z[1] ·kv−1 + ... + z[v−1] ·k ∈GF(2128) (9.18)
That’s it. Appending the two length ﬁelds to the GHASH input on line ( ∗) ensures that the
XOR-DUF property is maintained even for messages of diﬀerent lengths.
Security. The AEAD security of GCM is similar to the analysis we did for generic composition
of encrypt-then-MAC (Theorem 9.4), and follows from the security of the underlying block cipher
as a PRF. The main diﬀerence between GCM and our generic composition is that GCM “cuts a
few corners” when it comes to keys: it uses a single key kand uses E(k,0n) as the GHASH key, and
E(k,x) as the pad that is used to mask the output of GHASH. This is similar to, but not exactly
the sames as, what is done in Carter-Wegman. Importantly, the counter mode encryption begins
with the counter value x′ := x+ 1, so that the inputs to the PRF that are used to encrypt the
message are guaranteed to be distinct from the inputs used to derive the GHASH key and pad. The
above discussion focused on the case where the nonce is 96 bits. The other case, where GHASH is
applied to the nonce to compute x, requires a more involved analysis — see Exercise 9.14.
GCM has no nonce re-use resistance. If a nonce is accidentally re-used on two diﬀerent messages
then all secrecy for those messages is lost. Even worse, the GHASH secret key km is exposed
(Exercise 7.13) and this can be used to break ciphertext integrity. Hence, it is vital that nonces
not be re-used in GCM.
382
Optimizations and performance. There are many ways to optimize the implementation of
GCM and GHASH. In practice, the polynomial in (9.18) is evaluated using Horner’s method so that
processing each block of plaintext requires only one addition and one multiplication in GF(2 128).
Intel added a special instruction, called PCLMULQDQ, to their instruction set to quickly
carry out binary polynomial multiplication. This instruction cannot be used directly to imple-
ment GHASH because of incompatibility with how the standard represents elements in GF(2 128).
Fortunately, work of Gueron shows how to overcome these diﬃculties and use the PCLMULQDQ
instruction to speed-up GHASH on Intel platforms.
Since GHASH needs only one addition and one multiplication in GF(2128) per block, one would
expect that the bulk of the time during GCM encryption and decryption is spent on computing
AES in counter mode. However, due to improvements in hardware AES, especially pipelining of
the AES-NI instructions, this is not always the case. On Intel’s Haswell processors (introduced in
2013) GCM is about three times slower than pure counter mode due to the overhead of computing
GHASH. However, improvements in the hardware implementation of PCLMULQDQ make GCM
just slightly more expensive than pure AES counter mode, which is the best one can hope for.
9.8 Case study: the TLS 1.3 record protocol
The Transport Layer Security (TLS) protocol is by far the most widely deployed security protocol.
Virtually every online purchase is protected by TLS. Although TLS is primarily used to protect
Web traﬃc, it is a general protocol that can protect many types of traﬃc: email, messaging, and
many others.
The original version of TLS was designed at Netscape where it was called the Secure Socket
Layer protocol or SSL. SSL 2.0 was designed in 1994 to protect Web e-commerce traﬃc. SSL 3.0,
designed in 1995, corrected several signiﬁcant security problems in SSLv2. For example, SSL 2.0
uses the same key for both the cipher and the MAC. While this is bad practice — it invalidates
the proofs of security for MtE and EtM — it also implies that if one uses a weak cipher key, say
due to export restrictions, then the MAC key must also be weak. SSL 2.0 supported only a small
number of algorithms and, in particular, only supported MD5-based MACs.
The Internet Engineering Task Force (IETF) created the Transport Layer Security (TLS) work-
ing group to standardize an SSL-like protocol. The working group produced a speciﬁcation for the
TLS 1.0 protocol in 1999 [52]. TLS 1.0 is a minor variation of SSL 3.0 and is often referred to
as SSL version 3.1. Minor updates were introduced in 2006, and again in 2008, leading to TLS
version 1.2. Due to several security vulnerabilities in TLS 1.2, the protocol was overhauled in 2017,
resulting in a much stronger TLS version 1.3. TLS has become ubiquitous, and is used worldwide
in many software systems. Here we will focus on TLS 1.3
The TLS 1.3 record protocol. Abstractly, TLS consists of two components. The ﬁrst, called
TLS session setup, negotiates the cipher suite that will be used to encrypt the session and then
sets up a shared secret between the browser and server. The second, called the TLS record
protocol uses this shared secret to securely transmit data between the two sides. TLS session
setup uses public-key techniques and will be discussed later in Chapter 21. Here we focus on the
TLS record protocol.
In TLS terminology, the shared secret generated during session setup is called amaster-secret.
This high entropy master secret is used to derive two keys kb→s and ks→b. The key kb→s encrypts
383
messages from the browser to the server while ks→b encrypts messages in the reverse direction. TLS
derives the two keys by using the master secret and other randomness as a seed for a key derivation
function called HKDF (Section 8.10.5) to derive enough pseudo-random bits for the two keys. This
step is carried out by both the browser and server so that both sides have the keys kb→s and ks→b.
The TLS record protocol sends data in records whose size is at most 214 bytes. If one side needs
to transmit more than 214 bytes, the record protocol fragments the data into multiple records each
of size at most 2 14. Each party maintains a 64-bit write sequence number that is initialized to
zero and is incremented by one for every record sent by that party.
TLS 1.3 uses a nonce-based AEAD cipher ( E,D) to encrypt a record. Which nonce-based
AEAD cipher is used is determined by negotiation during TLS session setup. The AEAD encryption
algorithm is given the following arguments:
• secret key: kb→s or ks→b depending on whether the browser or server is encrypting.
• plaintext data: up to 2 14 bytes.
• associated data: empty (zero length).
• nonce (8 bytes or longer): the nonce is computed by (1) padding the encrypting party’s
64-bit write sequence number on the left with zeroes to the expected nonce length and (2)
XORing this padded sequence number with a random string (called client write iv or
server write iv, depending on who is encrypting) that was derived from the master secret
during session setup and is ﬁxed for the life of the session. TLS 1.3 could have used an
equivalent and slightly easier to comprehend method: choose the initial nonce value at random
and then increment it sequentially for each record. The method used by TLS 1.3 is a little
easier to implement.
The AEAD cipher outputs a ciphertext cwhich is then formatted into an encrypted TLS record
as follows:
type version length ciphertext c
where type is a 1-byte record type (handshake record or application data record), version is a
legacy 2-byte ﬁeld that is always set to 0301, length is a 2-byte ﬁeld indicating the length of c,
and c is the ciphertext. The type, version, and length ﬁelds are all sent in the clear. Notice that
the nonce is not part of the encrypted TLS record. The recipient computes the nonce by itself.
Why is the initial nonce value random and not simply set to zero? In networking protocols
the ﬁrst message block sent over TLS is usually a ﬁxed public value. If the nonce were set to zero
then the ﬁrst ciphertext would be computed as c0 ←E(k,m0,d, 0) where the adversary knows m0
and associated data d. This opens up the system to an exhaustive search attack for the key k
using a time-space tradeoﬀ discussed in Section 18.7. The attack shows that with a large amount
of pre-computation and suﬃcient storage, an attacker can quickly recover k from c0 with non-
negligible advantage — for 128-bit keys, such attacks may be feasible in the not-too-distant future.
Randomizing the initial nonce “future proofs” TLS against such attacks.
When a record is received, the receiving party runs the AEAD decryption algorithm to decryptc.
If decryption results in reject then the party sends a fatal bad record mac alert to its peer and
shuts down the TLS session.
384
The length ﬁeld. In TLS 1.3, as in earlier versions of TLS, the record length is sent in the clear.
Several attacks based on traﬃc analysis exploit record lengths to deduce information about the
record contents. For example, if an encrypted TLS record contains one of two images of diﬀerent
size then the length will reveal to an eavesdropper which image was encrypted. Chen et al. [41]
show that the lengths of encrypted records can reveal considerable information about private data
that a user supplies to a cloud application. They use an online tax ﬁling system as their example.
Other works show attacks of this type on many other systems. Since there is no complete solution
to this problem, it is often ignored.
When encrypting a TLS record the length ﬁeld is not part of the associated data and conse-
quently has no integrity protection. The reason is that due to variable length padding, the length
of c may not be known before the encryption algorithm terminates. Therefore, the length cannot
be given as input to the encryption algorithm. This does not compromise security: a secure AEAD
cipher will reject a ciphertext that is a result of tampering with the length ﬁeld.
Replay prevention. An attacker may attempt to replay a previous record to cause the wrong
action at the recipient. For example, the attacker could attempt to make the same purchase
order be processed twice, by simply replaying the record containing the purchase order. TLS uses
the 64-bit write sequence number to reject such replicated packets. TLS assumes in-order record
delivery so that the recipient already knows what sequence number to expect without any additional
information in the record. A replicated or out-of-order record will be discarded because the AEAD
decryption algorithm will be given the wrong nonce as input causing it to reject the ciphertext.
The cookie cutter attack. TLS provides a streaming interface, where records are sent as soon
as they are ready. While replay, re-ordering, and mid-stream deletion of records is prevented by
a 64-sequence number, there is no defense against deletion of the last record in a stream. In
particular, an active attacker can close the network connection mid-way through a session, and to
the participants this will look like the conversation ended normally. This can lead to a real-world
attack called cookie cutter. To see how this works, consider a victim web site and a victim web
browser. The victim browser visits a malicious web site that directs the browser to connect to
victim.com. Say that the encrypted response from the victim site looks as follows:
HTTP/1.1 302 Redirect
Location: https://victim.com/path
Set-Cookie: SID=[AuthenticationToken]; secure
Content-Length: 0 \r\n\r\n
The ﬁrst two lines indicate the type of response. Notice that the second line includes a “path” value
that is copied from the browser’s request. The third line sets a cookie that will be stored on the
browser. Here the “secure” attribute indicates that this cookie should only be sent to victim.com
over an encrypted TLS session. The fourth line indicates the end of the response.
Suppose that in the original browser request, the “path” value is suﬃciently long so that the
server’s response is split across two TLS frames:
frame 1: HTTP/1.1 302 Redirect
Location: https://victim.com/path
Set-Cookie: SID=[AuthenticationToken]
385
frame 2: ; secure
Content-Length: 0 \r\n\r\n
The network attacker shuts down the connection after the ﬁrst frame is sent, so that the second
frame never reaches the browser. This causes the browser to mark the cookie as non-secure. Now
the attacker directs the browser to the cleartext (http) version of victim.com, and the browser will
send the SID cookie in the clear, where the attacker can easily read it.
In eﬀect, the adversary was able to make the browser receive a message that the server did not
send: the server sent both frames, but the browser only received one and accepted it as a valid
message. This is despite proper use of authenticated encryption on every frame.
TLS assumes that the application layer will defend against this attack. In particular, the
server’s response ends with an end-of-message (EOM) mark in the form of \r\n\r\n. The browser
should not process an incoming message until it sees the EOM. In practice, however, it is tempting to
process headers as soon as they are received, resulting in the vulnerability above. Every application
that uses TLS must be aware of this issue, and defend against it using an EOM or equivalent
mechanism.
9.9 Case study: an attack on non-atomic decryption in SSH
SSH (secure shell) is a popular command line tool for securely exchanging information with a
remote host. SSH is designed to replace (insecure) UNIX tools such as telnet, rlogin, rsh, and rcp.
Here we describe a fascinating vulnerability in an older cipher suite used in SSH. This vulnerability
is an example of what can go wrong when decryption is not atomic, that is, when the decryption
algorithm releases fragments of a decrypted record before verifying integrity of the entire record.
First, a bit of history. The ﬁrst version of SSH, called SSHv1, was made available in 1995. It
was quickly pointed out that SSHv1 suﬀers from serious design ﬂaws.
• Most notably, SSHv1 provides data integrity by computing a Cyclic Redundancy Check
(CRC) of the plaintext and appending the resulting checksum to the ciphertext in the clear.
CRC is a simple keyless, linear function — so not only does this directly leak information
about the plaintext, it is also not too hard to break integrity either.
• Another issue is the incorrect use of CBC mode encryption. SSHv1 always sets the CBC
initial value (IV) to 0. Consequently, an attacker can tell when two SSHv1 packets contain
the same preﬁx. Recall that for CPA security one must choose the IV at random.
• Yet another problem, the same encryption key was used for both directions (user to server
and server to user).
To correct these issues, a revised and incompatible protocol called SSHv2 was published in 1996.
Session setup results in two keys ku→s, used to encrypt data from the user to the server, and ks→u,
used to encrypt data in the reverse direction. Here we focus only how these keys are used for
message transport in SSHv2.
SSHv2 encryption. Let us examine an older cipher suite used in SSHv2. SSHv2 combines a
CPA-secure cipher with a secure MAC using encrypt-and-MAC (Exercise 9.10) in an attempt to
construct a secure AEAD cipher. Speciﬁcally, SSHv2 encryption works as follows (Fig. 9.3):
386
Gray area is encrypted; Boxed area is authenticated by integrity tag
packet len
pad len
message
pad
integrity tag
32 bits
Figure 9.3: An SSHv2 packet
1. Pad. Pad the plaintext with random bytes so that the total length of
plaintext := packet-length ∥pad-length ∥message ∥pad
is a multiple of the cipher block length (16 bytes for AES). The pad length can be anywhere
from 4 bytes to 255 bytes. The packet length ﬁeld measures the length of the packet in bytes,
not including the integrity tag or the packet-length ﬁeld itself.
2. Encrypt. Encrypt the gray area in Fig. 9.3 using AES in randomized CBC mode with
either ku→s or ks→u, depending on the encrypting party. SSHv2 uses a defective version of
randomized CBC mode encryption described in Exercise 5.13.
3. MAC. A MAC is computed over a sequence-number and the plaintext data in the thick
box in Fig. 9.3. Here sequence-number is a 32-bit sequence number that is initialized to zero
for the ﬁrst packet, and is incremented by one after every packet. SSHv2 can use one of a
number of MAC algorithms, but HMAC-SHA1-160 must be supported.
When an encrypted packet is received the decryption algorithm works as follows: ﬁrst it decrypts
the packet-length ﬁeld using either ku→s or ks→u. Next, it reads that many more packets from
the network plus as many additional bytes as needed for the integrity tag. Next it decrypts the rest
of the ciphertext and veriﬁes validity of the integrity tag. If valid, it removes the pad and returns
the plaintext message.
Although SSH uses encrypt-and-MAC, which is not generally secure, we show in Exercise 9.10
that for certain combinations of cipher and MAC, including the required ones in SSHv2, encrypt-
and-MAC provides authenticated encryption.
387
SSH boundary hiding via length encryption. An interesting aspect of SSHv2 is that the
encryption algorithm encrypts the packet length ﬁeld, as shown in Fig. 9.3. The motivation for
this is to ensure that if a sequence of encrypted SSH packets are sent over an insecure network as a
stream of bytes, then an eavesdropper should be unable to determine the number of packets sent or
their lengths. This is intended to frustrate certain traﬃc analysis attacks that deduce information
about the plaintext from its size.
Hiding message boundaries between consecutive encrypted messages is outside the requirements
addressed by authenticated encryption. In fact, many secure AEAD modes do not provide this level
of secrecy. TLS 1.0, for example, sends the length of the every record in the clear making it easy
to detect boundaries between consecutive encrypted records. Enhancing authenticated encryption
to ensure boundary hiding has been formalized by Boldyreva, Degabriele, Paterson, and Stam [27],
proposing a number of constructions satisfying the deﬁnitions.
An attack on non-atomic decryption. Notice that CBC decryption is done in two steps: ﬁrst
the 32-bit packet-length ﬁeld is decrypted and used to decide how many more bytes to read from
the network. Next, the rest of the CBC ciphertext is decrypted.
Generally speaking, AEAD ciphers are not designed to be used this way: plaintext data should
not be used until the entire ciphertext decryption process is ﬁnished; however, in SSHv2 the de-
crypted length ﬁeld is used before its integrity has been veriﬁed.
Can this be used to attack SSHv2? A beautiful attack [3] shows how this non-atomic decryption
can completely compromise secrecy. Here we only describe the high-level idea, ignoring many
details. Suppose an attacker intercepts a 16-byte ciphertext block c and it wants to learn the ﬁrst
four bytes of the decryption of c. It does so by abusing the decryption process as follows: ﬁrst, it
sends the ciphertext block c to the server as if it were the ﬁrst block of a new encrypted packet.
The server decrypts cand interprets the ﬁrst four bytes as a length ﬁeld ℓ. The server now expects
to read ℓbytes of data from the network before checking the integrity tag. The attacker can slowly
send to the server arbitrary bytes, one byte at a time, waiting after each byte to see if the server
responds. Once the server reads ℓ bytes it attempts to verify the integrity tag on the bytes it
received and this most likely fails causing the server to send back an error message. Thus, once ℓ
bytes are read the attacker receives an error message. This tells the attacker the value of ℓ which
is what it wanted.
In practice, there are many complications in mounting an attack like this. Nevertheless, it shows
the danger of using decrypted data — the length ﬁeld in this case — before its integrity has been
veriﬁed. As mentioned above, we refer to [27] for encryption methods that securely hide packet
lengths.
A clever traﬃc analysis attack on SSH. SSHv2 operates by sending one network packet
for every user keystroke. This gives rise to an interesting traﬃc analysis attack reported in [149].
Suppose a network eavesdropper knows that the user is entering a password at his or her keyboard.
By measuring timing diﬀerences between consecutive packets, the eavesdropper obtains timing
information between consecutive keystrokes. This exposes information about the user’s password:
a large timing gap between consecutive keystrokes reveals information about the keyboard position
of the relevant keys. The authors show that this information can signiﬁcantly speed up an oﬄine
password dictionary attack. To make matters worse, password packets are easily identiﬁed since
applications typically turn oﬀ echo during password entry so that password packets do not generate
388
CRC(m)cleartext payload m
RC4( IV ∥k)
encrypted frameIV encrypted frame
⨁
Figure 9.4: WEP Encryption
an echo packet from the server.
Some SSH implementations defend against this problem by injecting randomly timed “dummy”
messages to make traﬃc analysis more diﬃcult. Dummy messages are identiﬁed by setting the
ﬁrst message byte to SSH MSG IGNORE and are ignored by the receiver. The eavesdropper cannot
distinguish dummy records from real ones thanks to encryption.
9.10 Case study: 802.11b WEP, a badly broken system
The IEEE 802.11b standard ratiﬁed in 1999 deﬁnes a protocol for short range wireless communica-
tion (WiFi). Security is provided by a Wired Equivalent Privacy (WEP) encapsulation of 802.11b
data frames. The design goal of WEP is to provide data privacy at the level of a wired network.
WEP, however, completely fails on this front and gives us an excellent case study illustrating how
a weak design can lead to disastrous results.
When WEP is enabled, all members of the wireless network share a long term secret key k. The
standard supports either 40-bit keys or 128-bit keys. The 40-bit version complies with US export
restrictions that were in eﬀect at the time the standard was drafted. We will use the following
notation to describe WEP:
• WEP encryption uses the RC4 stream cipher. We let RC4( s) denote the pseudo random
sequence generated by RC4 given the seed s.
• We let CRC(m) denote the 32-bit CRC checksum of a message m ∈{0,1}∗. The details of
CRC are irrelevant for our discussion and it suﬃces to view CRC as some ﬁxed function from
bit strings to {0,1}32.
Let mbe an 802.11b cleartext frame. The ﬁrst few bits of mencode the length of m. To encrypt
an 802.11b frame m the sender picks a 24-bit IV and computes:
c←
(
m∥CRC(m)
)
⊕ RC4(IV ∥k)
cfull ←(IV, c)
The WEP encryption process is shown in Fig. 9.4. The receiver decrypts by ﬁrst computing
c⊕RC4(IV ∥k) to obtain a pair ( m,s). The receiver accepts the frame if s= CRC(m) and rejects
it otherwise.
389
Attack 1: IV collisions. The designers of WEP understood that a stream cipher key should
never be reused. Consequently, they used the 24-bit IV to derive a per-frame key kf := IV ∥k.
The standard, however, does not specify how to choose the IVs and many implementations do so
poorly. We say that an IV collision occurs whenever a wireless station happens to send two frames,
say frame number i and frame number j, encrypted using the same IV. Since IVs are sent in the
clear, an eavesdropper can easily detect IV collisions. Moreover, once an IV collision occurs the
attacker can use the two-time pad attack discussed in Section 3.3.1 to decrypt both frames iand j.
So, how likely is an IV collision? By the birthday paradox, an implementation that chooses
a random IV for each frame will cause an IV collision after only an expected
√
224 = 212 = 4096
frames. Since each frame body is at most 1156 bytes, a collision will occur after transmitting about
4MB on average.
Alternatively, an implementation could generate the IV using a counter. The implementation
will exhaust the entire IV space after 224 frames are sent, which will take about a day for a wireless
access point working at full capacity. Even worse, several wireless cards that use the counter method
reset the counter to 0 during power-up. As a result, these cards will frequently reuse low value IVs,
making the traﬃc highly vulnerable to a two-time pad attack.
Attack 2: related keys. A far more devastating attack on WEP encryption results from the
use of related RC4 keys. In Chapter 3 we explained that a new and random stream cipher key must
be chosen for every encrypted message. WEP, however, uses keys 1 ∥k, 2 ∥k,... which are all
closely related — they all have the same suﬃx k. RC4 was never designed for such use, and indeed,
is completely insecure in these settings. Fluhrer, Mantin, and Shamir [63] showed that after about
a million WEP frames are sent, an eavesdropper can recover the entire long term secret key k.
The attack was implemented by Stubbleﬁeld, Ioannidis, and Rubin [151] and is now available in a
variety of hacking tools such as WepCrack and AirSnort.
Generating per frame keys should have been done using a PRF, for example, setting the key for
frame ito ki := F(k,IV) — the resulting keys would be indistinguishable from random, independent
keys. Of course, while this approach would have prevented the related keys problem, it would not
solve the IV collision problem discussed above, or the malleability problem discussed next.
Attack 3: malleability. Recall that WEP attempts to provide authenticated encryption by
using a CRC checksum for integrity. In a sense, WEP uses the MAC-then-encrypt method, but it
uses CRC instead of a MAC. We show that despite the encryption step, this construction utterly
fails to provide ciphertext integrity.
The attack uses the linearity of CRC. That is, given CRC( m) for some message m, it is easy to
compute CRC(m⊕∆) for any ∆. More precisely, there is a public function Lsuch that for any m
and ∆ ∈{0,1}ℓ we have that
CRC(m⊕∆) = CRC(m) ⊕L(∆)
This property enables an attacker to make arbitrary modiﬁcations to a WEP ciphertext without
ever being detected by the receiver. Let c be a WEP ciphertext, namely
c=
(
m,CRC(m)
)
⊕ RC4(IV ∥k)
390
For any ∆ ∈{0,1}ℓ, an attacker can create a new ciphertext c′←c⊕
(
∆,L(∆)
)
, which satisﬁes
c′ = RC4(IV ∥k) ⊕
(
m, CRC(m)
)
⊕
(
∆, L(∆)
)
=
RC4(IV ∥k) ⊕
(
m⊕∆, CRC(m) ⊕L(∆)
)
=
RC4(IV ∥k) ⊕
(
m⊕∆, CRC(m⊕∆)
)
Hence, c′ decrypts without errors to m⊕∆. We see that given the encryption of m, an attacker
can create a valid encryption of m⊕∆ for any ∆ of his choice. We explained in Section 3.3.2 that
this can lead to serious attacks.
Attack 4: Chosen ciphertext attack. The protocol is vulnerable to a chosen ciphertext attack
called chop-chop that lets the attacker decrypt an encrypted frame of its choice. We describe a
simple version of this attack in Exercise 9.5.
Attack 5: Denial of Service. We brieﬂy mention that 802.11b suﬀers from a number of serious
Denial of Service (DoS) attacks. For example, in 802.11b a wireless client sends a “disassociate”
message to the wireless station once the client is done using the network. This allows the station
to free memory resources allocated to that client. Unfortunately, the “disassociate” message is
unauthenticated, allowing anyone to send a disassociate message on behalf of someone else. Once
disassociated, the victim will take a few seconds to re-establish the connection to the base station.
As a result, by sending a single “disassociate” message every few seconds, an attacker can prevent
a computer of their choice from connecting to the wireless network. These attacks are implemented
in 802.11b tools such as Void11.
802.11i. Following the failures of the 802.11b WEP protocol, a new standard called 802.11i was
ratiﬁed in 2004. 802.11i provides authenticated encryption using a MAC-then-encrypt mode called
CCM. In particular, CCM uses (raw) CBC-MAC for the MAC and counter mode for encryption.
Both are implemented in 802.11i using AES as the underlying PRF. CCM was adopted by NIST
as a federal standard [128].
9.11 A fun application: private information retrieval
To be written.
9.12 Notes
Citations to the literature to be added.
9.13 Exercises
9.1 (AE-security: simple examples). Let (E,D) be an AE-secure cipher. Consider the fol-
lowing derived ciphers:
(a) E1(k,m) :=
(
E(k,m), E(k,m)
)
; D1
(
k, (c1,c2)
):=
{
D(k,c1) if D(k,c1) = D(k,c2)
reject otherwise
391
(b) E2(k,m) :=
{
c←E(k,m), output (c,c)
}
; D2
(
k, (c1,c2)
):=
{
D(k,c1) if c1 = c2
reject otherwise
Show that part (b) is AE-secure, but part (a) is not.
9.2 (AE-security: some insecure constructions). Let (E,D) be a CPA-secure cipher deﬁned
over (K,M,C) and let H1 : M→T and H2 : C→T be collision resistant hash functions. Deﬁne
the following two ciphers:
(a) E1(k,m) :=
{
c←R E(k,m), output
(
c,H1(m)
)}
;
D1
(
k, (c1,c2)
):=
{
D(k,c1) if H1(D(k,c1)) = c2
reject otherwise
(b) E2(k,m) :=
{
c←R E(k,m), output
(
c,H2(c)
)}
;
D2
(
k, (c1,c2)
):=
{
D(k,c1) if H2(c1) = c2
reject otherwise
Show that both ciphers are not AE-secure.
9.3 (An Android Keystore Attack). Let (E,D) be a secure block cipher deﬁned over ( K,X),
and let ( Ecbc,Dcbc) be the cipher derived from ( E,D) using randomized CBC mode, as in Sec-
tion 5.4.3. Let H : X≤L →X be a collision resistant hash function. Consider the following attempt
at building an AE-secure cipher deﬁned over ( K, X≤L, X≤L+2):
E′(k,m) := Ecbc
(
k, (H(m),m)
)
; D′(k,c) :=
{ (t,m) ←Dcbc(k,c)
if t= H(m) output m, otherwise reject
}
Show that (E′,D′) is not AE-secure by giving a chosen-ciphertext attack on it. This construction
was used to protect secret keys in the Android KeyStore. The chosen-ciphertext attack resulted in
a compromise of the key store [138].
9.4 (Redundant message encoding does not give AE). The attack in the previous exercise
can be generalized if instead of using CBC encryption as the underlying cipher, we use randomized
counter mode, as in Section 5.4.2. Let ( Ectr,Dctr) be such a counter-mode cipher, and assume that
its message space is {0,1}ℓ′
. Let f : {0,1}ℓ →{0,1}ℓ′
be a one-to-one function, so that ℓ′≥ℓ. Let
g: {0,1}ℓ′
→{0,1}ℓ∪{⊥}be the inverse of f in the sense that g(m′) = mwhenever m′= f(m) for
some m, and g(m′) = ⊥if m′is not in the image of f. Intuitively, f represents an “error detecting
code”: a message m∈{0,1}ℓ is “encoded” as m′= f(m). If m′gets modiﬁed into a value ˜m′, this
modiﬁcation will be detected if g( ˜m′) = ⊥. Now deﬁne a new cipher ( E2,D2) with message space
{0,1}ℓ as follows:
E2(k,m) := Ectr
(
k, f(m)
)
; D1(k,c) :=
{ m′←Dctr(k,c)
if g(m′) ̸= ⊥output g(m′), otherwise reject
}
Show that (E2,D2) is not AE-secure by giving a chosen-ciphertext attack on it.
9.5 (Chop-chop attack). The parity bit b for a message m∈{0,1}∗ is just the XOR of all the
bits in m. After appending the parity bit, the message m′= m∥bhas the property that the XOR
of all the bits is zero. Parity bits are sometimes used as a very simple form of error detection. They
392
are meant to provide a little protection against low-probability, random errors: if a single bit of m′
gets ﬂipped, this can be detected, since the XOR of the bits of the corrupted m′will now be one.
Consider a cipher where messages are variable length bit strings, and encryption is done using
randomized counter mode without any padding. No MAC is used, but before the plaintext is
encrypted, the sender appends a parity bit to the end of the plaintext. After the receiver decrypts,
it checks the parity bit and returns either the plaintext (with the parity bit removed) or reject.
Design a chosen-ciphertext attack that recovers the complete plaintext of every encrypted message.
Your attack should work even if the adversary learns only one bit for every chosen-ciphertext query
c; it only learns if the decryption ofcsucceeded or resulted in reject, and learns nothing else about c.
Hint: Use the fact that the system encrypts variable length messages.
Remark: A variant of this attack, called chopchop, was used successfully against encryption in
the 802.11b protocol. The name is a hint for how the attack works. Note that the previous exercise
already tells us that this scheme is not CCA-secure, but the attack in this exercise is much more
devastating.
9.6 (Nested encryption). Let (E,D) be an AE-secure cipher. Consider the following derived
cipher (E′,D′):
E′(
(k1,k2),m
):= E
(
k2,E(k1,m)
)
; D′(
(k1,k2), c
):=
{
D
(
k1,D(k2,c)
)
if D(k2,c) ̸= reject
reject otherwise
(a) Show that ( E′,D′) is AE-secure even if the adversary knows k1, but not k2.
(b) Show that ( E′,D′) is not AE-secure if the adversary knows k2 but not k1.
(c) Design a cipher built from ( E,D) where keys are pairs ( k1,k2) ∈K2 and the cipher remains
AE-secure even if the adversary knows one of the keys, but not the other.
9.7 (A format oracle attack). Let Ebe an arbitrary CPA-secure cipher, and assume that the
key space for Eis {0,1}n. Show how to “sabotage” Eto obtain another cipher E′ such that E′ is
still CPA secure, but E′is insecure against chosen ciphertext attack, in the following sense. In the
attack, the adversary is allowed to make several decryption queries, such that in each query, the
adversary only learns whether the result of the decryption was reject or not. Design an adversary
that makes a series of decryption queries as above, and then outputs the secret key in its entirety.
9.8 (Choose independent keys). Let us see an example of a CPA-secure cipher and a secure
MAC that are insecure when used in encrypt-then-MAC when the same secret keykis used for both
the cipher and the MAC. Let ( E,D) be a block cipher deﬁned over ( K,X) where X= {0,1}n and
|X|is super-poly. Consider randomized CBC mode encryption built from (E,D) as the CPA-secure
cipher for single block messages: an encryption of m∈X is the pair c:= (r, E(k, r⊕m)) where
r is the random IV. Use FCBC (from Fig. 6.3a) built from ( E,D) as the secure MAC. This MAC is
secure in this context because it is only being applied to ﬁxed length messages (messages in X2):
the tag on a ciphertext c ∈X 2 is t := E
(
k, E(k,c[0]) ⊕c[1]
)
. Show that using the same key k
for both the cipher and the MAC in encrypt-then-MAC results in a cipher that does not provide
authenticated encryption. Both CPA security and ciphertext integrity can be defeated.
393
9.9 (MAC-then-encrypt). Prove that MAC-then-encrypt provides authenticated encryption
when the underlying cipher is randomized CBC mode encryption and the MAC is a secure MAC.
Speciﬁcally, assume that the underlying cipher works on blocks of a ﬁxed size, a message m is a
sequence of full blocks, and the tag t for the MAC is one full block, so that the message that is
CBC-encrypted is the block sequence m∥t, and no CBC padding is needed.
9.10 (An AEAD from encrypt-and-MAC). Let (E,D) be randomized counter mode encryp-
tion deﬁned over ( K,M,C) where the underlying secure PRF has domain X. We let E(k,m; r)
denote the encryption of message m with key k using r ∈X as the IV. Let F be a secure PRF
deﬁned over (K, (M×D×N ), X). Show that the following cipher (E1,D1) is a secure nonce-based
AEAD cipher assuming |X|is super-poly.
E1
(
(ke,km), m, d,N
):=
{
t←F
(
km, (m,d, N )
)
, c←R E(kc,m; t), output (c,t)
}
D1
(
(ke,km), (c,t), d, N )
):=
{ m←D(ke,c; t)
if F
(
km, (m,d, N )
)
̸= t output reject, otherwise output m
}
This method is loosely called encrypt-and-MAC because the message m is both encrypted by the
cipher and is the input to the MAC signing algorithm, which here is a PRF.
Discussion: This construction is related to the authenticated SIV cipher (Exercise 9.11) and
oﬀers similar nonce re-use resistance. One down-side of this system is that the tag t cannot be
truncated as one often does with a PRF-based MAC.
9.11 (Authenticated SIV). We discuss a modiﬁcation of the SIV construction, introduced in
Exercise 5.8, that provides ciphertext integrity without enlarging the ciphertext any further. We
call this the authenticated SIV construction. With E = ( E,D), F, and E′ = ( E′,D′) as in
Exercise 5.8, we deﬁne E′′= (E′,D′′), where
D′′(
(k,k′), c
):=
{ m←D(k,c)
if E′((k,k′),m) = c output m, otherwise output reject
}
Assume that |R|is super-poly and that for every ﬁxed key k ∈ Kand m ∈ M, the function
E(k,m; ·) : R→C is one to one (which holds for counter and CBC mode encryption). Show that
E′′provides ciphertext integrity.
Note: Since the encryption algorithm of E′′is the same as that of E′we know that E′′is determin-
istic CPA-secure, assuming that Eis CPA-secure (as was shown in Exercise 5.8).
9.12 (Constructions based on strongly secure block ciphers). Let (E,D) be a block cipher
deﬁned over (K, M×R).
(a) As in Exercise 5.6, let ( E′,D′) be deﬁned as
E′(k,m) :=
{
r←R R, c←R E
(
k, (m,r)
)
, output c
}
D′(k,c) :=
{
(m,r′) ←D(k,c), output m
}
Show that (E′,D′) is CCA-secure provided (E,D) is astrongly secureblock cipher and 1/|R|is
negligible. This is an example of a CCA-secure cipher that clearly does not provide ciphertext
integrity.
394
(b) Let ( E′′,D′′) be deﬁned as
E′′(k,m) :=
{
r←R R, c←R E
(
k, (m,r)
)
, output (c,r)
}
D′′(
k,(c,r)
):=
{ (m,r′) ←D(k,c)
if r= r′output m, otherwise output reject
}
This cipher is deﬁned over
(
K, M, (M×R)×R
)
. Show that (E′′,D′′) is AE-secure provided
(E,D) is a strongly secure block cipher and 1 /|R|is negligible.
(c) Suppose that 0 ∈R and we modify algorithms E′′and D′′to work as follows:
˜E′′(k,m) :=
{
r←0, c←R E
(
k, (m,r)
)
, output c
}
˜D′′(
k,c
):=
{ (m,r′) ←D(k,c)
if r′= 0 output m, otherwise output reject
}
Show that ( ˜E′′, ˜D′′) is one-time AE-secure provided (E,D) is a strongly secure block cipher,
and 1/|R|is negligible.
9.13 (MAC from encryption). Let ( E,D) be a cipher deﬁned over ( K,M,C). Deﬁne the
following MAC system (S,V ) also deﬁned over ( K,M,C):
S(k,m) := E(k,m); V(k,m,t ) :=
{
accept if D(k,t) = m
reject otherwise
Show that if (E,D) has ciphertext integrity then ( S,V ) is a secure MAC system.
9.14 (GCM analysis). Give a complete security analysis of GCM (see Section 9.7). Show that
it is nonce-based AEAD secure assuming the security of the underlying block cipher as a PRF
and that GHASH is an XOR-DUF. Start out with the easy case when the nonce is 96-bits. Then
proceed to the more general case where GHASH may be applied to the nonce to compute x.
9.15 (Plaintext integrity). Consider a weaker notion of integrity called plaintext integrity,
or simply PI. The PI game is identical to the CI game except that the winning condition is relaxed
to:
• D(k,c) ̸= reject, and
• D(k,c) ̸∈{m1,m2,... }
Prove that the following holds:
(a) Show that MAC-then-Encrypt is both CPA and PI secure.
Note: The MAC-then-Encrypt counter-example (Section 9.4.2) shows that a system that is
CPA and PI secure is not CCA-secure (and, therefore, not AE-secure).
(b) Prove that a system that is CCA- and PI-secure is also AE-secure. The proof only needs a
weak version of CCA, namely where the adversary issues a single decryption query and is
told whether the ciphertext is accepted or rejected. Also, you may assume a super-poly-sized
message space.
395
9.16 (Encrypted UHF MAC). Let H be a hash function deﬁned over ( KH,M,X) and (E,D)
be a cipher deﬁned over ( KE,X,C). Deﬁne the encrypted UHF MAC system I= (S,V ) as
follows: for key ( k1,k2) and message m∈M deﬁne
S
(
(k1,k2), m
):= E
(
k1, H(k2,m)
)
V
(
(k1,k2), m, c
):=
{
accept if H(k2,m) = D(k1,c),
reject otherwise.
Show that Iis a secure MAC system assuming H is a computational UHF and ( E,D) provides
authenticated encryption. Recall from Section 7.4 that CPA security of ( E,D) is insuﬃcient for
this MAC system to be secure.
9.17 (Simpliﬁed OCB mode). OCB is an elegant and eﬃcient AE cipher built from a tweakable
block cipher (as deﬁned in Exercise 4.11). Let ( E,D) be a tweakable block cipher deﬁned over
(K,X,T) where X:= {0,1}n and the tweak set is T := N ×{−ℓ,...,ℓ }. Consider the following
nonce-based cipher ( E′,D′) with key space K, message space X≤ℓ, ciphertext space Xℓ+1, and
nonce space N . For simplicity, the cipher does not support associated data.
E′(k,m, N ) :=


create (uninitialized) c∈X|m|
checksum ←0n
for i= 0,..., |m|−1 :
c[i] ←E
(
k, m[i], (N ,i + 1)
)
checksum ←checksum ⊕m[i]
t←E
(
k, checksum, (N ,−|m|)
)
output (c,t)



D′(k,(c,t),N ) :=


create (uninitialized) m∈X|c|
checksum ←0n
for i= 0,..., |c|−1 :
m[i] ←D
(
k, c[i], (N ,i + 1)
)
checksum ←checksum ⊕m[i]
t′←E
(
k, checksum, (N ,−|c|)
)
if t= t′output m, else reject



(a) Prove that ( E′,D′) is a nonce-based AE-secure cipher assuming ( E,D) is a strongly secure
tweakable block cipher and |X|is super-poly.
(b) Show that if t were computed as t ←E
(
k, checksum, (N ,0)
)
then the scheme would be
insecure: it would have no ciphertext integrity.
9.18 (Non-committing encryption). Let (E,D) be a cipher. We say that the cipher is non-
committing if an adversary can ﬁnd a ciphertext c and two keys k0,k1 such that c decrypts
successfully under both k0 and k1 and the resulting plaintexts are diﬀerent. The non-committing
property means that the adversary can transmit c, but if he or she are later required to reveal the
decryption key, say for an internal audit, the adversary can “open” the ciphertext in two diﬀerent
ways.
(a) Let ( E,D) be an encrypt-then-MAC AE-secure cipher where the underlying encryption is
randomized counter mode built using a secure PRF. Show that ( E,D) is non-committing.
(b) Show that GCM mode encryption is non-committing.
(c) Describe a simple way in which the ciphers from parts (a) and (b) can be made committing.
396
9.19 (Middlebox encryption). In this exercise we develop a mode of encryption that lets a
middlebox placed between the sender and recipient inspect all traﬃc in the clear, but prevents
the middlebox from modifying traﬃc en-route. This is often needed in enterprise settings where a
middlebox ensures that no sensitive information is accidentally sent out. Towards this goal let us
deﬁne a middlebox cipher as a tuple of four algorithms ( E,D,D ′,K) where E(k,m) and D(k,c)
are the usual encryption and decryption algorithms used by the end-points, K is an algorithm
that derives a sub-key k′from the primary key k (i.e., k′←R K(k)), and D′(k′,c) is the decryption
algorithm used by the middlebox with the sub-key k′. We require the usual correctness properties:
D(k,c) and D′(k′,c) output m whenever c←R E(k,m) and k′←R K(k).
(a) Security for a middlebox cipher (E,D,D ′,K) captures our desired conﬁdentiality and integrity
requirements. In particular, we say that a middlebox cipher is secure if the following three
properties hold:
(i) the cipher is secure against a chosen plaintext attack (CPA security) when the adversary
knows nothing about k,
(ii) the cipher provides ciphertext integrity with respect to the decryption algorithmD′(k′,·),
when the adversary knows nothing about k, and
(iii) the cipher provides ciphertext integrity with respect to the decryption algorithm D(k,·),
when the adversary is given a sub-key k′←R K(k), but again knows nothing about k.
The second requirement says that the middlebox will only decrypt authentic ciphertexts. The
third requirement says that the receiving end-point will only decrypt authentic ciphertexts,
even if the middlebox is corrupt.
Formalize these requirements as attack games.
(b) Give a construction that satisﬁes your deﬁnition from part (a). You can use an AE secure
cipher and a secure MAC as building blocks.
397
Part II
Public key cryptography
398
Chapter 10
Public key tools
We begin our discussion of public-key cryptography by introducing several basic tools that will be
used in the remainder of the book. The main applications for these tools will emerge in the next
few chapters where we use them for public-key encryption, digital signatures, and key exchange.
Since we use some basic algebra and number theory in this chapter, the reader is advised to ﬁrst
brieﬂy scan through Appendix A.
We start with a simple toy problem: generating a shared secret key between two parties so that
a passive eavesdropping adversary cannot feasibly guess their shared key. The adversary can listen
in on network traﬃc, but cannot modify messages en-route or inject his own messages. In a later
chapter we develop the full machinery needed for key exchange in the presence of an active attacker
who may tamper with network traﬃc.
At the onset we emphasize that security against eavesdropping is typically not suﬃcient for
real world-applications, since an attacker capable of listening to network traﬃc is often also able
to tamper with it; nevertheless, this toy eavesdropping model is a good way to introduce the new
public-key tools.
10.1 A toy problem: anonymous key exchange
Two users, Alice and Bob, who have never met before talk on the phone. They are worried that an
eavesdropper is listening to their conversation and hence they wish to encrypt the session. Since
Alice and Bob have never met before they have no shared secret key with which to encrypt the
session. Thus, their initial goal is to generate a shared secret unknown to the adversary. They may
later use this secret as a session-key for secure communication. To do so, Alice and Bob execute
a protocol where they take turns in sending messages to each other. The eavesdropping adversary
can hear all these messages, but cannot change them or inject his own messages. At the end of the
protocol Alice and Bob should have a secret that is unknown to the adversary. The protocol itself
provides no assurance to Alice that she is really talking to Bob, and no assurance to Bob that he
is talking to Alice — in this sense, the protocol is “anonymous.”
More precisely, we model Alice and Bob as communicating machines. A key exchange proto-
col P is a pair of probabilistic machines ( A,B) that take turns in sending messages to each other.
At the end of the protocol, when both machines terminate, they both obtain the same value k. A
protocol transcript TP is the sequence of messages exchanged between the parties in one exe-
cution of the protocol. Since A and B are probabilistic machines, we obtain a diﬀerent transcript
399
every time we run the protocol. Formally, the transcript TP of protocol P is a random variable,
which is a function of the random bits generated by A and B. The eavesdropping adversary A
sees the entire transcript TP and its goal is to ﬁgure out the secret k. We deﬁne security of a key
exchange protocol using the following game.
Attack Game 10.1 (Anonymous key exchange). For a key exchange protocol P = (A,B)
and a given adversary A, the attack game runs as follows.
• The challenger runs the protocol between Aand B to generate a shared key k and
transcript TP. It gives TP to A.
• Aoutputs a guess ˆk for k.
We deﬁne A’s advantage, denoted AnonKEadv[A,P], as the probability that ˆk= k. 2
Deﬁnition 10.1. We say that an anonymous key exchange protocol P is secure against an eaves-
dropper if for all eﬃcient adversaries A, the quantity AnonKEadv[A,P] is negligible.
This deﬁnition of security is extremely weak, for three reasons. First, we assume the adversary
is unable to tamper with messages. Second, we only guarantee that the adversary cannot guess
k in its entirety. This does not rule out the possibility that the adversary can guess, say, half
the bits of k. If we are to use k as a secret session key, the property we would really like is
that k is indistinguishable from a truly random key. Third, the protocol provides no assurance
of the identities of the participants. We will strengthen Deﬁnition 10.1 to meet these stronger
requirements in Chapter 21.
Given all the tools we developed in Part 1, it is natural to ask if anonymous key exchange can
be done using an arbitrary secure symmetric cipher. The answer is yes, it can be done as we show
in Section 10.8, but the resulting protocol is highly ineﬃcient. To develop eﬃcient protocols we
must ﬁrst introduce a few new tools.
10.2 One-way trapdoor functions
In this section, we introduce a tool that will allow us to build an eﬃcient and secure key exchange
protocol. In Section 8.11, we introduced the notion of a one-way function. This is a function
F : X →Ythat is easy to compute, but hard to invert. As we saw in Section 8.11, there are a
number of very eﬃcient functions that are plausibly one-way. One-way functions, however, are not
suﬃcient for our purposes. We need one-way functions with a special feature, called a trapdoor.
A trapdoor is a secret that allows one to eﬃciently invert the function; however, without knowledge
of the trapdoor, the function remains hard to invert.
Let us make this notion more precise.
Deﬁnition 10.2 (Trapdoor function scheme). Let Xand Ybe ﬁnite sets. A trapdoor func-
tion scheme T, deﬁned over (X,Y), is a triple of algorithms (G,F,I ), where
• G is a probabilistic key generation algorithm that is invoked as (pk,sk) ←R G(), where pk is
called a public key and sk is called a secret key.
• F is a deterministic algorithm that is invoked as y ←F(pk,x), where pk is a public key (as
output by G) and x lies in X. The output y is an element of Y.
400
• I is a deterministic algorithm that is invoked as x ←I(sk,y), where sk is a secret key (as
output by G) and y lies in Y. The output x is an element of X.
Moreover, the following correctness property should be satisﬁed: for all possible outputs (pk,sk)
of G(), and for all x∈X, we have I(sk, F(pk,x) ) = x.
Observe that for every pk, the function F(pk,·) is a function from X to Y. The correctness
property says that sk is the trapdoor for inverting this function; note that this property also implies
that the function F(pk,·) is one-to-one. Note that we do not insist that F(pk,·) maps Xonto Y.
That is, there may be elements y ∈Y that do not have any preimage under F(pk,·). For such y,
we make no requirements on algorithm I — it can return some arbitrary element x∈X (one might
consider returning a special reject symbol in this case, but it simpliﬁes things a bit not to do this).
In the special case where X= Y, then F(pk,·) is not only one-to-one, but onto. That is, F(pk,·)
is a permutation on the set X. In this case, we may refer to ( G,F,I ) as a trapdoor permutation
scheme deﬁned over X.
The basic security property we want from a trapdoor permutation scheme is a one-wayness
property, which basically says that given pk and F(pk,x) for random x∈X, it is hard to compute
x without knowledge of the trapdoor sk. This is formalized in the following game.
Attack Game 10.2 (One-way trapdoor function scheme). For a given trapdoor function
scheme T = ( G,F,I ), deﬁned over ( X,Y), and a given adversary A, the attack game runs as
follows:
• The challenger computes
(pk,sk) ←R G(), x ←R X, y ←F(pk,x)
and sends (pk,y) to the adversary.
• The adversary outputs ˆx∈X.
We deﬁne the adversary’s advantage in inverting T, denoted OW adv[A,T], to be the probability
that ˆx= x. 2
Deﬁnition 10.3. We say that a trapdoor function scheme T is one way if for all eﬃcient adver-
saries A, the quantity OWadv[A,T] is negligible.
Note that in Attack Game 10.2, since the value x is uniformly distributed over Xand F(pk,·)
is one-to-one, it follows that the value y := F(pk,x) is uniformly distributed over the image of
F(pk,·). In the case of a trapdoor permutation scheme, where X= Y, the value of y is uniformly
distributed over X.
10.2.1 Key exchange using a one-way trapdoor function scheme
We now show how to use a one-way trapdoor function scheme T = (G,F,I ), deﬁned over ( X,Y),
to build a secure anonymous key exchange protocol. The protocol runs as follows, as shown in
Fig. 10.1:
• Alice computes (pk,sk) ←R G(), and sends pk to Bob.
• Upon receiving pk from Alice, Bob computes x←R X,y ←F(pk,x), and sends y to Alice.
401
Alice Bob
(pk,sk) ←
R
G() pk x←
R
X
y←F(pk,x)
x←I(sk,y) x
Figure 10.1: Key exchange using a trapdoor function scheme
• Upon receiving y from Bob, Alice computes x←I(sk,y).
The correctness property of the trapdoor function scheme guarantees that at the end of the protocol,
Alice and Bob have the same valuex— this is their shared, secret key. Now consider the security of
this protocol, in the sense of Deﬁnition 10.1. In Attack Game 10.1, the adversary sees the transcript
consisting of the two messages pk and y. If the adversary could compute the secret x from this
transcript with some advantage, then this very same adversary could be used directly to break the
trapdoor function scheme, as in Attack Game 10.2, with exactly the same advantage.
10.2.2 Mathematical details
We give a more mathematically precise deﬁnition of a trapdoor function scheme, using the termi-
nology deﬁned in Section 2.3.
Deﬁnition 10.4 (Trapdoor function scheme). A trapdoor function scheme is a triple of
eﬃcient algorithms (G,F,I ) along with families of spaces with system parameterization P:
X = {Xλ,Λ}λ,Λ,Y = {Yλ,Λ}λ,Λ.
As usual, λ∈Z≥1 is a security parameter and Λ ∈Supp(P(λ)) is a domain parameter. We require
that
1. X is eﬃciently recognizable and sampleable.
2. Y is eﬃciently recognizable.
3. G is an eﬃcient probabilistic algorithm that on input λ,Λ, where λ∈Z≥1, Λ ∈Supp(P(λ)),
outputs a pair (pk,sk), where pk and sk are bit strings whose lengths are always bounded by
a polynomial in λ.
4. F is an eﬃcient deterministic algorithm that on input λ,Λ,pk,x, where λ ∈ Z≥1, Λ ∈
Supp(P(λ)), (pk,sk) ∈Supp(G(λ,Λ)) for some sk, and x ∈Xλ,Λ, outputs an element of
Yλ,Λ.
402
5. I is an eﬃcient deterministic algorithm that on input λ,Λ,sk,y, where λ ∈ Z≥1, Λ ∈
Supp(P(λ)), (pk,sk) ∈Supp(G(λ,Λ)) for some pk, and y ∈Yλ,Λ, outputs an element of
Xλ,Λ.
6. For all λ ∈ Z≥1, Λ ∈ Supp(P(λ)), (pk,sk) ∈ Supp(G(λ,Λ)), and x ∈ Xλ,Λ, we have
I(λ,Λ; sk,F(λ,Λ; pk,x)) = x.
As usual, in deﬁning the one-wayness security property, we parameterize Attack Game 10.2 by
the security parameterλ, and the advantage OWadv[A,T] is actually a function ofλ. Deﬁnition 10.3
should be read as saying that OW adv[A,T](λ) is a negligible function.
10.3 A trapdoor permutation scheme based on RSA
We now describe a trapdoor permutation scheme that is plausibly one-way. It is called RSA
after its inventors, Rivest, Shamir, and Adleman. Recall that a trapdoor permutation is a special
case of a trapdoor function, where the domain and range are the same set. This means that for
every public-key, the function is a permutation of its domain, which is why we call it a trapdoor
permutation. Despite many years of study, RSA is essentially the only known reasonable candidate
trapdoor permutation scheme (there are a few others, but they are all very closely related to the
RSA scheme).
Here is how RSA works. First, we describe a probabilistic algorithm RSAGen that takes as
input an integer ℓ> 2, and an odd integer e> 2.
RSAGen(ℓ,e) :=
generate a random ℓ-bit prime p such that gcd(e,p −1) = 1
generate a random ℓ-bit prime q such that gcd(e,q −1) = 1 and q̸= p
n←pq
d←e−1 mod (p−1)(q−1)
output (n,d).
To eﬃciently implement the above algorithm, we need an eﬃcient algorithm to generate random
ℓ-bit primes. This is discussed in Appendix A. Also, we use the extended Euclidean algorithm
(Appendix A) to compute e−1 mod (p−1)(q−1). Note that since gcd( e,p−1) = gcd(e,q −1) = 1,
it follows that gcd(e,(p−1)(q−1)) = 1, and hence ehas a multiplicative inverse modulo (p−1)(q−1).
Now we describe the RSA trapdoor permutation scheme TRSA = (G,F,I ). It is parameterized
by ﬁxed values of ℓ and e.
• Key generation runs as follows:
G() := ( n,d) ←R RSAGen(ℓ,e), pk ←(n,e), sk ←(n,d)
output (pk,sk).
• For a given public key pk = (n,e), and x∈Zn, we deﬁne F(pk,x) := xe ∈Zn.
• For a given secret key sk = (n,d), and y∈Zn, we deﬁne I(sk,y) := yd ∈Zn.
Note that although the encryption exponent eis considered to be a ﬁxed system parameter, we
also include it as part of the public key pk.
403
A technicality. For each ﬁxed pk = ( n,e), the function F(pk,·) maps Zn into Zn; thus, the
domain and range of this function actually vary with pk. However, in our deﬁnition of a trapdoor
permutation scheme, the domain and range of the function are not allowed to vary with the public
key. So in fact, this scheme does not quite satisfy the formal syntactic requirements of a trapdoor
permutation scheme. One could easily generalize the deﬁnition of a trapdoor permutation scheme,
to allow for this. However, we shall not do this; rather, we shall state and analyze various schemes
based on a trapdoor permutation scheme as we have deﬁned it, and then show how to instantiate
these schemes using RSA. Exercise 10.27 explores an idea that builds a proper trapdoor permutation
scheme based on RSA.
Ignoring this technical issue for the moment, let us ﬁrst verify thatTRSA satisﬁes the correctness
requirement of a trapdoor permutation scheme. This is implied by the following:
Theorem 10.1. Let n= pq where p and q are distinct primes. Let e and d be integers such that
ed≡1 (mod ( p−1)(q−1)). Then for all x∈Z, we have xed ≡x (mod n).
Proof. The hypothesis that ed≡1 (mod ( p−1)(q−1)) just means that ed= 1 + k(p−1)(q−1)
for some integer k. Certainly, if x ≡0 (mod p), then xed ≡0 ≡x (mod p); otherwise, if x ̸≡0
(mod p), then by Fermat’s little theorem (Appendix A), we have
xp−1 ≡1 (mod p),
and so
xed ≡x1+k(p−1)(q−1) ≡x·
(
x(p−1))k(q−1) ≡x·1k(q−1) ≡x (mod p).
Therefore,
xed ≡x (mod p).
By a symmetric argument, we have
xed ≡x (mod q).
Thus, xed −x is divisible by the distinct primes p and q, and must therefore be divisible by their
product n, which means
xed ≡x (mod n). 2
So now we know that TRSA satisﬁes the correctness property of a trapdoor permutation scheme.
However, it is not clear that it is one-way. For TRSA, one-wayness means that there is no eﬃcient
algorithm that given n and xe, where x∈Zn is chosen at random, can eﬀectively compute x. It is
clear that if TRSA is one-way, then it must be hard to factor n; indeed, if it were easy to factor n,
then one could compute d in exactly the same way as is done in algorithm RSAGen, and then use
d to compute x= yd.
It is widely believed that factoring n is hard, provided ℓ is suﬃciently large — typically, ℓ
is chosen to be between 1000 and 1500. Moreover, the only known eﬃcient algorithm to invert
TRSA is to ﬁrst factor n and then compute d as above. However, there is no known proof that the
assumption that factoring n is hard implies that TRSA is one-way. Nevertheless, based on current
evidence, it seems reasonable to conjecture that TRSA is indeed one-way. We state this conjecture
now as an explicit assumption. As usual, this is done using an attack game.
Attack Game 10.3 (RSA). For given integers ℓ >2 and odd e >2, and a given adversary A,
the attack game runs as follows:
404
• The challenger and the adversary Atake (ℓ,e) as input.
• The challenger computes
(n,d) ←R RSAGen(ℓ,e), x ←R Zn, y ←xe ∈Zn
and sends (n,y) to the adversary.
• The adversary outputs ˆx∈Zn.
We deﬁne the adversary’s advantage in breaking RSA, denoted RSA adv[A,ℓ,e ], as the probability
that ˆx= x. 2
Deﬁnition 10.5 (RSA assumption). We say that the RSA assumption holds for (ℓ,e) if for all
eﬃcient adversaries A, the quantity RSAadv[A,ℓ,e ] is negligible.
We analyze the RSA assumption and present several known attacks on it later on in Chapter 16.
We next introduce some terminology that will be useful later. Suppose ( n,d) is an output of
RSAGen(ℓ,e), and suppose that x∈Zn and let y:= xe. The number nis called an RSA modulus,
the number e is called an encryption exponent , and the number d is called a decryption
exponent. We call ( n,y) an instance of the RSA problem, and we call x a solution to this
instance of the RSA problem. The RSA assumption asserts that there is no eﬃcient algorithm that
can eﬀectively solve the RSA problem.
10.3.1 Key exchange based on the RSA assumption
Consider now what happens when we instantiate the key exchange protocol in Section 10.2.1 with
TRSA. The protocol runs as follows:
• Alice computes (n,d) ←R RSAGen(ℓ,e), and sends ( n,e) to Bob.
• Upon receiving (n,e) from Alice, Bob computes x←R Zn, y←xe, and sends y to Alice.
• Upon receiving y from Bob, Alice computes x←yd.
The secret shared by Alice and Bob is x. The message ﬂow is the same as in Fig. 10.1. Under the
RSA assumption, this is a secure anonymous key exchange protocol.
10.3.2 Mathematical details
We give a more mathematically precise deﬁnition of the RSA assumption, using the terminology
deﬁned in Section 2.3.
In Attack Game 10.3, the parameters ℓ and e are actually poly-bounded and eﬃciently com-
putable functions of a security parameter λ. Likewise, RSA adv[A,ℓ,e ] is a function of λ. As usual,
Deﬁnition 10.5 should be read as saying that RSA adv[A,ℓ,e ](λ) is a negligible function.
There are a couple of further wrinkles we should point out. First, as already mentioned above,
the RSA scheme does not quite ﬁt our deﬁnition of a trapdoor permutation scheme, as the deﬁnition
of the latter does not allow the set X to vary with the public key. It would not be too diﬃcult
to modify our deﬁnition of a trapdoor permutation scheme to accommodate this generalization.
Second, the speciﬁcation of RSAGen requires that we generate random prime numbers of a given
405
bit length. In theory, it is possible to do this in (expected) polynomial time; however, the most
practical algorithms (see Appendix A) may — with negligible probability — output a number that
is not a prime. If that should happen, then it may be the case that the basic correctness requirement
— namely, that I(sk,F(pk,x)) = xfor all pk,sk,x — is no longer satisﬁed. It would also not be too
diﬃcult to modify our deﬁnition of a trapdoor permutation scheme to accommodate this type of
generalization as well. For example, we could recast this requirement as an attack game (in which
any eﬃcient adversary wins with negligible probability): in this game, the challenger generates
(pk,sk) ←R G() and sends ( pk,sk) to the adversary; the adversary wins the game if he can output
x∈X such that I(sk,F(pk,x)) ̸= x. While this would be a perfectly reasonable deﬁnition, using
it would require us to modify security deﬁnitions for higher-level constructs. For example, if we
used this relaxed correctness requirement in the context of key exchange, we would have to allow
for the possibility that the two parties end up with diﬀerent keys with some negligible probability.
10.4 Diﬃe-Hellman key exchange
In this section, we explore another approach to constructing secure key exchange protocols, which
was invented by Diﬃe and Hellman. Just as with the protocol based on RSA, this protocol will
require a bit of algebra and number theory. However, before getting in to the details, we provide
a bit of motivation and intuition.
Consider the following “generic” key exchange protocol that makes use of two functions E and
F. Alice chooses a random secret α, computes E(α), and sends E(α) to Bob over an insecure
channel. Likewise, Bob chooses a random secret β, computes E(β), and sends E(β) to Alice over
an insecure channel. Alice and Bob both somehow compute a shared key F(α,β). In this high-level
description, E and F are some functions that should satisfy the following properties:
1. E should be easy to compute;
2. given α and E(β), it should be easy to compute F(α,β);
3. given E(α) and β, it should be easy to compute F(α,β);
4. given E(α) and E(β), it should be hard to compute F(α,β).
Properties 1–3 ensure that Alice and Bob can eﬃciently implement the protocol: Alice computes
the shared key F(α,β) using the algorithm from Property 2 and her given data α and E(β). Bob
computes the same key F(α,β) using the algorithm from Property 3 and his given data E(α) and
β. Property 4 ensures that the protocol is secure: an eavesdropper who sees E(α) and E(β) should
not be able to compute the shared key F(α,β).
Note that properties 1–4 together imply that E is hard to invert; indeed, if we could compute
eﬃciently αfrom E(α), then by Property 2, we could eﬃciently compute F(α,β) from E(α),E(β),
which would contradict Property 4.
To make this generic approach work, we have to come up with appropriate functions E and F.
To a ﬁrst approximation, the basic idea is to implement E in terms of exponentiation to some ﬁxed
base g, deﬁning E(α) := gα and F(α,β) := gαβ. Notice then that
E(α)β = (gα)β = F(α,β) = (gβ)α = E(β)α.
406
Hence, provided exponentiation is eﬃcient, Properties 1–3 are satisﬁed. Moreover, if Property 4 is
to be satisﬁed, then at the very least, we require that taking logarithms (i.e., inverting E) is hard.
To turn this into a practical and plausibly secure scheme, we cannot simply perform exponen-
tiation on ordinary integers since the numbers would become too large. Instead, we have to work
in an appropriate ﬁnite algebraic domain, which we introduce next.
10.4.1 The key exchange protocol
Suppose pis a large prime and that q is a large prime dividing p−1 (think of pas being very large
random prime, say 2048 bits long, and think of q as being about 256 bits long).
We will be doing arithmetic mod p, that is, working in Zp. Recall that Z∗
p is the set of nonzero
elements of Zp. An essential fact is that since q divides p−1, Z∗
p has an element g of order q (see
Appendix A). This means that gq = 1 and that all of the powers ga, for a = 0 ,...,q −1, are
distinct. Let G := {ga : a = 0,...,q −1}, so that G is a subset of Z∗
p of cardinality q. It is not
hard to see that G is closed under multiplication and inversion; that is, for all u,v ∈G, we have
uv ∈G and u−1 ∈G. Indeed, ga ·gb = ga+b = gc with c := (a+ b) mod q, and ( ga)−1 = gd with
d:= (−a) mod q. In the language of algebra, G is called a subgroup of the group Z∗
p.
For every u∈G and integers a and b, it is easy to see that ua = ub if a≡bmod q. Thus, the
value of ua depends only on the residue class of a modulo q. Therefore, if α = [a]q ∈Zq is the
residue class of a modulo q, we can deﬁne uα := ua and this deﬁnition is unambiguous. From here
on we will frequently use elements of Zq as exponents applied to elements of G.
So now we have everything we need to describe the Diﬃe-Hellman key exchange protocol. We
assume that the description of G, including g ∈G and q, is a system parameter that is generated
once and for all at system setup time and shared by all parties involved. The protocol runs as
follows, as shown in Fig. 10.2:
1. Alice computes α←R Zq, u←gα, and sends u to Bob.
2. Bob computes β ←R Zq,v ←gβ and sends v to Alice.
3. Upon receiving v from Bob, Alice computes w←vα
4. Upon receiving u from Alice, Bob computes w←uβ
The secret shared by Alice and Bob is
w= vα = gαβ = uβ.
10.4.2 Security of Diﬃe-Hellman key exchange
For a ﬁxed element g∈G, diﬀerent from 1, the function from Zq to G that sends α∈Zq to gα ∈G
is called the discrete exponentiation function . This function is one-to-one and onto, and its
inverse function is called the discrete logarithm function , and is usually denoted Dlogg; thus,
for u∈G, Dlogg(u) is the unique α∈Zq such that u= gα. The value g is called the base of the
discrete logarithm.
If the Diﬃe-Hellman protocol has any hope of being secure, it must be hard to compute αfrom
gα for a random α; in other words, it must be hard to compute the discrete logarithm function.
407
Alice Bob
G,g,q G,g,q
α←
R
Zq u←gα β ←
R
Zq
v←gβ
w←vα = gαβ w←uβ = gαβ
Figure 10.2: Diﬃe-Hellman key exchange
There are a number of candidate group families G where the discrete logarithm function is believed
to be hard to compute. For example, when p and q are suﬃciently large, suitably chosen primes,
the discrete logarithm function in the order q subgroup of Z∗
p is believed to be hard to compute
(p should be at least 2048-bits, and q should be at least 256-bits). This assumption is called the
discrete logarithm assumption and is deﬁned in the next section.
Unfortunately, the discrete logarithm assumption by itself is not enough to ensure that the
Diﬃe-Hellman protocol is secure. Observe that the protocol is secure if and only if the following
holds:
given gα,gβ ∈G, where α←R Zq and β ←R Zq, it is hard to compute gαβ ∈G.
This security property is called the computational Diﬃe-Hellman assumption. Although the
computational Diﬃe-Hellman assumption is stronger than the discrete logarithm assumption, all
evidence still suggests that this is a reasonable assumption in groups where the discrete logarithm
assumption holds.
10.5 Discrete logarithm and related assumptions
In this section, we state the discrete logarithm and related assumptions more precisely and in
somewhat more generality, and explore in greater detail relationships among them.
The subset G of Z∗
p that we deﬁned above in Section 10.4 is a speciﬁc instance of a general type
of mathematical object known as a cyclic group . There are in fact other cyclic groups that are
very useful in cryptography, most notably, groups based on elliptic curves — we shall study elliptic
curve cryptography in Chapter 15. From now on, we shall state assumptions and algorithms in
terms of an abstract cyclic group G of prime order q generated by g ∈G. In general, such groups
may be selected by a randomized process, and again, the description of G, including g∈G and q,
is a system parameter that is generated once and for all at system setup time and shared by all
parties involved.
We shall use just a bit of terminology from group theory. The reader who is unfamiliar with the
concept of a group may wish to refer to Appendix A; alternatively, for the time being, the reader
408
may simply ignore this abstraction entirely:
• Whenever we refer to a “cyclic group,” the reader may safely assume that this means the
speciﬁc set G deﬁned above as a subgroup of Z∗
p.
• The “order of G” is just a fancy name for the size of the set G, which is q.
• A “generator of G” is an element g ∈G with the property that every element of G can be
expressed as a power of g.
• By convention, we assume that the description of G includes its order q and some generator
g∈G.
We begin with a formal statement of the discrete logarithm assumption, stated in our more
general language. As usual, we need an attack game.
Attack Game 10.4 (Discrete logarithm). Let G be a cyclic group of prime order q generated
by g∈G. For a given adversary A, deﬁne the following attack game:
• The challenger and the adversary Atake a description of G as input. The descrip-
tion includes the order q and a generator g∈G.
• The challenger computes
α←R Zq, u ←gα,
and sends u∈G to the adversary.
• The adversary outputs some ˆα∈Zq.
We deﬁne A’s advantage in solving the discrete logarithm problem for G, denoted
DLadv[A,G], as the probability that ˆα= α. 2
Deﬁnition 10.6 (Discrete logarithm assumption). We say that the discrete logarithm
(DL) assumption holds for G if for all eﬃcient adversaries Athe quantity DLadv[A,G] is neg-
ligible.
We say that gα is an instance of the discrete logarithm (DL) problem (for G), and that
α is a solution to this problem instance. The DL assumption asserts that there is no eﬃcient
algorithm that can eﬀectively solve the DL problem for G.
Note that the DL assumption is deﬁned in terms of a group G and generator g∈G. As already
mentioned, the group G and generator g are chosen and ﬁxed at system setup time via a process
that may be randomized. Also note that all elements of G\{1}are in fact generators for G, but we
do not insist that g is chosen uniformly among these (but see Exercise 10.19). Diﬀerent methods
for selecting groups and generators give rise to diﬀerent DL assumptions (and the same applies to
the CDH and DDH assumptions, deﬁned below).
Now we state the computational Diﬃe-Hellman assumption.
Attack Game 10.5 (Computational Diﬃe-Hellman). Let G be a cyclic group of prime
order q generated by g∈G. For a given adversary A, the attack game runs as follows.
• The challenger and the adversary Atake a description of G as input. The descrip-
tion includes the order q and a generator g∈G.
409
• The challenger computes
α,β ←R Zq, u ←gα, v ←gβ, w ←gαβ
and sends the pair ( u,v) to the adversary.
• The adversary outputs some ˆw∈G.
We deﬁne A’s advantage in solving the computational Diﬃe-Hellman problem for G,
denoted CDHadv[A,G], as the probability that ˆw= w. 2
Deﬁnition 10.7 (Computational Diﬃe-Hellman assumption). We say that the compu-
tational Diﬃe-Hellman (CDH) assumption holds for G if for all eﬃcient adversaries Athe
quantity CDHadv[A,G] is negligible.
We say that (gα,gβ) is an instance of the computational Diﬃe-Hellman (CDH) problem,
and that gαβ is a solution to this problem instance. The CDH assumption asserts that there is no
eﬃcient algorithm that can eﬀectively solve the CDH problem for G.
An interesting property of the CDH problem is that there is no general and eﬃcient algorithm
to even recognizecorrect solutions to the CDH problem, that is, given an instance (u,v) of the CDH
problem, and a group element ˆw, to determine if ˆwis a solution to the given problem instance. This
is in contrast to the RSA problem: given an instance ( n,e,y ) of the RSA problem, and an element
ˆx of Z∗
n, we can eﬃciently test if ˆx is a solution to the given problem instance simply by testing
if ˆxe = y. In certain cryptographic applications, this lack of an eﬃcient algorithm to recognize
solutions to the CDH problem can lead to technical diﬃculties. However, this apparent limitation
is also an opportunity: if we assume not only that solving the CDH problem is hard, but also that
recognizing solutions to the CDH problem is hard, then we can sometimes prove stronger security
properties for certain cryptographic schemes.
We shall now formalize the assumption that recognizing solutions to the CDH problem is hard.
In fact, we shall state a stronger assumption, namely, that even distinguishing solutions from
random group elements is hard. It turns out that this stronger assumption is equivalent to the
weaker one (see Exercise 10.10).
Attack Game 10.6 (Decisional Diﬃe-Hellman). Let G be a cyclic group of prime order q
generated by g∈G. For a given adversary A, we deﬁne two experiments.
Experiment b (b= 0,1):
• The challenger and the adversary Atake a description of G as input.
• The challenger computes
α,β,γ ←R Zq, u ←gα, v ←gβ, w 0 ←gαβ, w 1 ←gγ,
and sends the triple ( u,v,w b) to the adversary.
• The adversary outputs a bit ˆb∈{0,1}.
If Wb is the event that Aoutputs 1 in Experiment b, we deﬁne A’s advantage in solving the
decisional Diﬃe-Hellman problem for G as
DDHadv[A,G] :=
⏐⏐⏐Pr[W0] −Pr[W1]
⏐⏐⏐. 2
410
Deﬁnition 10.8 (Decisional Diﬃe-Hellman assumption). We say that the decisional
Diﬃe-Hellman (DDH) assumption holds for G if for all eﬃcient adversaries Athe quantity
DDHadv[A,G] is negligible.
For α,β,γ ∈Zq, we call ( gα,gβ,gγ) a DH-triple if γ = αβ; otherwise, we call it a non-
DH-triple. The DDH assumption says that there is no eﬃcient algorithm that can eﬀectively
distinguish between random DH-triples and random triples. More precisely, in the language of
Section 3.11, the DDH assumptions says that the uniform distribution over DH-triples and the
uniform distribution over G3 are computationally indistinguishable. It is not hard to show that
the DDH assumption implies that it is hard to distinguish between random DH-triples and random
non-DH-triples (see Exercise 10.7).
Clearly, the DDH assumption implies the CDH assumption: if we could eﬀectively solve the
CDH problem, then we could easily determine if a given triple ( u,v, ˆw) is a DH-triple by ﬁrst
computing a correct solution w to the instance ( u,v) of the CDH problem, and then testing if
w= ˆw.
In deﬁning the DL, CDH, and DDH assumptions, we have restricted our attention to prime
order groups. This is convenient for a number of technical reasons. See, for example Section 16.1.3
where we show that the DDH assumption for groups of even order is simply false.
10.5.1 Random self-reducibility
An important property of the discrete-log function in a group G is that it is either hard almost
everywhere in G or easy everywhere in G. A middle ground where discrete-log is easy for some
inputs and hard for others is not possible. We prove this by showing that the discrete-log function
has a random self reduction.
Consider a speciﬁc cyclic group G of prime order qgenerated by g∈G. Suppose Ais an eﬃcient
algorithm with the following property: if u∈G is chosen at random, then Pr[A(u) = Dlogg(u)] = ϵ.
That is, on a random input u, algorithm Acomputes the discrete logarithm of u with probability
ϵ. Here, the probability is over the random choice of u, as well as any random choices made by A
itself.1 Suppose ϵ= 0.1. Then the group G is of little use in cryptography since an eavesdropper
can use Ato break 10% of all Diﬃe-Hellman key exchanges. However, this does not mean that A
is able to compute Dlogg(u) with non-zero probability for all u∈G. It could be the case that for
10% of the inputs u∈G, algorithm Aalways computes Dlogg(u), while for the remaining 90%, it
never computes Dlogg(u).
We show how to convert Ainto an eﬃcient algorithm Bwith the following property: for all
u ∈ G, algorithm B on input u successfully computes Dlogg(u) with probability ϵ. Here, the
probability is only over the random choices made by B. We do so using a reduction that maps a
given discrete-log instance to a random discrete-log instance. Such a reduction is called a random
self reduction.
Theorem 10.2. Consider a speciﬁc cyclic group G of prime order qgenerated by g∈G. Suppose A
is an eﬃcient algorithm with the following property: if u∈G is chosen at random, then Pr[A(u) =
Dlogg(u)] = ϵ, where the probability is over the random choice of u and the random choices made
by A. Then there is an eﬃcient algorithm Bwith the following property: for all u∈G, algorithm B
1Technical note: the probability ϵ is not quite the same as DL adv[A,G], as the latter is also with respect to the
random choice of group/generator made at system setup time; here, we are viewing these as truly ﬁxed.
411
G G
Aworks for inputs here Bworks everywhere
=⇒
Figure 10.3: The eﬀect of a random self reduction
either outputs fail or Dlogg(u), and it outputs the latter with probability ϵ, where now the probability
is only over the random choices made by B.
Theorem 10.2 implements the transformation shown in Fig. 10.3. The point is that, unlike A,
algorithm Bworks for all inputs. To compute discrete-log of a particular u ∈G one can iterate
Bon the same input u several times, say n⌈1/ϵ⌉times for some n. Using the handy inequality
1 + x≤exp(x) (which holds for all x), this iteration will produce the discrete-log with probability
1−(1−ϵ)n⌈1/ϵ⌉≥1−exp(−n). In particular, if 1 /ϵis poly-bounded, we can eﬃciently compute the
discrete logarithm of any group element with negligible failure probability. In contrast, iterating A
on the same input umany times may never produce a correct answer. Consequently, if discrete-log
is easy for a non-negligible fraction of instances, then it will be easy for all instances.
Proof of Theorem 10.2. Algorithm Bworks as follows:
Input: u∈G
Output: Dlogg(u) or fail
σ←R Zq
u1 ←u·gσ ∈G
α1 ←A(u1)
if gα1 ̸= u1
then output fail
else output α←α1 −σ
Suppose that u = gα. Observe that u1 = gα+σ. Since σ is uniformly distributed over Zq, the
group element u1 is uniformly distributed over G. Therefore, on input u1, adversary Awill output
α1 = α+ σ with probability ϵ. When this happens, Bwill output α1 −σ = α, and otherwise, B
will output fail. 2
Why random self reducibility is important. Any hard problem can potentially form the
basis of a cryptosystem. For example, an NP-hard problem known as subset sum has attracted
attention for many years. Unfortunately, many hard problems, including subset sum, are only hard
in the worst case. Generally speaking, such problems are of little use in cryptography, where we
need problems that are not just hard in the worst case, but hard on average (i.e., for randomly
chosen inputs). For a problem with a random self-reduction, if it is hard in the worst case, then it
must be hard on average. This implication makes such problems attractive for cryptography.
412
One can also give random self reductions for both the CDH and DDH problems, as well as for
the RSA problem (in a more limited sense). These ideas are developed in the chapter exercises.
10.5.2 Mathematical details
As in previous sections, we give the mathematical details pertaining to the DL, CDH, and DDH
assumptions. We use the terminology introduced in Section 2.3. This section may be safely skipped
on ﬁrst reading with very little loss in understanding.
To state the assumptions asymptotically we introduce a security parameter λthat identiﬁes the
group in which the DL, CDH, and DDH games are played. We will require that the adversary’s
advantage in breaking the assumption is a negligible function of λ. As lambda increases the
adversary’s advantage in breaking discrete-log in the group deﬁned by λshould quickly go to zero.
To make sense of the security parameter λ we need a family of groups that increase in size as
λ increases. As in Section 2.3, this family of groups is parameterized by both λ and an additional
system parameter Λ. The idea is that once λ is chosen, a system parameter Λ is generated by a
system parameterization algorithm P. The pair ( λ,Λ) then fully identiﬁes the group Gλ,Λ
where the DL, CDH, and DDH games are played. Occasionally we will refer to Λ as a group
description. This Λ is a triple
Λ := ( Λ1, q, g)
where Λ1 is an arbitrary string, q is a prime number that represents the order of the group Gλ,Λ,
and g is a generator of Gλ,Λ.
Deﬁnition 10.9 (group family). A group family G consists of an algorithm Mul along with a
family of spaces:
G = {Gλ,Λ}λ,Λ
with system parameterization algorithm P, such that
1. G is eﬃciently recognizable.
2. Algorithm Mul is an eﬃcient deterministic algorithm that on input λ∈Z≥1, Λ ∈Supp(P(λ)),
u,v ∈Gλ,Λ, outputs w∈Gλ,Λ.
3. For all λ∈Z≥1, Λ = (Λ1,q,g ) ∈Supp(P(λ)), algorithm Mul is a multiplication operation on
Gλ,Λ that deﬁnes a cyclic group of prime order q generated by g.
The deﬁnition implies that all the spaces Gλ,Λ are eﬃciently sampleable. Since Λ = (Λ 1,q,g )
we can randomly sample a random element u of Gλ,Λ by picking a random α ←R Zq and setting
u ←gα. Speciﬁc group families may allow for a more eﬃcient method that generates a random
group element. The group identity element may always be obtained by raising g to the power q,
although for speciﬁc group families, there are most likely simpler and faster ways to do this.
An example. We deﬁne the asymptotic version of a subgroup of prime order q within Z∗
p, where
q is a prime dividing p−1, and p itself is prime. Here the system parameterization algorithm P
takes λ as input and outputs a group description Λ := (p,q,g ) where p is a random ℓ(λ)-bit prime
(for some poly-bounded length function ℓ) and g is an element of Z∗
p of order q. The group Gλ,Λ is
the subgroup of Z∗
p generated by g. Elements of Gλ,Λ may be eﬃciently recognized as follows: ﬁrst,
one can check that a given bit string properly encodes an element u of Z∗
p; second, one can check
that uq = 1.
413
Armed with the concept of a group family, we now parameterize the DL Attack Game 10.4
by the security parameter λ. In that game, the adversary is given the security parameter λ and
a group description Λ = (Λ 1,q,g ), where g is a generator for the group Gλ,Λ. It is also given a
random u ∈Gλ,Λ, and it wins the game if it computes Dlogg(u). Its advantage DL adv[A,G] is
now a function of λ, and for each λ, this advantage is a probability that depends on the random
choice of group and generator, as well as the random choices made by the the challenger and the
adversary. Deﬁnition 10.6 should be read as saying that DL adv[A,G](λ) is a negligible function.
We use the same approach to deﬁne the asymptotic CDH and DDH assumptions.
10.6 Collision resistant hash functions from number-theoretic
primitives
It turns out that the RSA and DL assumptions are extremely versatile, and can be used in many
cryptographic applications. As an example, in this section, we show how to build collision-resistant
hash functions based on the RSA and DL assumptions.
Recall from Section 8.1 that a hash function H deﬁned over (M,T) is an eﬃciently computable
function from Mto T. In most applications, we want the message space Mto be much larger
than the digest space T. We also deﬁned a notion of collision resistance, which says that for every
eﬃcient adversary A, its collision-ﬁnding advantage CRadv[A,H] is negligible. Here, CR adv[A,H]
is deﬁned to be the probability that Acan produce a collision, i.e., a pair m0,m1 ∈M such that
m0 ̸= m1 but H(m0) = H(m1).
10.6.1 Collision resistance based on DL
Before presenting our DL-based hash function, we introduce a simple but surprisingly useful con-
cept. Let G be a cyclic group of prime order q generated by g∈G. Suppose h∈G is an arbitrary
group element.
For u ∈G, a representation (relative to g and h) of u is a pair ( α,β) ∈Z2
q such that
gαhβ = u. For a given u ∈G, there are many representations. In fact, there are precisely q of
them: for every β ∈Zq, there exists a unique α∈Zq such that gα = uh−β.
The key to our hash function design is the following fact: given two diﬀerent representations of
the same group element, we can eﬃciently compute Dloggh. Indeed, suppose (α,β) and (α′,β′) are
two diﬀerent representations of the same group element. This means
gαhβ = gα′
hβ′
and ( α,β) ̸= (α′,β′).
This implies
gα−α′
= hβ′−β. (10.1)
Moreover, we must have β′−β ̸= 0, as otherwise, (10.1) (and the fact that g is a generator) would
imply α−α′ = 0, contradicting the assumption that ( α,β) ̸= (α′,β′). It follows that β−β′ has
a multiplicative inverse in Zq, which we can in fact eﬃciently compute (see Appendix A). Raising
both sides of (10.1) to the power 1 /(β′−β), we obtain
g(α−α′)/(β′−β) = h.
In other words, Dloggh= (α−α′)/(β′−β).
To summarize:
414
Fact 10.3 (Computing DL from two representations). Suppose we are given (α,β) and
(α′,β′), which are two diﬀerent representations (relative to g and h) of the same group element.
Then we can eﬃciently compute Dloggh as follows:
Dloggh= (α−α′)/(β′−β).
This fact suggests the following hash function Hdl, which is deﬁned over ( Zq ×Zq,G). This
hash function is parameterized by the group G and the generator g, along with a randomly chosen
h ∈G. Thus, the group G, along with the group elements g and h, are chosen once and for all;
together, these system parameters deﬁne the hash function Hdl. For α,β ∈Zq, we deﬁne
Hdl(α,β) := gαhβ.
The essential observation is that a collision on Hdl is a pair of distinct representations of the
same group element, and so from any collision, we can use Fact 10.3 to compute Dloggh.
Theorem 10.4. The hash function Hdl is collision resistant under the DL assumption.
In particular, for every collision-ﬁnding adversary A, there exists a DL adversary B, which is
an elementary wrapper around A, such that
CRadv[A,Hdl] = DLadv[B,G]. (10.2)
Proof. We use the given collision-ﬁnding adversary Ato build a DL adversary Bas follows. When
Breceives its challenge h∈G from its DL-challenger, Bruns Ausing Hdl, which is deﬁned using
G, g, and the given h. Suppose Aﬁnds a collision. This is a pair of distinct inputs ( α,β) ̸= (α′,β′)
such that
gαhβ = gα′
hβ′
.
In other words, ( α,β) and ( α′,β′) are distinct representations (relative to g and h) of the same
group element. From these, Bcan compute Dloggh as in Fact 10.3. 2
The function Hdl : Zq×Zq →G maps from a message space of size q2 to a digest space of size q.
The good news is that the message space is larger than the digest space, and so the hash function
actually compresses. The bad news is that the set of encodings of G may be much larger than the
set G itself. Indeed, if G is constructed as recommended in Section 10.4 as a subset of Z∗
p, then
elements of G are encoded as 2048-bit strings, even though the group G itself has order ≈2256. So
if we replace the set G by the set of encodings, the hash function Hdl is not compressing at all.
This problem can be avoided by using other types of groups with more compact encodings, such
as elliptic curve groups (see Chapter 15). See also Exercise 10.21 and Exercise 10.22.
10.6.2 Collision resistance based on RSA
We shall work with an RSA encryption exponent ethat is a prime. For this application, the bigger
e is, the more compression we get. Let Ie := {0,...,e −1}. Let n be an RSA modulus, generated
as in Section 10.3 using an appropriate length parameter ℓ. We also choose a random y∈Z∗
n. The
values e, n, and y are chosen once and for all, and together they determine a hash function Hrsa
deﬁned over (Z∗
n ×Ie, Z∗
n) as follows: for a∈Z∗
n and b∈Ie, we deﬁne
Hrsa(a,b) := aeyb.
415
We will show that Hrsa is collision resistant under the RSA assumption. Note that Hrsa can be
used directly as a compression function in the Merkle-Damg˚ ard paradigm (see Section 8.4) to build
a collision-resistant hash function for arbitrarily large message spaces. In applying Theorem 8.3,
we would take X= Z∗
n and Y= {0,1}⌊log2 e⌋.
To analyze Hrsa, we will need a couple of technical results. The ﬁrst result simply says that in
the RSA attack game, it is no easier to compute an eth root of a random element of Z∗
n than it is
to compute an eth root of a random element of Zn. To make this precise, suppose that we modify
Attack Game 10.3 so that the challenger chooses x←R Z∗
n, and keep everything else the same. Note
that since x is uniformly distributed over Z∗
n, the value y := xe is also uniformly distributed over
Z∗
n. Denote by uRSA adv[A,ℓ,e ] the adversary A’s advantage in this modiﬁed attack game.
Theorem 10.5. Let ℓ> 2 and odd e> 2 be integers. For every adversary A, there exists an adver-
sary B, which is an elementary wrapper around A, such that uRSAadv[A,ℓ,e ] ≤RSAadv[B,ℓ,e ].
Proof. Let Abe a given adversary. Here is how Bworks. Adversary Breceives a random element
y ∈Zn. If y ∈Z∗
n, then Bgives y to Aand outputs whatever Aoutputs. Otherwise, Bcomputes
an eth root x of y as follows. If y = 0, Bsets x:= 0; otherwise, by computing the GCD of y and
n, Bcan factor n, compute the RSA decryption exponent d, and then compute x:= yd.
Let W be the event that Bsucceeds. We have
Pr[W] = Pr[W |y∈Z∗
n] Pr[y∈Z∗
n] + Pr[W |y /∈Z∗
n] Pr[y /∈Z∗
n].
The result follows from the observations that
Pr[W |y∈Z∗
n] = uRSAadv[A,ℓ,e ]
and
Pr[W |y /∈Z∗
n] = 1 ≥uRSAadv[A,ℓ,e ]. 2
The above theorem shows that the standard RSA assumption implies a variant RSA assumption,
where the preimage is chosen at random from Z∗
n, rather than Zn. In Exercise 10.26, you are to
show the converse, that is, that this variant RSA assumption implies the standard RSA assumption.
We also need the following technical result, which says that given y∈Z∗
n, along with an integer
f that is relatively prime to e, and an eth root of yf, we can easily compute an eth root of y itself.
Just to get a feeling for the result, suppose e = 3 and f = 2. We have w ∈Z∗
n such that
w3 = y2. We want to compute x∈Z∗
n such that x3 = y. If we set x:= (y/w), then we have
x3 = y3/w3 = y3/y2 = y.
Theorem 10.6 (Shamir’s trick). There is an eﬃcient algorithm that takes as input n,e,f,w,y ,
where nis a positive integer, eand f are relatively prime integers, and w and y are elements of Z∗
n
that satisfy we = yf, and outputs x∈Z∗
n such that xe = y.
Proof. Using the extended Euclidean algorithm (Appendix A), we compute integers s and t such
that es+ ft = gcd(e,f), and output x:= yswt. If gcd( e,f) = 1 and we = yf, then
xe = (yswt)e = yeswet = yesyft = yes+ft = y1 = y. 2
Theorem 10.7. The hash function Hrsa is collision resistant under the RSA assumption.
416
In particular, for every collision-ﬁnding adversary A, there exists an RSA adversary B, which
is an elementary wrapper around A, such that
CRadv[A,Hrsa] ≤RSAadv[B,ℓ,e ]. (10.3)
Proof. We construct an adversary B′ that plays the alternative RSA attack game considered in
Theorem 10.5. We will show that CR adv[A,Hrsa] = uRSA adv[B′,ℓ,e ], and the theorem will the
follow from Theorem 10.5.
Our RSA adversary B′runs as follows. It receives ( n,y) from its challenger, where nis an RSA
modulus and y is a random element of Z∗
n. The values e,n,y deﬁne the hash function Hrsa, and
adversary B′runs adversary Awith this hash function. Suppose that Aﬁnds a collision. This is a
pair of inputs ( a,b) ̸= (a′,b′) such that
aeyb = (a′)eyb′
,
which we may rewrite as
(a/a′)e = yb′−b.
Using this collision, B′will compute an eth root of y.
Observe that b′−b ̸= 0, since otherwise we would have ( a/a′) = 1 and hence a = a′. Also
observe that since |b−b′|< eand e is prime, we must have gcd( e,b −b′) = 1. So now we simply
apply Theorem 10.6 with n, e, and y as given, and w:= a/a′and f := b′−b. 2
10.7 Attacks on the anonymous Diﬃe-Hellman protocol
The Diﬃe-Hellman key exchange is secure against a passive eavesdropper. Usually, however, an
attacker capable of eavesdropping on traﬃc is also able to inject its own messages. The protocol
completely falls apart in the presence of an active adversary who controls the network. The main
reason is the lack of authentication. Alice sets up a shared secret, but she has no idea with whom
the secret is shared. The same holds for Bob. An active attacker can abuse this to expose all traﬃc
between Alice and Bob. The attack, called a man in the middle attack , works against any key
exchange protocol that does not include authentication. It works as follows (see Fig. 10.4):
• Alice sends (g,gα) to Bob. The attacker blocks this message from reaching Bob. He picks a
random α′←R Zn and sends (g,gα′
) to Bob.
• Bob responds with gβ. The attacker blocks this message from reaching Alice. He picks a
random β′←R Zn and sends gβ′
to Alice.
• Now Alice computes the key kA := gαβ′
and Bob computes kB := gα′β. The attacker knows
both kA and kB.
At this point Alice thinks kA is a secret key shared with Bob and will use kA to encrypt messages
to him. Similarly for Bob with his key kB. The attacker can act as a proxy between the two. He
intercepts each message ci := E(kA,mi) from Alice, re-encrypts it as c′
i ←E(kB,mi) and forwards
c′
i to Bob. He also re-encrypts messages from Bob to Alice. The communication channel works
properly for both parties and they have no idea that this proxying is taking place. The attacker,
however, sees all plaintexts in the clear.
417
Alice Bob Adversary
α←
R
Zq u←gα
α′←
R
Zq u′←gα′
β ←
R
Zqv←gβ
β′←
R
Zqv′←gβ′
ka ←(v′)α = gαβ′
kb ←(u′)β = gα′β
ka, kb
Figure 10.4: Man in the middle attack
This generic attack explains why we view key exchange secure against eavesdropping as a toy
problem. Protocols secure in this model can completely fall apart once the adversary can tamper
with traﬃc. We will come back to this problem in Chapter 21, where we design protocols secure
against active attackers.
10.8 Merkle puzzles: a partial solution to key exchange using
block ciphers
Can we build a secure key exchange protocol using symmetric-key primitives? The answer is yes,
but the resulting protocol is very ineﬃcient. We show how to do key exchange using a block cipher
E= (E,D) deﬁned over (K,M). Alice and Bob want to generate a random s∈M that is unknown
to the adversary. They use a protocol called Merkle puzzles (due to the same Merkle from the
Merkle-Damg˚ ard hashing paradigm). The protocol, shown in Fig. 10.5, works as follows:
Protocol 10.1 (Merkle puzzles).
1. Alice chooses random pairs ( ki,si) ←R K×M for i= 1,...,L . We will determine the optimal
value for L later. She constructs L puzzles where puzzle P′
i is deﬁned as a triple:
P′
i :=
(
E(ki,si), E(ki,i), E(ki,0)
)
.
Next, she sends the L puzzles in a random order to Bob. That is, she picks a random
permutation π←R Perms[{1,...,L }] and sends ( P1,...,P L) := (P′
π(1),...,P ′
π(L)) to Bob.
2. Bob picks a random puzzle Pj = (c1,c2,c3) where j ←R {1,...,L }. He solves the puzzle by
brute force, by trying all keys k∈K until he ﬁnds one such that
D(k,c3) = 0. (10.4)
418
Alice Bob
Puzzles P1, ..., PL
j ←
R
{1,...,ℓ }
Pj = (c1,c2,c3)ℓ←D(k,c2)
k←sℓ k←sℓ
Figure 10.5: Merkle puzzles protocol
In the unlikely event that Bob ﬁnds two diﬀerent keys that satisfy (10.4), he indicates to
Alice that the protocol failed, and they start over. Otherwise, Bob computes ℓ ←D(k,c2)
and s←D(k,c1), and sends ℓ back to Alice.
3. Alice locates puzzle P′
ℓ and sets s←sℓ. Both parties now know the shared secret s∈M.
Clearly, when the protocol terminates successfully, both parties agree on the same secrets∈M.
Moreover, when |M|is much larger than |K|, the protocol is very likely to terminate successfully,
because under these conditions (10.4) is likely to have a unique solution.
The work for each party in this protocol is as follows:
Alice’s work = O(L), Bob’s work = O(|K|).
Hence, to make the workload for the two parties about the same we need to setL≈|K|. Either way,
the size of Land Kneeds to be within reason so that both parties can perform the computation in
a reasonable time. For example, one can set L≈|K|≈ 230. When using AES one can force Kto
have size 230 by ﬁxing the 98 most signiﬁcant bits of the key to zero.
Security. The adversary sees the protocol transcript which includes all the puzzles and the quan-
tity ℓ sent by Bob. Since the adversary does not know which puzzle Bob picked, intuitively, he
needs to solve all puzzles until he ﬁnds puzzle Pℓ. Thus, to recover s∈M the adversary must solve
L puzzles each one taking O(|K|) time to solve. Overall, the adversary must spend time O(L|K|).
One can make this argument precise, by modeling the block cipher Eas an ideal cipher, as we
did in Section 4.7. We can assume that |K|is poly-bounded, and that |M|is super-poly. Then
the analysis shows that if the adversary makes at most Q queries to the ideal cipher, then its
probability of learning the secret s ∈M is bounded by approximately Q/L|K|. Working out the
complete proof and the exact bound is a good exercise in working with the ideal cipher model.
Performance. Suppose we set L ≈|K|. Then the adversary must spend time O(L2) to break
the protocol, while each participant spends time O(L). This gives a quadratic gap between the
work of the participants and the work to break the protocol. Technically speaking, this doesn’t
satisfy our deﬁnitions of security — with constant work the adversary has advantage about 1 /L2
419
which is non-negligible. Even worse, in practice one would have to make Lextremely large to have
a reasonable level of security against a determined attacker. The resulting protocol is then very
ineﬃcient.
Nevertheless, the Merkle puzzles protocol is very elegant and shows what can be done using
block ciphers alone. As the story goes, Merkle came up with this clever protocol while taking a
seminar as an undergraduate student. The professor gave the students the option of submitting a
research paper instead of taking the ﬁnal exam. Merkle submitted his key exchange protocol as
the research project. These ideas, however, were ahead of their time and the professor rejected the
paper. Merkle still had to take the ﬁnal exam. Subsequently, for his Ph.D. work, Merkle chose to
move to a diﬀerent school to work with Martin Hellman.
It is natural to ask if a better key exchange protocol, based on block ciphers, can achieve better
than quadratic separation between the participants and the adversary. Unfortunately, a result by
Impagliazzo and Rudich [92] suggests that one cannot achieve better separation using block ciphers
alone.
10.9 A fun application: accumulators
In Section 8.9 we saw how Alice can use a Merkle tree to commit to an ordered tuple of elements
(x1,...,x n) ∈Xn. She can later prove to Bob that xi is the value at position i, for any 1 ≤i≤n
of her choice, where each such proof is a sequence of log 2 n hashes.
The tools developed in this chapter give a very diﬀerent construction that has some advan-
tages over Merkle trees. Our goal here is to show how Alice can commit to an unordered set
S = {x1,...,x n}⊆X , so that she can later prove to Bob that a particular x∈X satisﬁes x∈S
(a membership proof), or satisﬁes x ̸∈S (a non-membership proof). A commitment scheme for
unordered sets that supports both membership and non-membership proofs is called an accumu-
lator. An accumulator is said to be a dynamic accumulator if it is easy to add elements to the
set S. In particular, there is an eﬃcient algorithm that takes as input a commitment to a set S
along with an element x̸∈S, and outputs a commitment to the set S′:= S∪{x}
Groups of unknown order. We construct a simple dynamic accumulator using an algebraic tool
called a group of unknown order or GUO. A GUO is a pair ( G,g), where G is the description
of a ﬁnite abelian group, and g is an element of G. As the name suggests, it should be diﬃcult to
deduce the size of G from (G,g). A group of unknown order is generated by a probabilistic group
generation algorithm GGen that is invoked as GGen() →(G,g).
We say that GGen satisﬁes the strong RSA assumption if it is diﬃcult to ﬁnd a non-trivial
root of g. In particular, it should be diﬃcult to ﬁnd a pair ( x,e) ∈G ×Z, such that xe = g and
e̸= ±1. For an adversary Awe deﬁne the strong RSA game as follows: the game begins by running
(G,g) ←R GGen() and sending (G,g) to A. Then Aoutputs (x,e) and wins the game if xe = g and
e̸= ±1. We say that GGen satisﬁes the strong RSA assumption if no eﬃcient adversary can win
the strong RSA game with respect to GGen with non-negligible advantage.
This assumption is called strong RSA because the adversary is given g and gets to choose e
however it wants (as long as e ̸= ±1). In the standard RSA assumption (Deﬁnition 10.5), the
adversary is given g, and must ﬁnd an e-th root of g for a pre-speciﬁed e. Giving the adversary the
ﬂexibility to choose emakes the adversary’s job easier. Hence, assuming that this seemingly easier
problem is hard, is a stronger assumption than standard RSA.
420
If the order of g is a known value u∈Z, then the strong RSA assumption is clearly false. The
adversary could choose some integer v >1 that is relatively prime to u, compute x:= g(v−1 mod u),
and output the pair ( x,v). Then xv = g, which breaks the assumption. Hence, for strong RSA to
hold, the order of g in G, or even a multiple of the order, must be unknown given ( G,g).
There are a number of candidate groups of unknown order where the strong RSA assumption
is believed to hold. We saw one example in Section 10.3: the group Z∗
n of integers modulo an RSA
modulus n = pq, where n ←RSAGen(ℓ,e), for some ℓ and e. In particular, GGen operates as
follows:
GGen() =
{
n←R RSAGen(ℓ,e), g ←R Z∗
n, output (n,g)
}
.
The order of Z∗
n is ϕ(n) = ( p−1)(q−1) which is, as far as we know, diﬃcult to compute just
given n. Moreover, the strong RSA assumption is believed to hold for this GGen. We will discuss
other candidate groups of unknown order at the end of the section.
An accumulator construction. Let’s assume that a group of unknown order (G,g) has already
been generated and is known to all parties. We treat (G,g) as the public parameters of the scheme.
In addition, the accumulator construction needs a collision resistant hash function H : X →
Primes(L), where Primes(L) is the set of smallest Lprime numbers: Primes(L) := {2,3,5,...,p L}
where pL is the L-th prime number. We need L to be suﬃciently large so that H can be collision
resistant. Taking L= 2256 is suﬃcient.
The GUO accumulator, an accumulator from a group of unknown order, works as follows:
• Alice commits to an unordered set S = {x1,...,x n}⊆X as follows:
C(S) :=
{
ei ←H(xi) for i= 1,...,n
E ←e1 ·e2 ···en ∈Z, output c:= gE
}
(10.5)
She sends c= C(S) ∈G to Bob as a short commitment to the entire set S.
This accumulator is clearly dynamic: given a commitment cto a set S along with an element
x̸∈S, the commitment to S′:= S∪{x}is simply c′:= cH(x).
• A membership proof for x ∈X with respect to c. Alice has ( S,x), Bob has ( c,x), and
Alice wants to convince Bob that x is in S. Observe that if x ∈S then β := E/H(x) is an
integer, where E is deﬁned in (10.5).
– Alice can prove to Bob thatxis in Sby computing b←gβ = gE/H(x) ∈G and outputting
the short proof πx := b.
– Bob veriﬁes the proof πx = b by checking that bH(x) = c holds in G.
• A non-membership proof for x ∈X with respect to c. Alice has ( S,x), Bob has ( c,x),
and Alice wants to convince Bob that x is not in S. We will argue below that if x̸∈S then
the integers H(x) and E must be relatively prime. The extended Euclidean algorithm can
then be used to ﬁnd β,γ ∈Z such that
β·H(x) + γ·E = 1 and |γ|≤ H(x).
– Alice proves to Bob that x is not in S by computing b ←gβ and outputting the short
proof π¬x := (b,γ) ∈G ×Z.
421
– Bob veriﬁes the proof π¬x = (b,γ) by checking that bH(x) ·cγ = g.
That’s the entire construction. We say that an accumulator is correct if for all sets S ⊆X and
all x∈X, if x∈S then Alice can generate a membership proof for x, and if x̸∈S then Alice can
generate a non-membership proof for x. For the GUO accumulator, if x∈S then Alice can always
generate a membership proof for x. However, if x̸∈S, Alice can only generate a non-membership
proof when gcd( H(x),E) = 1, for E is deﬁned in (10.5). Let’s show that it is infeasible to ﬁnd
a set S and an x ̸∈S for which gcd( H(x),E) > 1. The argument proceeds in two steps. First,
because H(x) is a prime number, this would imply that H(x) must be one of the prime factors of
E. Second, the only way H(x) is one of the prime factors of E is if there is some y ∈S such that
H(x) = H(y). Since x ̸∈S and y ∈S we deduce that x ̸= y and H(x) = H(y). Then x and y
are a collision for H, which contradicts the collision resistance of H. Therefore, if H is collision
resistant, then gcd(H(x),E) = 1 for any explicit x̸∈S, and Alice can generate a non-membership
proof for this x. We conclude that the GUO accumulator is correct, but correctness relies on the
collision resistance of H.
Security of the GUO accumulator. A correct accumulator is said to be secure if no eﬃcient
adversary can simultaneously prove that x ∈S and x ̸∈S, for some committed set S and some
x∈X. More precisely, no eﬃcient adversary can output a 4-tuple ( c,x,π x,π¬x) so that (i) πx is a
valid membership proof for xwith respect to c, and (ii) π¬x is a valid non-membership proof for x
with respect to c.
For the GUO accumulator, suppose an adversary breaks security by outputting a 4-tuple(
c,x,a, (b,γ)
)
where both the membership proof a and the non-membership proof ( b,γ) are valid.
Then:
aH(x) = c and bH(x) ·cγ = g.
The left equality implies ( aγ)H(x) = cγ. Substituting ( aγ)H(x) for cγ in the right equality gives
(b·aγ)H(x) = g, which breaks the strong RSA assumption. It follows that if the strong RSA
assumption holds for GGen then the GUO accumulator is secure.
Accumulator proof size. Membership and non-membership proofs for the GUO accumulator
are quite short; their size is independent of the number of elements n in the accumulator. This is
better than Merkle tree proofs where the proof size grows with log 2 n. In practice, the GUO proofs
are shorter than Merkle tree proofs whenever n> 256.
GUO accumulator proofs have another advantage over Merkle tree proofs: Alice can aggregate
multiple proofs into a single proof. Suppose Alice wants to prove that both x and y are in S, for
some x ̸= y. Alice can do so by producing a single proof πxy := b, where b ←gE/(H(x)·H(y)) ∈G.
Bob veriﬁes the proof by checking that bH(x)·H(y) = c. Hence, a single group element can convince
Bob that both x and y are in S. If needed, Bob can recover the individual proofs πx and πy from
πxy by computing πx ←bH(y) and πy ←bH(x). Hence, the aggregate proof b is purely a proof
compression mechanism that cannot harm security. Notice that Bob has to do a little more work
to verify the aggregate proof πxy because the exponent H(x) ·H(y) is a little bigger than usual.
An interesting property of aggregate proofs is that anyone can aggregate membership proofs.
Suppose Carol obtains distinct x,y ∈S, along with their membership proofs πx = gE/H(x) and
πy = gE/H(y). Carol can construct the short aggregate proof πxy = gE/(H(x)·H(y)) herself using
Shamir’s trick described in Theorem 10.6. Indeed, πH(x)
x = πH(y)
y = gE, and therefore applying
422
Theorem 10.6 lets Carol compute the aggregate proof πxy = gE/(H(x)·H(y)). Hence, anyone can
compress two membership proofs into one.
More generally, Alice can prove that an entire setT is a subset of S using a proof that is a single
group element, namely πT := gE/∏
x∈T H(x). Naively, Bob’s work to verify the proof would grow
linearly in |T|due to the larger exponent needed to verify the proof. However, using additional
tools, Bob’s work to verify the proof can be reduced to about the same as verifying a membership
proof for a single element (see [31, §4.2]). Proofs of non-membership can be similarly aggregated:
Alice can construct a constant size proof to prove that a set T is disjoint from S (i.e., T ∩S = ∅).
This requires more sophisticated techniques that we will not describe here (see [31, §4.2]).
In comparison, Merkle tree proofs cannot be aggregated. Proving membership or non-
membership for t elements requires a proof whose size grows linearly in t.
Remark 10.1. Suppose Alice commits to a set S of size n, and wants to compute the membership
proof for every element x ∈S. Naively, these n membership proofs take quadratic time in n to
compute. However, we show in Exercise 10.32 that Alice can compute all n membership proofs in
time proportional to nlog n. 2
More groups of unknown order. The GUO accumulator needs a group of unknown order
(G,g) where no one knows the order of the group G. The group Z∗
n, where n = pq is an RSA
modulus, is a good candidate. However, it requires a trusted party to generate two primes p and
q, publish n = pq, and then erase the primes p and q. If the trusted party does not erase p and
q, then it could create false membership and non-membership proofs for the GUO accumulator, as
discussed in Exercise 10.31.
It would better if we had a group of unknown order that does not require a trusted setup. One
option is to use the group Z∗
n, where nis a large random integer, so large that no one can factor n.
This way, no one would know the size ofZ∗
n. Unfortunately, nneeds to be more than sixty thousand
bits long to ensure that a random integer nis hard to factor with high probability. This makes the
group operation very slow, and makes group elements quite large.
A better option is to use a family of groups, called ideal class groups, from algebraic number
theory. In particular, the class group of an imaginary quadratic ﬁeld is deﬁned by a negative integer
∆, called the discriminant. Once ∆ is ﬁxed, the class group, denoted Cl(∆), is completely deﬁned.
For cryptographic applications one typically takes ∆ to be a negative prime number where ∆ ≡1
(mod 4). The group size is then approximately |∆|1/2. The best known algorithm for computing
the size of Cl(∆) takes sub-exponential time in log |∆|. In practice, choosing the discriminant ∆ as
a random 2048 bit prime seems suﬃcient to obtain a group of unknown order. See [43] for more
information about computing in ideal class groups.
One caveat in using Cl(∆) is that there is an eﬃcient algorithm for computing square roots
in this group [37]. As a result, we need to amend the Strong RSA assumption to require that
the exponent e in the adversary’s output satisﬁes |e|> 2. In addition, the number 2 needs to be
removed from the range of the hash function H used in the GUO accumulator.
10.10 Notes
Citations to the literature to be added.
423
10.11 Exercises
10.1 (Computationally unbounded adversaries). Show that an anonymous key exchange
protocol P (as in Deﬁnition 10.1) cannot be secure against a computationally unbounded adversary.
This explains why all protocols in this chapter must rely on computational assumptions.
10.2 (Key exchange requires randomness from both parties). Consider an anonymous
key exchange protocol P between two probabilistic machines A and B (as in Deﬁnition 10.1).
Suppose that one of the machines is deterministic, so that it uses no random bits when participating
in the protocol. Show that protocol P cannot be secure: there is an adversary Afor which
AnonKEadv[A,P] = 1. This explains why in all the protocols in this chapter, both parties are
randomized.
10.3 (DDH PRG). Let G be a cyclic group of prime order q generated by g ∈G. Consider the
following PRG deﬁned over (Z2
q, G3):
G(α,β) := (gα,gβ,gαβ).
Show that G is a secure PRG assuming DDH holds in G.
10.4 (The Naor-Reingold PRF). Let G be a cyclic group of prime order q generated by g∈G.
Let us show that the following PRF deﬁned over
(
Zn+1
q , {0,1}n, G
)
is secure assuming DDH holds
in G:
FNR
(
(α0,α1,...,α n), (x1,...,x n)
)
:= g(α0·αx1
1 ···αxnn )
This secure PRF is called the Naor-Reingold PRF.
(a) We prove security of FNR using Exercise 4.18. First, show that FNR is an augmented tree
construction constructed from the PRG: GNR(α,gβ) := (gβ,gαβ).
(b) Second, show that GNR satisﬁes the hypothesis of Exercise 4.18 part (b), assuming DDH
holds in G. Use the result of Exercise 10.11.
Security of FNR now follows from Exercise 4.18 part (b).
Discussion: See Exercise 11.2 for a simpler PRF from the DDH assumption, but in the random
oracle model.
10.5 (Random self-reduction for CDH (I)). Consider a speciﬁc cyclic group G of prime order
q generated by g ∈G. For u= gα ∈G and v = gβ ∈G, deﬁne [ u,v] = gαβ, which is the solution
instance (u,v) of the CDH problem. Consider the randomized mapping from G2 to G2 that sends
(u,v) to (˜u,v), where
ρ←R Zq, ˜u←gρu.
Show that
(a) ˜u is uniformly distributed over G;
(b) [˜u,v] = [u,v] ·vρ.
424
10.6 (Random self-reduction for CDH (II)). Continuing with the previous exercise, suppose
Ais an eﬃcient algorithm that solves the CDH problem with success probability ϵ on random
inputs. That is, if u,v ∈ G are chosen at random, then Pr[ A(u,v) = [ u,v]] = ϵ, where the
probability is over the random choice of uand v, as well as any random choices made by A. Using
A, construct an eﬃcient algorithm Bthat solves the CDH problem with success probability ϵ for
all inputs. More precisely, for all u,,v ∈G, we have Pr[B(u,v) = [u,v]] = ϵ, where the probability
is now only over the random choices made by B.
Remark: If we iterate Bon the same input ( u,v) many times, say n⌈1/ϵ⌉times for some n, at
least one of these iterations will output the correct result [ u,v] with probability 1 −(1 −ϵ)n⌈1/ϵ⌉≥
1 −exp(−n). Unfortunately, assuming the DDH is true, we will have no way of knowing which of
these outputs is the correct result.
10.7 (An alternative DDH characterization). Let G by a cyclic group of prime order q
generated by g∈G. Let Pbe the uniform distribution overG3. Let Pdh be the uniform distribution
over the set of all DH-triples ( gα,gβ,gαβ). Let Pndh be the uniform distribution over the set of all
non-DH-triples (gα,gβ,gγ),γ ̸= αβ.
(a) Show that the statistical distance (as in Deﬁnition 3.5) between Pand Pndh is 1/q.
(b) Using part (a), deduce that under the DDH assumption, the distributions Pdh and Pndh are
computationally indistinguishable (as in Deﬁnition 3.4). In particular, show that for every
adversary A, we have Distadv[A,Pdh,Pndh] ≤DDHadv[A,G] + 1/q.
10.8 (Random self-reduction for DDH (I)). Consider a speciﬁc cyclic group G of prime order
q generated by g∈G. Let DH be the set of all DH-triples, i.e.,
DH := {(gα,gβ,gαβ) ∈G3 : α,β ∈Zq}.
For ﬁxedu∈G, and let Tu be the subset of G3 whose ﬁrst coordinate is u. Consider the randomized
mapping from G3 to G3 that sends (u,v,w ) to (u,v∗,w∗), where
σ←R Zq, τ ←R Zq, v∗←gσvτ, w∗←uσwτ.
Prove the following:
(a) if ( u,v,w ) ∈DH, then (u,v∗,w∗) is uniformly distributed over DH ∩Tu;
(b) if ( u,v,w ) /∈DH, then (u,v∗,w∗) is uniformly distributed over Tu.
10.9 (Random self-reduction for DDH (II)). Continuing with the previous exercise, consider
the randomized mapping from G3 to G3 that sends (u,v,w ) to (˜u,v, ˜w), where
ρ←R Zq, ˜u←gρu, ˜w←vρw.
Prove the following:
(a) ˜u is uniformly distributed over G;
(b) ( u,v,w ) ∈DH ⇐⇒(˜u,v, ˜w) ∈DH;
(c) if we apply the randomized mapping from the previous exercise to (˜ u,v, ˜w), obtaining the
triple (˜u,v∗, ˜w∗), then we have
425
• if (u,v,w ) ∈DH, then (˜u,v∗, ˜w∗) is uniformly distributed over DH;
• if (u,v,w ) /∈DH, then (˜u,v∗, ˜w∗) is uniformly distributed over G3.
10.10 (Random self-reduction for DDH (III)). Continuing with the previous exercise, prove
the following. Suppose Ais an eﬃcient algorithm that takes as input three group elements and
outputs a bit, and which satisﬁes the following property: if α,β,γ ∈Zq are chosen at random, then
⏐⏐⏐Pr[A(gα,gβ,gαβ) = 1] −Pr[A(gα,gβ,gγ) = 1]
⏐⏐⏐= ϵ,
where the probability is over the random choice of α,β,γ , as well as any random choices made by
A. Assuming that 1/ϵis poly-bounded, show how to use Ato build an eﬃcient algorithm Bthat for
all inputs (u,v,w ) correctly decides whether or not (u,v,w ) ∈DH with negligible error probability.
That is, adversary Bmay output an incorrect answer, but for all inputs, the probability that its
answer is incorrect should be negligible.
Hint: Use a Chernoﬀ bound.
10.11 (Multi-DDH (I)). Let G be a cyclic group of prime order q generated by g ∈G. Let n
and m be positive integers. Deﬁne the following two distributions over Gn+2nm:
D: gαi (i= 1,...,n ), g βij, gαiβij (i= 1,...,n, j = 1,...,m ),
and
R: gαi (i= 1,...,n ), g βij, gγij (i= 1,...,n, j = 1,...,m ).
where the αi’s, βij’s, and γij’s are uniformly and independently distributed over Zq. Show that
under the DDH assumption, Dand Rare computationally indistinguishable (as in Deﬁnition 3.4).
In particular, show that for every adversary Athat distinguishes Dand R, there exists a DDH
adversary B(which is an elementary wrapper around A) such that
Distadv[A,D,R] ≤1/q+ DDHadv[B,G].
Hint: Apply Exercises 10.7, 10.8, and 10.9.
10.12 (Multi-DDH (II)). Let G be a cyclic group of prime order q generated by g ∈G. Let
n≤m be positive integers. Deﬁne the following two distributions over Gn·m+n+m:
D: gαi (i= 1,...,n ), g βj (j = 1,...,m )
gαiβj (i= 1,...,n, j = 1,...,m ),
and
R: gαi (i= 1,...,n ), g βj (j = 1,...,m )
gγij (i= 1,...,n, j = 1,...,m ).
where the αi’s, βj’s, and γij’s are uniformly and independently distributed over Zq. Show that
under the DDH assumption, Dand Rare computationally indistinguishable (as in Deﬁnition 3.4).
426
In particular, show that for every adversary Athat distinguishes Dand R, there exists a DDH
adversary B(which is an elementary wrapper around A) such that
Distadv[A,D,R] ≤n·(1/q+ DDHadv[B,G]).
Hint: First give a proof for the case n = 1 using the results of Exercise 10.7 and Exercise 10.8,
and then generalize to arbitrary n using a hybrid argument.
Discussion: This result gives us a DDH-based PRG G deﬁned over (Zn+m
q , Gn·m+n+m), with a
nice expansion rate, given by
G
(
{αi}n
i=1, {βj}m
j=1
)
:=
(
{gαi}n
i=1, {gβj}m
j=1, {gαiβj}i=1,...,n
j=1,...,m
)
.
The reader should also compare this exercise to the previous one: security in this construction
degrades linearly in n, while the security in the construction in the previous exercise does not
degrade at all as n increases.
10.13 (Matrix DDH). Let G be a cyclic group of prime order q generated by g∈G. Let n and
mbe positive integers, and assume n≤m. For A= (αij) ∈Zn×m
q (i.e., Ais an n×mmatrix with
entries in Zq), let gA be the n×mmatrix whose entry at row icolumn j is the group element gαij.
For k= 1,...,n , deﬁne the random variable R(k) to be a random matrix uniformly distributed over
all n×mmatrices over Zq of of rank at most k. Let 1 ≤k1 <k2 ≤n. Show that gR(k1) and gR(k2)
are computationally indistinguishable under the DDH. In particular, show that for every adversary
Athat distinguishes gR(k1) and gR(k2) there exists a DDH adversary B(which is an elementary
wrapper around A) such that
Distadv[A,gR(k1),gR(k2)] ≤(k2 −k1) ·(1/q+ DDHadv[B,G]).
Hint: Use the fact that if A∈Zn×m
q is a ﬁxed matrix of rank k, and if U ∈Zn×n
q and V ∈Zm×m
q
are a random invertible matrices, then the matrix UAV ∈Zn×m
q is uniformly distributed over all
n×m matrices of rank k. You might also try to prove this fact, which is not too hard.
Discussion: For k1 = 1 and k2 = n, this result is almost the same as Exercise 10.12. In this
sense, this exercise is a generalization of Exercise 10.12.
10.14 (A trapdoor function from DDH). Let q := |G|and n ≥2 log2 q. In this exercise
we construct a trapdoor function scheme ( G,F,I ) deﬁned over ( X,Y), where X := {0,1}n and
Y:= Gn. Security will follow from DDH in G. The scheme works as follows:
• Algorithm G chooses a random matrix A ←R Zn×n
q and outputs pk ← gA ∈ Gn×n and
sk ←A−1 ∈Zn×n
q (the notation gA is deﬁned in the previous exercise). Note that A is
invertible with overwhelming probability, so that sk is well deﬁned.
• For pk = gA and x∈X deﬁne F(pk,x) := gA·x ∈Y.
(a) Show how to use sk to invert the function: given sk and y:= gA·x ∈Y as input, show how to
compute x∈X.
427
(b) Show that if DDH holds in G then the function is one way in the sense of Deﬁnition 10.3.
Hint: Use Exercise 10.13, and the DDH assumption, to argue that the advantage of an
inversion adversary will be only negligibly aﬀected if the public key matrix A ∈Zn×n
q is
replaced by a random matrix A′ ∈Zn×n
q of rank 1. Once the change is made, using the
assumption that n≥2 log2 q, argue that it is likely that there are several 0/1 vectors x′∈X
such that F(pk,x) = F(pk,x′), and so the adversay is likely to pick the wrong inverse.
Discussion: This trapdoor function is quite diﬀerent from the RSA trapdoor permutation. In par-
ticular, it is not known how to randomly sample from its image without ﬁrst sampling a preimage.
This prevents it from being used in certain key applications, such as full domain hash signatures
discussed in Section 13.3.
10.15 (A trapdoor test). Consider a speciﬁc cyclic group G of prime order qgenerated by g∈G.
Let u∈G and f : G →G3. Now set
σ←R Zq, τ ←R Zq, ¯u←gσuτ, (v,w, ¯w) ←f(¯u).
Let S be the event that ( u,v,w ) and (¯u,v, ¯w) are both DH-triples. Let T be the event that
¯w= vσwτ. Show that:
(a) ¯u is uniformly distributed over G;
(b) Pr[ S∧¬T] = 0;
(c) Pr[ ¬S∧T] ≤1/q.
Remark: This result gives us a kind of trapdoor test. Suppose a group element u∈G is given (it
could be chosen at random or adversarially chosen). Then we can generate a random element ¯uand
a “trapdoor” ( σ,τ). Using this trapdoor, given group elements v,w, ¯w∈G (possibly adversarially
chosen in a way that depends on ¯ u), we can reliably test if ( u,v,w ) and (¯u,v, ¯w) are both DH-
triples, even though we do not know either Dlogg(u) or Dlogg(¯u), and even though we cannot tell
whether (u,v,w ) and (¯u,v, ¯w) are individually DH-triples. This rather technical result has several
nice applications, one of which is developed in the following exercise.
10.16 (A CDH self-corrector). Consider a speciﬁc cyclic group G of prime order q generated
by g ∈G. Let Abe an eﬃcient algorithm with the following property: if α,β ∈Zq are chosen at
random, then Pr[ A(gα,gβ) = gαβ] = ϵ. Here, the probability is over the random choice of α and
β, as well as any random choices made by A. Assuming 1 /ϵis poly-bounded and |G|is super-poly,
show how to use Ato build an eﬃcient algorithm Bthat solves the CDH problem on all inputs with
negligible error probability; that is, on every input ( gα,gβ), algorithm Boutputs a single group
element w, and w ̸= gαβ with negligible probability (and this probability is just over the random
choices made by B).
Here is a high-level sketch of how Bmight work on input ( u,v).
somehow choose ¯u∈G
somehow use Ato generate lists L,¯L of group elements
for each w in L and each ¯w in ¯L do
if (u,v,w ) and (¯u,v, ¯w) are both DH-triples then
output w and halt
output an arbitrary group element
428
As stated, this algorithm is not fully speciﬁed. Nevertheless, you can use this rough outline, com-
bined with the CDH random self reduction in Exercise 10.5 and the trapdoor test in Exercise 10.15,
to prove the desired result.
For the next problem, we need the following notions from complexity theory:
• We say problem Ais deterministic poly-time reducible to problem B if there exists
a deterministic algorithm Rfor solving problem Aon all inputs that makes calls to
a subroutine that solves problem B on all inputs, where the running time of R(not
including the running time for the subroutine for B) is polynomial in the input
length.
• We say that A and B are deterministic poly-time equivalent if A is deterministic
poly-time reducible to B and B is deterministic poly-time reducible to A.
10.17 (Problems equivalent to CDH). Consider a speciﬁc cyclic group G of prime order q
generated by g∈G. Show that the following problems are deterministic poly-time equivalent:
(a) Given gα and gβ, compute gαβ (this is just the Computational Diﬃe-Hellman problem).
(b) Given gα, compute g(α2).
(c) Given gα with α̸= 0, compute g1/α.
(d) Given gα and gβ with β ̸= 0, compute gα/β.
Note that all problem instances are deﬁned with respect to the same group G and generator g∈G.
10.18 (System parameters). In formulating the discrete-log Attack Game 10.4, we assume that
the description of G, including g ∈G and q, is a system parameter that is generated once and for
all at system setup time and shared by all parties involved. This parameter may be generated via
some randomized process, in which case the advantage ϵ = DLadv[A,G] is a probability over the
choice of system parameter, as well as the random choice of α ∈Zq made by the challenger and
any random choices made by adversary. So we can think of the system parameter as a random
variable Λ, and for any speciﬁc system parameter Λ0, we can consider the corresponding conditional
advantage ϵ(Λ0) given that Λ = Λ 0, which is a probability just over the random choice of α ∈Zq
made by the challenger and any random choices made by adversary. Let us call Λ 0 a “vulnerable”
parameter if ϵ(Λ0) ≥ϵ/2.
(a) Prove that the probability that Λ is vulnerable is at least ϵ/2.
Note that even if an adversary breaks the DL with respect to a randomly generated system
parameter, there could be many particular system parameters for which the adversary cannot
or will not break the DL (it is helpful to imagine an adversary that is all powerful yet
capricious, who simply refuses to break the DL for certain groups and generators which he
ﬁnds distasteful). This result says, however, that there is still a non-negligible fraction of
vulnerable system parameters for which the adversary breaks the DL.
(b) State and prove an analogous result for the CDH problem.
(c) State and prove an analogous result for the DDH problem.
429
10.19 (Choice of generators). In formulating the DL, CDH, and DDH assumptions, we work
with a cyclic group G of prime order q generated by g ∈G. We do not specify how the generator
g is chosen. Indeed, it may be desirable to choose a speciﬁc g that allows for more eﬃcient
implementations. Conceivably, such a g could be a “weak” generator that makes it easier for an
adversary to break the DL, CDH, or DDH assumptions. So to be on the safe side, we might insist
that the generator gis uniformly distributed over G\{1}. If we do this, we obtain new assumptions,
which we call the rDL, rCDH, and rDDH assumptions. Show that:
(a) the rDL and DL assumptions are equivalent;
(b) the rCDH and CDH assumptions are equivalent;
(c) the DDH assumption implies the rDDH assumption.
Hint: To start with, you might ﬁrst consider the setting where we are working with a speciﬁc
group, then generalize your result to incorporate all the aspects of the asymptotic attack game (see
Section 10.5.2), including the security parameter and the system parameter (where the group is
selected at system setup time).
Remark: The rDDH assumption is not known to imply the DDH assumption, so for applications
that use the DDH assumption, it seems safest to work with a random generator.
10.20 (Relation ﬁnding is as hard as discrete log). Let G be a cyclic group of prime order q,
and let nbe a poly-bounded parameter. A relation ﬁnding algorithm Atakes as input (g1,...,g n) ∈
Gn, and outputs α:= (α1,...,α n) ∈Zn
q such that gα1
1 ···gαn
n = 1 where α̸= (0,..., 0). Such an
α∈Zn
q is called a relation for g1,...,g n ∈G. Let us show that ﬁnding a relation among a random
set of group elements is as hard as computing discrete log in G. For a relation ﬁnding algorithm
A, random generators ( g1,...,g n) ←R Gn, and (α1,...,α n) ←R A(g1,...,g n), deﬁne
RELadv[A,G] := Pr
[
gα1
1 ···gαn
n = 1
]
.
Show that for every relation ﬁnding algorithm A, there exists a DL adversary B, which is an
elementary wrapper around A, such that RELadv[A,G] ≤DLadv[B,G] + 1/q.
10.21 (Collision resistance from discrete-log). Let G be a cyclic group of prime order q
generated by g∈G. Let n be a poly-bounded parameter. Let us deﬁne a hash function H deﬁned
over (Zn
q,G). The hash function is parameterized by the group G and n randomly chosen group
elements g1,...,g n ∈G. For (α1,...,α n) ∈Zn
q, deﬁne
H(α1,...,α n) := gα1
1 ···gαn
n .
Our goal is to show that H is collision resistant under the DL assumption for G. A collision ﬁnding
adversary Afor H takes as input (g1,...,g n) ∈Gn and outputs a collision forH. Use Exercise 10.20
to show that for every collision-ﬁnding adversary Afor H, there exists a DL adversary B, which
is an elementary wrapper around A, such that CR adv[A,H] ≤DLadv[B,G] + 1/q. Note that this
result is a generalization of the collision resistant hash function from Section 10.6.1.
10.22 (Collision resistance in Z∗
p). This exercise asks you to prove that the hash function
presented in Section 8.5.1 is collision resistant under an appropriate DL assumption. Let us deﬁne
things a bit more precisely. Let p be a large prime such that q := (p−1)/2 is also prime. The
430
prime qis called a Sophie Germain prime, and pis sometimes called a “strong” prime. Such primes
are often very convenient to use in cryptography. Suppose x is a randomly chosen integer in the
range [2,q] and y is a randomly chosen integer in the range [1 ,q]. These parameters deﬁne a hash
function H that takes as input two integers in [1 ,q] and outputs an integer in [1 ,q], as speciﬁed in
(8.3). Let G be the subgroup of order q in Z∗
p, and consider the DL assumption for G with respect
to a randomly chosen generator. Show that H is collision resistant under this DL assumption.
Hint: Use the fact that the map that sends α ∈Z∗
p to α2 ∈Z∗
p is a group homomorphism with
image G and kernel ±1; also use the fact that there is an eﬃcient algorithm for taking square roots
in Z∗
p.
10.23 (A broken CRHF). Consider the following variation of the hash construction in the
previous exercise. Let p be a large prime such that q := (p−1)/2 is also prime. Let x and y be
randomly chosen integers in the range [2,p −2] (so neither can be ±1 (mod p)). These parameters
deﬁne a hash function H that takes as input two integers in [1 ,p −1] and outputs an integer in
[1,p −1], as follows:
H(a,b) := xayb mod p.
Give an eﬃcient, deterministic algorithm that takes as input p,x,y as above, and computes a
collision on the corresponding H. Your algorithm should work for all inputs p,x,y .
10.24 (DDH is easy in groups of even order). We have restricted the DL, CDH, and DDH
assumptions to prime order groups G. Consider the DDH assumption for a cyclic group G of even
order q with generator g∈G. Except for dropping the restriction that q is prime, the attack game
is identical to Attack Game 10.6. Give an eﬃcient adversary that has advantage 1 /2 in solving the
DDH for G.
Remark: For a prime p> 2, the group Z∗
p is a cyclic group of even order p−1. This exercise shows
that the DDH assumption is false in this group. Exercise 10.23 gives another reason to restrict
ourselves to groups of prime order. See also Exercise 16.2 for a generalization of this result.
10.25 (RSA variant (I)). Let n be an RSA modulus generated by RSAGen( ℓ,e). Let X and
X∗be random variables, where X is uniformly distributed over Zn and X∗is uniformly distributed
over Z∗
n. Show that the statistical distance ∆[ X,X∗] is less than 2 −(ℓ−2).
10.26 (RSA variant (II)). In Theorem 10.5, we considered a variant of the RSA assumption
where the challenger chooses the preimage x at random from Z∗
n, rather than Zn. That theorem
showed that the standard RSA assumption implies this variant RSA assumption. In this exercise,
you are to show the converse. In particular, show that RSAadv[A,ℓ,e ] ≤uRSAadv[B,ℓ,e ] + 2−(ℓ−2)
for every adversary A.
Hint: Use the result of the previous exercise.
10.27 (A proper trapdoor permutation scheme based on RSA). As discussed in Sec-
tion 10.3, our RSA-based trapdoor permutation scheme does not quite satisfy our deﬁnitions,
simply because the domain on which it acts varies with the public key. This exercise shows one way
to patch things up. Let ℓ and e be parameters used for RSA key generation, and let G be the key
generation algorithm, which outputs a pair ( pk,sk). Recall that pk = (n,e), where n is an RSA
modulus, which is the product of two ℓ-bit primes, and e is the encryption exponent. The secret
key is sk = (n,d), where dis the decryption exponent corresponding to the encryption exponent e.
431
Choose a parameter L that is a substantially larger than 2 ℓ, so that n/2L is negligible. Let X be
the set of integers in the range [0,2L). We shall present a trapdoor permutation scheme (G,F∗,I∗),
deﬁned over X. The function F∗takes two inputs: a public key pk as above and an integer x∈X,
and outputs an integer y ∈X, computed as follows. Divide x by n to obtain the integer quotient
Q and remainder R, so that x = nQ+ R and 0 ≤R < n. If Q >2L/n−1, then set S := R;
otherwise, set S := Re mod n. Finally, set y:= nQ+ S.
(a) Show that F∗(pk,·) is a permutation on X, and give an eﬃcient inversion function I∗ that
satisﬁes I∗(sk,F∗(pk,x)) = x for all x∈X.
(b) Show under the RSA assumption, ( G,F∗,I∗) is one-way.
10.28 (Random self-reduction for RSA). Suppose we run ( n,d) ←R RSAGen(ℓ,e). There
could be “weak” RSA moduli n for which an adversary can break the the RSA assumption with
some probability ϵ. More precisely, suppose that there is an eﬃcient algorithm Asuch that for
any such “weak” modulus n, if x ∈Z∗
n is chosen at random, then Pr[ A(xe) = x] ≥ϵ, where the
probability is over the random choice of x, as well as any random choices made by A. Using A,
construct an eﬃcient algorithm Bsuch that for every “weak” modulus n, and every x ∈Zn, we
have Pr[A(xe) = x] ≥ϵ, where the probability is now only over the random choices made by B.
Hint: Use the randomized mapping from Z∗
n to Z∗
n that sends y to ˜y, where r ←R Z∗
n, ˜y ←rey.
Show that for every y∈Z∗
n, the value ˜y is uniformly distributed over Z∗
n.
10.29 ( n-product CDH). Let G be a cyclic group of prime order q generated by g ∈G. The
following attack game deﬁnes then-product CDH problem(here, nis a poly-bounded parameter,
not necessarily constant). The challenger begins by choosing αi ←R Zq for i = 1 ,...,n . The
adversary then makes a sequence of queries. In each query, the adversary submits a proper subset
of indices S ⊊ {1,...,n }, and the challenger responds with
g
∏
i∈Sαi.
The adversary wins the game if it outputs
gα1···αn.
We relate the hardness of solving the n-product CDH problem to another problem, called the n-
power CDH problem . In the attack game for this problem, the challenger begins by choosing
α←R Z∗
q, and gives
g,gα,...,g αn−1
to the adversary. The adversary wins the game if it outputs g(αn).
Show that if there is an eﬃcient adversary Athat breaks n-product CDH with non-negligible
probability, then there is an eﬃcient adversary Bthat breaks n-power CDH with non-negligible
probability.
10.30 (Trapdoor collison resistance). Let us show that the collision resistant hash functions
Hdl and Hrsa, presented in Section 10.6, are trapdoor collision resistant.
432
(a) Recall that Hdl is deﬁned as Hdl(α,β) := gαhβ ∈G, where g and hare parameters chosen at
setup. Show that anyone who knows the discrete-log of h base g (the trapdoor), can break
the 2nd-preimage resistance of Hdl. That is, given ( α,β) as input, along with the trapdoor,
one can eﬃciently compute ( α′,β′) ̸= (α,β) such that Hdl(α′,β′) = Hdl(α,β).
(b) Recall that Hrsa is deﬁned as Hrsa(a,b) := aeyb ∈Zn, where n,e and y are parameters chosen
at setup. Show that anyone who knows the eth root of y in Zn (the trapdoor), can break the
2nd-preimage resistance of Hrsa.
(c) Continuing with part (b), show that anyone who knows the factorization of n(the trapdoor),
can invert Hrsa. That is, given z∈Zn as input, one can ﬁnd ( a,b) such that Hrsa(a,b) = z.
Discussion: Part (c) shows that the factorization of nis a “stronger” trapdoor for Hrsa than the
eth root of y. The latter only breaks 2nd-preimage resistance of Hrsa, whereas the former enables
complete inversion. Both trapdoors break collision resistance.
10.31 (An attack on the GUO accumulator). Consider again the accumulator described in
Section 10.9 which uses a ﬁnite group of unknown order ( G,g) and a hash function H. Let d be
the unknown order of g in G, and suppose that Mel somehow discovers the value of d∈Z.
(a) Show that Mel can use d to create a false accumulator membership proof. Speciﬁcally, show
that Mel can produce a tuple ( S,c,x,b ), where S ⊆X is some set, c= g
∏
y∈SH(y) ∈G is the
accumulator commitment to S, x∈X is not a member of S, but b∈G is a valid membership
proof for x with respect to c, so that, bH(x) = c. This b is a false membership proof for x
in S. Show that Mel can produce the required tuple ( S,c,x,b ).
(b) Show that Mel can use d to create a false accumulator non-membership proof. Speciﬁcally,
show that Mel can produce a tuple ( S,c,x,b,γ ), where S and c are as in part (a), x∈X is
a member of S, but ( b,γ) ∈G ×Z is a valid non-membership proof for x with respect to c.
That is, bH(x) ·cγ = g. This ( b,γ) is a false non-membership proof for xin S. Show that Mel
can produce the required tuple ( S,c,x,b,γ ).
10.32 (Fast generation of accumulator proofs). Let e1 <...<e n be a set of distinct prime
numbers, and let E = e1 ···en ∈Z. Let ( G,g) be a group of unknown order (GUO), as discussed
in Section 10.9. Show how to generate the set of n group elements
{
gE/ei
}n
i=1 using only nlog2 n
exponentiations in G where every exponent is no bigger than en. Your solution gives the algorithm
mentioned in Remark 10.1.
433
Chapter 11
Public key encryption
In this chapter, we consider again the basic problem of encryption. As a motivating example,
suppose Alice wants to send Bob an encrypted email message, even though the two of them do not
share a secret key (nor do they share a secret key with some common third party). Surprisingly,
this can be done using a technology called public-key encryption.
The basic idea of public-key encryption is that the receiver, Bob in this case, runs a key gener-
ation algorithm G, obtaining a pair of keys:
(pk,sk) ←R G().
The key pk is Bob’s public key, and sk is Bob’s secret key. As their names imply, Bob should keep
sk secret, but may publicize pk.
To send Bob an encrypted email message, Alice needs two things: Bob’s email address, and
Bob’s public key pk. How Alice reliably obtains this information is a topic we shall explore later in
Section 13.8. For the moment, one might imagine that this information is placed by Bob in some
kind of public directory to which Alice has read-access.
So let us assume now that Alice has Bob’s email address and public key pk. To send Bob an
encryption of her email message m, she computes the ciphertext
c←R E(pk,m).
She then sends cto Bob, using his email address. At some point later, Bob receives the ciphertext
c, and decrypts it, using his secret key:
m←D(sk,c).
Public-key encryption is sometimes called asymmetric encryption to denote the fact that
the encryptor uses one key, pk, and the decryptor uses a diﬀerent key, sk. This is in contrast with
symmetric encryption, discussed in Part 1, where both the encryptor and decryptor use the same
key.
A few points deserve further discussion:
• Once Alice obtains Bob’s public key, the only interaction between Alice and Bob is the actual
transmission of the ciphertext from Alice to Bob: no further interaction is required. In fact,
we chose encrypted email as our example problem precisely to highlight this feature, as email
delivery protocols do not allow any interaction beyond delivery of the message.
434
• As we will discuss later, the same public key may be used many times. Thus, once Alice ob-
tains Bob’s public key, she may send him encrypted messages as often as she likes. Moreover,
other users besides Alice may send Bob encrypted messages using the same public key pk.
• As already mentioned, Bob may publicize his public key pk. Obviously, for any secure public-
key encryption scheme, it must be hard to compute sk from pk, since anyone can decrypt
using sk.
11.1 Two further example applications
Public-key encryption is used in many real-world settings. We give two more examples.
11.1.1 Sharing encrypted ﬁles
In a modern encrypted ﬁle system, a user Alice can grant read access to other users. In particular,
for every ﬁle that Alice owns, she can select a group of users who are allowed to decrypt the stored
ﬁle and read its contents in the clear. This is done using a combination of public-key encryption
and an ordinary symmetric cipher.
Here is how it works. Alice encrypts a ﬁle f with a random key k using an ordinary symmetric
cipher. The resulting ciphertext cf is stored on the ﬁle system. If Alice wants to grant Bob access
to the contents of f, she encrypts kunder Bob’s public key pkB by computing cB ←R E(pkB,k). The
short ciphertext cB is then stored near the ciphertext cf, say, as part of the metadata associated
with the ﬁle f. Now when Bob wants to read f, he decrypts cB using his secret key skB to obtain k,
and then decrypts cf using kto obtain f in the clear. Alice grants access to herself just as she does
for Bob, by encrypting k under her own public key pkA, and storing the resulting ciphertext cA as
part of the ﬁle metadata.
This scheme scales nicely when Alice wants to grant access to more users. Only one copy of the
encrypted ﬁle f is stored on the ﬁle system. In addition, for each user that is granted access to f,
Alice writes an appropriate encryption of the key k in the ﬁle’s metadata section. Each of these
encryptions of k is fairly short, even if the ﬁle itself is large.
11.1.2 Key escrow
Consider a company that deploys an encrypted ﬁle system such as the one described above. One
day Alice is traveling, but her manager needs to read one of her ﬁles to prepare for a meeting
with an important client. Unfortunately, the manager is unable to decrypt the ﬁle because it is
encrypted and Alice is unreachable.
Large companies solve this problem using a mechanism called key escrow. The company runs
a key escrow server that works as follows: at setup time the key escrow server generates a secret key
skES and a corresponding public key pkES. It keeps the secret key to itself and makes the public
key available to all employees.
When Alice encrypts a ﬁle f using a random symmetric key k, she also encrypts k under
pkES, and stores the resulting ciphertext cES in the ﬁle metadata section. Every ﬁle created by an
employee is encrypted this way. Now, if Alice’s manager needs access tof, and Alice is unreachable,
the manager sends cES to the escrow service. The server decrypts cES to obtain k, and sends k
back to the manager. The manager can then use this k to decrypt cf and obtain f.
435
Public-key encryption makes it possible for the escrow server to remain oﬄine, until someone
needs to decrypt an inaccessible ﬁle. Although the escrow service enables Alice’s manager to access
her ﬁles, the escrow service itself cannot read Alice’s ﬁles, since it does not get to see any encrypted
ﬁle.
We will discuss this application in more detail in Section 12.2.3.
11.2 Basic deﬁnitions
We begin by deﬁning the basic syntax and correctness properties of a public-key encryption scheme.
Deﬁnition 11.1. A public-key encryption scheme E= (G,E,D ) is a triple of eﬃcient algo-
rithms: a key generation algorithm G, an encryption algorithm E, a decryption algorithm
D.
• G is a probabilistic algorithm that is invoked as (pk,sk) ←R G(), where pk is called a public
key and sk is called a secret key.
• E is a probabilistic algorithm that is invoked as c ←R E(pk,m), where pk is a public key (as
output by G), m is a message, and c is a ciphertext.
• D is a deterministic algorithm that is invoked as m←D(sk,c), where sk is a secret key (as
output by G), c is a ciphertext, and m is either a message, or a special reject value (distinct
from all messages).
• As usual, we require that decryption undoes encryption; speciﬁcally, for all possible outputs
(pk,sk) of G, and all messages m, we have
Pr[D(sk, E(pk, m) ) = m] = 1.
• Messages are assumed to lie in some ﬁnite message space M, and ciphertexts in some ﬁnite
ciphertext space C. We say that E= (G,E,D ) is deﬁned over (M,C).
We next deﬁne the notion of semantic security for a public-key encryption scheme. We stress
that this notion of security only models an eavesdropping adversary. We will discuss stronger
security properties in the next chapter.
Attack Game 11.1 (semantic security). For a given public-key encryption scheme E =
(G,E,D ), deﬁned over (M,C), and for a given adversary A, we deﬁne two experiments.
Experiment b (b= 0,1):
• The challenger computes (pk,sk) ←R G(), and sends pk to the adversary.
• The adversary computes m0,m1 ∈M, of the same length, and sends them to the challenger.
• The challenger computes c←R E(pk,mb), and sends c to the adversary.
• The adversary outputs a bit ˆb∈{0,1}.
436
Challenger A
m0, m1 2 M
ˆb 2 {0, 1}
(Experiment b)
c
pk(pk , sk ) R
 G()
c R
 E(pk ,m b )
Figure 11.1: Experiment b of Attack Game 11.1
If Wb is the event that Aoutputs 1 in Experiment b, we deﬁne A’s advantage with respect to
Eas
SSadv[A,E] :=
⏐⏐⏐Pr[W0] −Pr[W1]
⏐⏐⏐. 2
Note that in the above game, the events W0 and W1 are deﬁned with respect to the probability
space determined by the random choices made by the key generation and encryption algorithms,
and the random choices made by the adversary. See Fig. 11.1 for a schematic diagram of Attack
Game 11.1.
Deﬁnition 11.2 (semantic security). A public-key encryption scheme Eis semantically se-
cure if for all eﬃcient adversaries A, the value SSadv[A,E] is negligible.
As discussed in Section 2.2.5, Attack Game 11.1 can be recast as a “bit guessing” game, where
instead of having two separate experiments, the challenger chooses b∈{0,1}at random, and then
runs Experiment b against the adversary A. In this game, we measure A’s bit-guessing advantage
SSadv∗[A,E] as |Pr[ˆb= b] −1/2|. The general result of Section 2.2.5 (namely, (2.11)) applies here
as well:
SSadv[A,E] = 2 ·SSadv∗[A,E]. (11.1)
11.2.1 Mathematical details
We give a more mathematically precise deﬁnition of a public-key encryption scheme, using the
terminology deﬁned in Section 2.3.
Deﬁnition 11.3 (public-key encryption scheme).A public-key encryption scheme consists
of three algorithms, G, E, and D, along with two families of spaces with system parameterization
P:
M = {Mλ,Λ}λ,Λ and C = {Cλ,Λ}λ,Λ,
such that
1. M and C are eﬃciently recognizable.
437
2. M has an eﬀective length function.
3. Algorithm G is an eﬃcient probabilistic algorithm that on input λ,Λ, where λ ∈Z≥1, Λ ∈
Supp(P(λ)), outputs a pair (pk,sk), where pk and sk are bit strings whose lengths are always
bounded by a polynomial in λ.
4. Algorithm E is an eﬃcient probabilistic algorithm that on input λ,Λ,pk,m, where λ∈Z≥1,
Λ ∈Supp(P(λ)), (pk,sk) ∈Supp(G(λ,Λ)) for some sk, and m ∈Mλ,Λ, always outputs an
element of Cλ,Λ.
5. Algorithm D is an eﬃcient deterministic algorithm that on input λ,Λ,sk,c, where λ∈Z≥1,
Λ ∈Supp(P(λ)), (pk,sk) ∈Supp(G(λ,Λ)) for some pk, and c ∈Cλ,Λ, outputs either an
element of Mλ,Λ, or a special symbol reject /∈Mλ,Λ.
6. For all λ,Λ,pk,sk,m,c , where λ ∈Z≥1, Λ ∈Supp(P(λ)), (pk,sk) ∈Supp(G(λ,Λ)), k ∈
Kλ,Λ, m∈Mλ,Λ, and c∈Supp(E(λ,Λ; pk,m)), we have D(λ,Λ; sk,c) = m.
As usual, the proper interpretation of Attack Game 11.1 is that both challenger and adversary
receive λ as a common input, and that the challenger generates Λ and sends this to the adversary
before the game begins. The advantage is actually a function of λ, and security means that this is
a negligible function of λ.
11.3 Implications of semantic security
Before constructing semantically secure public-key encryption schemes, we ﬁrst explore a few con-
sequences of semantic security. We ﬁrst show that any semantically secure public-key scheme must
use a randomized encryption algorithm. We also show that in the public-key setting, semantic
security implies CPA security. This was not true for symmetric encryption schemes: the one-time
pad is semantically secure, but not CPA secure.
11.3.1 The need for randomized encryption
Let E= (G,E,D ) be a semantically secure public-key encryption scheme deﬁned over (M,C) where
|M|≥ 2. We show that the encryption algorithm E must be randomized, otherwise the scheme
cannot be semantically secure.
To see why, supposeEis deterministic. Then the following adversary Abreaks semantic security
of E= (G,E,D ):
• Areceives a public key pk from its challenger.
• Achooses two distinct messages m0 and m1 in Mand sends them to its challenger. The
challenger responds with c:= E(pk,mb) for some b∈{0,1}.
• Acomputes c0 := E(pk,m0) and outputs 0 if c= c0. Otherwise, it outputs 1.
Because E is deterministic, we know that c = c0 whenever b = 0. Therefore, when b = 0 the
adversary always outputs 0. Similarly, when b= 1 it always outputs 1. Therefore
SSadv[A,E] = 1
438
showing that Eis insecure.
This generic attack explains why semantically secure public-key encryption schemes must be
randomized. All the schemes we construct in this chapter and the next use randomized encryption.
This is quite diﬀerent from the symmetric key settings where a deterministic encryption scheme
can be semantically secure; for example, the one-time pad.
11.3.2 Semantic security against chosen plaintext attack
Recall that when discussing symmetric ciphers, we introduced two distinct notions of security:
semantic security, and semantic security against chosen plaintext attack (or CPA security, for
short). We showed that for symmetric ciphers, semantic security does not imply CPA security.
However, for public-key encryption schemes, semantic securitydoes imply CPA security. Intuitively,
this is because in the public-key setting, the adversary can encrypt any message he likes, without
knowledge of any secret key material. The adversary does so using the given public key and never
needs to issue encryption queries to the challenger. In contrast, in the symmetric key setting, the
adversary cannot encrypt messages on his own.
The attack game deﬁning CPA security in the public-key setting is the natural analog of the
corresponding game in the symmetric setting (see Attack Game 5.2 in Section 5.3):
Attack Game 11.2 (CPA security). For a given public-key encryption scheme E= (G,E,D ),
deﬁned over (M,C), and for a given adversary A, we deﬁne two experiments.
Experiment b (b= 0,1):
• The challenger computes (pk,sk) ←R G(), and sends pk to the adversary.
• The adversary submits a sequence of queries to the challenger.
For i= 1,2,..., the ith query is a pair of messages, mi0,mi1 ∈M, of the same length.
The challenger computes ci ←R E(pk,mib), and sends ci to the adversary.
• The adversary outputs a bit ˆb∈{0,1}.
If Wb is the event that Aoutputs 1 in Experiment b, then we deﬁne A’s advantage with respect
to Eas
CPAadv[A,E] :=
⏐⏐⏐Pr[W0] −Pr[W1]
⏐⏐⏐. 2
Deﬁnition 11.4 (CPA security). A public-key encryption scheme E is called semantically
secure against chosen plaintext attack , or simply CPA secure, if for all eﬃcient adversaries
A, the value CPAadv[A,E] is negligible.
Theorem 11.1. If a public-key encryption scheme Eis semantically secure, then it is also CPA
secure.
In particular, for every CPA adversary Athat plays Attack Game 11.2 with respect to E, and
which makes at most Q queries to its challenger, there exists an SS adversary B, where Bis an
elementary wrapper around A, such that
CPAadv[A,E] = Q·SSadv[B,E].
439
Proof. The proof is a straightforward hybrid argument, and is very similar to the proof of The-
orem 5.1. Suppose E= (G,E,D ) is deﬁned over ( M,C). Let Abe a CPA adversary that plays
Attack Game 11.2 with respect to E, and which makes at most Q queries to its challenger.
We describe the relevant hybrid games. For j = 0,...,Q , Hybrid j is played between Aand a
challenger who works as follows:
(pk,sk) ←R G()
Send pk to A
Upon receiving the ith query (mi0,mi1) ∈M2 from Ado:
if i>j
then ci ←R E(pk,mi0)
else ci ←R E(pk,mi1)
send ci to A.
Put another way, the challenger in Hybrid j encrypts
m11,...,m j1, m (j+1)0,...,m Q0,
As usual, we deﬁne pj to be the probability that Aoutputs 1 in Hybrid j. Clearly,
CPAadv[A,E] = |pQ −p0|.
Next, we deﬁne an appropriate adversary Bthat plays Attack Game 11.1 with respect to E:
First, Bchooses ω∈{1,...,Q }at random.
Then, Bplays the role of challenger to A: it obtains a public key pk from its own
challenger, and forwards this to A; when Amakes a query ( mi0,mi1), Bcomputes its
response ci as follows:
if i>ω then
c←R E(pk,mi0)
else if i= ω then
Bsubmits (mi0,mi1) to its own challenger
ci is set to the challenger’s response
else / / i<ω
ci ←R E(pk,mi1).
Finally, Boutputs whatever Aoutputs.
The crucial diﬀerence between the proof of this theorem and that of Theorem 5.1 is that for i̸= ω,
adversary Bcan encrypt the relevant message using the public key.
For b = 0,1, let Wb be the event that Boutputs 1 in Experiment b of its attack game. It is
clear that for j = 1,...,Q ,
Pr[W0 |ω= j] = pj−1 and Pr[ W1 |ω= j] = pj,
and the theorem follows by the usual telescoping sum calculation. 2
One can also consider multi-key CPA security, where the adversary sees many encryptions under
many public keys. In the public-key setting, semantic security implies not only CPA security, but
multi-key CPA security — see Exercise 11.10.
440
11.4 Encryption based on a trapdoor function scheme
In this section, we show how to use a trapdoor function scheme (see Section 10.2) to build a
semantically secure public-key encryption scheme. In fact, this scheme makes use of a hash function,
and our proof of security works only when we model the hash function as a random oracle (see
Section 8.10.2). We then present a concrete instantiation of this scheme, based on RSA (see
Section 10.3).
Our encryption scheme is called ETDF, and is built out of several components:
• a trapdoor function scheme T = (G,F,I ), deﬁned over (X,Y),
• a symmetric cipher Es = (Es,Ds), deﬁned over (K,M,C),
• a hash function H : X→K .
The message space for ETDF is M, and the ciphertext space is Y×C . We now describe the key
generation, encryption, and decryption algorithms for ETDF.
• The key generation algorithm for ETDF is the key generation algorithm for T.
• For a given public key pk, and a given message m ∈M, the encryption algorithm runs as
follows:
E(pk,m) := x←R X, y ←F(pk,x), k ←H(x), c ←R Es(k,m)
output (y,c).
• For a given secret key sk, and a given ciphertext ( y,c) ∈Y×C , the decryption algorithm
runs as follows:
D(sk, (y,c) ) := x←I(sk,y), k ←H(x), m ←Ds(k,c)
output m.
Thus, ETDF = (G,E,D ), and is deﬁned over ( M,Y×C ).
The correctness property for T immediately implies the correctness property for ETDF. If H
is modeled as a random oracle (see Section 8.10), one can prove that ETDF is semantically secure,
assuming that T is one-way, and that Es is semantically secure.
Recall that in the random oracle model, the function H is modeled as a random function O
chosen at random from the set of all functions Funs[ X,K]. More precisely, in the random oracle
version of Attack Game 11.1, the challenger chooses Oat random. In any computation where
the challenger would normally evaluate H, it evaluates Oinstead. In addition, the adversary is
allowed to ask the challenger for the value of the function Oat any point of its choosing. The
adversary may make any number of such “random oracle queries” at any time of its choosing. We
use SSroadv[A,ETDF] to denote A’s advantage against ETDF in the random oracle version of Attack
Game 11.1.
Theorem 11.2. Assume H : X →Kis modeled as a random oracle. If T is one-way and Es is
semantically secure, then ETDF is semantically secure.
In particular, for every SS adversary Athat attacks ETDF as in the random oracle version of
Attack Game 11.1, there exist an inverting adversary Bow that attacks T as in Attack Game 10.2,
441
and an SS adversary Bs that attacks Es as in Attack Game 2.1, where Bow and Bs are elementary
wrappers around A, such that
SSroadv[A,ETDF] ≤2 ·OWadv[Bow,T] + SSadv[Bs,Es]. (11.2)
Proof idea. Suppose the adversary sees the ciphertext ( y,c), where y = F(pk,x). If H is modeled
as a random oracle, then intuitively, the only way the adversary can learn anything at all about
the symmetric key k used to generate c is to explicitly evaluate the random oracle representing H
at the point x; however, if he could do this, we could easily convert the adversary into an adversary
that inverts the function F(pk,·), contradicting the one-wayness assumption. Therefore, from the
adversary’s point of view, k is completely random, and semantic security for ETDF follows directly
from the semantic security of Es. In the detailed proof, we implement the random oracle using
the same “faithful gnome” technique as was used to eﬃciently implement random functions (see
Section 4.4.2); that is, we represent the random oracle as a table of input/output pairs corresponding
to points at which the adversary actually queried the random oracle (as well as the point at which
the challenger queries the random oracle when it runs the encryption algorithm). We also use many
of the same proof techniques introduced in Chapter 4, speciﬁcally, the “forgetful gnome” technique
(introduced in the proof of Theorem 4.6) and the Diﬀerence Lemma (Theorem 4.7). 2
Proof. It is convenient to prove the theorem using the bit-guessing versions of the semantic security
game. We prove:
SSroadv∗[A,ETDF] ≤OWadv[Bow,T] + SSadv∗[Bs,Es]. (11.3)
Then (11.2) follows by (11.1) and (2.10).
Deﬁne Game 0 to be the game played between Aand the challenger in the bit-guessing version
of Attack Game 11.1 with respect to ETDF. We then modify the challenger to obtain Game 1. In
each game, b denotes the random bit chosen by the challenger, while ˆb denotes the bit output by
A. Also, for j = 0 ,1, we deﬁne Wj to be the event that ˆb = b in Game j. We will show that
|Pr[W1] −Pr[W0]|is negligible, and that Pr[W1] is negligibly close to 1/2. From this, it follows that
SSroadv∗[A,ETDF] =
⏐⏐Pr[W0] −1/2
⏐⏐ (11.4)
is also negligible.
Game 0. Note that the challenger in Game 0 also has to respond to the adversary’s random oracle
queries. The adversary can make any number of random oracle queries, but at most one encryption
query. Recall that in addition to direct access to the random oracle via explicit random oracle
queries, the adversary also has indirect access to the random oracle via the encryption query, where
the challenger also makes use of the random oracle. In describing this game, we directly implement
the random oracle as a “faithful gnome.” This is done using an associative array Map : X →K.
The details are in Fig. 11.2. In the initialization step, the challenger prepares some quantities
that will be used later in processing the encryption query. In particular, in addition to computing
(pk,sk) ←R G(), the challenger precomputes x←R X, y←F(pk,x), k←R K. It also sets Map[x] ←k,
which means that the value of the random oracle at x is equal to k.
Game 1. This game is precisely the same as Game 0, except that we make our gnome “forgetful”
by deleting line (3) in Fig. 11.2.
Let Z be the event that the adversary queries the random oracle at the point x in Game 1.
Clearly, Games 0 and 1 proceed identically unless Z occurs, and so by the Diﬀerence Lemma, we
442
initialization:
(1) ( pk,sk) ←R G(), x←R X, y←F(pk,x)
initialize an empty associative array Map : X→K
(2) k←R K, b←R {0,1}
(3) Map[x] ←k
send the public key pk to A;
upon receiving a single encryption query ( m0,m1) ∈M2:
(4) c←R Es(k,mb)
send (y,c) to A;
upon receiving a random oracle query ˆx∈X:
if ˆx /∈Domain(Map) then Map[ˆx] ←R K
send Map[ˆx] to A
Figure 11.2: Game 0 challenger
have ⏐⏐Pr[W1] −Pr[W0]
⏐⏐≤Pr[Z]. (11.5)
If event Z happens, then one of the adversary’s random oracle queries is the inverse of y under
F(pk,·). Moreover, in Game 1, the value x is used only to deﬁne y = F(pk,x), and nowhere else.
Thus, we can use adversary Ato build an eﬃcient adversary Bow that breaks the one-wayness
assumption for T with an advantage equal to Pr[ Z].
Here is how adversary Bow works in detail. This adversary plays Attack Game 10.2 against a
challenger Cow, and plays the role of challenger to Aas in Fig. 11.2, except with the following lines
modiﬁed as indicated:
(1) obtain (pk,y) from Cow
(3) (deleted)
Additionally,
when Aterminates:
if F(pk,ˆx) = y for some ˆx∈Domain(Map)
then output ˆx
else output “failure”.
To analyze Bow, we may naturally view Game 1 and the game played between Bow and Cow
as operating on the same underlying probability space. By deﬁnition, Z occurs if and only if
x∈Domain(Map) when Bow ﬁnishes its game. Therefore,
Pr[Z] = OWadv[Bow,T]. (11.6)
Observe that in Game 1, the key k is only used to encrypt the challenge plaintext. As such,
the adversary is essentially attacking Es as in the bit-guessing version of Attack Game 2.1 at this
443
point. More precisely, we derive an eﬃcient SS adversary Bs based on Game 1 that uses Aas a
subroutine, such that ⏐⏐Pr[W1] −1/2
⏐⏐= SSadv∗[Bs,Es]. (11.7)
Adversary Bs plays the bit-guessing version of Attack Game 2.1 against a challenger Cs, and plays
the role of challenger to Aas in Fig. 11.2, except with the following lines modiﬁed as indicated:
(2) (deleted)
(3) (deleted)
(4) forward (m0,m1) to Cs, obtaining c
Additionally,
when Aoutputs ˆb:
output ˆb
To analyze Bs, we may naturally view Game 1 and the game played between Bs and Cs as
operating on the same underlying probability space. By construction, Bs and Aoutput the same
thing, and so (11.7) holds.
Combining (11.4), (11.5), (11.6), and (11.7), yields (11.3). 2
11.4.1 Instantiating ETDF with RSA
Suppose we now use RSA (see Section 10.3) to instantiate T in the above encryption scheme ETDF.
This scheme is parameterized by two quantities: the length ℓ of the prime factors of the RSA
modulus, and the encryption exponent e, which is an odd, positive integer. Recall that the RSA
scheme does not quite ﬁt the deﬁnition of a trapdoor permutation scheme, because the domain of
the trapdoor permutation is not a ﬁxed set, but varies with the public key. Let us assume that X
is a ﬁxed set into which we may embed Zn, for every RSA modulus n generated by RSAGen(ℓ,e)
(for example, we could take X = {0,1}2ℓ). The scheme also makes use of a symmetric cipher
Es = (Es,Ds), deﬁned over (K,M,C), as well as a hash function H : X→K .
The basic RSA encryption scheme is ERSA = (G,E,D ), with message space Mand ciphertext
space X×C , where
• the key generation algorithm runs as follows:
G() := ( n,d) ←R RSAGen(ℓ,e), pk ←(n,e), sk ←(n,d)
output (pk,sk);
• for a given public key pk = (n,e), and message m ∈M, the encryption algorithm runs as
follows:
E(pk,m) := x←R Zn, y ←xe, k ←H(x), c ←R Es(k,m)
output (y,c) ∈X×C ;
• for a given secret key sk = (n,d), and a given ciphertext ( y,c) ∈X×C , where y represents
an element of Zn, the decryption algorithm runs as follows:
D(sk, (y,c) ) := x←yd, k ←H(x), m ←Ds(k,c)
output m.
444
Theorem 11.3. Assume H : X→K is modeled as a random oracle. If the RSA assumption holds
for parameters (ℓ,e), and Es is semantically secure, then ERSA is semantically secure.
In particular, for any SS adversary Athat attacks ERSA as in the random oracle version of
Attack Game 11.1, there exist an RSA adversary Brsa that breaks the RSA assumption for (ℓ,e)
as in Attack Game 10.3, and an SS adversary Bs that attacks Es as in Attack Game 2.1, where
Brsa and Bs are elementary wrappers around A, such that
SSroadv∗[A,ERSA] ≤RSAadv[Brsa,ℓ,e ] + SSadv∗[Bs,Es].
Proof. The proof of Theorem 11.2 carries over, essentially unchanged. 2
11.5 ElGamal encryption
In this section we show how to build a public-key encryption scheme from Diﬃe-Hellman. Security
will be based on either the CDH or DDH assumptions from Section 10.5.
The encryption scheme is a variant of a scheme ﬁrst proposed by ElGamal, and we call it EEG.
It is built out of several components:
• a cyclic group G of prime order q with generator g∈G,
• a symmetric cipher Es = (Es,Ds), deﬁned over (K,M,C),
• a hash function H : G2 →K.
The message space for EEG is M, and the ciphertext space is G ×C. We now describe the key
generation, encryption, and decryption algorithms for EEG.
• the key generation algorithm runs as follows:
G() := α←R Zq, u←gα
pk ←u, sk ←α
output (pk,sk);
• for a given public key pk = u ∈G and message m ∈M, the encryption algorithm runs as
follows:
E(pk,m) := β ←R Zq, v←gβ, w←uβ, k←H(v,w), c←Es(k,m)
output (v,c);
• for a given secret key sk = α∈Zq and a ciphertext ( v,c) ∈G ×C, the decryption algorithm
runs as follows:
D(sk, (v,c) ) := w←vα, k←H(v,w), m←Ds(k,c)
output m.
Thus, EEG = (G,E,D ), and is deﬁned over ( M,G ×C).
Note that the description of the group G and generator g ∈G is considered to be a system
parameter, rather than part of the public key.
445
Remark 11.1 (Hashing (v,w) vs hashing only w). In EEG, we derive the key k by hashing
both v and w. In a variant, which we call E′
EG, we could derive the key k by hashing just w. The
security results that we prove in this chapter hold for E′
EG as well as for EEG (and the proofs carry
over with little change). However, in the next chapter we prove further security properties of EEG
that do not hold as cleanly for E′
EG. As such, the scheme EEG is generally preferred, and is not
signiﬁcantly more computationally expensive than E′
EG. 2
11.5.1 Semantic security of ElGamal in the random oracle model
We shall analyze the security of EEG under two diﬀerent sets of assumptions. In this section we do
the analysis modeling H : G2 →K as a random oracle, under the CDH assumption for G, and the
assumption that Es is semantically secure. In the next section we analyze EEG without the random
oracle model, but using the stronger DDH assumption for G.
Theorem 11.4. Assume H : G2 →K is modeled as a random oracle. If the CDH assumption
holds for G, and Es is semantically secure, then EEG is semantically secure.
In particular, for every SS adversary Athat plays the random oracle version of Attack Game 11.1
with respect to EEG, and makes at most Q queries to the random oracle, there exist a CDH
adversary Bcdh that plays Attack Game 10.5 with respect to G, and an SS adversary Bs that
plays Attack Game 2.1 with respect to Es, where Bcdh and Bs are elementary wrappers around
A, such that
SSroadv[A,EEG] ≤2Q·CDHadv[Bcdh,G] + SSadv[Bs,Es]. (11.8)
Proof idea. Suppose the adversary sees the ciphertext ( v,c), where v = gβ. If H is modeled as
a random oracle, then intuitively, the only way the adversary can learn anything at all about the
symmetric key k used to generate c is to explicitly evaluate the random oracle representing H at
the point ( v,w), where w = vα; however, if he could do this, we could convert the adversary into
an adversary that breaks the CDH assumption for G. One wrinkle is that we cannot recognize
the correct solution to the CDH problem when we see it (if the DDH assumption is true), so we
simply guess by choosing at random from among all of the adversary’s random oracle queries.
This is where the factor of Q in (11.8) comes from. So unless the adversary can break the CDH
assumption, from the adversary’s point of view, k is completely random, and semantic security for
EEG follows directly from the semantic security of Es. 2
Proof. It is convenient to prove the theorem using the bit-guessing version of the semantic security
game. We prove:
SSroadv∗[A,EEG] ≤Q·CDHadv[Bcdh,G] + SSadv∗[Bs,Es]. (11.9)
Then (11.8) follows from (11.1) and (2.10).
We deﬁne Game 0 to be the game played between Aand the challenger in the bit-guessing
version of Attack Game 11.1 with respect to EEG. We then modify the challenger to obtain Game 1.
In each game, b denotes the random bit chosen by the challenger, while ˆb denotes the bit output
by A. Also, for j = 0,1, we deﬁne Wj to be the event that ˆb = b in Game j. We will show that⏐⏐Pr[W1]−Pr[W0]
⏐⏐is negligible, and that Pr[W1] is negligibly close to 1/2. From this, it follows that
SSroadv∗[A,EEG] =
⏐⏐Pr[W0] −1/2
⏐⏐ (11.10)
is negligible.
446
initialization:
(1) α,β ←R Zq, u←gα, v←gβ, w←gαβ
initialize an empty associative array Map : G2 →K
(2) k←R K, b←R {0,1}
(3) Map[v,w] ←k
send the public key u to A;
upon receiving a single encryption query ( m0,m1) ∈M2:
(4) c←R Es(k,mb)
send (v,c) to A;
upon receiving a random oracle query (ˆv, ˆw) ∈G2:
if (ˆv, ˆw) /∈Domain(Map) then Map[ˆv, ˆw] ←R K
send Map[ˆv, ˆw] to A
Figure 11.3: Game 0 challenger
Game 0. The adversary can make any number of random oracle queries, but at most one encryption
query. Again, recall that in addition to direct access to the random oracle via explicit random oracle
queries, the adversary also has indirect access to the random oracle via the encryption query, where
the challenger also makes use of the random oracle. The random oracle is implemented using an
associative array Map : G2 →K. The details are in Fig. 11.3. At line (3), we eﬀectively set the
random oracle at the point ( v,w) to k.
Game 1. This is the same as Game 0, except we delete line (3) in Fig. 11.3.
Let Z be the event that the adversary queries the random oracle at ( v,w) in Game 1. Clearly,
Games 0 and 1 proceed identically unless Z occurs, and so by the Diﬀerence Lemma, we have
⏐⏐Pr[W1] −Pr[W0]
⏐⏐≤Pr[Z]. (11.11)
If event Z happens, then one of the adversary’s random oracle queries is ( v,w), where w is the
solution to the instance ( u,v) of the CDH problem. Moreover, in Game 1, the values α and β are
only needed to compute u and v, and nowhere else. Thus, we can use adversary Ato build an
adversary Bcdh to break the CDH assumption: we simply choose one of the adversary’s random
oracle queries (ˆv, ˆw) at random, and output ˆw — with probability at least Pr[ Z]/Q, this will be
the solution to the given instance of the CDH problem.
In more detail, adversary Bcdh plays Attack Game 10.5 against a challenger Ccdh, and plays the
role of challenger to Aas in Fig. 11.3, except with the following lines modiﬁed as indicated:
(1) obtain (u,v) from Ccdh
(3) (deleted)
Additionally,
when Aterminates:
if Domain(Map) ̸= ∅
then (ˆv, ˆw) ←R Domain(Map), output ˆw
else output “failure”
447
To analyze Bcdh, we may naturally view Game 1 and the game played between Bcdh and Ccdh
as operating on the same underlying probability space. By deﬁnition, Z occurs if and only if
(v,w) ∈ Domain(Map) when Bcdh ﬁnishes its game. Moreover, since
⏐⏐Domain(Map)
⏐⏐ ≤ Q, it
follows that
CDHadv[Bcdh,G] ≥Pr[Z]/Q. (11.12)
Observe that in Game 1, the key k is only used to encrypt the challenge plaintext. We leave it
to the reader to describe an eﬃcient SS adversary Bs that uses Aas a subroutine, such that
⏐⏐Pr[W1] −1/2
⏐⏐= SSadv∗[Bs,Es]. (11.13)
Combining (11.10), (11.11), (11.12), and (11.13), yields (11.9), which completes the proof of
the theorem. 2
11.5.2 Semantic security of ElGamal without random oracles
As we commented in Section 8.10.2, security results in the random oracle model do not necessarily
imply security in the real world. When it does not hurt eﬃciency, it is better to avoid the random
oracle model. By replacing the CDH assumption by the stronger, but still reasonable, DDH as-
sumption, and by making an appropriate, but reasonable, assumption about H, we can prove that
the same system EEG is semantically secure without resorting to the random oracle model.
We thus obtain two security analyses of EEG: one in the random oracle model, but using the
CDH assumption. The other, without the random oracle model, but using the stronger DDH
assumption. We are thus using the random oracle model as a hedge: in case the DDH assumption
turns out to be false in the group G, the scheme remains secure assuming CDH holds in G, but
in a weaker random oracle semantic security model. In Exercise 11.14 we develop yet another
analysis of ElGamal without random oracles, but using a weaker assumption than DDH called
hash Diﬃe-Hellman (HDH) which more accurately captures the exact requirement needed to
prove security.
To carry out the analysis using the DDH assumption in G we make a speciﬁc assumption about
the hash function H : G2 →K, namely that H is a secure key derivation function , or KDF
for short. We already introduced a very general notion of a key derivation function in Section 8.10.
What we describe here is more focused and tailored precisely to our current situation.
Intuitively, H : G2 →K is a secure KDF if no eﬃcient adversary can eﬀectively distinguish
between ( v,H(w)) and ( v,k), where v and w are randomly chosen from G, and k is randomly
chosen from K. To be somewhat more general, we consider an arbitrary, eﬃciently computable
hash function F : X×Y→Z , where X, Y, and Zare arbitrary, ﬁnite sets.
Attack Game 11.3 (secure key derivation). For a given hash function F : X×Y→Z , and
for a given adversary A, we deﬁne two experiments.
Experiment b (b= 0,1):
• The challenger computes
x←R X, y ←R Y, z 0 ←F(x,y), z 1 ←R Z,
and sends (x,zb) to the adversary.
448
• The adversary outputs a bit ˆb∈{0,1}.
If Wb is the event that Aoutputs 1 in Experiment b, then we deﬁne A’s advantage with respect
to F as
KDFadv[A,F] :=
⏐⏐Pr[W0] −Pr[W1]
⏐⏐. 2
Deﬁnition 11.5 (secure key derivation). A hash function F : X×Y→Z is a secure KDF
if for every eﬃcient adversary A, the value KDFadv[A,F] is negligible.
It is plausible to conjecture that an “oﬀ the shelf” hash function, like SHA256 or HKDF (see
Section 8.10.5), is a secure KDF. In fact, one may justify this assumption modeling the hash
function as a random oracle; however, using this explicit computational assumption, rather than
the random oracle model, yields more meaningful results.
One may even build a secure KDF without making any assumptions at all: the construction in
Section 8.10.4 based on a universal hash function and the leftover hash lemma yields an uncondi-
tionally secure KDF. Even though this construction is theoretically attractive and quite eﬃcient,
it may not be a wise choice from a security point of view: as already discussed above, if the DDH
turns out to be false, we can still rely on the CDH in the random oracle model, but for that, it is
better to use something based on SHA256 or HKDF, which can more plausibly be modeled as a
random oracle.
Theorem 11.5. If the DDH assumption holds for G, H : G2 →K is a secure KDF, and Es is
semantically secure, then EEG is semantically secure.
In particular, for every SS adversary Athat plays Attack Game 11.1 with respect to EEG, there
exist a DDH adversary Bddh that plays Attack Game 10.6 with respect to G, a KDF adversary
Bkdf that plays Attack Game 11.3 with respect to H, and an SS adversary Bs that plays Attack
Game 2.1 with respect to Es, where Bddh, Bkdf, and Bs are elementary wrappers around A, such
that
SSadv[A,EEG] ≤2 ·DDHadv[Bddh,G] + 2·KDFadv[Bkdf,H] + SSadv[Bs,Es]. (11.14)
Proof idea. Suppose the adversary sees the ciphertext ( v,c), where v = gβ and c is a symmetric
encryption created using the key k := H(v,w), where w = uβ. Suppose the challenger replaces w
by a random independent group element ˜w ∈G and constructs k as k := H(v, ˜w). By the DDH
assumption the adversary cannot tell the diﬀerence between w and ˜w and hence its advantage is
only negligibly changed. Under the KDF assumption, k := H(v, ˜w) looks like a random key in K,
independent of the adversary’s view, and therefore security follows by semantic security of Es. 2
Proof. More precisely, it is convenient to prove the theorem using the bit-guessing version of the
semantic security game. We prove:
SSadv∗[A,EEG] ≤DDHadv[Bddh,G] + KDFadv[Bkdf,H] + SSadv∗[Bs,Es]. (11.15)
Then (11.14) follows by (11.1) and (2.10).
Deﬁne Game 0 to be the game played between Aand the challenger in the bit-guessing version
of Attack Game 11.1 with respect to EEG. We then modify the challenger to obtain Games 1 and
2. In each game, bdenotes the random bit chosen by the challenger, while ˆbdenotes the bit output
by A. Also, for j = 0,1,2, we deﬁne Wj to be the event that ˆb= b in Game j. We will show that⏐⏐Pr[W2]−Pr[W0]
⏐⏐is negligible, and that Pr[W2] is negligibly close to 1/2. From this, it follows that
SSadv∗[A,EEG] =
⏐⏐Pr[W0] −1/2
⏐⏐ (11.16)
is negligible.
449
initialization:
(1) α,β ←R Zq, γ ←αβ, u←gα, v←gβ, w←gγ
(2) k←H(v,w)
b←R {0,1}
send the public key u to A;
upon receiving (m0,m1) ∈M2:
c←Es(k,mb), send (v,c) to A
Figure 11.4: Game 0 challenger
Game 0. The logic of the challenger in this game is presented in Fig. 11.4.
Game 1. We ﬁrst play our “DDH card.” The challenger in this game is as in Fig. 11.4, except
that line (1) is modiﬁed as follows:
(1) α,β ←R Zq, γ ←R Zq, u←gα, v←gβ, w←gγ
We describe an eﬃcient DDH adversary Bddh that uses Aas a subroutine, such that
⏐⏐Pr[W0] −Pr[W1]
⏐⏐= DDHadv[Bddh,G]. (11.17)
Adversary Bddh plays Attack Game 10.6 against a challenger Cddh, and plays the role of challenger
to Aas in Fig. 11.4, except with line (1) modiﬁed as follows:
(1) obtain (u,v,w ) from Cddh
Additionally,
when Aoutputs ˆb:
if b= ˆb then output 1 else output 0
Let p0 be the probability that Bddh outputs 1 when Cddh is running Experiment 0 of the DDH
Attack Game 10.6, and let p1 be the probability that Bddh outputs 1 when Cddh is running Exper-
iment 1. By deﬁnition, DDH adv[Bddh,G] = |p1 −p0|. Moreover, if Cddh is running Experiment 0,
then adversary Ais playing our Game 0, and so p0 = Pr[W0], and if Cddh is running Experiment 1,
then Ais playing our Game 1, and so p1 = Pr[W1]. Equation (11.17) now follows immediately.
Game 2. Observe that in Game 1, w is completely random, and is used only as an input to H.
This allows us to play our “KDF card.” The challenger in this game is as in Fig. 11.4, except with
the following lines modiﬁed as indicated:
(1) α,β ←R Zq, γ ←R Zq, u←gα, v←gβ, w←gγ
(2) k←R K
We may easily derive an eﬃcient KDF adversary Bkdf that uses Aas a subroutine, such that
|Pr[W1] −Pr[W2]|= KDFadv[Bkdf,H]. (11.18)
Adversary Bkdf plays Attack Game 11.3 against a challenger Ckdf, and plays the role of challenger
to Aas in Fig. 11.4, except with the following lines modiﬁed as indicated:
450
(1) α←R Zq, γ ←R Zq, u←gα, w←gγ
(2) obtain (v,k) from Ckdf
Additionally,
when Aoutputs ˆb:
if b= ˆb then output 1 else output 0
We leave it to the reader to verify (11.18).
Observe that in Game 2, the key k is only used to encrypt the challenge plaintext. As such,
the adversary is essentially just playing the SS game with respect to Es at this point. We leave it
to the reader to describe an eﬃcient SS adversary Bs that uses Aas a subroutine, such that
|Pr[W2] −1/2|= SSadv∗[Bs,Es]. (11.19)
Combining (11.16), (11.17), (11.18), and (11.19), yields (11.15), which completes the proof of
the theorem. 2
11.6 A fun application: oblivious transfer based on Diﬃe-Hellman
Bob likes to be well informed. Every day he logs into his favorite newspaper site, reads the headlines,
and downloads the articles that he wants to read. The newspaper charges Bob a few cents for every
article that he reads, a price that Bob is happy to pay to support the newspaper. All articles have
the same price.
One day Bob realizes that the newspaper can track what articles he is downloading and use
this information to learn his political leanings, age group, and other personal information that Bob
would rather keep private. He asks his friend Alice the following question:
The newspaper has articles m1,...,m n ∈M and I am interested in reading article
number i ∈ {1,...,n }. Can I download article mi without the newspaper learning
what article I downloaded?
Alice says that the newspaper can simply send Bob all the articles m1,...,m n and Bob will read
the ones he is interested in. The newspaper does not learn what articles Bob reads.
When Bob asked the newspaper to implement this scheme, they refused saying that this is a
sure way for the newspaper to go out of business. Their readers will pay a few cents for a single
article and get the entire newspaper. The newspaper would go along with the proposal if it could
ensure that Bob only receives one article per request.
Oblivious transfer. Let’s call the newspaper the sender and call Bob the receiver. Bob wants a
solution to the following problem:
The sender has data m1,...,m n ∈M and the receiver has an index i ∈{1,...,n }.
They want a protocol with the following property: when the protocol completes the
receiver learns mi and nothing else, while the sender learns nothing about i.
451
This problem is called 1-out-of- n oblivious transfer, or simply 1-out-of- n OT. It comes up in
many settings beyond newspapers, such as privately buying digital content from an online store
without the store learning what item is being purchased. More generally, OT is useful whenever
there is a need to privately request a record in a proprietary database. It is also a central building
block in many cryptographic protocols, as we will see in Chapter 23.
A warning. Deﬁning security for OT protocols is quite subtle. Here we present two very simple
OT protocols and discuss their security somewhat informally. We caution that both of these proto-
cols require modiﬁcation to ensure that they can be used securely when running several instances
of the protocols concurrently, and when using them as subprotocols in arbitrary applications. Our
goal here is to just get across some of the essential ideas, not to present protocols that should be
deployed “as is” in arbitrary applications. We will discuss these security issues in Chapter 23.
11.6.1 A secure OT from ElGamal encryption
Our ﬁrst 1-out-of- n OT protocol satisﬁes the basic OT security properties in the random oracle
model, assuming only CDH. We let Mdenote the message space for the sender’s messages, so that
m1,...,m n ∈M. The protocol uses the following ingredients:
• a cyclic group G of prime order q with generator g∈G;
• a hash function H : G2 →K, which will be modeled as a random oracle;
• a semantically secure symmetric cipher ( Es,Ds) with key space Kand message space M.
We also assume that the sender and receiver communicate over a secure channel (i.e., one that
utilizes authenticated encryption as discussed in Chapter 9).
With these ingredients in place, we are ready to describe an elegant OT protocol using ElGamal
encryption in the group G. We use a variant of the system EEG from Section 11.5. The sender
generates nElGamal public keys u1,...,u n ∈G and encrypts message mj under public key uj, for
j = 1,...,n . The clever idea is a mechanism that ensures that the recipient knows the secret key
for the public key ui, but does not know the secret key for any the other key. This lets the recipient
decrypt the ith ciphertext, obtaining mi, but reveals nothing else. Let’s see how the protocol works
and then discuss its security.
• step 1: the sender chooses β ←R Zq, computes v←gβ ∈G, and sends v to the receiver.
• step 2: the receiver chooses α←R Zq, computes u←gαv−i ∈G, and sends u to the sender.
• step 3: for j = 1,...,n the sender computes
uj ←u·vj ∈G / / construct an ElGamal public key uj = gαvj−i
wj ←uβ
j / / construct an ElGamal ciphertext (v,cj) = (gβ,cj)
kj ←H(v,wj) ∈K / / using randomness β and public key uj,
cj ←R Es(kj,mj) / / that is the encryption of mj ∈M
and sends the vector of ciphertexts C := (c1,...,c n) to the receiver. Note that all nElGamal
ciphertexts (v,c1),..., (v,cn) are generated using the same encryption randomness β.
452
• step 4: the receiver, who has the secret key α for the ElGamal public-key ui = gα, decrypts
ci as follows:
compute w←vα, k←H(v,w)
output m←Ds(k,ci)
Let’s verify that this works correctly. During decryption in step 4, the key k is computed as
k←H(v,w), where w= vα = gαβ. In step 3 the key ki is computed as ki ←H(v,wi), where
wi = uβ
i = (uvi)β =
(
(gαv−i)vi)β = gαβ.
Hence, when the parties honestly follow the protocol, we have wi = w and ki = k, and thus the
receiver correctly obtains mi.
Is the protocol secure? The only information that the sender learns about the receiver’s input
i ∈{1,...,n }is the group element u = gαv−i computed in step 2. Because α is not used in any
other message sent to the sender, the quantity gα is uniformly distributed over the group G, and
therefore perfectly hides v−i. Hence, the sender learns nothing about i, as required.
To argue that the receiver learns at most one message, it suﬃces to argue that the receiver
can query the random oracle representing H at no more than one point of the form ( v,wj) for
j = 1,...,n . This will ensure that the receiver learns at most one decryption key, and hence at
most one message. We brieﬂy argue that if the receiver could query the random oracle at two
such points, say ( v,wj1) and ( v,wj2), then we could break the CDH for G. In particular, we can
transform such a receiver into an algorithm that takes as input v= gβ and produces a tuple
(
u,j1,j2, wj1 = (uvj1)β, wj2 = (uvj2)β)
(11.20)
for some j1 ̸= j2. Dividing the two right most terms in (11.20) one by the other, and raising the
result to the power of 1 /(j1 −j2) gives vβ which is the same as g(β2). Hence, we get an algorithm
that takes as input gβ and computes g(β2). We showed in Exercise 10.17 that this problem is
equivalent to CDH in G. We conclude that if CDH holds in G then the receiver can learn at most
one message.
Remark 11.2. As we cautioned at the beginning of the section, the protocol above must be
modiﬁed in order to satisfy stronger security properties. See [10] for a similar protocol that can be
proven to satisfy such properties. In particular, the above informal security analysis did not take
into account the possibility of attacks that can arise when multiple instances of the protocol run
concurrently. Exercise 11.20 discusses such attacks, and shows how to prevent them by modifying
the protocol in two ways. First, in deriving the secret keys using the hash function H, the hash
function should also take as input the identities of the two parties (and possibly some type of
“channel binding” or “session ID”, as in Section 21.8). Second, instead of a semantically secure
symmetric cipher, we should use a cipher that provides one-time authenticated encryption. 2
11.6.2 Adaptive oblivious transfer
Going back to our newspaper scenario, the OT protocol presented above satisﬁes the security
properties we want. However, when trying to deploy it, the newspaper hits a serious performance
problem. For every article that Bob wants to download, the newspaper has to re-encrypt all
the articles in the newspaper under new keys and send all these fresh ciphertexts to Bob. The
453
computation and communication costs per download would be prohibitive. In case of an online
bookstore, the entire bookstore would need to be re-encrypted for every purchase.
It would be much better if the newspaper could encrypt all the articles once and post the
encrypted articles on its web site for anyone to download. Bob, and other readers, will download
the entire encrypted newspaper. Later, every time Bob pays for an article, the newspaper will
quickly open the article for Bob without learning which article was opened. The computation and
communication costs should be independent of the number of articles n. Bob will repeat this for
as many articles as he wants to read.
An OT protocol that operates this way is called an adaptive OT. The word “adaptive” refers
to Bob choosing the next article to open in a way that depends on the previously opened articles.
Here we describe a simple protocol that satisﬁes the basic adaptive OT security requirements. That
is, a receiver (even a malicious one) learns at most one message per run of the protocol, and the
sender (even a malicious one) learns nothing about which messages were opened. In addition, the
protocol should ensure that a malicious sender cannot make the receiver open the wrong message.
This will provide consistency: whenever one or several receivers request the ith message, they
always get the same message.
As we cautioned above, this protocol must be modiﬁed in order to satisfy stronger security
properties. See [79] for a protocol that can be proven to satisfy such properties.
Before describing the adaptive OT we need a quick interlude to describe a useful, related concept
called an oblivious PRF.
11.6.3 Oblivious PRFs
Let F be a secure PRF deﬁned over (K,X,Y). Suppose that one party, the sender, has the PRF key
k∈K. Another party, the receiver, has an input x∈X. An oblivious PRF protocol, or simply
an OPRF protocol , is a protocol that lets the receiver learn the output y = F(k,x) without
learning anything else about the value of the PRF at any other input; in addition, the sender learns
nothing about the input x. Moreover, the OPRF protocol may be run many times, allowing the
receiver to learn the value of F(k,·) for several inputs. The sender should learn nothing about any
of these inputs, and the receiver should learn nothing about the value of F(k,·) at any other inputs.
Actually, all we will really require here is that the receiver cannot learn anything about the value
of F(k,·) at ℓ inputs unless it interacts with the sender ℓ or more times.
We can construct a simple OPRF protocol from the secure PRF F′presented in Exercise 11.3.
That PRF makes use of a cyclic group G of prime order q with generator g∈G and is deﬁned as
F′(k,x) := H′
(
x, H(x)k
)
for k∈Zq, x∈X. (11.21)
Here H : X →G and H′ : X× G →Y are hash functions modeled as random oracles. The key
space is K= Zq. In Exercise 11.3, you are asked to prove that this PRF is secure assuming CDH
holds for G, and we model H and H′as random oracles.
Protocol OPRF1
We now describe an OPRF protocol forF′, which we call Protocol OPRF1. Suppose that the sender
has a key k∈Zq. In each run of the protocol, the receiver has an input x∈X, and the sender and
receiver interact (over a secure channel) as follows:
454
• receiver: choose ρ←R Zq \{0}, compute v←H(x)ρ ∈G, and send v to the sender.
• sender: compute w←vk ∈G and send w to the receiver.
• receiver: compute and output y←H′(
x, w1/ρ)
∈Y.
First, observe that if sender and receiver are honest, then in the last step, the receiver computes
w1/ρ = (vk)1/ρ =
((
H(x)ρ)k)1/ρ
= H(x)k,
and so the receiver ends up with the correct output.
Second, observe that because of the way the receiver computes v, the value v leaks no infor-
mation to the sender about the input x: the sender just sees a random group element, statistically
independent of x.
Now suppose the receiver is honest but the sender is malicious. In this case, the sender still
learns nothing about x; however, the sender could send a wrong value for w, causing the receiver to
end up with an incorrect value for the output y= F′(k,x). Thus, Protocol OPRF1 is only partially
secure if the sender is malicious: while the sender does not learn the receiver’s input, the receiver
may end up with the wrong output.
Now consider how much information a possibly malicious receiver can obtain by interacting with
the sender. We want to argue that a receiver cannot learn anything about the value of F′(k,·) at ℓ
inputs unless it interacts with the sender at least ℓ times. This follows directly from a reasonable
assumption called the one-more Diﬃe-Hellman or 1MDH assumption, if we model H and H′
as random oracles.
Very brieﬂy, we can describe the 1MDH assumption in terms of the following attack game. In
this game, the challenger chooses α ←R Zq, computes u ←gα ∈G, and sends u to the adversary.
Next, the adversary makes a sequence of queries to the challenger, each of which can be one of the
following types:
Challenge query: the challenger chooses v←R G, and sends v to the adversary.
CDH solve query: the adversary submits ˆv ∈G to the challenger, who computes and
sends ˆw←ˆvα to the adversary.
At the end of the game, the adversary outputs a list of distinct pairs, each of the form ( i,w), where
iis a positive integer bounded by the number of challenge queries, and w∈G. We call such a pair
(i,w) correct if w = vα and v is the challenger’s response to the ith challenge query. We say the
adversary wins the game if the number of correct pairs exceeds the number of CDH solve queries.
The 1MDH assumption says that every eﬃcient adversary wins this game with at most negligible
probability. In particular, an eﬃcient adversary who sees a number of challenges raised to the
power α, cannot produce any other challenge raised to the power α.
Clearly, the 1MDH assumption implies the CDH assumption. It is also not too hard to see
that the 1MDH assumption implies that in the above OPRF construction, a receiver cannot learn
anything about the value of F′(k,·) at ℓ inputs unless it interacts with the sender at least ℓ times.
In particular, the receiver cannot query the random oracle representing H′ at ℓ distinct points of
the form (x,H(x)k) unless it interacts with the sender at least ℓ times. We leave this argument as
an exercise to the reader.
We note that in certain groups G, winning in the 1MDH game can be slightly easier than
computing discrete log in G. In these groups there is also an attack on the scheme OPRF1 that is
slightly faster than computing discrete log in G. The details are in Section 16.1.4.
455
Remark 11.3. When using the scheme OPRF1, it is important that the sender verify that v is in
G before responding, otherwise the sender’s response could inadvertently expose the secret key k
(e.g., using an attack similar to the one in Exercises 12.3 or 15.1). 2
Protocol OPRF2
As we saw, Protocol OPRF1 is only partially secure in the presence of a malicious sender. We can
easily modify the protocol to get one that provides a stronger security property in the presence of a
malicious sender. In particular, this modiﬁed protocol, which we call Protocol OPRF2, guarantees
that if the receiver’s computed output y is wrong, then it is in fact a random value in Ythat is
completely independent of the sender’s view. The protocol begins by having the sender publish the
value u = gk, where k is the sender’s secret key. In each run of the protocol, the receiver has an
input x∈X, and the sender and receiver interact (over a secure channel) as follows:
• receiver: choose ρ←R Zq \{0}, τ ←R Zq, compute v ←H(x)ρ ·gτ ∈G, and send v to sender.
Note that this v is computed diﬀerently than in OPRF1.
• sender: compute w←vk ∈G and send w to the receiver.
• receiver: compute and output y←H′(
x, (w/uτ)1/ρ)
∈Y.
Observe that if sender and receiver are honest, then in the last step, the receiver computes
(w/uτ)1/ρ =
(
vk/uτ
)1/ρ
=
((
H(x)ρ ·gτ)k/gkτ
)1/ρ
= H(x)k,
and so the receiver ends up with the correct output y= F′(k,x).
Next, observe that because of the way the receiver computes v, the value vleaks no information
to the sender about the input x, and also leaks no information about the exponent ρ.
Now suppose the receiver is honest but the sender is malicious. If the sender responds in the
second step with a wrong value for w, say w= vk ·d, where d̸= 1, then the receiver computes
(w/uτ)1/ρ =
(
vkd/uτ
)1/ρ
= H(x)k ·d1/ρ.
As we observed above, the sender knows nothing about ρ, and so from the sender’s point of view,
the value d1/ρ ∈G is just a random group element (uniformly distributed over G). It then follows
that the output value y computed by the receiver is also just a random element of Y.
Finally, suppose the sender is honest but the receiver is malicious. Just as for Protocol OPRF1,
under the 1MDH assumption, if we model H and H′ as random oracles, then the receiver cannot
learn anything about the value of F′(k,·) at ℓ inputs unless it interacts with the sender ℓ or more
times.
11.6.4 A simple adaptive OT from an oblivious PRF
We will need the following ingredients:
• A PRF F deﬁned over (Kprf,{1,...,n },K).
• An OPRF protocol for a PRF F, like Protocol OPRF2 in Section 11.6.3, which is secure
against a malicious sender, in the sense that the receiver will either get a correct output or a
random output.
456
• A symmetric cipher ( Es,Ds) deﬁned over ( K,M,C) that provides one-time authenticated
encryption.
The protocol works as follows. Recall that the sender has messages m1,...,m n ∈M.
• step 1: the sender begins by doing:
k←R Kprf
for i= 1,...,n : ki ←F(k,i), ci ←R Es(ki,mi) / / encrypt all messages
send C := (c1,...,c n) to the receiver / / send all ciphertexts to the receiver
• step 2: when the receiver wantsmi, the receiver and sender establish a secure channel between
them and use the OPRF protocol to compute F(k,i):
the sender has k and the receiver has i∈{1,...,n },
when done, the receiver has ki = F(k,i)
Finally, the receiver computes mi ←Ds(ki,ci).
That’s it. If both parties honestly follow the protocol then the receiver obtains mi, as required.
Moreover, step 2 can be repeated as many times as needed. Every time the receiver gets to open
another message of its choice from the sender’s list. The amount of traﬃc generated in each
invocation of step 2 is relatively small, and in particular, is independent of the size of the sender’s
list.
As for security, ﬁrst suppose that the sender is honest but the receiver is malicious. In this case,
from the security property of the OPRF protocol, we know that the number of messages that the
receiver can decrypt is no more than the number of times the receiver runs step 2 of the protocol.
Second, suppose that the receiver is honest but the sender is malicious. In this case, from
the security property of the OPRF protocol, we know that the sender learns nothing about which
ciphertexts get decrypted by the receiver. Moreover, we know that if the sender deviates from the
OPRF protocol, the receiver will obtain a random decryption key, and by the ciphertext integrity
property of a symmetric cipher, the receiver will reject the ciphertext with overwhelming probability.
A ﬁnal word of caution. To conclude this section we note that there are some inherent limi-
tations to the security of oblivious transfer against corrupt parties. Consider again our newspaper
example. Suppose that a corrupt newspaper wants to learn if Bob is reading articles in the ﬁnance
section of the newspaper. The newspaper could choose its input articles m1,...,m n to the OT
protocol so that all the articles in the ﬁnance section are blank, while articles in other sections
are correct newspaper articles. Now, when Bob wants to read an article in the ﬁnance section he
will carry out an OT with the newspaper and end up with a blank article, despite having paid
the newspaper for the article. Bob will undoubtedly complain that the oblivious transfer failed, at
which point the newspaper learns that Bob requested an article in the ﬁnance section. This violates
OT privacy. Of course, Bob could preserve his privacy by not complaining, but that is diﬃcult to
enforce in real life.
11.7 Notes
Citations to the literature to be added.
457
11.8 Exercises
11.1 (From weak PRF to PRF via hashing). This exercise gives a simple application of the
random oracle model that will be useful later. Let F′(k,x) be a PRF deﬁned over ( K,X,Y) and
suppose that F′ is weakly secure (as in Deﬁnition 4.3). Let H : M→X be a hash function, and
deﬁne a new PRF over ( K,M,Y):
F(k,m) := F′(
k,H(m)
)
.
Show that F is a secure PRF (in the standard sense of a secure PRF) when H is modeled as a
random oracle. In particular, you should show that for every adversary Aattacking F as a PRF,
there exists an adversary Battacking F′as a weak PRF, which is an elementary wrapper aroundA,
such that PRFroadv[A,F] ≤wPRFadv[B,F′].
11.2 (Simple PRF from DDH). Let G be a cyclic group of prime order q generated by g∈G.
Let H : M→ G be a hash function, which we shall model as a random oracle (see Section 8.10.2).
Let F be the PRF deﬁned over ( Zq,M,G) as follows:
F(k,m) := H(m)k for k∈Zq, m∈M.
Show that F is a secure PRF in the random oracle model for H under the DDH assumption
for G. In particular, you should show that for every adversary Aattacking F as a PRF, there
exists a DDH adversary B, which is an elementary wrapper around A, such that PRFroadv[A,F] ≤
DDHadv[B,G] + 1/q.
Hint: First show that DDH implies that the PRF F′(k,v) := vk deﬁned over (Zq,G,G) is a weak
PRF (as in Deﬁnition 4.3). Use Exercise 10.11 for this. Then use Exercise 11.1 to deduce that F
is a secure PRF.
11.3 (Simple PRF from CDH). Continuing with Exercise 11.2, let H′: M×G →Y be a hash
function, which we again model as a random oracle. Let F′be the PRF deﬁned over (Zq,M,Y) as
follows:
F′(k,m) := H′
(
m, H(m)k
)
for k∈Zq, m∈M.
Show that F′ is a secure PRF assuming CDH for G, and when H and H′ are modeled as random
oracles. Section 11.6.2 gives an interesting application for this PRF, exploiting the fact that it can
be evaluated obliviously.
Hint: Use the result of Exercise 10.5.
11.4 (Broken variant of RSA). Consider the following broken version of the RSA public-key
encryption scheme: key generation is as in ERSA, but to encrypt a message m∈Zn with public key
pk = (n,e) do E(pk,m) := me in Zn. Decryption is done using the RSA trapdoor.
Clearly this scheme is not semantically secure. Even worse, suppose one encrypts arandom message
m ∈ {0,1,..., 264}to obtain c := me mod n. Show that for 35% of plaintexts in [0 ,264], an
adversary can recover the complete plaintext m from c using only 235 eth powers in Zn.
Hint: Use the fact that about 35% of the integers m in [0,264] can be written as m = m1 ·m2
where m1,m2 ∈[0,234].
458
11.5 (Multiplicative ElGamal). Let G be a cyclic group of prime order q generated by g ∈G.
Consider a simple variant of the ElGamal encryption system EMEG = (G,E,D ) that is deﬁned over
(G,G2). The key generation algorithm G is the same as in EEG, but encryption and decryption
work as follows:
• for a given public key pk = u∈G and message m∈G:
E(pk,m) := β ←R Zq, v←gβ, e←uβ ·m, output ( v,e)
• for a given secret key sk = α∈Zq and a ciphertext ( v,e) ∈G2:
D(sk, (v,e) ) := e/vα
(a) Show that EMEG is semantically secure assuming the DDH assumption holds in G. In par-
ticular, you should show that the advantage of any adversary Ain breaking the semantic
security of EMEG is bounded by 2 ϵ, where ϵ is the advantage of an adversary B(which is an
elementary wrapper around A) in the DDH attack game.
(b) Show that EMEG is not semantically secure if the DDH assumption does not hold in G.
(c) Show that EMEG has the following property: given a public key pk, and two ciphertexts
c1 ←R E(pk,m1) and c2 ←R E(pk,m2), it is possible to create a new ciphertext c which is an
encryption of m1 ·m2. This property is called a multiplicative homomorphism.
11.6 (An attack on multiplicative ElGamal). Let pand q be large primes such that q divides
p−1. Let G be the order q subgroup of Z∗
p generated by g ∈ G and assume that the DDH
assumption holds in G. Suppose we instantiate the ElGamal system from Exercise 11.5 with the
group G. However, plaintext messages are chosen from the entire group Z∗
p so that the system is
deﬁned over (Z∗
p,G ×Z∗
p). Show that the resulting system is not semantically secure.
11.7 (Extending the message space). Suppose that we have a public-key encryption scheme
E= (G,E,D ) with message space M. From this, we would like to build an encryption scheme
with message space M2. To this end, consider the following encryption scheme E2 = (G2,E2,D2),
where
G2() := ( pk0,sk0) ←R G(), (pk1,sk1) ←R G(),
output pk := (pk0,pk1) and sk := (sk0,sk1)
E2(
pk,(m0,m1)
) :=
(
E(pk0,m0), E(pk1,m1)
)
D2(
sk,(c0,c1)
) :=
(
D(sk0,c0), D(sk1,c1)
)
Show that E2 is semantically secure, assuming Eitself is semantically secure.
11.8 (Encrypting many messages with multiplicative ElGamal). Consider again the mul-
tuplicative ElGamal scheme in Exercise 11.5. To increase the message space from a single group
element to several, say n, group elements, we could proceed as in the previous exercise. However,
the following scheme, EMMEG = (G,E,D ) deﬁned over (Gn,Gn+1), is more eﬃcient.
• the key generation algorithm runs as follows:
G() := αi ←R Zq, ui ←gαi (i= 1,...,n )
pk ←(u1,...,u n), sk ←(α1,...,α n)
output (pk,sk)
459
• for a given public key pk = (u1,...,u n) ∈Gn and message m= (m1,...,m n) ∈Gn:
E(pk,m) := β ←R Zq, v←gβ
ei ←uβ
i ·mi (i= 1,...,n )
output (v,e1,...,e n)
• for a given secret key sk = (α1,...,α n) ∈Zn
q and a ciphertext c= (v,e1,...,e n) ∈Gn+1:
D(sk,c) := (e1/vα1,...,e n/vαn)
(a) Show that EMMEG is semantically secure assuming the DDH assumption holds in G. In
particular, you should show that the advantage of any adversary Ain breaking the semantic
security of EMMEG is bounded by 2( ϵ+ 1/q), where ϵ is the advantage of an adversary B,
which is an elementary wrapper around A, in the DDH attack game.
Hint: Use Exercise 10.11 with n= 1.
(b) Theorem 11.1 shows that semantic security implies CPA security, but the concrete security
bound degrades by a factor equal to the number Q of encryption queries. Show that for
EMMEG, the security bound for CPA security only degrades by a factor ofn, which is typically
much smaller than Q. In particular, show that for every CPA adversary Athere is a DDH
adversary B, which is an elementary wrapper around A, such that CPA adv[A,EMMEG] is
bounded by 2n·(ϵ+ 1/q), where ϵ is the advantage of Bin the DDH attack game.
Hint: Use Exercise 10.12.
11.9 (Modular hybrid construction). Both of the encryption schemes presented in this chapter,
ETDF in Section 11.4 and EEG in Section 11.5, as well as many other schemes used in practice, have a
“hybrid” structure that combines an asymmetric component and a symmetric component in a fairly
natural and modular way. The symmetric part is, of course, the symmetric cipher Es = (Es,Ds),
deﬁned over (K,M,C). The asymmetric part can be understood in abstract terms as what is called
a key encapsulation mechanism, or KEM.
A KEM Ekem consists of a tuple of algorithms (G,Ekem,Dkem). Algorithm Gis invoked as (pk,sk) ←R
G(). Algorithm Ekem is invoked as ( k,ckem) ←R Ekem(pk), where k ∈K and ckem ∈Ckem. Algorithm
Dkem is invoked as k←Dkem(sk,ckem), where k∈K∪{ reject}and ckem ∈Ckem. We say that Ekem is
deﬁned over (K,Ckem). We require that Ekem satisﬁes the following correctness requirement:
for all possible outputs ( pk,sk) of G(), and all possible outputs ( k,ckem) of Ekem(pk), we have
Dkem(sk,ckem) = k.
We can deﬁne a notion of semantic security in terms of an attack game between a challenger and
an adversary A, as follows. In Experiment b, for b= 0,1, the challenger computes
(pk,sk) ←R G(), (k0,ckem) ←R Ekem(pk), k1 ←R K,
and sends ( kb,ckem) to A. Finally, Aoutputs ˆb ∈{0,1}. As usual, if Wb is the event that A
outputs 1 in Experiment b, we deﬁne A’s advantage with respect to Ekem as SS adv[A,Ekem] :=
|Pr[W0] −Pr[W1]|, and if this advantage is negligible for all eﬃcient adversaries, we say that Ekem
is semantically secure.
460
Now consider the hybrid public-key encryption scheme E= (G,E,D ), constructed out of Ekem
and Es, and deﬁned over ( M,Ckem ×C). The key generation algorithm for Eis the same as that of
Ekem. The encryption algorithm E works as follows:
E(pk,m) :=
{
(k,ckem) ←R Ekem(pk), c←R Es(k,m), output (ckem,c)
}
.
The decryption algorithm D works as follows:
D(sk,(ckem,c)) :=
{
m←reject, k←Dkem(sk,ckem), if k̸= reject then m←Ds(k,c),
output m
}
.
(a) Prove that Esatisﬁes the correctness requirement for a public key encryption scheme, assum-
ing Ekem and Es satisfy their corresponding correctness requirements.
(b) Prove that Eis semantically secure, assuming that Ekem and Es are semantically secure. You
should prove a concrete security bound that says that for every adversary Aattacking E,
there are adversaries Bkem and Bs (which are elementary wrappers around A) such that
SSadv[A,E] ≤2 ·SSadv[Bkem,Ekem] + SSadv[Bs,Es].
(c) Describe the KEM corresponding to ETDF and prove that it is semantically secure (in the
random oracle model, assuming T is one way).
(d) Describe the KEM corresponding to EEG and prove that it is semantically secure (in the
random oracle model, under the CDH assumption for G).
(e) Let Ea = (G,Ea,Da) be a public-key encryption scheme deﬁned over (K,Ca). Deﬁne the KEM
Ekem = (G,Ekem,Da), where
Ekem(pk) :=
{
k←R K, ckem ←R Ea(pk,k), output (k,ckem)
}
.
Show that Ekem is semantically secure, assuming that Ea is semantically secure.
Discussion: Part (e) shows that one can always build a KEM from a public-key encryption scheme
by just using the encryption scheme to encrypt a symmetric key; however, parts (c) and (d) show
that there are more direct and eﬃcient ways to do this.
11.10 (Multi-key CPA security). Generalize the deﬁnition of CPA security for a public-key
encryption scheme to the multi-key setting. In this attack game, the adversary gets to obtain
encryptions of many messages under many public keys. Show that semantic security implies multi-
key CPA security. You should show that security degrades linearly in QkQe, where Qk is a bound
on the number of keys, and Qe is a bound on the number of encryption queries per key. That is,
the advantage of any adversary Ain breaking the multi-key CPA security of a scheme is at most
QkQe ·ϵ, where ϵ is the advantage of an adversary B(which is an elementary wrapper around A)
that breaks the scheme’s semantic security.
11.11 (A tight reduction for multiplicative ElGamal). We proved in Exercise 11.10 that
semantic security for a public-key encryption scheme implies multi-key CPA security; however,
the security degrades signiﬁcantly as the number of keys and encryptions increases. Consider the
461
multiplicative ElGamal encryption scheme EMEG from Exercise 11.5. You are to show a tight
reduction from multi-key CPA security for EMEG to the DDH assumption, which does not degrade
at all as the number of keys and encryptions increases. In particular, you should show that the
advantage of any adversary Ain breaking the multi-key CPA security of EMEG is bounded by
2(ϵ+ 1/q), where ϵis the advantage of an adversary B(which is an elementary wrapper around A)
in the DDH attack game.
Note: You should assume that in the multi-key CPA game, the same groupG and generator g∈G
is used throughout.
Hint: Use Exercise 10.11.
11.12 (An easy discrete log group). Let nbe a large integer and consider the following subset
of Z∗
n2:
Gn :=
{
[an+ 1]n2 ∈Z∗
n2 : a∈{0,...,n −1}
}
(a) Show that Gn is a multiplicative subgroup of Z∗
n2 of order n.
(b) Which elements of Gn are generators?
(c) Choose an arbitrary generator g∈Gn and show that the discrete log problem in Gn is easy.
11.13 (Paillier encryption). Let us construct another public-key encryption scheme ( G,E,D )
that makes use of RSA composites:
• The key generation algorithm is parameterized by a ﬁxed value ℓ and runs as follows:
G(ℓ) := generate two distinct random ℓ-bit primes p and q,
n←pq, d ←(p−1)(q−1)/2
pk ←n, sk ←d
output (pk,sk)
• for a given public key pk = n and message m∈{0,...,n −1}, set g:= [n+ 1]n2 ∈Z∗
n2. The
encryption algorithm runs as follows:
E(pk,m) := h←R Z∗
n2, c ←R gmhn ∈Z∗
n2, output c.
(a) Explain how the decryption algorithm D(sk,c) works.
Hint: Using the notation of Exercise 11.12, observe that cd falls in the subgroup Gn which
has an easy discrete log.
(b) Show that this public-key encryption scheme is semantically secure under the following as-
sumption:
let n be a product of two random ℓ-bit primes,
let u be uniform in Z∗
n2,
let v be uniform in the subgroup ( Zn2)n := {hn : h∈Z∗
n2},
then the distribution ( n,u) is computationally indistinguishable from the distribution ( n,v).
Discussion: This encryption system, called Paillier encryption, has a useful property called
an additive homomorphism: for ciphertexts c0 ←R E(pk,m0) and c1 ←R E(pk,m1), the product
c←c0 ·c1 is an encryption of m0 + m1 mod n.
462
11.14 (Hash Diﬃe-Hellman). Let G be a cyclic group of prime order q generated by g ∈G.
Let H : G →K be a hash function. We say that the Hash Diﬃe-Hellman (HDH) assumption
holds for ( G,H) if the distribution
(
gα,gβ,H(gβ,gαβ)
)
is computationally indistinguishable from
the distribution (gα,gβ,k) where α,β ←R Zq and k←R K.
(a) Show that if H is modeled as a random oracle and the CDH assumption holds for G, then
the HDH assumption holds for ( G,H).
(b) Show that if H is a secure KDF and the DDH assumption holds for G, then the HDH
assumption holds for ( G,H).
(c) Prove that the ElGamal public-key encryption scheme EEG is semantically secure if the HDH
assumption holds for ( G,H).
11.15 (Anonymous public-key encryption). Suppose t people publish their public-keys
pk1,..., pkt. Alice sends an encrypted message to one of them, say pk5, but she wants to en-
sure that no one (other than user 5) can tell which of the tusers is the intended recipient. You may
assume that every user, other than user 5, who tries to decrypt Alice’s message with their secret
key, obtains fail.
(a) Deﬁne a security model that captures this requirement. The adversary should be given t
public keys pk1,..., pkt and it then selects the message m that Alice sends. Upon receiving
a challenge ciphertext, the adversary should learn nothing about which of the t public keys
is the intended recipient. A system that has this property is said to be an anonymous
public-key encryption scheme.
(b) Show that the ElGamal public-key encryption system EEG is anonymous.
(c) Show that the RSA public-key encryption system ERSA is not anonymous. Assume that all t
public keys are generated using the same RSA parameters ℓ and e.
11.16 (Proxy re-encryption). Bob works for the Acme corporation and publishes a public-key
pkbob so that all incoming emails to Bob are encrypted under pkbob. When Bob goes on vacation
he instructs the company’s mail server to forward all his incoming encrypted email to Alice. Alice’s
public key is pkalice. The mail server needs a way to translate an email encrypted under public-key
pkbob into an email encrypted under public-key pkalice. This would be easy if the mail server had
skbob, but then the mail server can read all of Bob’s incoming email.
Consider the variation E′
EG of EEG, in which we only hash w instead of ( v,w) (see Remark 11.1).
Suppose that pkbob and pkalice are public keys for E′
EG. Then the mail server can do the translation
from pkbob to pkalice while learning nothing about the email contents.
(a) Suppose pkalice = gα and pkbob = gα′
. Show that giving τ := α′/α to the mail server
lets it translate an email encrypted under pkbob into an email encrypted under pkalice, and
vice-versa.
(b) Assume that E′
EG is semantically secure. Show that the adversary cannot break semantic
security for Alice, even if it is given Bob’s public key gα′
along with the translation key τ.
11.17 (Online-oﬄine oblivious transfer). In Section 11.6 we looked at protocols for oblivious
transfer where one party, a sender, has messages m0,...,m n−1 ∈M = {0,1}ℓ and another party, a
463
receiver, has an index i∈Zn. At the end of the protocol the receiver hasmi and neither party learns
anything else about the other party’s data. Suppose that the sender and receiver had previously
established a random OT instance: the sender has random x0,...,x n−1 ←R Mand the recipient
has (j,xj) for some random j ←R Zn. This random data is called an OT correlation. Let’s show
that the parties can quickly solve the OT problem using a random OT correlation:
Sender has: ( m1,...,m n−1), (x1,...,x n−1), Receiver has: ( i,j,x j).
Receiver wants mi.
• receiver: compute ∆ ←(j−i) ∈Zn and send ∆ to the sender.
• sender: for all u = 0 ,1,...,n −1 compute cu ←mu ⊕x(u+∆) and send C :=
(c0,...,c n−1) to the receiver. Here each index ( u+ ∆) is an element in Zn.
• receiver: output ci ⊕xj.
(a) Show that when both parties honestly follow the protocol, the receiver learns the required
value mi.
(b) Prove that a (possibly malicious) sender learns nothing about the receiver’s index i. Similarly,
a (possibly malicious) receiver cannot learn anything about any messages besides mi, where i
is deﬁned as i:= j−∆ ∈Zn.
(c) A protocol between the sender and the receiver that constructs a random OT correlation,
but where the recipient can choose the j ∈Zn that it wants, is called a Random OT or
ROT protocol. ROT is discussed in more detail in Section 23.7. Adapt the protocol in this
exercise to show that an ROT protocol implies an OT protocol.
11.18 (All-but-one oblivious transfer). All-but-one OT refers to the following problem: a
sender has messages m1,...,m n ∈M and a receiver has an index i∈{1,...,n }. At the end of the
protocol the receiver should have all of m1,...,m n except for message mi. Neither party should
learn anything else about the other party’s inputs. Here is a simple solution using a CPA secure
cipher (E,D) deﬁned over (K,M,C):
• sender: choose a random k in Kand compute cj ←R E(k,mj) for j = 1,...,n .
• both: the sender and receiver engage in n independent 1-out-of-2 oblivious transfers. In OT
instance number j = 1,...,n , the sender’s input data is the pair ( cj,k), and the receiver’s
input data is a bit bj in {0,1}.
(a) Explain why the receiver can learn at most n−1 of the sender’s messages, and why the sender
learns nothing about i.
(b) Generalize the protocol to allow the receiver to learn at most n−2 of the sender’s messages,
where the sender learns nothing about which ones.
Hint: try using n independent 1-out-of-3 oblivious transfers.
Discussion: The all-but-one OT protocol described in this problem make use of n executions of
1-out-of-2 oblivious transfer. It is possible to implement all-but-one OT using only log2 nexecutions
of 1-out-of-2 oblivious transfer [142, Sec. 3].
464
11.19 (Expanding oblivious transfer). Suppose we are given a 1-out-of-2 oblivious transfer
protocol Π. We wish to construct a 1-out-of- n oblivious transfer protocol, for n >2, using only
⌈log2 n⌉parallel executions of the 1-out-of-2 OT protocol Π. For simplicity assume n= 2t, for some
t> 1. We will need a hash function H : Kt →K′, where K:= {0,1}w, and a cipher ( E,D) deﬁned
over (K′,M,C). The 1-out-of- n OT protocol Π′works as follows:
Sender has: m0,...,m n−1 ∈M, Receiver has: i∈{0,...,n −1}.
Receiver wants mi.
• Setup: The receiver computes the binary representation of i, namely b0,...,b t−1 ∈{0,1}so
that i= ∑t−1
j=0 2jbj. The sender chooses random kj[0],kj[1] ←R Kfor j = 0,...,t −1.
• OT phase: The sender and receiver engage in t = log 2 n parallel instances of the 1-out-
of-2 OT protocol Π, where in instance number j, the receiver’s input is bj ∈ {0,1}, and
the sender’s input is ( kj[0],kj[1]) ∈ K2. At the end of the t protocols, the receiver has
k0[b0],...,k t−1[bt−1] ∈K.
• Sender: For ℓ= 0,...,n −1, the sender does:
– let d0,...,d t−1 ∈{0,1}be the binary representation of ℓ,
– compute kℓ ←H
(
k0[d0],...,k t−1[dt−1]
)
∈K′,
– set cℓ ←R E(kℓ,mℓ).
Send c= (c0,...,c n−1) to the receiver.
• Receiver: set ˆki ←H
(
k0[b0],...,k t−1[bt−1]
)
∈K′and outputs ˆm←D(ˆki,ci).
(a) Show that protocol Π ′is correct so that ˆm= mi.
(b) Let’s prove a minimal security property for Π ′: the receiver learns nothing about mj for
j ̸= i. More precisely, consider an eﬃcient adversary that outputs i, and two tuples m =
(m0,...,m n−1) and m′= (m′
0,...,m ′
n−1), with mi = m′
i. The adversary then plays the role
of receiver with input i. We say that Π ′ is secure if the adversary cannot distinguish the
message cfrom the sender when the sender takes mas input versus when the sender takes
m′as input. Show that when H is modeled as a random oracle, (E,D) is semantically secure,
and the 1-out-of-2 OT protocol Π is secure, then the 1-out-of- n OT protocol Π′is secure.
Discussion: This protocol runs many instances of 1-out-of-2 OT. In Section 23.7 we will show
how to eﬃciently construct many instances of 1-out-of-2 OT, as needed here, using a technique
called OT extension.
11.20 (An attack on oblivious transfer). The ﬁrst OT protocol presented in Section 11.6 is
subject to a kind of “man in the middle” attack, when multiple instances of the protocol may run
concurrently. However, the protocol is easily repaired to prevent this. Suppose we run two OT
instances of the protocol: in one instance the adversary plays the role of receiver and interacts with
an honest sender; in the other instance the adversary plays the role of sender and interacts with
an honest receiver. The adversary relays all messages unchanged between the honest sender and
honest receiver. However, when the honest receiver sends u:= gαv−i, the adversary sends ˆu:= u·v
to the honest sender.
(a) Show that when the two OT protocols complete, the receiver incorrectly obtains the message
mi−1 instead of mi. You may assume i> 1.
465
(b) Now suppose we modify the protocol along the lines discussed on Remark 11.2. Explain why
the attack does not work any more.
Discussion: One of the reasons we use secure channels in this protocol is to prevent the adversary
from modifying protocol messages. However, even with secure channels, the protocol as stated is
subject to a “man in the middle attack” as illustrated in this exercise. Here, the honest sender
and honest receiver are talking to the adversary in two separate protocol instances, and we would
ideally like these two protocol instances to run relatively independently of each other, without any
“strange interactions” between them, such as that illustrated in this exercise. This type of subtle
attack, which arises when we run protocol instances concurrently, is one reason why we need better
security deﬁnitions for interactive protocols, which we will discuss in Section 23.5.
11.21 (Group key exchange). Suppose n parties A1,...,A n want to setup a group secret key
that they can use for encrypted group messaging. They have at their disposal a public bulletin board
that they can all post messages to and whose contents are public. No peer-to-peer communication
is allowed. Your goal is to design a group key exchange that is secure against passive eavesdropping.
(a) Use a public-key encryption scheme ( G,E,D ) to design a protocol that runs in two rounds.
One party, say A1, reads and writes nvalues to and from the bulletin board. All other parties
read and write only one value to the bulletin board.
(b) Deﬁne a security model for a group key exchange protocol that ensures security against a
passive eavesdropper. Prove security of your protocol from part (a) assuming ( G,E,D ) is
semantically secure.
(c) Design a fairer protocol where the work is evenly distributed. Speciﬁcally, design a protocol
based on two-party Diﬃe-Hellman that runs in at most k rounds, assuming n = 2k. Let G
be a group of prime order q with generator g ∈G and let H : G →Zq be a hash function.
In each round every party posts at most one group element in G to the bulletin board and
reads at most one group element from the bulletin board. At the end of the protocol, after
k rounds, all parties obtain the same secret key and there are at most 2 n group elements on
the bulletin board. You may assume that the parties know each other’s identities and can
order themselves lexicographically by identity.
Hint: think of an n-leaf binary tree where each leaf corresponds to one party.
(d) Prove security of your protocol from part (c) against a passive eavesdropper assuming CDH
holds in G and H is modeled as a random oracle.
(e) Suppose one of the n parties has a malfunctioning random number generator — whenever
that party needs to sample a random value, its random number generator always returns “5”.
Show that this will sink the entire protocol from parts (a) and (c), even if all the other
participants have well functioning random number generators. Speciﬁcally, show that an
eavesdropper will learn the group secret key.
(f) Is it possible to design a group key exchange protocol that is secure even if at most one of
the participants suﬀers from the problem in part (e)?
466
Chapter 12
Chosen ciphertext secure public key
encryption
In Chapter 11, we introduced the notion of public-key encryption. We also deﬁned a basic form
of security called semantic security , which is completely analogous to the corresponding notion
of semantic security in the symmetric-key setting. We observed that in the public-key setting,
semantic security implies security against a chosen plaintext attack, i.e., CPA security.
In this chapter, we study the stronger notion of security against chosen ciphertext attack, or
CCA security. In the CPA attack game, the decryption key is never used, and so CPA security
provides no guarantees in any real-world setting in which the decryption key is actually used to
decrypt messages. The notion of CCA security is designed to model a wide spectrum of real-world
attacks, and it is considered the “gold standard” for security in the public-key setting.
We brieﬂy introduced the notion of CCA security in the symmetric-key setting in Section 9.2,
and the deﬁnition in the public-key setting is a straightforward translation of the deﬁnition in the
symmetric-key setting. However, it turns out CCA security plays a more fundamental role in the
public-key setting than in the symmetric-key setting.
12.1 Basic deﬁnitions
As usual, we formulate this notion of security using an attack game, which is a straightforward
adaptation of the CCA attack game in the symmetric setting (Attack Game 9.2) to the public-key
setting.
Attack Game 12.1 (CCA security). For a given public-key encryption scheme E= (G,E,D ),
deﬁned over (M,C), and for a given adversary A, we deﬁne two experiments.
Experiment b (b= 0,1):
• The challenger computes (pk,sk) ←R G() and sends pk to the adversary.
• Athen makes a series of queries to the challenger. Each query can be one of two types:
– Encryption query: for i= 1,2,..., the ith encryption query consists of a pair of messages
(mi0,mi1) ∈M2, of the same length. The challenger computes ci ←R E(pk,mib) and
sends ci to A.
467
– Decryption query: for j = 1 ,2,..., the jth decryption query consists of a ciphertext
ˆcj ∈C that is not among the responses to the previous encryption queries, i.e.,
ˆcj /∈{c1,c2,... }.
The challenger computes ˆmj ←D(sk,ˆcj), and sends ˆmj to A.
• At the end of the game, the adversary outputs a bit ˆb∈{0,1}.
Let Wb be the event that Aoutputs 1 in Experiment band deﬁne A’s advantage with respect
to Eas
CCAadv[A,E] :=
⏐⏐Pr[W0] −Pr[W1]
⏐⏐. 2
Deﬁnition 12.1 (CCA Security). A public-key encryption scheme E is called semantically
secure against a chosen ciphertext attack , or simply CCA secure, if for all eﬃcient adver-
saries A, the value CCAadv[A,E] is negligible.
Just as we did in the symmetric-key setting, we can consider a restricted attack game in which
the adversary makes only a single encryption query:
Deﬁnition 12.2 (1CCA security). In Attack Game 12.1, if the adversary Ais restricted to
making a single encryption query, we denote its advantage by 1CCAadv[A,E]. A public-key en-
cryption scheme E is one-time semantically secure against chosen ciphertext attack , or
simply, 1CCA secure, if for all eﬃcient adversaries A, the value 1CCAadv[A,E] is negligible.
Notice that if we strip away the decryption queries, 1CCA security corresponds to semantic
security, and CCA security corresponds to CPA security. We showed in Theorem 11.1 that semantic
security for a public-key encryption scheme implies CPA security. A similar result holds with respect
to chosen ciphertext security, namely, that 1CCA security implies CCA security.
Theorem 12.1. If a public-key encryption scheme Eis 1CCA secure, then it is also CCA secure.
In particular, for every CCA adversary Athat plays Attack Game 12.1 with respect to E, and
which makes at most Qe encryption queries to its challenger, there exists a 1CCA adversary B
as in Deﬁnition 12.2, where Bis an elementary wrapper around A, such that
CCAadv[A,E] = Qe ·1CCAadv[B,E].
The proof is a simple hybrid argument that is almost identical to that of Theorem 11.1, and
we leave the details as an easy exercise to the reader. Using another level of hybrid argument, one
can also extend this to the multi-key setting as well — see Exercise 12.6.
Since 1CCA security implies CCA security, if we want to prove that a particular public-key
encryption scheme is CCA secure, we will typically simply prove 1CCA security. So it will be
helpful to study the 1CCA attack game in a bit more detail. We can view the 1CCA attack game
as proceeding in a series of phases:
Initialization phase: the challenger generates (pk,sk) ←R G() and sends pk to the adversary.
Phase 1: the adversary submits a series of decryption queries to the challenger; each such query
is a ciphertext ˆc∈C, to which the challenger responds with ˆm←D(sk,ˆc).
468
Encryption query: the adversary submits a single encryption query ( m0,m1) to the challenger;
in Experiment b (where b= 0,1), the challenger responds with c←R E(pk,mb).
Phase 2: the adversary again submits a series of decryption queries to the challenger; each such
query is a ciphertext ˆc ∈C, subject to the restriction that ˆ c ̸= c, to which the challenger
responds with ˆm←D(sk,ˆc).
Finish: at the end of the game, the adversary outputs a bit ˆb∈{0,1}.
As usual, as discussed in Section 2.2.5, Attack Game 12.1 can be recast as a “bit guessing”
game, where instead of having two separate experiments, the challenger chooses b ∈ {0,1}at
random, and then runs Experiment b against the adversary A. In this game, we measure A’s bit-
guessing advantage CCAadv∗[A,E] (and 1CCA adv∗[A,E]) as |Pr[ˆb= b] −1/2|. The general result
of Section 2.2.5 applies here as well:
CCAadv[A,E] = 2 ·CCAadv∗[A,E]. (12.1)
And similarly, for adversaries restricted to a single encryption query, we have:
1CCAadv[A,E] = 2 ·1CCAadv∗[A,E]. (12.2)
12.2 Understanding CCA security
The deﬁnition of CCA security may seem rather unintuitive at ﬁrst. Indeed, one might ask: in the
attack game, why can the adversary get any message decrypted except the ones he really wants
to decrypt? One answer is that without this restriction, it would be impossible to satisfy the
deﬁnition. However, this is not a very satisfying answer, and it begs the question as to whether the
entire deﬁnitional framework makes sense.
In this section, we explore the deﬁnition of CCA security from several angles. Hopefully, by the
end, the reader will understand why this deﬁnition makes sense, and what it is good for.
12.2.1 CCA security and ciphertext malleability
Our ﬁrst example illustrates an important property of CCA secure systems: they are non-
malleable. We will not formally deﬁne it here, but very roughly speaking, non-malleability means
that given the encryptions of several unknown messages, it is infeasible for an adversary to cre-
ate new ciphertexts that decrypt to messages that bear some speciﬁc relation with the original
messages.
Consider a professor, Bob, who collects homework by email. Moreover, assume that Bob gen-
erates a public key/secret key pair ( pk,sk) for a public-key encryption scheme, and gives pk to all
of his students. When a student Alice submits an email, she encrypts it under pk.
To make things concrete, suppose that the public-key encryption scheme is the semantically
secure scheme ETDF presented in Section 11.4, which is based on a trapdoor function along with
some symmetric cipher Es. The only requirement on Es is that it is semantically secure, so let us
assume that Es is a stream cipher (such as AES in counter mode).
When Alice encrypts the email message m containing her homework using ETDF and pk, the
resulting ciphertext is of the form ( y,c), where y = F(pk,x) and c= G(H(x)) ⊕m. Here, H is a
hash function and G is a PRG.
469
As we saw in Section 3.3.2, any stream cipher is extremely malleable, and the public-key scheme
ETDF inherits this weakness. In particular, an attacker Molly can do essentially the same thing
here as she did in Section 3.3.2. Namely, assuming that Alice’s email message m starts with the
header From:Alice, by ﬂipping a few bits of the symmetric-key ciphertext c, Molly obtains another
ciphertext c′that decrypts (under the same symmetric key) to a message m′that is identical to m,
except that the header now reads From:Molly. Molly can do this without knowing anything at all
about Alice’s email message other than the header.
Using the above technique, Molly can “steal” Alice’s homework as follows. She intercepts Alice’s
ciphertext (y,c). She then modiﬁes the symmetric-key ciphertext cto obtain c′as above, and sends
the public-key ciphertext (y,c′) to Bob. Now, when Professor Bob decrypts (y,c′), he will essentially
see Alice’s homework, but Bob will mistakenly think that the homework was submitted by Molly,
and give Molly credit for it.
The attack described so far is a good example of a chosen ciphertext attack, which could not
succeed if the public-key encryption scheme were actually CCA secure. Indeed, if given ( y,c) it is
possible for Molly to create a new ciphertext ( y,c′) where the header From:Alice is changed to
From:Molly, then the system cannot be CCA secure. For such a system, we can design a simple
CCA adversary Athat has advantage 1 in the CCA security game. Here is how.
• Create a pair of messages, each with the same header, but diﬀerent bodies. Our adversary A
submits this pair as an encryption query, obtaining ( y,c).
• Athen uses Molly’s algorithm to create a ciphertext ( y,c′), which should encrypt a message
with a diﬀerent header but the same body.
• Athen submits ( y,c′) as a decryption query, and outputs 0 or 1, depending on which body
it sees.
As we have shown, if Alice encrypts her homework using a CCA-secure system, she is assured
that no one can steal her homework by modifying the ciphertext she submitted. CCA security,
however, does not prevent all attacks on this homework submission system. An attacker can
maliciously submit a homework on behalf of Alice, and possibly hurt her grade in the class. Indeed,
anyone can send an encrypted homework to the professor, and in particular, a homework that
begins with From:Alice. Preventing this type of attack requires tools that we will develop later.
In Section 13.7, where we develop the notion of signcryption, which is one way to prevent this
attack.
12.2.2 CCA security vs. authentication
When we ﬁrst encountered the notion of CCA security in the symmetric-key setting, back in
Section 9.2, we saw that CCA security was implied by AE security, i.e., ciphertext integrity plus
CPA security. Moreover, we saw that ciphertext integrity could be easily added to any CPA-secure
encryption scheme using the encrypt-then-MAC method. We show here that this does not work in
the public-key setting: simply adding an authentication wrapper does not make the system CCA
secure.
Consider again the homework submission system example in the previous section. If we start
with a scheme, like ETDF, which is not itself CCA secure, we might hope to make it CCA secure
using encrypt-then-MAC: Alice wraps the ciphertext (y,c) with some authentication data computed
from (y,c). Say, Alice computes a MAC tag tover (y,c) using a secret key that she shares with Bob
470
and sends (y,c,t ) to Bob (or, instead of a MAC, she computes a digital signature on (y,c), a concept
discussed in Chapter 13). Bob can check the authentication data to make sure the ciphertext was
generated by Alice. However, regardless of the authentication wrapper used, Molly can still carry
out the attack described in the previous section. Here is how. Molly intercepts Alice’s ciphertext
(y,c,t ), and computes ( y,c′) exactly as before. Now, since Molly is a registered student in Bob’s
course, she presumably is using the same authentication mechanism as all other students, so she
simply computes her own authentication tag t′on ciphertext (y,c′) and sends (y,c′,t′) to Bob. Bob
receives (y,c′,t′), and believes the authenticity of the ciphertext. When Bob decrypts ( y,c′), the
header From:Molly will look perfectly consistent with the authentication results.
What went wrong? Why did the strategy of authenticating ciphertexts provide us with CCA
security in the symmetric-key setting, but not in the public-key setting? The reason is simply
that in the public-key setting, anyone is allowed to send an encrypted message to Bob using Bob’s
public key. The added ﬂexibility that public-key encryption provides makes it more challenging to
achieve CCA security, yet CCA security is vital for security in real-world systems. (We will discuss
in detail how to securely combine CCA-secure public-key encryption and digital signatures when
we discuss signcryption in Section 13.7.)
12.2.3 CCA security and key escrow
Consider again the key escrow example discussed in Section 11.1.2. Recall that in that example,
Alice encrypts a ﬁle f using a symmetric key k. Among other things, Alice stores along with the
encrypted ﬁle an escrow of the ﬁle’s encryption key. Here, the escrow is an encryption cES of k
under the public key of some escrow service. If Alice works for some company, then if need be,
Alice’s manager or other authorized entity can retrieve the ﬁle’s encryption key by presenting cES
to the escrow service for decryption.
If the escrow service uses a CCA-secure encryption scheme, then it is possible to implement an
access control policywhich can guard against potential abuse. This can be done as follows. Suppose
that in forming the escrow-ciphertext cES, Alice encrypts the pair ( k,h) under the escrow service’s
public key, where his a collision-resistant hash of the metadata md associated with the ﬁle f: this
might include the identity of owner of the ﬁle (Alice, in this case), the name of the ﬁle, the time
that it was created and/or modiﬁed. Let us also assume that all of this metadata md is stored on
the ﬁle system in the clear along with the encrypted ﬁle.
Now suppose a requesting entity presents the escrow-ciphertext cES to the escrow service, along
with the corresponding metadata md. The escrow service may impose some type of access control
policy, based on the given metadata, along with the identity or credentials of the requesting entity.
Such a policy could be very speciﬁc to a particular company or organization. For example, the
requesting entity may be Alice’s manager, and it is company policy that Alice’s manager should
have access to all ﬁles owned by Alice. Or the requesting entity may be an external auditor that is
to have access to all ﬁles created by certain employees on a certain date.
To actually enforce this access control policy, not only must the escrow service verify that the
requesting entity’s credentials and the supplied metadata conform to the access control policy, the
escrow service must also perform the following check: after decrypting the escrow-ciphertext cES
to obtain the pair ( k,h), it must check that h matches the hash of the metadata supplied by the
requesting entity. Only if these match does the escrow service release the key k to the requesting
entity.
This type of access control can prevent certain abuses. For example, consider the external
471
auditor who has the right to access all ﬁles created by certain employees on a certain date. Suppose
the auditor himself is a bit too nosy, and during the audit, wants to ﬁnd out some information in a
personal ﬁle of Alice that is not one of the ﬁles targeted by the audit. The above implementation
of the escrow service, along with CCA security, ensures that the nosy auditor cannot obtain this
unauthorized information. Indeed, suppose cES is the escrow-ciphertext associated with Alice’s
personal ﬁle, which is not subject to the audit, and that this ﬁle has metadata md. Suppose the
auditor submits a pair ( c′
ES,md′) to the escrow service. There are several cases to consider:
• if md′= md, then the escrow service will reject the request, as the metadata md of Alice’s
personal ﬁle does not ﬁt the proﬁle of the audit;
• if md′̸= md and c′
ES = cES, then the collision resistance of the hash ensures that the escrow
service will reject the request, as the hash embedded in the decryption of c′
ES will not match
the hash of the supplied metadata md′;
• if md′̸= md and c′
ES ̸= cES, then the escrow service may or may not accept the request, but
even if it does, CCA security and the fact that c′
ES ̸= cES ensures that no information about
the encryption key for Alice’s personal ﬁle is revealed.
This implementation of an escrow service is pretty good, but it is far from perfect:
• It assumes that Alice follows the protocol of actually encrypting the ﬁle encryption key along
with the correct metadata. Actually, this may not be such an unreasonable assumption, as
these tasks will be performed automatically by the ﬁle system on Alice’s behalf, and so it
may not be so easy for a misbehaving Alice to circumvent this protocol.
• It assumes that the requesting entity and the escrow service do not collude.
Treating the metadata as associated data. In Section 12.7 we deﬁne public-key encryption
with associated data, which is the public-key analogue of symmetric encryption with associated
data from Section 9.5. Here the public-key encryption and decryption algorithms take a third
input called associated data. The point is that decryption reveals no useful information if the given
associated data used in decryption is diﬀerent from the one used in encryption.
The metadata information md in the escrow system above can be treated as associated data,
instead of appending it to the plaintext. This will result in a smaller ciphertext while achieving the
same security goals. In fact, associating metadata to a ciphertext for the purpose described above
is a very typical application of associated data in a public-key encryption scheme.
12.2.4 Encryption as an abstract interface
To conclude our motivational discussion of CCA security we show that it abstractly captures a
“correct” and very natural notion of security. We do this by describing encryption as an abstract
interface, as discussed in Section 9.3 in the symmetric case.
The setting is as follows. We have a sender S and receiver R, who are participating in some
protocol, during which S drops messages m1,m2,... into his out-box, and R retrieves messages
from his in-box. While S and R do not share a secret key, we assume that R has generated public
key/secret key pair (pk,sk), and that S knows R’s public key pk.
472
That is the abstract interface. In a real implementation, when mi is placed in S’s out-box, it is
encrypted under pk, yielding a corresponding ciphertext ci, which is sent over the wire to R. On
the receiving end, when a ciphertext ˆc is received at R’s end of the wire, it is decrypted using sk,
and if the decryption is a message ˆm̸= reject, the message ˆm is placed in R’s in-box.
Note that while we are syntactically restricting ourselves to a single sender S, this restriction is
superﬁcial: in a system with many users, all of them have access to R’s public key, and so we can
model such a system by allowing all users to place messages in S’s out-box.
Just as in Section 9.3, an attacker may attempt to subvert communication in several ways:
• The attacker may drop, re-order, or duplicate the ciphertexts sent by S.
• The attacker may modify ciphertexts sent by S, or inject ciphertexts computed in some
arbitrary fashion.
• The attacker may have partial knowledge — or even inﬂuence the choice — of the messages
sent by S.
• The attacker can obtain partial knowledge of some of the messages retrieved by R, and
determine if a given ciphertext delivered to R was rejected.
We now describe an ideal implementation of this interface. It is slightly diﬀerent from the ideal
implementation in Section 9.3 — in that section, we were working with the notion of AE security,
while here we are working with the notion of CCA security. When S drops mi in its out-box,
instead of encrypting mi, the ideal implementation creates a ciphertext ci by encrypting a dummy
message dummyi, that has nothing to do with mi (except that it should be of the same length).
Thus, ci serves as a “handle” for mi, but does not contain any information about mi (other than its
length). When ci arrives at R, the corresponding message mi is magically copied from S’s out-box
to R’s in-box. If a ciphertext ˆ c arrives at R that is not among the previously generated ci’s, the
ideal implementation decrypts ˆc using sk as usual .
CCA security implies that this ideal implementation of the service is for all practical purposes
equivalent to the real implementation. In the ideal implementation, we see that messages magically
jump from S to R, in spite of any information the adversary may glean by getting R to decrypt
other ciphertexts — the ciphertexts generated by S in the ideal implementation serve simply as
handles for the corresponding messages, but do not carry any other useful information. Hopefully,
analyzing the security properties of a higher-level protocol will be much easier using this ideal
implementation.
Note that even in the ideal implementation, the attacker may still drop, re-order, or dupli-
cate ciphertexts, and these will cause the corresponding messages to be dropped, re-ordered, or
duplicated. A higher-level protocol can easily take measures to deal with these issues.
We now argue informally that when Eis CCA secure, the real world implementation is indis-
tinguishable from the ideal implementation. The argument is similar to that in Section 9.3. It
proceeds in two steps, starting with the real implementation, and in each step, we make a slight
modiﬁcation.
• First, we modify the real implementation of R’s in-box, as follows. When a ciphertext ˆ c
arrives on R’s end, the list of ciphertexts c1,c2,... previously generated by S is scanned, and
if ˆc = ci, then the corresponding message mi is magically copied from S’s out-box into R’s
in-box, without actually running the decryption algorithm.
473
The correctness property of Eensures that this modiﬁcation behaves exactly the same as the
real implementation. Note that in this modiﬁcation, any ciphertext that arrives at R’s end
that is not among the ciphertexts previously generated by S will be decrypted as usual using
sk.
• Second, we modify the implementation ofS’s out-box, replacing the encryption of mi with the
encryption of dummyi. The implementation of R’s in-box remains as in the ﬁrst modiﬁcation.
Here is where we use the CCA security property: if the attacker could distinguish the second
modiﬁcation from the ﬁrst, we could use the attacker to break the CCA security of E.
Since the second modiﬁcation is identical to the ideal implementation, we see that the real and
ideal implementations are indistinguishable from the adversary’s point of view.
Just as in Section 9.3, we have ignored the possibility that theci’s generated bySare not unique.
Certainly, if we are going to view the ci’s as handles in the ideal implementation, uniqueness would
seem to be an essential property. Just as in the symmetric case, CPA security (which is implied by
CCA security) guarantees that the ci’s are unique with overwhelming probability (the reader can
verify that the result of Exercise 5.12 holds in the public-key setting as well).
12.3 CCA-secure encryption from trapdoor function schemes
We now turn to constructing CCA-secure public-key encryption schemes. We begin with a construc-
tion from a general trapdoor function scheme satisfying certain properties. We use this to obtain
a CCA-secure system from RSA. Later, in Section 12.6, we will show how to construct suitable
trapdoor functions (in the random oracle model) from arbitrary, CPA-secure public-key encryp-
tion schemes. Using the result in this section, all these trapdoor functions give us CCA-secure
encryption schemes.
Consider again the public-key encryption scheme ETDF = (G,E,D ) discussed in Section 11.4,
which is based on an arbitrary trapdoor function scheme T = (G,F,I ), deﬁned over (X,Y). Let us
brieﬂy recall this scheme: it makes use of a symmetric cipher Es = (Es,Ds), deﬁned over (K,M,C),
and a hash function H : X→K , which we model as a random oracle. The message space for ETDF
is Mand the ciphertext space is Y×C . The key generation algorithm for ETDF is the same as the
key generation algorithm for T, and encryption and decryption work as follows:
E(pk,m) := x←R X, y←F(pk,x), k←H(x), c←R Es(k,m)
output (y,c);
D(sk, (y,c) ) := x←I(sk,y), k←H(x), m←Ds(k,c)
output m.
If X ̸= Y, that is, if T is not a trapdoor permutation scheme, we have to modify the scheme
slightly to get a scheme that is CCA secure. Basically, we modify the decryption algorithm to
explicitly check that the given value y ∈Y is actually in the image of F(pk,·). So the scheme we
will analyze is E′
TDF = (G,E,D ′), where
D′(sk, (y,c) ) := x←I(sk,y)
if F(pk,x) = y
then k←H(x), m←Ds(k,c)
else m←reject
output m.
474
We will prove that E′
TDF is CCA secure if we model H as a random oracle, under appropriate
assumptions. The ﬁrst assumption we will make is that Es is 1CCA secure (see Section 9.6). We
also have to assume that T is one-way. However, when X ̸= Y, we need a somewhat stronger
assumption: that T is one-way even given access to an “image oracle”. Essentially, this means that
given pk and y = F(pk,x) for randomly chosen x∈X, it is hard to compute x, even given access
to an oracle that will answer arbitrary questions of the form “does a given ˆy∈Y lie in the image of
F(pk,·)?”. We formalize this notion by giving an attack game that is similar to Attack Game 10.2,
but where the adversary has access to an image oracle.
Attack Game 12.2 (One-way trapdoor function scheme even with image oracle). For a
given trapdoor function scheme T = (G,F,I ), deﬁned over ( X,Y), and a given adversary A, the
attack game runs as follows:
• The challenger computes
(pk,sk) ←R G(), x ←R X, y ←F(pk,x)
and sends (pk,y) to the adversary.
• The adversary makes a series of image oracle queries to the challenger. Each such query is
of the form ˆy ∈Y , to which the challenger replies “yes” if F(pk,I(sk,ˆy)) = ˆy, and “no”
otherwise.
• The adversary outputs ˆx∈X.
We deﬁne the adversary’s advantage in inverting T given access to an image oracle, denoted
IOWadv[A,T], to be the probability that ˆx= x. 2
Deﬁnition 12.3. We say that a trapdoor function scheme T is one way given an image oracle
if for all eﬃcient adversaries A, the quantity IOWadv[A,T] is negligible.
In Exercise 12.15 we show that (in the random oracle model) every one way trapdoor function
scheme can be easily converted into one that is one way given an image oracle.
The next theorem proves the CCA security of E′
TDF, assuming T is one-way given an image ora-
cle, Es is 1CCA secure (see Deﬁnition 9.6), and H is modeled as a random oracle. In Exercise 12.14
we explore an alternative analysis of this scheme under diﬀerent assumptions.
In proving this theorem, we just prove that E′
TDF is 1CCA secure (see Deﬁnition 12.2). By
virtue of Theorem 12.1, this is suﬃcient. Recall that in the random oracle model (see Section 8.10),
the function H is modeled as a random function Ochosen at random from the set of all functions
Funs[X,K]. This means that in the random oracle version of the 1CCA attack game, the challenger
chooses Oat random. In any computation where the challenger would normally evaluate H, it
evaluates Oinstead. In addition, the adversary is allowed to ask the challenger for the value of the
function Oat any point of its choosing. The adversary may make any number of such “random
oracle queries” at any time of its choosing, arbitrarily interleaved with its usual encryption and
decryption queries. We use 1CCA roadv[A,E′
TDF] to denote A’s advantage against E′
TDF in the
random oracle version of the 1CCA attack game.
Theorem 12.2. Assume H : X →Kis modeled as a random oracle. If T is one-way given an
image oracle, and Es is 1CCA secure, then E′
TDF is CCA secure.
475
In particular, for every 1CCA adversary Athat attacks E′
TDF as in the random oracle version of
Deﬁnition 12.2, there exist an inverting adversary Biow that breaks the one-wayness assumption
for T as in Attack Game 12.2, and a 1CCA adversary Bs that attacks Es as in Deﬁnition 9.6,
where Biow and Bs are elementary wrappers around A, such that
1CCAroadv[A,E′
TDF] ≤2 ·IOWadv[Biow,T] + 1CCAadv[Bs,Es]. (12.3)
For applications of this theorem in the sequel, we record here some further technical properties
that the adversary Biow satisﬁes.
If Amakes at most Qd decryption queries, then Biow makes at most Qd image-oracle queries.
Also, the only dependence of Biow on the function F is that it invokes F(pk,·) as a subroutine,
at most Qro times, where Qro is a bound on the number of random-oracle queries made by A;
moreover, if Biow produces an output ˆx, it always evaluates F(pk,·) at ˆx.
Proof idea. The crux of the proof is to show that the adversary’s decryption queries do not help
him in any signiﬁcant way. What this means technically is that we have to modify the challenger so
that it can compute responses to the decryption queries without using the secret key sk . The trick
to achieve this is to exploit the fact that our challenger is in charge of implementing the random
oracle, maintaining a table of all input/output pairs. Assume the target ciphertext (i.e., the one
resulting from the encryption query) is ( y,c), where y = F(pk,x), and suppose the challenger is
given a decryption query (ˆy,ˆc), where y̸= ˆy= F(pk,ˆx).
• If the adversary has previously queried the random oracle at ˆ x, and if ˆk was the output of
the random oracle at ˆx, then the challenger simply decrypts ˆc using ˆk.
• Otherwise, if the adversary has not made such a random oracle query, then the challenger
does not know the correct value of the symmetric key — but neither does the adversary. The
challenger is then free to choose a key ˆkat random, and decrypt ˆcusing this key; however, the
challenger must do some extra book-keeping to ensure consistency, so that if the adversary
ever queries the random oracle in the future at the point ˆx, then the challenger “back-patches”
the random oracle, so that its output at ˆx is set to ˆk.
We also have to deal with decryption queries of the form ( y,ˆc), where ˆc ̸= c. Intuitively, under
the one-wayness assumption for T, the adversary will never query the random oracle at x, and so
from the adversary’s point of view, the symmetric key k used in the encryption query, and used in
decryption queries of the form ( y,ˆc), is as good as random, and so CCA security for E′
TDF follows
immediately from 1CCA security for Es.
In the above, we have ignored ciphertext queries of the form (ˆy,ˆc) where ˆyhas no preimage under
F(pk,·). The real decryption algorithm rejects such queries. This is why we need to assume T is
one-way given an image oracle — in the reduction, we need this image oracle to reject ciphertexts
of this form. 2
Proof. It is convenient to prove the theorem using the bit-guessing versions of the 1CCA attack
games. We prove:
1CCAroadv∗[A,E′
TDF] ≤IOWadv[Biow,T] + 1CCAadv∗[Bs,Es]. (12.4)
Then (12.3) follows by (12.2) and (9.2).
476
As usual, we deﬁne Game 0 to be the game played between Aand the challenger in the bit-
guessing version of the 1CCA attack game with respect to E′
TDF. We then modify the challenger to
obtain Game 1. In each game, b denotes the random bit chosen by the challenger, while ˆb denotes
the bit output by A. Also, for j = 0,1, we deﬁne Wj to be the event that ˆb= b in Game j.
Game 0. The logic of the challenger is shown in Fig. 12.1. The challenger has to respond to
random oracle queries, in addition to encryption and decryption queries. The adversary can make
any number of random oracle queries, and any number of decryption queries, but at most one
encryption query. Recall that in addition to direct access to the random oracle via explicit random
oracle queries, the adversary also has indirect access to the random oracle via the encryption and
decryption queries, where the challenger also makes use of the random oracle. In the initialization
step, the challenger computes (pk,sk) ←R G(); we also have our challenger make those computations
associated with the encryption query that can be done without yet knowing the challenge plaintext.
To facilitate the proof, we want our challenger to use the secret key sk as little as possible in
processing decryption queries. This will motivate a somewhat nontrivial strategy for implementing
the decryption and random oracle queries.
As usual, we will make use of an associative array to implement the random oracle. In the
proof of Theorem 11.2, which analyzed the semantic security of ETDF, we did this quite naturally
by using an associative array Map : X→K . We could do the same thing here, but because we want
our challenger to use the secret key as little as possible, we adopt a diﬀerent strategy. Namely, we
will represent the random oracle using associative array Map′: Y→K , with the convention that
if the value of the oracle at ˆx ∈X is equal to ˆk ∈K, then Map′[ˆy] = ˆk, where ˆy = F(pk,ˆx). We
will also make use of an associative array Pre : Y→X that is used to track explicit random oracle
queries made by the adversary: if Pre[ˆy] = ˆx, this means that the adversary queried the oracle at
the point ˆx, and ˆy= F(pk,ˆx). Note that Map′will in general be deﬁned at points other than those
at which Pre is deﬁned, since the challenger also makes random oracle queries.
In preparation for the encryption query, in the initialization step, the challenger precomputes
x←R X, y←F(pk,x), k←R K. It also sets Map′[y] ←k, which means that the value of the random
oracle at x is equal to k. Also note that in the initialization step, the challenger sets c←⊥, and
in processing the encryption query, overwrites c with a ciphertext in C. Thus, decryption queries
processed while c= ⊥are phase 1 queries, while those processed while c̸= ⊥are phase 2 queries.
To process a decryption query (ˆy,ˆc), making minimal use of the secret key, the challenger uses
the following strategy.
• If ˆy= y, the challenger just uses the prepared key k directly to decrypt ˆc.
• Otherwise, the challenger checks if Map′ is deﬁned at the point ˆy, and if not, it assigns to
Map′[ˆy] a random value ˆk. If ˆy has a preimage ˆx and Map′was not deﬁned at ˆy, this means
that neither the adversary nor the challenger previously queried the random oracle at ˆx, and
so this new random value ˆk represents the value or the random oracle at ˆx; in particular, if
the adversary later queries the random oracle at the point ˆ x, this same value of ˆk will be
used. If ˆy has no preimage, then assigning Map′[ˆy] a random value ˆk has no real eﬀect — it
just streamlines the logic a bit.
• Next, the challenger tests if ˆ y is in the image of F(pk,·). If ˆ y is not in the image, the
challenger just rejects the ciphertext. In Fig. 12.1, we implement this by invoking the function
477
initialization:
(pk,sk) ←R G(), x←R X, y←F(pk,x)
c←⊥
initialize empty associative arrays Pre : Y→X and Map′: Y→K
k←R K, b←R {0,1}
(1) Map′[y] ←k
send the public key pk to A;
upon receiving an encryption query ( m0,m1) ∈M2:
c←R Es(k,mb), send (y,c) to A;
upon receiving a decryption query (ˆy,ˆc) ∈X×C , where (ˆy,ˆc) ̸= (y,c):
if ˆy= y then
ˆm←Ds(k,ˆc)
else
if ˆy /∈Domain(Map′) then Map′[ˆy] ←R K
(2) if Image(pk,sk,ˆy) = “no” / / i.e., ˆy is not in the image of F(pk,·)
then ˆm←reject
else ˆk←Map′[ˆy], ˆm←Ds(ˆk,ˆc)
send ˆm to A;
upon receiving a random oracle query ˆx∈X:
ˆy←F(pk,ˆx), Pre[ˆy] ←ˆx
if ˆy /∈Domain(Map′) then Map′[ˆy] ←R K
send Map′[ˆy] to A
Figure 12.1: Game 0 challenger in the proof of Theorem 12.2
Image(pk,sk,ˆy). For now, we can think of Image as being implemented as follows:
Image(pk,sk,ˆy) :=
{
return “yes” if F(pk,I(sk,ˆy)) = ˆy and “no” otherwise
}
.
This is the only place where our challenger makes use of the secret key.
• Finally, if ˆy is in the range of F(pk,·), the challenger simply decrypts ˆ c directly using the
symmetric key ˆk = Map′[ˆy], which at this point is guaranteed to be deﬁned, and represents
the value of the random oracle at the preimage ˆx of ˆy. Note that our challenger can do this,
without actually knowing ˆx. This is the crux of the proof.
Despite this somewhat involved bookkeeping, it should be clear that our challenger behaves
exactly as in the usual attack game.
Game 1. This game is precisely the same as Game 0, except that we delete the line marked (1) in
Fig. 12.1.
Let Z be the event that the adversary queries the random oracle at x in Game 1. Clearly,
Games 0 and 1 proceed identically unless Z occurs, and so by the Diﬀerence Lemma, we have
|Pr[W1] −Pr[W0]|≤ Pr[Z]. (12.5)
478
If event Z happens, then at the end of Game 1, we have Pre[y] = x. What we want to do,
therefore, is use Ato build an eﬃcient adversary Biow that breaks the one-wayness assumption
for T with an advantage equal to Pr[ Z], with the help of an image oracle. The logic of Biow is
very straightforward. Basically, after obtaining the public key pk and y∈Y from its challenger in
Attack Game 12.2, Biow plays the role of challenger to Aas in Game 1. The value of x is never
explicitly used in that game (other than to compute y), and the value of the secret key sk is not
used, except in the evaluation of the Image function, and for this, Biow can use the image oracle
provided to it in Attack Game 12.2. At the end of the game, if y∈Domain(Pre), then Biow outputs
x= Pre[y]. It should be clear, by construction, that
Pr[Z] = IOWadv[Biow,T]. (12.6)
Finally, note that in Game 1, the key k is only used to encrypt the challenge plaintext, and to
process decryption queries of the form ( y,ˆc), where ˆc̸= c. As such, the adversary is essentially just
playing the 1CCA attack game against Es at this point. More precisely, we can easily derive an
eﬃcient 1CCA adversary Bs based on Game 1 that uses Aas a subroutine, such that
|Pr[W1] −1/2|= 1CCAadv∗[Bs,Es]. (12.7)
This adversary Bs generates (pk,sk) itself and uses sk to answer queries from A.
Combining (12.5), (12.6) and (12.7), we obtain (12.4). That completes the proof of the theorem.
2
12.3.1 Instantiating E′
TDF with RSA
Suppose we instantiate E′
TDF using RSA just as we did in Section 11.4.1. The underlying trapdoor
function is actually a permutation on Zn. This implies two things. First, we can omit the check in
the decryption algorithm that y is in the image of the trapdoor function, and so we end up with
exactly the same scheme ERSA as was presented in Section 11.4.1. Second, the implementation of
the image oracle in Attack Game 12.2 is trivial to implement, and so we end up back with Attack
Game 10.2. Theorem 12.2 specializes as follows:
Theorem 12.3. Assume H : X→K is modeled as a random oracle. If the RSA assumption holds
for parameters (ℓ,e), and Es is 1CCA secure, then ERSA is CCA secure.
In particular, for every 1CCA adversary Athat attacks ERSA as in the random oracle version
of Deﬁnition 12.2, there exist an RSA adversary Brsa that breaks the RSA assumption for (ℓ,e)
as in Attack Game 10.3, and a 1CCA adversary Bs that attacks Es as in Deﬁnition 9.6, where
Brsa and Bs are elementary wrappers around A, such that
1CCAroadv[A,ERSA] ≤2 ·RSAadv[Brsa,ℓ,e ] + 1CCAadv[Bs,Es].
12.4 CCA-secure ElGamal encryption
We saw that the basic RSA encryption schemeERSA could be shown to be CCA secure in the random
oracle model under the RSA assumption (and assuming the underlying symmetric cipher was
1CCA secure). It is natural to ask whether the basic ElGamal encryption scheme EEG, discussed in
Section 11.5, is CCA secure in the random oracle model, under the CDH assumption. Unfortunately,
479
this is not the case: it turns out that a slightly stronger assumption than the CDH assumption is
both necessary and suﬃcient to prove the security of EEG.
Recall the basic ElGamal encryption scheme, EEG = (G,E,D ), introduced in Section 11.5. It
is deﬁned in terms of a cyclic group G of prime order q generated by g ∈G, a symmetric cipher
Es = (Es,Ds), deﬁned over (K,M,C), and a hash function H : G2 →K. The message space of EEG
is Mand the ciphertext space is G ×C. Public keys are of the form u∈G and secret keys are of
the form α∈Zq. The algorithms G, E, and D are deﬁned as follows:
G() := α←R Zq, u←gα, pk ←u, sk ←α
output (pk,sk);
E(u,m) := β ←R Zq, v←gβ, w←uβ, k←H(v,w), c←R Es(k,m)
output (v,c);
D(α, (v,c) ) := w←vα, k←H(v,w), m←Ds(k,c)
output m.
To see why the CDH assumption by itself is not suﬃcient to establish the security of EEG
against chosen ciphertext attack, suppose the public key is u = gα. Now, suppose an adversary
selects group elements ˆvand ˆwin some arbitrary way, and computesˆk←H(ˆv, ˆw) and ˆc←R Es(ˆk, ˆm)
for some arbitrary message ˆm. Further, suppose the adversary can obtain the decryption m∗of the
ciphertext (ˆv,ˆc). Now, it is very likely that ˆm= m∗if and only if ˆw= ˆvα, or in other words, if and
only if (u,ˆv, ˆw) is a DH-triple. Thus, in the chosen ciphertext attack game, decryption queries can
be eﬀectively used by the adversary to answer questions of the form “is ( u,ˆv, ˆw) a DH-triple?” for
group elements ˆvand ˆwof the adversary’s choosing. In general, the adversary would not be able to
eﬃciently answer such questions on his own (this is the DDH assumption), and so these decryption
queries may potentially leak some information about the secret key α. Based on the current state
of our knowledge, this leakage does not seem to compromise the security of the scheme; however,
we do need to state this as an explicit assumption.
Intuitively, the interactive CDH assumption states that given a random instance ( gα,gβ)
of the DH problem, it is hard to compute gαβ, even when given access to a “DH-decision oracle”
that recognizes DH-triples of the form ( gα,·,·). More formally, this assumption is deﬁned in terms
of the following attack game.
Attack Game 12.3 (Interactive Computational Diﬃe-Hellman). Let G be a cyclic group
of prime order q generated by g∈G. For a given adversary A, the attack game runs as follows.
• The challenger computes
α,β ←R Zq, u←gα, v←gβ, w←gαβ
and gives (u,v) to the adversary.
• The adversary makes a sequence of DH-decision oracle queries to the challenger. Each query
is of the form (˜v, ˜w) ∈G2. Upon receiving such a query, the challenger tests if ˜vα = ˜w; if so,
he sends “yes” to the adversary, and otherwise, sends “no” to the adversary.
• Finally, the adversary outputs some ˆw∈G.
We deﬁne A’s advantage in solving the interactive computational Diﬃe-Hellman prob-
lem, denoted ICDHadv[A,G], as the probability that ˆw= w. 2
480
We stress that in the above attack game, the adversary can ask the challenger for help in
determining whether certain triples are DH-triples, but only triples of the form ( u,·,·), where u is
generated by the challenger.
Deﬁnition 12.4 (Interactive Computational Diﬃe-Hellman assumption). We say that the
interactive computational Diﬃe-Hellman (ICDH) assumption holds for G if for all eﬃcient
adversaries Athe quantity ICDHadv[A,G] is negligible.
By the above discussion, we see (at least heuristically) that the ICDH assumption is necessary
to establish the CCA security of EEG. Conversely, one can prove that EEG is CCA secure in the
random oracle model under the ICDH assumption (and assuming also that Es is 1CCA secure).
Remark 12.1 (Group membership veriﬁcation). To prove the CCA security of EEG, we must
insist that given a ciphertext (v,c), the decryption algorithm veriﬁes that v∈G. For example, if G
is a subgroup of Z∗
p of order q, where p is a large prime, the decryption algorithm should not only
check that v∈Z∗
p (which means, as an integer, it is in the range [1 ,p)), but should also check that
vq = 1 (which costs another exponentiation). Without this check, the scheme may be vulnerable to
a CCA attack (see Exercise 12.3). Later, in Chapter 15, we will see other cryptographically useful
groups (elliptic curves) where group membership veriﬁcation can be much less expensive. 2
Remark 12.2 (Hashing (v,w) vs hashing only w). Analogous to Remark 11.1, the variantE′
EG,
which hashes only w instead of ( v,w), is also CCA secure under the same assumptions; however,
the security reduction for EEG is simpler and more eﬃcient. 2
Theorem 12.4. Assume H : G2 →K is modeled as a random oracle. If the ICDH assumption
holds for G, and Es is 1CCA secure, then EEG is CCA secure.
In particular, for every 1CCA adversary Athat attacks EEG as in the random oracle version
of Deﬁnition 12.2, there exist an ICDH adversary Bicdh for G as in Attack Game 12.3, and
a 1CCA adversary Bs that attacks Es as in Deﬁnition 9.6, where Bicdh and Bs are elementary
wrappers around A, such that
1CCAroadv[A,EEG] ≤2 ·ICDHadv[Bicdh,G] + 1CCAadv[Bs,Es]. (12.8)
In addition, the number of DH-decision oracle queries made by Bicdh is bounded by the number
of random oracle queries made by A.
Proof. The basic structure of the proof is very similar to that of Theorem 12.2. As in that proof,
it is convenient to use the bit-guessing versions of the 1CCA attack games. We prove
1CCAroadv∗[A,EEG] ≤ICDHadv[Bicdh,G] + 1CCAadv∗[Bs,Es]. (12.9)
Then (12.8) follows by (12.2) and (9.2).
We deﬁne Games 0 and 1. Game 0 is the bit-guessing version of Attack Game 12.1 played by
Awith respect to EEG. In each game, b denotes the random bit chosen by the challenger, while ˆb
denotes the bit output by A. For j = 0,1, we deﬁne Wj to be the event that ˆb= b in Game j.
Game 0. The logic of the challenger is shown in Fig. 12.2. The adversary can make any number
of random oracle queries, and any number of decryption queries, but at most one encryption query.
As usual, in addition to direct access the random oracle via explicit random oracle queries, the
481
adversary also has indirect access to the random oracle via the encryption and decryption queries,
where the challenger also makes use of the random oracle.
In the initialization step, the challenger computes the secret key α ∈Zq and the public key
u = gα; it also makes those computations associated with the encryption query that can be done
without yet knowing the challenge plaintext. As in the proof of Theorem 12.2, we want our
challenger to use the secret key α as little as possible in processing decryption queries, and again,
we use a somewhat nontrivial strategy for implementing the decryption and random oracle queries.
Nevertheless, despite the signiﬁcant superﬁcial diﬀerences, this implementation will be logically
equivalent to the actual attack game.
As usual, we will implement the random oracle using an associative array Map : G2 →K.
However, we will also make use of an auxiliary associative array Map′ : G →K. The convention
is that if ( u,ˆv, ˆw) is a DH-triple, and the value of the random oracle at the point (ˆ v, ˆw) is ˆk, then
Map[ˆv, ˆw] = Map′[ˆv] = ˆk. However, in processing a decryption query (ˆ v,ˆc), we may speculatively
assign a random value ˆk to Map′[ˆv], and then later, if the adversary queries the random oracle
at the point (ˆv, ˆw), where ( u,ˆv, ˆw) is a DH-triple, we assign the value ˆk to Map[ˆv, ˆw], in order to
maintain consistency.
Now for more details. In preparation for the encryption query, in the initialization step, the
challenger precomputes β ←R Zq, v ←gβ, w ←gαβ, k ←R K. It also sets Map[v,w] and Map′[v] to
k, which means that the value of the random oracle at ( v,w) is equal to k. Also note that in the
initialization step, the challenger sets c←⊥, and in processing the encryption query, overwrites c
with a ciphertext in C. Thus, decryption queries processed while c= ⊥are phase 1 queries, while
those processed while c̸= ⊥are phase 2 queries.
Processing random oracle queries. When processing a random oracle query (ˆv, ˆw), if Map[ˆv, ˆw] has
not yet been deﬁned, the challenger proceeds as follows.
• First, it tests if ( u,ˆv, ˆw) is a DH-triple. In Fig. 12.2, we implement this by invoking the
function DHP(α,ˆv, ˆw). For now, we can think of DHP as being implemented as follows:
DHP(α,ˆv, ˆw) := ˆ vα = ˆw.
This is the only place where our challenger makes use of the secret key.
• If (u,ˆv, ˆw) is a DH-triple, the challenger sets Map′[ˆv] to a random value, if it is not already
deﬁned, and then sets Map[ˆv, ˆw] ←Map′[ˆv]. It also sets Sol[ˆv] ← ˆw, where Sol : G →G is
another associative array. The idea is that Sol records solutions to Diﬃe-Hellman instances
(u,ˆv) that are discovered while processing random oracle queries.
• If (u,ˆv, ˆw) is not a DH-triple, then the challenger just sets Map[ˆv, ˆw] to a random value.
The result of the random oracle query is always Map[ˆv, ˆw].
Processing decryption queries. In processing a decryption query (ˆv,ˆc), the challenger proceeds as
follows.
• If ˆv= v, the challenger just uses the prepared key k directly to decrypt ˆc.
• Otherwise, the challenger checks if Map′ is deﬁned at the point ˆv, and if not, it assigns to
Map′[ˆv] a random value. It then uses the value ˆk = Map′[ˆv] directly to decrypt ˆc. Observe
that our challenger performs the decryption without using the solution ˆ w to the instance
482
initialization:
α,β ←R Zq, u←gα, v←gβ, w←gαβ
k←R K, b←R {0,1}
c←⊥
initialize three empty associative arrays
Map : G2 →K, Map′: G →K, and Sol : G →G
(1) Map[v,w] ←k, Map′[v] ←k
send the public key u to A;
upon receiving an encryption query ( m0,m1) ∈M2:
c←R Es(k,mb), send (v,c) to A;
upon receiving a decryption query (ˆv,ˆc) ∈G ×C, where (ˆv,ˆc) ̸= (v,c):
if ˆv= v then
ˆm←Ds(k,ˆc)
else
if ˆv /∈Domain(Map′) then Map′[ˆv] ←R K
ˆk←Map′[ˆv], ˆm←Ds(ˆk,ˆc)
send ˆm to A;
upon receiving a random oracle query (ˆv, ˆw) ∈G2:
if (ˆv, ˆw) /∈Domain(Map) then
if DHP(α,ˆv, ˆw) then
if ˆv /∈Domain(Map′) then Map′[ˆv] ←R K
Map[ˆv, ˆw] ←Map′[ˆv], Sol[ˆv] ←ˆw
else
Map[ˆv, ˆw] ←R K
send Map[ˆv, ˆw] to A
Figure 12.2: Game 0 challenger in the proof of Theorem 12.4
(u,ˆv) of the CDH problem. However, if the adversary queries the random oracle at the point
(ˆv, ˆw), the adversary will see the same value ˆk, and so consistency is maintained.
Hopefully, it is clear that our challenger behaves exactly as in the usual attack game, despite
the more elaborate bookkeeping.
Game 1. This game is the same as Game 0, except that we delete line (1) in Fig. 12.2.
Let Z be the event that Aqueries the random oracle at ( v,w) in Game 1. It is not hard to see
that Games 0 and 1 proceed identically, unless Z occurs. By the Diﬀerence Lemma, we have
|Pr[W1] −Pr[W0]|≤ Pr[Z]. (12.10)
If event Z happens, then at the end of Game 1, we have Sol[v] = w. What we want to do,
therefore, is use Ato build an eﬃcient adversary Bicdh that breaks the CDH assumption for G,
with the help of a DH-decision oracle, with an advantage equal to Pr[ Z]. The logic of Bicdh is very
straightforward. Basically, after obtaining u and v from its challenger in Attack Game 12.3, Bicdh
483
plays the role of challenger to Aas in Game 1. Besides the computation of u, the value of α is
never explicitly used in that game, other than in the evaluation of the DHP function, and for this,
Bicdh can use the DH-decision oracle provided to it in Attack Game 12.3. At the end of the game,
if v∈Domain(Sol), then Bicdh outputs w= Sol[v].
By construction, it is clear that
Pr[Z] = ICDHadv[Bicdh,G]. (12.11)
Finally, note that in Game 1, the key k is only used to encrypt the challenge plaintext, and to
process decryption queries of the form ( v,ˆc), where ˆc̸= c. As such, the adversary is essentially just
playing the 1CCA attack game against Es at this point. More precisely, we can easily derive an
eﬃcient 1CCA adversary Bs based on Game 1 that uses Aas a subroutine, such that
|Pr[W1] −1/2|= 1CCAadv∗[Bs,Es]. (12.12)
We leave the details of Bs to the reader.
Combining (12.10), (12.11), and (12.12), we obtain (12.9). That completes the proof of the
theorem. 2
Discussion. We proved that EEG is CCA-secure, in the random oracle model, under the ICDH
assumption. Is the ICDH assumption reasonable? On the one hand, in Chapter 15 we will see groups
G where the ICDH assumption is equivalent to the CDH assumption. In such groups there is no
harm in assuming ICDH. On the other hand, the ElGamal system is most commonly implemented
in groups where ICDH is not known to be equivalent to CDH. Is it reasonable to assume ICDH
in such groups? Currently, we do not know of any group where CDH holds, but ICDH does not
hold. As such, it appears to be a reasonable assumption to use when constructing cryptographic
schemes. Later, in Section 12.6.2, we will see a variant of ElGamal encryption that is CCA-secure,
in the random oracle model, under the normal CDH assumption. See also Exercise 12.31, where
we develop a more modular analysis of EEG based on a new assumption, called the interactive hash
Diﬃe-Hellman (IHDH) assumption, which itself is implied by the ICDH assumption.
12.5 CCA security from DDH without random oracles
In Section 11.5.2, we proved that EEG was semantically secure without relying on the random oracle
model. Rather, we used the DDH assumption (among other assumptions). Unfortunately, it seems
unlikely that we can ever hope to prove that EEG is CCA secure without relying on random oracles.
In this section, we present a public key encryption scheme that can be proved CCA secure
without relying on the random oracle heuristic. The scheme is based on the DDH assumption (as
well as a few other standard assumptions). The scheme is a variant of one designed by Cramer and
Shoup, and we call it ECS.
12.5.1 Universal projective hash functions
We introduce here the tool used in the design and analysis of ECS. Deﬁning this tool in its full
generality would take us too far aﬁeld. Rather, we give just an intuitive description of this tool
in its general form, and instantiate it more rigorously in the speciﬁc form in which we will need it
here.
484
The tool is called a projective hash function . It can perhaps be best understood as a form
of function delegation. Suppose Alice has a secret function f : Y→Z . She would like to delegate
the ability to evaluate f to Bob — but not entirely. Speciﬁcally, she wants to give Bob just enough
information about f so that he can evaluate f on a speciﬁc subset L⊆Y , but nowhere else. We
denote by h the information about f that Alice gives to Bob. In our applications, Lwill always
be the image of some function θ : X→Y , and to eﬃciently evaluate f at a point y ∈L, Bob will
need x∈X such that θ(x) = y, along with the auxiliary information h provided to him by Alice.
Such a scheme is called a projective hash function . Given the auxiliary information h, the
behavior of f is completely deﬁned on L. However, we also require that h does not reveal any
information about the behavior of f outside of L. Somewhat more precisely, the requirement is
that if f is chosen at random (from some family of functions), then for every y∈Y\L , the values
f(y) and hare independent, with f(y) uniformly distributed over Z. If this additional requirement
is satisﬁed, then we say this scheme is a universal projective hash function.
A concrete instantiation. We now give a concrete example of the above idea. Suppose G is a
cyclic group of prime order q with generator g ∈G. Further, suppose u ∈G is some ﬁxed group
element. The set Yabove will consist of all pairs ( v,w) ∈G2, while the set L= Lu will consist
of those pairs ( v,w) for which ( u,v,w ) is a DH-triple. Note that the set Lu is the image of the
function
θ: Zq →G2,
β ↦→(gβ,uβ).
The function f := fσ,τ is indexed by randomly chosen σ,τ ∈Zq, and is deﬁned as follows:
fσ,τ : G2 →G,
(v,w) ↦→vσwτ. (12.13)
The auxiliary information h that deﬁnes f on Lu is
h:= f(g,u) = gσuτ. (12.14)
So if Alice chooses σ,τ ∈ Zq at random, which deﬁnes f, and gives h to Bob, then for any
(v,w) = (gβ,uβ) = θ(β) ∈Lu, Bob can compute f(v,w) as hβ, since
f(v,w) = vσwτ = (gβ)σ(uβ)τ = (gσuτ)β = hβ.
So this is a projective hash function. To show that it is universal, it suﬃces to show that hand
f(v,w) are uniformly and independently distributed over G, for all ( v,w) ∈G2 \Lu.
Lemma 12.5. Suppose σ and τ are uniformly and independently distributed over Zq. Then for all
u,v,w,h,z ∈G, if (u,v,w ) is not a DH-triple, then
Pr[gσuτ = h ∧vσwτ = z] = 1
q2 .
Proof. Let u,v,w,h,z ∈G be ﬁxed, and assume that ( u,v,w ) is not a DH-triple. Suppose u= gα,
v = gβ, and w = gγ. Since ( u,v,w ) is not a DH-triple, we have γ ̸= αβ. Consider the event
485
gσuτ = h ∧vσwτ = z. Taking discrete logarithms, we can write this as a matrix equation:
(Dloggh
Dloggz
)
=
(1 α
β γ
)
  
=:M
(σ
τ
)
. (12.15)
We claim that the matrix M is non-singular. One way to see this is to calculate its determinant
det(M) = γ−αβ ̸= 0. Another way to see this is to observe that the second row of M cannot be
a scalar multiple of the ﬁrst: if it were, then by looking at the ﬁrst column of M, the second row
of M would have to be equal to β times the ﬁrst, and by looking at the second column of M, this
would imply γ = αβ, which is not the case.
Since M is non-singular, (12.15) is satisﬁed by a unique pair ( σ,τ). Moreover, since σ and τ
are distributed uniformly and independently over Zq, this happens with probability 1 /q2. 2
The way we will use the above property in the analysis of our encryption scheme ECS is char-
acterized by the following game:
Attack Game 12.4 (Universal distinguishing game). For a given adversaryA, we deﬁne two
experiments.
Experiment b (b= 0,1):
• Achooses u∈G and (v,w) ∈G2 \Lu, and sends ( u,v,w ) to the challenger.
• The challenger chooses σ,τ ∈Zq at random, deﬁning f := fσ,τ as in (12.13), and computes
the auxiliary information hthat deﬁnes f on Lu as in (12.14). The challenger then computes
z0 ←f(v,w), z 1 ←R G,
and sends both h and zb to A.
• Athen makes a series of evaluation queries to the challenger. Each such query is of the form
(˜v, ˜w) ∈Lu, to which the challenger replies with ˜z←f(˜v, ˜w).
• At the end of the game, the adversary outputs a bit ˆb∈{0,1}.
Let Wb is the event that Aoutputs 1 in Experiment b. 2
Lemma 12.6. In Attack Game 12.4, Pr[W0] = Pr[W1] for all adversaries A.
Proof sketch. The proof follows almost immediately from Lemma 12.5, which says that h and z0
are independent, so replacing z0 by random z1 does not change the distribution of the adversary’s
view. The only additional observation is that the evaluation queries do not leak any additional
information about f, since if (˜v, ˜w) ∈Lu, the value f(˜v, ˜w) is completely determined by h. 2
Note that in Attack Game 12.4, the challenger does not explicitly check that (˜v, ˜w) ∈Lu for the
evaluation queries — we just assume that the adversary adheres to this restriction. In any case,
the result of Lemma 12.6 applies to computationally unbounded adversaries, so this is not really
an issue. Additionally, in our eventual application of Lemma 12.6, the adversary will in fact know
α= Dloggu. See Exercise 12.26 for an analysis of a stronger version of Attack Game 12.4.
486
12.5.2 Universal 2 projective hash functions
In our encryption scheme, we will need an independence property that is a bit stronger than
universal, which is called universal2. Again, we present the intuitive idea in terms of function
delegation. As before, we have a function θ: X→Y , and L⊆Y is the image of θ. In this scenario,
Alice has a function f′: Y×T →Z, and she wants to give Bob auxiliary information h′that will
allow him to compute f′ on L×T . The values in the set T may be thought of as “tags” that
are used to separate the inputs to the function. The stronger property we want is this: for all
y,ˆy ∈Y\L and t,ˆt∈T with t̸= ˆt, the values h′, f′(y,t), and f′(ˆy,ˆt) are mutually independent,
with f′(y,t) and f′(ˆy,ˆt) each uniformly distributed over Z. In particular, given h′and f′(y,t), the
value f′(ˆy,ˆt) is completely unpredictable.
We can easily extend our universal projective hash function scheme forLu ⊆G2 in Section 12.5.1
to obtain a universal 2 projective hash function scheme for Lu. In this scheme, our “tags” will be
elements of Zq. For σ,τ ∈Zq, let fσ,τ : G2 →G be deﬁned as in (12.16). We deﬁne a new function
f′:= f′
σ1,τ1,σ2,τ2, indexed by randomly chosen σ1,τ1,σ2,τ2 ∈Zq, as follows
f′
σ1,τ1,σ2,τ2 : G2 ×Zq →G,
(v,w,ρ ) ↦→fσ1,τ1(v,w) ·
(
fσ2,τ2(v,w)
)ρ = vσ1+ρσ2wτ1+ρτ2.
(12.16)
The auxiliary information that deﬁnes f′ on Lu ×Zq is h′ := (h1,h2), where hi is the auxiliary
information that deﬁnes fσi,τi on Lu; that is
hi := fσi,τi(g,u) = gσiuτi for i= 1,2. (12.17)
It should be clear that if Alice chooses σ1,τ1,σ2,τ2 ∈Zq at random, which deﬁnes f′, and gives
h′= (h1,h2) to Bob, then for any (v,w) = (gβ,uβ) ∈Lu, and any ρ∈Zq, Bob can compute f′(v,w)
as (h1hρ
2)β. The universal 2 independence property is established by the following lemma, which
says that for all (v,w),(ˆv, ˆw) ∈G2 \Lu and ρ̸= ˆρ, the values h1, h2, f′(v,w,ρ ), and f′(ˆv, ˆw,ˆρ) are
uniformly and independently distributed over G.
Lemma 12.7. Suppose σ1,τ1,σ2,τ2 are uniformly and independently distributed over Zq. Then for
all u,v,w, ˆv, ˆw,h1,h2,z, ˆz ∈G and all ρ,ˆρ ∈Zq, if (u,v,w ) and (u,ˆv, ˆw) are not DH-triples, and
ρ̸= ˆρ, then
Pr[gσ1uτ1 = h1 ∧ gσ2uτ2 = h2 ∧ vσ1+ρ1σ2wτ1+ρ1τ2 = z ∧ ˆvσ1+ρ2σ2 ˆwτ1+ρ2τ2 = ˆz] = 1
q4 .
Proof sketch. The basic idea is the same as the proof of Lemma 12.5. The relevant matrix equation
now is: 

Dloggh1
Dloggh2
Dloggz
Dloggˆz

=


1 α 0 0
0 0 1 α
β γ ρβ ργ
ˆβ ˆγ ˆρˆβ ˆρˆγ


  
=:M


σ1
τ1
σ2
τ2

. (12.18)
Here, u = gα, v = gβ, w = gγ, ˆv = gˆβ, and ˆw = gˆγ. The key fact is that the matrix M is
non-singular. Indeed, one can again just compute the determinant
det(M) = (ρ−ˆρ)(γ−αβ)(ˆγ−αˆβ),
487
which is nonzero under our assumptions. 2
The way we will use the above property in the analysis of our encryption scheme ECS is char-
acterized by the following game:
Attack Game 12.5 (Universal 2 guessing game). For a given adversary A, the game runs as
follows.
• Achooses u∈G, (v,w) ∈G2 \Lu, and ρ∈Zq, and sends ( u,v,w,ρ ) to the challenger.
• The challenger chooses σ1,τ1,σ2,τ2 ∈Zq at random, deﬁning f′:= f′
σ1,τ1,σ2,τ2 as in (12.16).
In addition, the challenger computes the auxiliary information ( h1,h2) that deﬁnes f′ on
Lu ×Zq as in (12.17). The challenger then computes
z←f′(v,w,ρ )
and sends h1, h2, and z to A.
• Athen makes a series of evaluation queries to the challenger. Each such query is of the form
(˜v, ˜w,˜ρ) ∈G2 ×Zq, where (˜v, ˜w) ∈Lu, to which the challenger replies with ˜z←f′(˜v, ˜w,˜ρ).
• At the end of the game, Aoutputs a list of tuples
(ˆzi,ˆvi, ˆwi,ˆρi) ∈G3 ×Zq (i= 1,...,Q ).
We say Awins the game if for some i= 1,...,Q , we have
(ˆvi, ˆwi) /∈Lu, ˆρi ̸= ρ, and ˆ zi = f′(ˆvi, ˆwi,ˆρi). 2
Lemma 12.8. In Attack Game 12.5, for any adversary Athat outputs at most Q tuples, the
probability that it wins is at most Q/q.
Proof sketch. The proof follows almost immediately from the Union Bound, along with Lemma 12.7,
which says that for each i= 1,...,Q , the values h1, h2, z, and zi are mutually independent. As we
observed in the proof of Lemma 12.6, the evaluation queries do not leak any additional information
about f′, since if (˜v, ˜w) ∈Lu, the value f′(˜v, ˜w,˜ρ) is completely determined by ( h1,h2). 2
See Exercise 12.26 for an analysis of a stronger version of Attack Game 12.5.
12.5.3 The ECS scheme
Without further ado, we present the scheme ECS. It makes use of
• a cyclic group G of prime order q with generator g∈G,
• a symmetric cipher Es = (Es,Ds), deﬁned over (K,M,C),
• a hash function H : G2 →K,
• a hash function H′: G2 →Zq.
The message space for ECS is M, and the ciphertext space is G3 ×C. We now describe the key
generation, encryption, and decryption algorithms for ECS.
488
• the key generation algorithm runs as follows:
G() := α←R Zq, u←gα
σ,τ ←R Zq, h←gσuτ
σ1,τ1,σ2,τ2 ←R Zq, h1 ←gσ1uτ1, h2 ←gσ2uτ2
pk ←(u,h,h 1,h2), sk ←(σ,τ,σ 1,τ1,σ2,τ2)
output (pk,sk);
• for a given public key pk = (u,h,h 1,h2) ∈G4 and message m∈M, the encryption algorithm
runs as follows:
E(pk,m) := β ←R Zq, v←gβ, w←uβ, ρ←H′(v,w)
z←hβ, z′←(h1hρ
2)β
k←H(v,z), c←R Es(k,m)
output (v,w,z ′,c);
• for a given secret key sk = (σ,τ,σ 1,τ1,σ2,τ2) ∈Z6
q and a ciphertext (v,w,z ′,c) ∈G3 ×C, the
decryption algorithm runs as follows:
D(sk, (v,w,z ′,c) ) := ρ←H′(v,w)
if vσ1+ρσ2wτ1+ρτ2 = z′
then z←vσwτ, k←H(v,z), m←Ds(k,c)
else m←reject
output m.
To understand what is going on, it is best to view the above construction in terms of the
projective hash functions deﬁned in Sections 12.5.1 and 12.5.2.
• The key generation algorithm chooses u∈G at random, which deﬁnes Lu = {(gβ,uβ) : β ∈
Zq}. The choice of σ,τ deﬁnes the function f = fσ,τ as in (12.13), and the value h is the
auxiliary information that deﬁnes f on Lu, as in (12.14). The choice of σ1,τ1,σ2,τ2 deﬁnes
the function f′= f′
σ1,τ1,σ2,τ2 as in (12.16), and the value ( h1,h2) is the auxiliary information
that deﬁnes f′on Lu ×Zq, as in (12.17).
• The encryption algorithm chooses a random ( v,w) ∈Lu, and computes z = f(v,w) and
z′ = f′(v,w,ρ ), where ρ = H′(v,w). These computations are done using the auxiliary
information in the public key. A symmetric key is then derived from ( v,z) using H, which is
used to encrypt m using Es.
• The decryption algorithm ﬁrst checks that z′= f′(v,w,ρ ), where ρ= H′(v,w). If this check
passes, the algorithm then computes z = f(v,w), derives a symmetric key from ( v,z) using
H, and uses this to decrypt c using Ds.
These observations immediately imply that decryption undoes encryption, so the basic correct-
ness requirements are met. Combined with Lemmas 12.6 and 12.8, these observations will also
allow us to prove that ECS is CCA secure under the DDH assumption.
Theorem 12.9. If the DDH assumption holds in G, Es is 1CCA secure, H is a secure KDF, and
H′ is collision resistant, then ECS is CCA secure.
489
In particular, for every 1CCA adversary Athat attacks ECS as in Deﬁnition 12.2, and makes at
most Qd decryption queries, there exist a DDH adversary Bddh for G as in Attack Game 10.6,
a 1CCA adversary Bs that attacks Es as in Deﬁnition 9.6, a KDF adversary Bkdf that attacks
H as in Attack Game 11.3, and a collision-ﬁnding adversary Bcr that attacks H′ as in Attack
Game 8.1, where Bddh, Bs, Bkdf, Bcr are elementary wrappers around A, such that
1CCAadv[A,ECS] ≤2
(
DDHadv[Bddh,G] + KDFadv[Bkdf,H]
+ CRadv[Bcr,H′] + Qd + 1
q
)
+ 1CCAadv[Bs,Es].
(12.19)
Proof. As usual, it is convenient to use the bit-guessing versions of the 1CCA attack games. We
prove
1CCAadv∗[A,ECS] ≤DDHadv[Bddh,G] + KDFadv[Bkdf,H]
+ CRadv[Bcr,H′] + Qd + 1
q + 1CCAadv∗[Bs,Es]. (12.20)
Then (12.19) follows by (12.2) and (9.2).
We deﬁne a series of games, Game j for j = 0,..., 7. Game 0 is the bit-guessing version of
Attack Game 12.1 played by Awith respect to ECS. In each game, bdenotes the random bit chosen
by the challenger, while ˆb denotes the bit output by A. For j = 0,..., 7, we deﬁne Wj to be the
event that ˆb= b in Game j.
Game 0. The logic of the challenger is shown in Fig. 12.3. The adversary can make any number
of decryption queries, but at most one encryption query. Note that in the initialization step, the
challenger performs those computations associated with the encryption query that it can, without
yet knowing the challenge plaintext. Also note that in the initialization step, the challenger sets
c←⊥, and in processing the encryption query, overwritescwith a ciphertext inC. Thus, decryption
queries processed while c= ⊥are phase 1 queries, while those processed while c̸= ⊥are phase 2
queries. The game is described using the terminology of projective hash functions, as discussed
above.
Game 1. We replace the lines marked (2) and (3) in Fig. 12.3 as follows:
(2) z←f(v,w)
(3) z′←f′(v,w,ρ )
Here, instead of using the auxiliary information that allows us to compute f on Lu and f′ on
Lu×Zq, we compute them directly, using the secret key sk. This does not change the result of the
computation in any way. Therefore,
Pr[W1] = Pr[W0]. (12.21)
The motivation for making this change is that now, the only place where we use the exponents
α, β, and γ is in the deﬁnition of the group elements u, v, and w, which allows us to play the “DDH
card” in the next step of the proof.
Game 2. We replace the line marked (1) in Fig. 12.3 with
(1) γ ←R Zq
490
initialization:
α,β ←R Zq
(1) γ ←αβ
u←gα, v←gβ, w←gγ
ρ←H′(v,w)
σ,τ ←R Zq, h←gσuτ, let f := fσ,τ (as in (12.13))
σ1,τ1,σ2,τ2 ←R Zq, h1 ←gσ1uτ1, h2 ←gσ2uτ2, let f′:= f′
σ1,τ1,σ2,τ2 (as in (12.16))
(2) z←hβ
(3) z′←(h1hρ
2)β
(4) k←H(v,z)
b←R {0,1}, c←⊥
send the public key ( u,h,h 1,h2) to A;
upon receiving an encryption query ( m0,m1) ∈M2:
c←R Es(k,mb), send (v,w,z ′,c) to A;
upon receiving a decryption query (ˆv, ˆw,ˆz′,ˆc) ∈G3 ×C, where (ˆv, ˆw,ˆz′,ˆc) ̸= (v,w,z ′,c):
if (ˆv, ˆw,ˆz′) = (v,w,z ′) then
ˆm←Ds(k,ˆc)
else
ˆρ←H′(ˆv, ˆw)
(5) if ˆ z′̸= f′(ˆv, ˆw,ˆρ)
then ˆm←reject
else ˆz←f(ˆv, ˆw), ˆk←H(ˆv,ˆz), ˆm←Ds(ˆk,ˆc)
send ˆm to A.
Figure 12.3: Game 0 challenger in the proof of Theorem 12.9
491
It is easy to see that ⏐⏐Pr[W1] −Pr[W2]
⏐⏐≤DDHadv[Bddh,G] (12.22)
for an eﬃcient DDH adversary Bddh, which works as follows. After it obtains its DDH problem
instance ( u,v,w ) from its own challenger, adversary Bddh plays the role of challenger to Ain
Game 1, but using the given valuesu,v,w . If (u,v,w ) is a random DH-triple, then this is equivalent
to Game 1, and if (u,v,w ) is a random triple, this is equivalent to Game 2. At the end of the game,
Bddh outputs 1 if ˆb= b and 0 otherwise.
Game 3. We replace the line marked (1) in Fig. 12.3 with
(1) γ ←R Zq \{αβ}
Since the statistical distance between the uniform distribution on all triples and the uniform
distribution on all non-DH-triples is 1 /q (see Exercise 10.7), it follows that:
⏐⏐Pr[W2] −Pr[W3]
⏐⏐≤1
q. (12.23)
Game 4. We now play our “CR card”. Let us deﬁne Coll u,v(ˆv, ˆw) to be true if (ˆv, ˆw) ̸= (v,w) and
H′(ˆv, ˆw) = H′(v,w), and to be false, otherwise. In this game, we “widen” the rejection rule at line
(5), replacing it with
(5) if Coll u,v(ˆu,ˆv) or z′̸= f′(ˆv, ˆw,ˆρ)
Let Z4 be the event that in Game 4, some decryption query, which would not have triggered
the rejection rule of Game 3, does trigger the wider rejection rule in Game 4. Clearly, Games 3
and 4 proceed identically unless event Z4 occurs. By the Diﬀerence Lemma, we have
⏐⏐Pr[W3] −Pr[W4]
⏐⏐≤Pr[Z4]. (12.24)
It should be clear that
Pr[Z4] ≤CRadv[Bcr,H′]. (12.25)
for an eﬃcient collision-ﬁnding adversary Bcr. Indeed, adversary Bcr just plays Game 4 and waits
for the event Z4 to happen.
Game 5. We again widen the rejection rule at line (5), replacing it with:
(5) if ˆ vα ̸= ˆw or Coll u,v(ˆu,ˆv) or z′̸= f′(ˆv, ˆw,ˆρ)
So this rule will reject the ciphertext if (ˆv, ˆw) /∈Lu.
Let Z5 be the event that in Game 5, some decryption query, which would not have triggered
the rejection rule of Game 4, does trigger the wider rejection rule in Game 5. Clearly, Games 4
and 5 proceed identically unless event Z5 occurs. By the Diﬀerence Lemma, we have
⏐⏐Pr[W4] −Pr[W5]
⏐⏐≤Pr[Z5]. (12.26)
We will argue that
Pr[Z5] ≤Qd
q . (12.27)
492
Suppose Z5 happened on a particular decryption query (ˆ v, ˆw,ˆz′,ˆc). We claim that for this
ciphertext, we have (i) (ˆv, ˆw) /∈Lu, (ii) ˆρ ̸= ρ, and (iii) ˆz′ = f′(ˆv, ˆw,ˆρ). Clearly, we must have
(i), as otherwise, this ciphertext could not have triggered the rejection rule in Game 5. We must
also have (iii), as otherwise, this ciphertext would have been rejected under the original rejection
rule. Suppose (ii) did not hold. Then we must have (ˆ v, ˆw) = (v,w), as otherwise, this ciphertext
would have been rejected under the collision rule added in Game 4. So we have ˆ z′= f′(ˆv, ˆw,ˆρ) =
f(v,w,ρ ) = z. But then this decryption query would not even have reached line (5) in the ﬁrst
place (it would have been decrypted directly as Ds(k,ˆc) three lines above).
Using the claim, we will show how to design an adversary that wins Attack Game 12.5 with
probability at least Pr[ Z5], and then use Lemma 12.8 to get an upper bound on Pr[ Z5]. We shall
refer to Attack Game 12.5 as “the guessing game” from here on out.
We can play the guessing game by running Game 5, but using the challenger in the guessing
game to evaluate f′, as needed. That challenger gives us f′(v,w,ρ ), along with h1 and h2, at the
beginning of the guessing game. Now, whenever Amakes a decryption query (ˆv, ˆw,ˆz′,ˆc) that brings
us to line (5), we ﬁrst check if ˆvα = ˆw; if so, we evaluate the rest of the test at line (5) by making
the evaluation query (ˆv, ˆw,ˆρ) in the guessing game, obtaining the value f′(ˆv, ˆw,ˆρ), and comparing
this to ˆz′; otherwise, we simply reject the decryption query, and append (ˆz′,ˆv, ˆw,ˆρ) to our output
list in the guessing game. The reader may verify that we win the guessing game with probability
at least Pr[ Z5]. The bound (12.27) follows from Lemma 12.8, and the fact that our output list in
the guessing game contains at most Qd guesses.
Game 6. Everything we did so far was leading to this point, which is the crux of the proof. We
replace line (2) in Fig. 12.3 with
(2) z←R G
We claim that
Pr[W6] = Pr[W5]. (12.28)
This follows from Lemma 12.6, and the fact that in processing decryption queries in Game 5, we
only need to evaluate f(ˆv, ˆw) at points (ˆv, ˆw) ∈Lu.
Game 7. Finally, the stage is set to play our “KDF card” and “1CCA card”. We replace the line
marked (4) by
(4) k←R K
It should be clear that
⏐⏐Pr[W6] −Pr[W7]
⏐⏐≤KDFadv[Bkdf,H] (12.29)
and ⏐⏐Pr[W7] −1/2
⏐⏐= 1CCAadv∗[Bs,Es], (12.30)
where Bkdf is an eﬃcient adversary attacking H as a KDF, and Bs is a 1CCA adversary attacking
Es.
The bound (12.20) now follows directly from (12.21)–(12.30). 2
Remark 12.3 (Group membership veriﬁcation). For reasons similar to that discussed in
Remark 12.1, it is essential that given a ciphertext ( v,w,z ′,c), the decryption algorithm for ECS
veriﬁes that v and w are in G. It is not necessary to explicitly check that z′is in G, since the check
that vσ1+ρσ2wτ1+ρτ2 = z′implies that z′is in G. 2
493
12.6 CCA security via a generic transformation
We have presented several constructions of CCA-secure public key encryption schemes. In Sec-
tion 12.3, we saw how to achieve CCA security in the random oracle model using a trapdoor
function scheme, and in particular (in Section 12.3.1) with RSA. In Section 12.4, we saw how to
achieve CCA security in the random oracle model under the interactive CDH assumption, and with
a bit more eﬀort, we were able to achieve CCA security in Section 12.5 without resorting to the
random oracle model, but under the DDH assumption.
It is natural to ask if there is a generic transformation that converts any CPA-secure public key
encryption scheme into one that is CCA-secure, as we did for symmetric encryption in Chapter 9.
The answer is yes. In the random oracle model it is possible to give a simple and eﬃcient transfor-
mation from CPA-security to CCA-security. This transformation, called the Fujisaki-Okamoto
transformation, allows one to eﬃciently convert any public-key encryption scheme that satisﬁes a
very weak security property (weaker than CPA security) into a public-key encryption scheme that
is CCA-secure in the random oracle model. It is possible, in principle, to give a similar transfor-
mation without relying on random oracles; however, the known constructions are too ineﬃcient to
be used in practice [56].
Applications. We show in Section 12.6.2 that applying the Fujisaki-Okamoto transformation to
a variant of ElGamal encryption, gives a public key encryption scheme that is CCA-secure in the
random oracle model under the ordinary CDH assumption, rather than the stronger, interactive
CDH assumption. (Exercise 12.33 develops another approach to achieving the same result, with a
tighter security reduction to the CDH assumption).
Beyond ElGamal, the Fujisaki-Okamoto transformation can be applied to other public key
encryption schemes, such as Regev’s lattice-based encryption scheme discussed in Chapter 17, the
McEliece coding-based scheme [111], and the NTRU scheme [88]. All these systems can be made
CCA secure, in the random oracle model, using the technique in this section.
The Fujisaki-Okamoto transformation. It is best to understand the Fujisaki-Okamoto trans-
formation as a technique that allows us to build a trapdoor function scheme TFO that is one way,
even given an image oracle (as in Deﬁnition 12.3), starting from any one-way, probabilistic public-
key encryption scheme Ea = ( Ga,Ea,Da). We can then plug TFO into the construction E′
TDF
presented in Section 12.3, along with a 1CCA symmetric cipher, to obtain a public-key encryption
scheme EFO that is CCA secure in the random oracle model.
Let Ea = (Ga,Ea,Da) be an arbitrary public-key encryption scheme with message space Xand
ciphertext space Y.
• The encryption algorithm Ea may be probabilistic, and in this case, it will be convenient to
make its random coin tosses explicit. To this end, let us view Ea as a deterministic algorithm
that takes three inputs: a public key pk, a message x∈X, and a randomizer r∈R, where R
is some ﬁnite randomizer space. To encrypt a message x∈X under a public key pk, one
chooses r∈R at random, and then computes the ciphertext Ea(pk,x; r).
• In general, the decryption algorithm Da may return the special symbolreject; however, we will
assume that this is not the case. That is, we will assume that Da always returns an element in
the message space X. This is not a serious restriction, as we can always modify the decryption
494
algorithm so as to return some default message instead ofreject. This assumption will simplify
the presentation somewhat.
The Fujisaki-Okamoto transformation applied to Ea = (Ga,Ea,Da) works as follows. We will
also need a hash function U : X →R, mapping messages to randomizers, which will be modeled
as a random oracle in the security analysis. The trapdoor function scheme is TFO = (Ga,F,D a),
deﬁned over (X,Y), where
F(pk,x) := Ea(pk,x; U(x)). (12.31)
To prove that TFO is one way given an image oracle, in addition to modeling U as a random
oracle, we will need to make the following assumptions, which will be made more precise below:
1. Ea is one way, which basically means that given an encryption of a random message x∈X,
it is hard to compute x;
2. Ea is unpredictable, which basically means that a random re-encryption of any ciphertext
y∈Y is unlikely to be equal to y.
We now make the above assumptions more precise. As usual, the one-wayness property is
deﬁned in terms of an attack game.
Attack Game 12.6 (One-way encryption). For a given public-key encryption scheme Ea =
(Ga,Ea,Da) with message space X, ciphertext space Y, and randomizer space R, and a given
adversary A, the attack game proceeds as follows:
• The challenger computes
(pk,sk) ←R Ga(), x←R X, r←R R, y←Ea(pk,x; r),
and sends (pk,y) to the adversary.
• The adversary outputs ˆx∈R.
We say Awins the above game if ˆx= x, and we deﬁne A’s advantage OWadv[A,Ea] to be the
probability that Awins the game. 2
Deﬁnition 12.5 (One-way encryption). A public-key encryption scheme Ea is one way if for
every eﬃcient adversary A, the value OWadv[A,Ea] is negligible.
Note that because Ea may be probabilistic, an adversary that wins Attack Game 12.6 may not
even know that they have won the game.
We deﬁne unpredictable encryption as follows.
Deﬁnition 12.6 (Unpredictable encryption). Let Ea = ( Ga,Ea,Da) be a given public-key
encryption scheme with message space X, ciphertext space Y, and randomizer space R. We say Ea
is ϵ-unpredictable if for every possible output (pk,sk) of Ga and every y∈Y, if we choose r∈R
at random, then we have
Pr[Ea(pk,Da(sk,y); r) = y] ≤ϵ.
We say Ea is unpredictable if it is ϵ-unpredictable for negligible ϵ.
495
We note that the one-wayness assumption is implied by semantic security (see Exercise 12.9).
We also note that, any public-key encryption scheme that is semantically secure typically is also
unpredictable, even though this is not implied by the deﬁnition. Moreover, any public-key encryp-
tion scheme can be easily transformed into one that satisﬁes this assumption, without aﬀecting the
one-wayness assumption (see Exercise 12.10).
Theorem 12.10. If U is modeled as a random oracle, and if Ea is one way and unpredictable, then
the trapdoor function scheme TFO, resulting from the Fujisaki-Okamoto transformation (12.31), is
one way given an image oracle.
In particular, assume that Ea is ϵ-unpredictable. Also assume that adversary Aattacks TFO as
in the random oracle version of Attack Game 12.2, and makes at most Qio image oracle queries
and Qro random oracle queries. Moreover, assume that Aalways includes its output among
its random oracle queries. Then there exists an adversary Bow that breaks the one-wayness
assumption for Ea as in Attack Game 12.6, where Bow is an elementary wrapper around A, such
that
IOWroadv[A,TFO] ≤Qio ·ϵ+ Qro ·OWadv[Bow,Ea]. (12.32)
Proof. We deﬁne Game 0 to be the game played betweenAand the challenger in the random oracle
version of Attack Game 12.2 with respect to TFO = (Ga,F,D a). We then modify the challenger
several times to obtain Games 1, 2, and so on. In each game, x denotes the random element of
X chosen by the challenger. For j = 0,1,..., we deﬁne Wj to be the event that x is among the
random oracle queries made by Ain Game j. As stated above, we assume that Aalways queries
the random oracle at its output value: this is a reasonable assumption, and we can always trivially
modify an any adversary to ensure that it behaves this way, increasing its random-oracle queries
by at most 1. Clearly, we have
IOWroadv[A,TFO] ≤Pr[W0]. (12.33)
Game 0. The challenger in Game 0 has to respond to random oracle queries, in addition to image
oracle queries. We make use of an associative array Map : X→R to implement the random oracle
representing the hash function U. The logic of the challenger is shown in Fig. 12.4. The adversary
can make any number of random oracle queries and any number of image queries. The associative
array Pre : Y→X is used to track the adversary’s random oracle queries. Basically, Pre[ˆy] = ˆx
means that ˆy is the image of ˆx under F(pk,·).
Game 1. In this game, we make the following modiﬁcation to the challenger. The line marked (2)
in the logic for processing decryption queries is modiﬁed as follows:
(2) if ˆ y∈Domain(Pre)
Let Z1 be the event that in Game 1, the adversary submits an image oracle query ˆ y such that
ˆy̸= y, ˆy /∈Domain(Pre), and Ea(pk,ˆx; ˆr) = ˆy,
where ˆxand ˆrare computed as in the challenger. It is clear that Games 0 and 1 proceed identically
unless Z1 occurs, and so by the Diﬀerence Lemma, we have
|Pr[W1] −Pr[W0]|≤ Pr[Z1]. (12.34)
496
initialization:
(pk,sk) ←R Ga(), x←R X, r←R R, y←Ea(pk,x; r)
initialize empty associative arrays Map : X→R and Pre : Y→X
(1) Map[x] ←r
send the public key pk to A;
upon receiving an image oracle query ˆy∈Y:
if ˆy= y then
result ←“yes”
else
ˆx←Da(sk,ˆy)
if ˆx /∈Domain(Map) then Map[ˆx] ←R R
ˆr←Map[ˆx]
(2) if Ea(pk,ˆx; ˆr) = ˆy
then result ←“yes”
else result ←“no”
send result to A;
upon receiving a random oracle query ˆx∈X:
if ˆx /∈Domain(Map) then Map[ˆx] ←R R
ˆr←Map[ˆx], ˆy←Ea(pk,ˆx; ˆr), Pre[ˆy] ←ˆx
send ˆr to A
Figure 12.4: Game 0 challenger in the proof of Theorem 12.10
497
upon receiving an image oracle query ˆy∈Y:
if ˆy∈{y}∪Domain(Pre):
then result ←“yes”
else result ←“no”
send result to A
Figure 12.5: Modiﬁed logic for image oracle queries
We argue that
Pr[Z1] ≤Qio ·ϵ, (12.35)
where we are assuming that Ea is ϵ-unpredictable. Indeed, observe that in Game 1, if Amakes an
image query ˆy with
ˆy̸= y and ˆ y /∈Domain(Pre),
then either
• ˆx= x, and so Ea(pk,ˆx; ˆr) = y̸= ˆy with certainty, or
• ˆx̸= x, and so ˆr is independent of A’s view, from which it follows that Ea(pk,ˆx; ˆr) = ˆy with
probability at most ϵ.
The inequality (12.35) then follows by the union bound.
Game 2. This game is the same Game 1, except that we implement the image oracle queries using
the logic described in Fig. 12.5. The idea is that in Game 1, we do not really need to use the secret
key to implement the image oracle queries.
It should be clear that
Pr[W2] = Pr[W1]. (12.36)
Since we do not use the secret key at all in Game 2, this makes it easy to play our “one-wayness
card.”
Game 3. In this game, we delete the line marked (1) in Fig. 12.4.
We claim that
Pr[W3] = Pr[W2]. (12.37)
Indeed, Games 2 and 3 proceed identically until Aqueries the random oracle at x. So if W2 does
not occur, neither does W3, and if W3 does not occur, neither does W2. That is, W2 and W3 are
identical events.
We sketch the design of an eﬃcient adversary Bsuch that
Pr[W3] ≤Qro ·OWadv[B,Ea]. (12.38)
The basic idea, as usual, is that Bplays the role of challenger to A, as in Game 3, except that the
values pk, sk, x, r, and y are generated by B’s OW challenger, from which Bobtains the values
pk and y. Adversary Binteracts with Ajust as the challenger in Game 3. The key observation is
that Bdoes not need to know the values sk, x, and r in order to carry out its duties. At the end of
498
the game, if Amade a random oracle query at the point x, then the value x will be contained in
the set Domain( Map). In general, it may not be easy to determine which of the values in this set
is the correct decryption of y, and so we use our usual guessing strategy; namely, Bsimply chooses
an element at random from Domain( Map) as its guess at the decryption of y. It is clear that the
inequality (12.38) holds.
The inequality (12.32) now follows from (12.33)–(12.38). That proves the theorem. 2
12.6.1 A generic instantiation
Putting all the pieces together, we get the following public-key encryption scheme EFO. The com-
ponents consist of:
• a public-key encryption scheme Ea = (Ga,Ea,Da), with message space X, ciphertext space
Y, and randomizer space R;
• a symmetric cipher Es = (Es,Ds), with key space Kand message space M;
• hash functions H : X→K and U : X→R .
The scheme EFO = (Ga,E,D ) has message space Mand ciphertext space Y×C . Encryption and
decryption work as follows:
E(pk,m) := x←R X, r←U(x), y←Ea(pk,x; r)
k←H(x), c←R Es(k,m)
output (y,c);
D(sk, (y,c) ) := x←Da(sk,y), r←U(x)
if Ea(pk,x; r) ̸= y
then m←reject
else k←H(x), m←Ds(k,c)
output m.
Combining Theorem 12.2 and Theorem 12.10, we immediately get the following:
Theorem 12.11. If H and U are modeled as a random oracles, Ea is one way and unpredictable,
and Es is 1CCA secure, then the above public-key encryption scheme EFO is CCA secure.
In particular, assume that Ea is ϵ-unpredictable. Then for every 1CCA adversary Athat attacks
EFO as in the random oracle version of Deﬁnition 12.2, and which makes at most Qd decryption
queries, QH queries to the random oracle for H, and QU queries to the random oracle for
U, there exist an adversary Bow that breaks the one-wayness assumption for Ea as in Attack
Game 12.6, and a 1CCA adversary Bs that attacks Es as in Deﬁnition 9.6, where Bow and Bs
are elementary wrappers around A, such that
1CCAroadv[A,EFO] ≤2(QH + QU) ·OWadv[Bow,Ea] + 2Qd ·ϵ+ 1CCAadv[Bs,Es]. (12.39)
12.6.2 A concrete instantiation with ElGamal
In the Fujisaki-Okamoto transformation, we can easily use a variant of ElGamal encryption in the
role of Ea. Let G be a cyclic group of prime order q generated by g ∈G. We deﬁne a public-key
encryption scheme Ea = (Ga,Ea,Da), with message space G, ciphertext space G2, and randomizer
space Zq. Public keys are of the form u∈G and secret keys of the form α∈Zq. Key generation,
encryption, and decryption work as follows:
499
Ga() := α←R Zq, u←gα, pk ←u, sk ←α
output (pk,sk);
Ea(u,x; β) := v←gβ, w←uβ, y←wx
output (v,y);
Da(α, (v,y)) := w←vα, x←y/w
output x.
We called this scheme multiplicative ElGamal in Exercise 11.5, where we showed that it is seman-
tically secure under the DDH assumption. It easily veriﬁed that Ea has the following properties:
• Ea is one-way under the CDH assumption. Indeed, an adversary Athat breaks the one-
wayness assumption for Ea is easily converted to an adversary Bthat breaks the CDH with
same advantage. Given an instance ( u,v) ∈G2 of the CDH problem, adversary Bplays the
role of challenger against Ain Attack Game 12.6 as follows:
– Bsets y←R G, and gives Athe public key u and the ciphertext ( v,y);
– when Aoutputs x∈G, adversary Boutputs w←y/x.
Clearly, if x is the decryption of ( v,y), then w = y/x is the solution to the given instance
(u,v) of the CDH problem.
• Ea is 1/q-unpredictable. Moreover, under the CDH assumption, it must be the case that 1 /q
is negligible.
Putting all the pieces together, we get the following public-key encryption scheme EEG
FO = (G,E,D ).
The components consist of:
• a cyclic group G of prime order q generated by g∈G;
• a symmetric cipher Es = (Es,Ds), with key space Kand message space M;
• hash functions H : G →K and U : G →Zq.
The message space of EEG
FO is Mand its ciphertext space is G2 ×C. Public keys are of the form u∈G
and secret keys of the form α ∈Zq. The key generation, encryption, and decryption algorithms
work as follows:
G() := α←R Zq, u←gα, pk ←u, sk ←α
output (pk,sk);
E(u,m) := x←R G, β←U(x), v←gβ, w←uβ, y←w·x
k←H(x), c←R Es(k,m)
output (v,y,c );
D(α, (v,y,c )) := w←vα, x←y/w, β ←U(x)
if gβ = v
then k←H(x), m←Ds(k,c)
else m←reject
output m.
500
Here, we have optimized the decryption algorithm a bit: ifv= gβ, then it follows thatEa(pk,x; β) =
(gβ,uβx) = (v,y), and so it is unnecessary to execute all of algorithm Ea.
As a special case of Theorem 12.11, we get the following:
Theorem 12.12. If H and U are modeled as a random oracles, the CDH assumption holds for G,
and Es is 1CCA secure, then the above public-key encryption scheme EEG
FO is CCA secure.
In particular, for every 1CCA adversary Athat attacks EEG
FO as in the random oracle version
of Deﬁnition 12.2, and which makes at most Qd decryption queries, QH queries to the random
oracle for H, and QU queries to the random oracle for U, there exist an adversary Bcdh that
breaks the CDH assumption for G as in Attack Game 10.5, and a 1CCA adversary Bs that
attacks Es as in Deﬁnition 9.6, where Bcdh and Bs are elementary wrappers around A, such that
1CCAroadv[A,EEG
FO ] ≤2(QH + QU) ·CDHadv[Bcdh,G] + 2Qd/q+ 1CCAadv[Bs,Es]. (12.40)
Contrast this result to the construction in Section 12.4: to achieve CCA security, instead of the
ordinary CDH assumption, that scheme requires the stronger, interactive CDH assumption. See
Exercise 12.33 for another scheme with a tighter reduction to CDH.
Remark 12.4 (Group membership veriﬁcation). Based on the discussion in Remark 12.1,
one might presume that given a ciphertext ( v,y,c ), the decryption algorithm for EEG
FO should verify
that v and y are in G. However, the check gβ = v already ensures that v is in G. This leaves the
question of whether the decryption algorithm needs to check that y is in G. It turns out that this
check is unnecessary (see Exercise 12.13 for details). 2
12.7 CCA-secure public-key encryption with associated data
In Section 9.6, we introduced the notion of CCA security for symmetric-key ciphers with associated
data. In this section, we brieﬂy sketch how this notion can be adapted to public-key encryption.
First, we have to deal with the syntactic changes. A public-key encryption schemeE= (G,E,D )
with associated data, or AD public-key encryption scheme , has the same basic structure as
an ordinary public-key encryption scheme, except that the encryption algorithm E and decryption
algorithm D each take an additional input d, called the associated data. Thus, E gets invoked
as c←R E(pk,m,d ), and D gets invoked as m←D(sk,c,d ). As usual, we require that ciphertexts
generated by E are correctly decrypted by D, as long as both are given the same associated data.
That is, for all possible outputs ( pk,sk) of G, and all messages m and associated data d, we have
Pr[D(sk, E(pk, m, d), d) = m] = 1.
Messages lie in some ﬁnite message space M, ciphertexts in some ﬁnite ciphertext space C, and
associated data in some ﬁnite space D. We say that Eis deﬁned over (M,D,C).
Deﬁnition 12.7 (CCA and 1CCA security with associated data). The deﬁnition of CCA
security for ordinary public-key encryption schemes carries over naturally to AD public-key en-
cryption schemes. Attack Game 12.1 is modiﬁed as follows. For encryption queries, in addition
to a pair of messages (mi0,mi1), the adversary also submits associated data di, and the challenger
computes ci ←RE(pk,mib,di). For decryption queries, in addition to a ciphertext ˆcj, the adversary
submits associated data ˆdj, and the challenger computes ˆmj ←D(sk,ˆcj, ˆdj). The restriction is that
501
the pair (ˆcj, ˆdj) may not be among the pairs (c1,d1),(c2,d2),... corresponding to previous encryp-
tion queries. An adversary A’s advantage in this game is denoted CCAadadv[A,E], and the scheme
is said to be CCA secure if this advantage is negligible for all eﬃcient adversaries A. If we
restrict the adversary to a single encryption query, as in Deﬁnition 12.2, the advantage is denoted
1CCAadadv[A,E], and the scheme is said to be 1CCA secure if this advantage is negligible for all
eﬃcient adversaries A.
Observations. We make a couple of simple observations.
• Theorem 12.1 carries over to AD schemes. That is, if an AD public-key encryption scheme is
1CCA secure, then it is also CCA secure. The proof and concrete security bounds go through
with no real changes.
• All of the CCA-secure public-key encryption schemes presented in this chapter can be triv-
ially converted to CCA-secure AD public-key encryption schemes, simply by replacing the
symmetric cipher Es used in each construction with a 1CCA-secure AD cipher. The associ-
ated data for the AD public-key scheme is simply passed through to the AD symmetric-key
cipher, in both the encryption and decryption algorithms. See part (g) of Exercise 12.5; see
also Exercise 12.17 for an alternative approach.
Applications. CCA-secure AD public-key encryption has a number of natural applications. One
such application is the key-escrow application, which we discussed in Section 12.2.3. In this appli-
cation, we escrowed a ﬁle-encryption key k by encrypting the pair ( k,h) under the public-key of a
key escrow service. Here, h was the collision-resistant hash of some metadata md associated with
the ﬁle, and the public-key encryption scheme used by the escrow service was assumed CCA se-
cure. By encrypting the pair ( k,h), the escrow service could enforce various access control policies,
based on the metadata and the identity or credentials of an entity requesting the key k. However,
the metadata itself was considered public information, and it did not really need to be encrypted,
except that we wanted it to be bundled in some non-malleable way with the keyk. This same eﬀect
can be achieved more naturally and eﬃciently by using a CCA-secure AD public-key encryption
scheme, as follows. When the key k is escrowed, the escrow-ciphertext is generated by encrypting
k using the metadata md as associated data. When a requesting entity presents a pair ( c,md)
to the escrow service, the service checks that the requesting entity’s credentials and the supplied
metadata conform to the access control policy, and if so, decryptscusing the supplied metadata md
as associated data. The access control policy is enforced by the CCA-security property: attempting
to decrypt the escrow-ciphertext using non-matching metadata as associated data will not leak any
information about the corresponding ﬁle-encryption key.
We will also make use of CCA-secure AD public-key encryption in building signcryption schemes
(see Section 13.7.3).
12.7.1 AD-only CCA security
A somewhat weaker notion of security that is suﬃcient in many applications is called AD-only
CCA security. This notion of security is obtained by changing Deﬁnition 12.7 so that the restric-
tion placed on a decryption query ( ˆCj, ˆdj) is that ˆdj should not be equal to any of the associated
data values di submitted as part of any previous encryption query. Otherwise, the deﬁnition of
AD-only CCA security is exactly the same as the deﬁnition of CCA security, with the corresponding
502
advantage denoted CCAadoadv[A,E]. We can also naturally deﬁne AD-only 1CCA security by
restricting the adversary to a single decryption query, with the corresponding advantage denoted
1CCAadoadv[A,E].
Observations. We make a few simple observations.
• AD-only CCA security constrains the adversary’s ability to issue decryption queries compared
to CCA security in Deﬁnition 12.7. Therefore, the notion of AD-only CCA security is no
stronger than CCA security. However, if the associated data space Dis suﬃciently large,
then under reasonable assumptions, one can convert an AD-only CCA secure scheme into a
CCA secure scheme. This is explored in Exercise 14.13.
• AD-only CCA secure schemes can produce ciphertexts that are somewhat more compact that
CCA secure schemes. See Exercise 12.19 for this and other results.
• Theorem 12.1 carries over to AD-only security. That is, if an AD public-key encryption
scheme is AD-only 1CCA secure, then it is also AD-only CCA secure. Again, the proof and
concrete security bounds go through with no real changes.
• In the key-escrow application discussed above, where we use associated data to enforce an
access control policy, it is typically enough to use an AD-only CCA secure encryption scheme
to prevent the types of abuses discussed in Section 12.2.3.
Indeed, suppose Alice encrypts a ﬁle-encryption key k with associated data equal to the
metadata md of the ﬁle f: the metadata includes Alice’s identity, the name of the ﬁle, as well
as the time the ﬁle was created and/or modiﬁed. Assuming Alice encrypts k using an AD-
only CCA-secure scheme, one can prove the following: if an auditor is to get any information
about k, then she must submit a decryption request to the escrow service with credentials
that are consistent with md (but not necessarily with an identical ciphertext).
12.8 Case study: PKCS1, OAEP, OAEP +, and SAEP
The most widely used public-key encryption scheme using RSA is described in a standard from
RSA Labs called PKCS1. This scheme is quite diﬀerent from the scheme ERSA we presented in
Section 12.3.1.
Why does the PKCS1 standard not use ERSA? The reason is that when encrypting a short
message — much shorter than the RSA modulus n — a PKCS1 ciphertext is more compact than
an ERSA ciphertext. The ERSA scheme outputs a ciphertext ( y,c) where y is in Zn and c is a
symmetric ciphertext, while a PKCS1 ciphertext is only a single element of Zn.
Public-key encryption for short messages is used in a variety of settings. For example, in some
key exchange protocols, public-key encryption is only applied to short messages: a symmetric key
and some metadata. Similarly, in some access control systems, one encrypts a short access token
and nothing else. In these settings, schemes like PKCS1 are more space eﬃcient than ERSA. It
is worth noting, however, that the ElGamal scheme EEG can produce even shorter ciphertexts
(although encryption time with ElGamal is typically higher than with RSA).
Our goal in this section is to study PKCS1, and more generally, public-key encryption schemes
based on a trapdoor function T = (G,F,I ) deﬁned over (X,Y), where the ciphertext is just a single
element of Y.
503
12.8.1 Padding schemes
Let T = (G,F,I ) be a trapdoor function deﬁned over ( X,Y), and let Mbe some message space,
where |M|≪|X| . Our goal is to design a public-key encryption scheme where a ciphertext is just
a single element in Y. To do so, we use the following general paradigm: to encrypt a message
m ∈M, the encryptor “encodes” the given message as an element of X, and then applies the
trapdoor function to the encoded element to obtain a ciphertext c∈Y. The decryptor inverts the
trapdoor function at c, and decodes the resulting value to obtain the message m.
As a ﬁrst naive attempt, suppose X := {0,1}t and M:= {0,1}s, where, say, t = 2048 and
s= 256. To encrypt a message m∈M using the public key pk do
E(pk,m) := F
(
pk, 0t−s ∥m
)
.
Here we pad the message m in Mwith zeros so that it is in X. To decrypt a ciphertext c, invert
the trapdoor function by computing I(sk,c) and strip oﬀ the ( t−s) zeros on the left.
This naive scheme uses deterministic encryption and is therefore not even CPA secure. It should
never be used. Instead, to build a secure public-key scheme we need a better way to encode the
message m ∈M into the domain X of the trapdoor function. The encoding should be invertible
to enable decryption, and should be randomized to have some hope of providing CPA security, let
alone CCA security. Towards this goal, let us deﬁne the notion of a padding scheme.
Deﬁnition 12.8. A padding scheme PS= (P,U), deﬁned over (M,R,X), is a pair of eﬃcient
algorithms, P and U, where P : M×R→X and U : X →M∪{ reject }is its inverse in the
following sense: U(x) = m whenever x= P(m,r) for some (m,r) ∈M×R , and U(x) = reject if
x is not in the image of P.
For a given padding scheme (P,U) deﬁned over (M,R,X), let us deﬁne the following public-key
encryption scheme Epad = (G,E,D ) derived from the trapdoor function T = (G,F,I ):
E(pk,m) := D(sk,c) :=
r←R R, x ←P(m,r), x ←I(sk,c),
c←F(pk,x), m ←U(x),
output c; output m.
(12.41)
When the trapdoor function T is RSA it will be convenient to call this scheme RSA-PSencryption.
For example, when RSA is coupled with PKCS1 padding we obtain RSA-PKCS1 encryption.
The challenge now is to design a padding scheme PSfor which Epad can be proven CCA secure,
in the random oracle, under the assumption that T is one way. Many such padding schemes have
been developed with varying properties. In the next subsections we describe several such schemes,
their security properties, and limitations.
12.8.2 PKCS1 padding
The oldest padding scheme, which is still in use today, is called PKCS1 padding.
To describe this padding scheme let us assume from now on that the domain Xof the trapdoor
function is 0 8 ×{0,1}t−8, where t is a multiple of 8. That is, X consists of all t-bit strings whose
left-most 8 bits are zero. These zero bits are meant to accommodate a t-bit RSA modulus, so that
all such strings are binary encodings of numbers that are less than the RSA modulus. The message
504
00 02 non-zero random bytes r 00 mx:=
16 bits s bits
t bits
Figure 12.6: PKCS1 padding (mode 2)
space Mconsists of all bit strings whose length is a multiple of 8, but at most t−88. The PKCS1
standard is very much byte oriented, which is why all bit strings are multiples of 8. The number
88 is speciﬁed in the standard: the message to be encrypted must be at least 11 bytes (88 bits)
shorter than the RSA modulus. For an RSA modulus of size 2048 bits, the message can be at most
245 bytes (1960 bits). In practice, messages are often only 32 bytes (256 bits).
The PKCS1 padding algorithm is shown in Fig. 12.6. A double-digit number, like 00 or 02, in
the ﬁgure denotes a one-byte (8-bit) value in hexadecimal notation. Here, s is the length of the
message m. The randomizer r shown in the ﬁgure is a sequence of ( t−s)/8 −3 random non-zero
bytes.
The PKCS1 padding scheme ( P,U) works as follows. We can take the randomizer space Rto
be the set of of all strings r′of non-zero bytes of length t/8 −3; to pad a particular message m, we
use a preﬁx r of r′ of appropriate length so that the resulting string x is exactly t-bits long. Here
are the details of algorithms P and U.
Algorithm P(m,r′):
output x:=
(
00 ∥02 ∥r∥00 ∥m
)
∈{0,1}t,
where r is the appropriate preﬁx of r′
Algorithm U(x):
(1) parse x as
(
00 ∥02 ∥non-zero bytes r∥00 ∥m
)
if x cannot be parsed this way, output reject
else, output m
Because the string rcontains only non-zero bytes, parsing xin line (1) can be done unambiguously
by scanning the string x from left to right. The 16 bits representing 00 02 at the left of the string
is the reason why this padding is called PKCS1 mode 2 (mode 1 is discussed in the next chapter).
By coupling PKCS1 padding with RSA, as in (12.41), we obtain the RSA-PKCS1 encryption
scheme. What can we say about the security of RSA-PKCS1? As it turns out, not much. In fact,
there is a devastating chosen ciphertext attack on it, which we discuss next.
12.8.3 Bleichenbacher’s attack on the RSA-PKCS1 encryption scheme
RSA-PKCS1 encryption is not secure against chosen ciphertext attacks. We describe an attack, due
to Bleichenbacher, as it applies to the SSL 3.0 protocol used to establish a secure session between a
client and a server. The SSL 3.0 protocol was later replaced by an improved protocol called TLS 1.0
that defends against this attack, as discussed below. The latest version of TLS, called TLS 1.3, has
moved away from RSA encryption altogether (see Section 21.10).
505
The only details of SSL 3.0 relevant to this discussion are the following:
• During session setup, the client chooses a random 48-byte (384-bit) string, called the
pre master secret, and encrypts it with RSA-PKCS1 under the server’s public-key. It
sends the resulting ciphertext c to the server in a message called client key exchange.
• When the server receives a client key exchange message it extracts the ciphertext c and
attempts to decrypt it. If PKCS1 decoding returns reject, the server sends an abort message
to the client. Otherwise, it continues normally with session setup.
Let us show a signiﬁcant vulnerability in this system that is a result of a chosen ciphertext
attack on RSA-PKCS1. Suppose the attacker has a ciphertext cthat it intercepted from an earlier
SSL session with the server. This c is an encryption generated using the server’s RSA public key
(n,e), with RSA modulus nand encryption exponent e. The attacker’s goal is to decrypt c. Let x
be the eth root of c in Zn, so that xe = c in Zn. We show how the attacker can learn x, which is
suﬃcient to decrypt c.
The attacker’s strategy is based on the following observation: let r be some element in Zn and
deﬁne c′←c·re in Zn; then
c′= c·re = (x·r)e ∈Zn.
The attacker plays the role of a client and attempts to establish a SSL connection with the
server. The attacker creates a client key exchange message that contains c′ as the encrypted
pre master secret and sends the message to the server. The server, following the protocol, com-
putes the eth root of c′to obtain x′= x·r in Zn. Next, the server checks if x′is a proper PKCS1
encoding: does x′ begin with the two bytes 00 02, and if so, is it followed by non-zero bytes, then
a zero byte, and then 48 additional (message) bytes? If not, the server sends an abort message to
the attacker. Otherwise, decryption succeeds and it sends the next SSL message to the attacker.
Consequently, the server’s response to the attacker’s client key exchange message reveals some
information about x′= x·r. It tells the attacker if x′is a valid PKCS1 encoding.
The attacker can repeat this process over and over with diﬀerent values ofr∈Zn of its choosing.
Every time the attacker learns ifx·ris a valid PKCS1 encoding or not. In eﬀect, the server becomes
an oracle that implements the following predicate for the attacker:
Px(r) :=
{
1 if x·r in Zn is a valid PKCS1 encoding;
0 otherwise.
The attacker can query this predicate for any r∈Zn of its choice and as many times as it wants.
Bleichenbacher showed that for a 2048-bit RSA modulus, this oracle is suﬃcient to recover
all of x with several million queries to the server. Exercise 12.21 gives a simple example of this
phenomenon.
This attack is a classic example of a real-world chosen ciphertext attack. The adversary has
a challenge ciphertext c that it wants to decrypt. It does so by creating a number of related
ciphertexts and asks the server to “partially decrypt” those ciphertexts (i.e., evaluate the predicate
Px). After enough queries, the adversary is able to obtain the decryption of c. Clearly, this attack
would not be possible if RSA-PKCS1 were CCA-secure: CCA security implies that such attacks
are not possible even given a full decryption oracle, let alone a partial decryption oracle like Px.
This devastating attack lets the attacker eavesdrop on any SSL session of its choice. Given the
wide deployment of RSA-PKCS1 encryption, the question then is how to best defend against this
attack.
506
The TLS defense. When Bleichenbacher’s attack was discovered in 1998, there was a clear need
to ﬁx SSL. Moving away from PKCS1 to a completely diﬀerent padding scheme would have been
diﬃcult since it would have required updating both clients and servers, and this can take decades for
everyone to update. The challenge was to ﬁnd a solution that requires only server-side changes, so
that deployment can be done server-side only. This will protect all clients, old and new, connecting
to an updated server.
The solution, implemented in TLS 1.0, changes the RSA-PKCS1 server-side decryption process
to the following procedure:
1. generate a string r of 48 random bytes,
2. decrypt the RSA-PKCS1 ciphertext to recover the plaintext m,
3. if the PKCS1 padding is invalid, or the length of m is not exactly 48 bytes:
4. set m←r
5. return m
In other words, when PKCS1 parsing fails, simply choose a random plaintext r and use this r as
the decrypted value. Clearly, the TLS session setup will fail further down the line and setup will
abort, but presumably doing so at that point reveals no useful information about the decryption
of c. Some justiﬁcation for this process is provided by Jonsson and Kaliski [95]. The TLS 1.2
standard goes further and includes the following warning about this decryption process:
In any case, a TLS server MUST NOT generate an alert if processing an RSA-encrypted
pre-master secret message fails [...] Instead, it MUST continue the handshake with a
randomly generated pre-master secret. It may be useful to log the real cause of failure for
troubleshooting purposes; however, care must be taken to avoid leaking the information
to an attacker (through, e.g., timing, log ﬁles, or other channels.)
Note the point about side channels, such as timing attacks, in the last sentence. Suppose the server
takes a certain amount of time to respond to a client key exchange message when the PKCS1
padding is valid, and a diﬀerent amount of time when it is invalid. Then by measuring the server’s
response time, the Bleichenbacher attack is easily made possible again.
The DROWN attack. To illustrate the cost of cryptographic mistakes, we mention an in-
teresting attack called DROWN [9]. While implementations of TLS 1.0 and above are immune
to Bleichenbacher’s attack, a very old version of the protocol, called SSL 2.0, is still vulnerable.
SSL 2.0 is still supported by some Internet servers so that old clients can connect. The trouble is
that, in a common TLS deployment, the server has only one TLS public-key pair. The same public
key is used to establish a session when the latest version of TLS is used, as when the old SSL 2.0
is used. As a result, an attacker can record the ciphertext c used in a TLS 1.2 session, encrypted
under the server’s public key, and then use Bleichenbacher’s attack on the SSL 2.0 implementation
to decrypt this c. This lets the attacker decrypt the TLS session, despite the fact that TLS is
immune to Bleichenbacher’s attack. Eﬀectively, the old SSL 2.0 implementation compromises the
modern TLS.
This attack shows that once a cryptographically ﬂawed protocol is deployed, it is very diﬃcult
to get rid of it. Even more troubling is that ﬂaws in a protocol can be used to attack later versions
of the protocol that have supposedly corrected those ﬂaws. The lesson is: make sure to get the
cryptography right the ﬁrst time. The best way to do that is to only use schemes that have been
properly analyzed.
507
d 00 00 00 ... 00 00 01 m
r ⨁
(t−h−8) bits
h bits
r W
H
⨁
00 r′ z′
8
00
t bits
z:=
x:=
Figure 12.7: OAEP padding using hash functions H and W, and optional associated data d
12.8.4 Optimal Asymmetric Encryption Padding (OAEP)
The failure of RSA-PKCS1 leaves us with the original question: is there a padding scheme ( P,U)
so that the resulting encryption scheme Epad from (12.41) can be shown to be CCA-secure, in the
random oracle model, based on the one-wayness of the trapdoor function?
The answer is yes, and the ﬁrst attempt at such a padding scheme was proposed by Bellare and
Rogaway in 1994. This padding, is called Optimal Asymmetric Encryption Padding (OAEP), and
the derived public-key encryption scheme was standardized in the PKCS1 version 2.0 standard. It
is called “optimal” because the ciphertext is a single element of Y, and nothing else.
The OAEP padding scheme ( P,U) is deﬁned over ( M,R,X), where R:= {0,1}h and X :=
08 ×{0,1}t−8. As usual, we assume that h and t are multiples of eight so that lengths can be
measured in bytes. As before, in order to accommodate a t-bit RSA modulus, we insist that the
left-most 8 bits of any element in Xare zero. The message space Mconsists of all bit strings whose
length is a multiple of 8, but at most t−2h−16.
The scheme also uses two hash functions H and W, where
H : {0,1}t−h−8 →R , W : R→{ 0,1}t−h−8. (12.42)
The set Rshould be suﬃciently large to be the range of a collision resistant hash. Typically,
SHA256 is used as the function H and we set h:= 256. The function W is derived from SHA256
(see Section 8.10.3 for recommended derivation techniques).
OAEP padding is used to build a public-key encryption scheme with associated data (as dis-
cussed in Section 12.7). As such, the padding algorithm P takes an optional third argument
d∈R = {0,1}h, representing the associated data. To support associated data that is more than h
bits long one can ﬁrst hash the associated data using a collision resistant hash to obtain an element
of R. If no associated data is provided as input to P, then dis set to a constant that identiﬁes the
hash function H, as speciﬁed in the standard. For example, for SHA256, one sets dto the following
508
256-bit hex value:
d:= E3B0C442 98FC1C14 9AFBF4C8 996FB924 27AE41E4 649B934C A495991B 7852B855.
Algorithm P(m,r,d) is shown in Fig. 12.7. Every pair of digits in the ﬁgure represents one byte
(8 bits). The variable length string of zeros in z is chosen so that the total length of z is exactly
(t−h−8) bits. The algorithm outputs an x∈X.
The inverse algorithm U, on input x∈X and d∈R, is deﬁned as follows:
parse x as (00 ∥r′∥z′) where r′∈R and z′∈{0,1}t−h−8
(1) if x cannot be parsed this way, set m←reject
else
r←H(z′) ⊕r′, z ←W(r) ⊕z′
parse z as (d∥00 ... 00 01 ∥m) where d∈R and m∈M
if z cannot be parsed this way, set m←reject
output m
Finally, the public-key encryption scheme RSA-OAEP is obtained by combining the RSA trap-
door function with the OAEP padding scheme, as in (12.41). When referring to OAEP coupled
with a general trapdoor function T = ( G,F,I ), we denote the resulting encryption scheme by
EOAEP = (G,E,D ).
The security of EOAEP. One might hope to prove CCA security of EOAEP in the random oracle
model using only the assumption that T is one-way. Unfortunately, that is unlikely because of
a counter-example: there is a plausible trapdoor function T for which the resulting EOAEP is
vulnerable to a CCA attack. See Exercise 12.23.
Nevertheless, it is possible to prove security of EOAEP by making a stronger one-wayness as-
sumption about T, called partial one-wayness. Recall that in the game deﬁning a one-way function,
the adversary is givenpk and y←F(pk,x), for some pk and random x∈X, and is asked to produce
x. In the game deﬁning a partial one-way function, the adversary is given pk and y, but is only
asked to produce, say, certain bits of x. If no eﬃcient adversary can accomplish even this simpler
task, then we say that T is partial one-way. More generally, instead of producing some bits of x,
the adversary is asked to produce a particular function f of x. This is captured in the following
game.
Attack Game 12.7 (Partial one-way trapdoor function scheme). For a given trapdoor
function scheme T = (G,F,I ), deﬁned over
(
X, Y
)
, a given eﬃciently computable function f :
X→Z , and a given adversary A, the attack game runs as follows:
• The challenger computes
(pk,sk) ←R G(), x ←X, y ←F
(
pk,x
)
and sends (pk,y) to the adversary.
• The adversary outputs ˆz∈Z.
We deﬁne the adversary’s advantage, denoted POWadv[A,T,f], to be the probability that ˆz= f(x).
2
509
Deﬁnition 12.9. We say that a trapdoor function scheme T deﬁned over
(
X, Y
)
is partial one
way with respect to f : X→Z if, for all eﬃcient adversaries A, the quantity POWadv[A,T,f]
is negligible.
Clearly, a partial one-way trapdoor function is also a one-way trapdoor function: if an adversary
can recover xit can also recover f(x). Therefore, the assumption that a trapdoor function is partial
one way is at least as strong as assuming that the trapdoor function is one way.
The following theorem, due to Fujisaki, Okamoto, Pointcheval, and Stern, shows that EOAEP is
CCA-secure in the random oracle model, assuming T is partial one-way. The proof can be found
in their paper [66].
Theorem 12.13. Let t, h, X, H, and W be as in the OAEP construction. Assume H and W
are modeled as random oracles. Let T = (G,F,I ) be a trapdoor function deﬁned over
(
X,Y). Let
f : X→{ 0,1}t−h−8 be the function that returns the right-most (t−h−8) bits of its input. If T is
partial one way with respect to f, and 2h is super-poly, then EOAEP is CCA secure.
Given Theorem 12.13 the question is then: is RSA a partial one-way function? We typically
assume RSA is one-way, but is it partial one-way when the adversary is asked to compute only
(t−h−8) bits of the pre-image? As it turns out, if RSA is one-way then it is also partial one-
way. More precisely, suppose there is an eﬃcient adversary Athat given an RSA modulus n
and encryption exponent e, along with y ←xe ∈Zn as input, outputs more than half the least
signiﬁcant bits of x. Then there is an eﬃcient adversary Bthat uses Aand recovers all the bits of
x. See Exercise 12.24.
As a result of this wonderful fact, we obtain as a corollary of Theorem 12.13 that RSA-OAEP is
CCA-secure in the random oracle model assuming only that RSA is a one-way function. However,
the concrete security bounds obtained when proving CCA security of RSA-OAEP based on the
one-wayness of RSA are quite poor.
Manger’s timing attack. RSA-OAEP is tricky to implement securely. Suppose the OAEP
algorithm U(x,d) were implemented so that it takes a certain amount of time when the input is
rejected because of the test on line (1), and a diﬀerent amount of time when the test succeeds. Notice
that rejection on line (1) occurs when the eight most signiﬁcant bits of x are not all zero. Now,
consider again the setting of Bleichenbacher’s attack on PKCS1. The adversary has a ciphertext
c, generated using the server’s RSA public key, with RSA modulus n and encryption exponent e.
The adversary wants to decrypt c. It can repeatedly interact with the server, sending it c′←c·re
in Zn, for various values of rof the adversary’s choice. By measuring the time that the server takes
to respond, the attacker can tell if rejection happened because of line (1). Therefore, the attacker
learns if the eight most signiﬁcant bits of ( c′)1/e in Zn are all zero. As in Bleichenbacher’s attack,
this partial decryption oracle is suﬃcient to decrypt all of c. See Exercise 12.21, or Manger [106],
for the full details.
12.8.5 OAEP + and SAEP+
In the previous section we saw that RSA-OAEP is CCA-secure assuming RSA is a one-way function.
However, for other one-way trapdoor functions, the derived schemeEOAEP may not be CCA-secure.
The next question is then: is there a padding scheme ( P,U) that, when coupled with a general
trapdoor function, gives a CCA-secure scheme in the random oracle model? The answer is yes,
510
and a padding scheme that does so, called OAEP+, is a variation of OAEP [147]. The diﬀerence,
essentially, is that the block of zero bytes in Fig. 12.7 is replaced with the value H′(m,r) for
some hash function H′. This block is veriﬁed during decryption by recomputing H′(m,r) from the
recovered values for m and r. The ciphertext is rejected if the wrong value is found in this block.
For RSA speciﬁcally, it is possible to use a simpler CCA-secure padding scheme. This simpler
padding scheme, called SAEP+, eliminates the hash function H and the corresponding xor on the
left of H in Fig. 12.7. The randomizer r needs to be longer than in OAEP. Speciﬁcally, r must be
slightly longer than half the size of the modulus, that is, slightly more than t/2 bits. RSA-SAEP+
is CCA-secure, in the random oracle model, assuming the RSA function is one-way [28]. It provides
a simple alternative padding scheme for RSA.
12.9 A fun application: private set intersection
This application has little to do with CCA security, but rather, is a continuation of the fun appli-
cation in the previous chapter (see Section 11.6).
Alice and Bob are sick: they both caught the ﬂu. Alice has a list of people Sa = {u1,...,u n}⊆
IDthat she recently came in contact with. Similarly, Bob has a list of people Sb ⊆ID that he
recently came in contact with. They want to identify the subset of people that they both came in
contact with, namely the people in Sa ∩Sb, who could have been the source of the ﬂu. The problem
is that Bob does not want to reveal his contacts Sb to Alice, and similarly, Alice does not want to
reveal her contacts Sa to Bob. How can they compute the intersection?
This problem is called private set intersection : each party should learn the items in the in-
tersection of Sa and Sb, but nothing else should be revealed about the sets. An elegant solution
uses a mechanism called an oblivious PRF that we previously discussed in Section 11.6.3. Let F
be a secure PRF deﬁned over ( K, ID, Y), where Y= {0,1}b for, say, b = 256. Suppose Alice
has some u ∈ID, and Bob has a random k ∈K. Recall that an oblivious PRF evaluation is a
protocol between Alice and Bob, where at the end of the protocol Alice learns y := F(k,u), but
Bob learns nothing about u, and Alice learns nothing else about k. In Section 11.6.3 we saw a
simple construction for an oblivious PRF based on the one-more Diﬃe-Hellman assumption in the
random oracle model.
Now, suppose Alice has Sa = {u1,...,u n}⊆ID , and Bob has Sb = {v1,...,v m}⊆ID . To
compute the intersection Sa ∩Sb, they decide to use the following protocol:
− step 1: Bob chooses k←R K.
− step 2: Alice and Bob run the oblivious PRF protocol n times, once for each element in Sa.
Alice learns ˆui := F(k,ui) for i= 1,...,n .
− step 3: Bob computes ˆvj := F(k,vj) for all j = 1,...,m and sends ˆv1,..., ˆvm ∈Y to Alice.
− step 4: Alice ﬁnds all u∈Sa such that ˆu:= F(k,u) is in {ˆv1,..., ˆvm}. She outputs the set of
all such u as the intersection Sa ∩Sb.
Clearly, if Alice and Bob honestly follow the protocol, then once the protocol terminates, Alice
learns the intersection Sa ∩Sb. Alice can then send the intersection to Bob. The running time for
each party is linear in |Sa|+ |Sb|, as is the total communication.
511
Notice that the protocol leaks the size of the sets: Bob learns the size of Sa, and Alice learns
the size of Sb. If needed, this leakage can be prevented by ﬁrst padding the sets to a maximum
size using distinct unused dummy elements.
Let’s see why the protocol reveals nothing beyond the intersection and the size of the sets. First,
thanks to the oblivious property of the evaluation protocol, Bob learns nothing about Sa beyond
its size. Second, Alice learns nothing about Sb beyond its size and the elements in the intersection.
To see why, observe that for each v ∈Sb that is not in the intersection, Alice learns ˆv := F(k,v).
Because F is a secure PRF, and Alice knows nothing about k, this ˆv is indistinguishable from a
random element in Ythat is independent of v. Hence, Alice learns nothing about elements v∈Sb
that are outside of the intersection.
A note on security. The above protocol is only secure when both Alice and Bob actually follow
the protocol — this is the so-called “honest but curious” model. There are protocols for this
problem that remain secure even for arbitrarily malicious parties, but they are more complicated.
12.10 Notes
Citations to the literature to be added.
12.11 Exercises
12.1 (Insecurity of multiplicative ElGamal). Show that multiplicative ElGamal from Exer-
cise 11.5 is not CCA secure. Your adversary should have an advantage of 1 in the 1CCA attack
game.
12.2 (Sloppy CCA). Let E= (G,E,D ) be a CCA-secure public-key encryption scheme deﬁned
over (M,C) where C := {0,1}ℓ. Consider the encryption scheme E′ = ( G,E′,D′) deﬁned over
(M,C′) where C:= {0,1}ℓ+1 as follows:
E′(pk,m) := E(pk,m) ∥0 and D′(sk,c) := D(sk,c[0 . .ℓ−1]).
That is, the last ciphertext bit can be 0 or 1, but the decryption algorithm ignores this bit. Show
that E′ is not CCA secure. Your adversary should have an advantage of 1 in the 1CCA attack
game.
Discussion: Clearly, adding a bit to the ciphertext does not harm security in practice, yet it
breaks CCA security of the scheme. This issue suggests that the deﬁnition of CCA security may be
too strong. A diﬀerent notion, called generalized CCA (gCCA), weakens the deﬁnition of CCA
security so that simple transformations of the ciphertext, like the one in E′, do not break gCCA
security. More formally, we assume that for each key pair ( pk,sk), there is an equivalence relation
≡pk on ciphertexts such that
c≡pk c′ =⇒ D(sk,c) = D(sk,c′).
Moreover, we assume that given pk,c,c ′, it is easy to tell if c≡pk c′. Note that the relation ≡pk is
speciﬁc to the particular encryption scheme. Then, in Attack Game 12.1, we insist each decryption
query is not equivalent to (as opposed to not equal to) any ciphertext arising from a previous
encryption query.
512
12.3 (Small subgroup attack). We mentioned in Remark 12.1 that the decryption algorithm for
EEG should verify that in a given ciphertext ( v,c), the element v actually belongs to the group G.
This exercise illustrates why this is important. Suppose that G is a subgroup of Z∗
p of prime order q,
where pis prime. We assume that the ICDH assumption holds for G. Suppose that the decryption
algorithm checks that v∈Z∗
p (which is typically quite trivial to do), but does not check that v∈G
(which can be more costly). In particular, the decryption algorithm just computes w ←vα ∈Z∗
p
and uses v,w,c to decrypt the given ciphertext. Here, we treat α as an integer in the range [0 ,q),
rather than an element of Zq. We also view H as a function H : Z∗
p ×Z∗
p →K.
Suppose p−1 can be written as a product p−1 = q·t1 ···tr, where q,t1,...,t r are distinct primes,
and each ti is poly-bounded. Show that it is possible to completely recover the secret key via
a chosen ciphertext attack. The number of decryption queries and the computation time of the
adversary in this attack is poly-bounded and its success probability is 1 −ϵ, where ϵ is negligible.
To simplify the analysis of your adversary’s success probability, you may model H : Z∗
p ×Z∗
p →K
as a random oracle and assume that the symmetric cipher provides one-time ciphertext integrity.
Hint: Use the fact that for each i= 1,...,t , you can eﬃciently ﬁnd an element gi ∈Z∗
p of order ti.
Use this gi to learn αmod ti.
12.4 (Extending the message space). Continuing with Exercise 11.7. Show that even if Eis
CCA secure, E2 is not CCA secure. For this, you should assume Mis non-trivial (i.e., contains at
least two messages of the same length).
Note: The next exercise presents a correct way to extend the message space of a CCA-secure
encryption scheme.
12.5 (Modular hybrid construction). All of the public-key encryption schemes presented in this
chapter can be viewed as special cases of the general hybrid construction introduced in Exercise 11.9.
Consider a KEM Ekem = ( G,Ekem,Dkem), deﬁned over ( K,Ckem). We deﬁne 1CCA security for
Ekem in terms of an attack game, played between a challenger and an adversary A, as follows. In
Experiment b, for b= 0,1, the challenger ﬁrst computes
(pk,sk) ←R G(), (k0,ckem) ←R Ekem(pk), k1 ←R K,
and sends ( kb,ckem) to A. Next, the adversary submits a sequence of decryption queries to the
challenger. Each such query is of the form ˆ ckem ∈Ckem, subject to the constraint that ˆckem ̸= ckem,
to which the challenger responds with Dkem(sk,ˆckem). Finally, Aoutputs ˆb ∈{0,1}. As usual,
if Wb is the event that Aoutputs 1 in Experiment b, we deﬁne A’s advantage with respect to
Ekem as 1CCAadv[A,Ekem] := |Pr[W0] −Pr[W1]|, and if this advantage is negligible for all eﬃcient
adversaries, we say that Ekem is 1CCA secure.
If Es is a symmetric cipher deﬁned over ( K,M,C), then as in Exercise 11.9, we also consider the
hybrid public-key encryption scheme E= (G,E,D ), deﬁned over (M,Ckem ×C), constructed out of
Ekem and Es.
(a) Prove that Eis CCA secure, assuming that Ekem and Es are 1CCA secure. You should prove a
concrete security bound that says that for every adversaryAattacking E, there are adversaries
Bkem and Bs (which are elementary wrappers around A) such that
1CCAadv[A,E] ≤2 ·1CCAadv[Bkem,Ekem] + 1CCAadv[Bs,Es].
513
Discussion: Using this result, one can arbitrarily extend the message space of any CCA-
secure encryption scheme whose message space is already large enough to contain the key
space for a 1CCA-secure symmetric cipher. For example, in practice, a 128-bit message space
suﬃces. Interestingly, one can arbitrarily extend the message space even when starting from
a CCA-secure scheme for 1-bit messages [118, 89].
(b) Describe the KEM corresponding to E′
TDF in Section 12.3 and prove that it is 1CCA secure
(in the random oracle model, assuming T is one way given an image oracle).
(c) Describe the KEM corresponding to EEG in Section 12.4 and prove that it is 1CCA secure (in
the random oracle model, under the ICDH assumption for G).
(d) Describe the KEM corresponding to ECS in Section 12.5 and prove that it is 1CCA secure
(assuming the DDH, H is a secure KDF, and H′is collision resistant).
(e) Give examples that show that if one of Ekem and Es is 1CCA secure, while the other is only
semantically secure, then Eneed not be CCA secure.
(f) Let Ea be a public-key encryption scheme. Consider the KEM Ekem constructed out of Ea as in
part (e) of Exercise 11.9. Show that Ekem is 1CCA secure, assuming that Ea is 1CCA secure.
(g) Assume Ekem is a 1CCA-secure KEM. AssumeEs is a 1CCA-secure AD cipher (see Section 9.6).
Suppose we modify the hybrid public-key encryption scheme Efrom Exercise 11.9 so that it
supports associated data, where the associated data is simply passed through to the symmetric
AD cipher. Show that the resulting scheme is a 1CCA-secure AD public-key encryption.
12.6 (Multi-key CCA security). Generalize the deﬁnition of CCA security for a public-key
encryption scheme to the multi-key setting. In this attack game, the adversary gets to obtain
encryptions of many messages under many public keys, and can make as decryption queries with
respect to any of these keys. Show that 1CCA security implies multi-key CCA security. You should
show that security degrades linearly in QkQe, where Qk is a bound on the number of keys, and Qe
is a bound on the number of encryption queries per key. That is, the advantage of any adversary A
in breaking the multi-key CCA security of a scheme is at most QkQe ·ϵ, where ϵ is the advantage
of an adversary B(which is an elementary wrapper around A) that breaks the scheme’s 1CCA
security.
12.7 (Multi-key CCA security of ElGamal). Consider a slight modiﬁcation of the public-key
encryption scheme EEG, which was presented an analyzed in Section 12.4. This new scheme, which
we call xEEG, is exactly the same as EEG, except that instead of deriving the symmetric key as
k = H(v,w), we derive it as k = H(u,v,w ). Consider the security of xEEG in the multi-key CCA
attack game, discussed above in Exercise 12.6. In that attack game, suppose Qte is a bound on the
total number of encryptions — clearly, Qte is at most QkQe, but it could be smaller. Let Abe an
adversary that attacks the multi-key CCA security of xEEG. Show that A’s advantage is at most
2ϵicdh + Qte ·ϵs,
where ϵicdh is that advantage of an ICDH adversary Bicdh attacking G and ϵs is the advantage of a
1CCA adversary Bs attacking Es (where both Bicdh and Bs are elementary wrappers around A).
Hint: Use the random self reduction for CDH (see Exercise 10.5).
514
12.8 (Fujisaki-Okamoto with veriﬁable ciphertexts). Consider the Fujisaki-Okamoto trans-
formation presented in Section 12.6. Suppose that the asymmetric cipher Ea has veriﬁable cipher-
texts, which means that there is an eﬃcient algorithm that given a public key pk, along with x∈X
and y ∈Y, determines whether or not y is an encryption of x under pk. Under this assumption,
improve the security bound (12.32) to
IOWroadv[A,TFO] ≤Qio ·ϵ+ OWadv[B,Ea].
Notice that this bound does not degrade as Qro grows.
12.9. Show that any semantically secure public-key encryption scheme with a super-poly-sized
message space is one way (as in Deﬁnition 12.5).
12.10 (Any cipher can be made unpredictable). Let (Ga,Ea,Da) be a public key encryption
scheme with message space X, ciphertext space Y, and randomizer space R. Let S be some
super-poly-sized ﬁnite set. Consider the encryption scheme ( Ga,E′
a,D′
a), with message space X,
ciphertext space Y×S, and randomizer space R×S, where E′
a(pk,x; (r,s)) := (Ea(pk,x; r),s) and
D′
a(sk,(y,s)) := Da(sk,y). Show that ( Ga,E′
a,D′
a) is unpredictable (as in Deﬁnition 12.6). Also
show that if (Ga,Ea,Da) is one way (as in Deﬁnition 12.5), then so is ( Ga,E′
a,D′
a).
12.11 (Fujisaki-Okamoto with semantically secure encryption). Consider the Fujisaki-
Okamoto transformation presented in Section 12.6. Suppose that the asymmetric cipher Ea is
semantically secure. Under this assumption, improve the security bound (12.32) to
IOWroadv[A,TFO] ≤Qio ·ϵ+ SSadv[B,Ea] + Qro/|X|.
12.12 (Analysis of a more general version of Fujisaki-Okamoto). This exercise develops
an analysis of a slightly more general version of the Fujisaki-Okamaoto transform in which we allow
the value x∈X to be chosen from some arbitrary distribution P on X. We assume that there is
an eﬃcient, probabilistic algorithm that samples elements of Xaccording to P.
(a) Suppose that in Attack Game 12.2, the value x ∈X is sampled according to P. Show that
Theorem 12.2 still holds.
(b) Suppose that in Attack Game 12.6, the value x ∈X is sampled according to P. Show that
Theorem 12.10 still holds.
12.13 (Subgroup membership checks for EEG
FO ). This exercise justiﬁes the claim made in Re-
mark 12.4. Consider the concrete instantiation EEG
FO of Fujisaki-Okamoto using the multiplicative
ElGamal encryption scheme over a group G of prime order q generated by g ∈G. Let us assume
that G is a subgroup of some larger group G′. For example, we might have G′= Z∗
p. The point is,
checking membership in G′may be much cheaper that checking membership in G. Now consider a
variant of the multiplicative ElGamal encryption scheme, where the plaintext space is G′ and the
ciphertext space is G ×G′.
(a) Show that if the plaintext x is sampled uniformly over G, then this ElGamal variant is one-
way under the CDH, using the generalized notion of one-way as discussed in part (b) of the
previous exercise (using the uniform distribution over G rather than over the entire plaintext
space G′).
(b) Show that this ElGamal variant is still 1 /q-unpredictable.
515
(c) Using part (b) of the previous exercise, show that if we instantiate Fujisaki-Okamoto with
this ElGamal variant, Theorem 12.12 still holds.
Discussion: This exercise shows that while EEG
FO decryption should check that v and y are in G′,
it need not explicitly check that they are in G ⊆G′. As discussed in Exercise 15.1, the check that
v and y are in G′is vitally important, as otherwise, a CCA attack could result in key exposure.
12.14 (An analysis of E′
TDF without image oracles). Theorem 12.2 shows that E′
TDF is CCA-
secure assuming the trapdoor function scheme T is one-way given access to an image oracle, and Es
is 1CCA secure. It is possible to prove security of E′
TDF assuming only that T is one-way (i.e.,
without assuming it is one-way given access to an image oracle), provided that Es is 1AE secure
(see Section 9.1.1). Note that we are making a slightly stronger assumption about Es (1AE instead
of 1CCA), but prove security under a weaker assumption on T. Prove the following statement: if
H : X→K is modeled as a random oracle, T is one-way, and Es is 1AE secure, then E′
TDF is CCA
secure.
Hint: The proof is similar to the proof of Theorem 12.2. Let (ˆy,ˆc) be a decryption query from the
adversary where ˆy̸= y. If Es provides ciphertext integrity, then in testing whether ˆy is in the image
of F(pk,·), we can instead test if the adversary queried the random oracle at a preimage ˆxof ˆy. If
not, we can safely reject the ciphertext — ciphertext integrity implies that the original decryption
algorithm would have anyway rejected the ciphertext with overwhelming probability.
Discussion: The analysis in this exercise requires that when a ciphertext ( y,c) fails to decrypt,
the adversary does not learn why. In particular, the adversary must not learn if decryption failed
because the inversion of y failed, or because the symmetric decryption of c failed. This means, for
example, if the time to decrypt is not the same in both cases, and this discrepancy is detectable
by the adversary, then the analysis in this exercise no longer applies. By contrast, the analysis
in Theorem 12.2 is unaﬀected by this side-channel leak: the adversary is given an image oracle
and can determine, by himself, the reason for a decryption failure. In this respect, the analysis
of Theorem 12.2 is more robust to side-channel attacks and is the preferable way to think of this
system.
12.15 (Immunizing against image queries). Let ( G,F,I ) be a trapdoor function scheme
deﬁned over (X,Y). Let U : X →Rbe a hash function. Consider the trapdoor function scheme
(G,F′,I′) deﬁned over (X,Y×R), where F′(pk,x) := (F(pk,x),U(x)) and I′(sk,(y,r)) := I(sk,y).
Show that if U is modeled as a random oracle, ( G,F,I ) is one way, and |R|is super-poly, then
(G,F′,I′) is one way given an image oracle.
12.16 (A broken CPA to CCA transformation). Consider the following attempt at trans-
forming a CPA-secure scheme to a CCA-secure one. Let ( G,E,D ) be a CPA-secure encryption
scheme deﬁned over (K×M, C), and let (S,V ) be a secure MAC with key space K. We construct
a new encryption scheme ( G,E′,D′), with message space M, as follows:
E′(pk,m) :=



k←R K,
c←R E
(
pk, (k,m)
)
,
t←R S(k,c),
output (c,t)



D′(
sk,(c,t)
):=



(k,m) ←D(sk,c),
if V(k,c,t ) = accept output m,
otherwise output reject



One might expect this scheme to be CCA-secure because a change to a ciphertext ( c,t) will invali-
date the MAC tag t. Show that this is incorrect. That is, show a CPA-secure encryption scheme
516
(G,E,D ) for which (G,E′,D′) is not CCA-secure (for any choice of MAC).
12.17 (Public-key encryption with associated data). In Section 12.7 we deﬁned public-key
encryption with associated data. We mentioned that the CCA-secure schemes in this chapter can
be made into public-key encryption schemes with associated data by replacing the symmetric cipher
used with an AD symmetric cipher. Here we develop another approach.
(a) Consider the scheme E′
TDF from Section 12.3. Suppose that we add an extra input d to the
encryption and decryption algorithms, representing the associated data, and that in both
algorithms we compute k as k←H(x,d), rather than k←H(x). Show that under the same
assumptions used in the analysis of E′
TDF, this modiﬁed scheme is a CCA-secure scheme with
associated data.
(b) Consider the scheme EEG from Section 12.4. Suppose that we add an extra input d to the
encryption and decryption algorithms, representing the associated data, and that in both
algorithms we compute k as k←H(v,w,d ), rather than k←H(v,w). Show that under the
same assumptions used in the analysis of EEG, this modiﬁed scheme is a CCA-secure scheme
with associated data.
(c) Consider the scheme ECS from Section 12.5. Suppose that we add an extra input d to the
encryption and decryption algorithms, representing the associated data, and that in both
algorithms we compute ρas ρ←H′(v,w,d ), rather than ρ←H′(v,w). Show that under the
same assumptions used in the analysis of ECS, this modiﬁed scheme is a CCA-secure scheme
with associated data.
12.18 (KEMs with associated data). Exercise 12.5 introduced the notion of a CCA secure key
encapsulation mechanism (KEM). One might also consider a KEM with associated data (AD KEM),
so that both encryption and decryption take as input associated datad. Because the input dmay be
adversarially chosen, we have to modify the attack game in Exercise 12.5, so that the adversary is
ﬁrst given pk, then makes a series of decryption queries, followed by one encryption query, followed
by a sequence of additional decryption queries. In the encryption query, the adversary supplies
d, the challenger computes ( k0,ckem) ←R Ekem(pk,d) and k1 ←R K, and sends either ( k0,ckem) or
(k1,ckem) to the adversary. Decryption queries work just as in Exercise 12.5, except the adversary
chooses the associated data ˆd as well as the ciphertext ˆ ckem, with the restriction that after the
encryption query is made, (ˆckem, ˆd) ̸= (ckem,d).
(a) Flesh out the details of the above attack game to obtain a formal deﬁnition of 1CCA-secure
AD KEM.
(b) Assume Ea is a 1CCA-secure AD KEM. Assume Es is a 1CCA-secure cipher. Suppose we
modify the hybrid public-key encryption scheme Ein Exercise 12.5 so that is supports asso-
ciated data, where the associated data is simply passed through to the AD KEM. Show that
the resulting scheme is a 1CCA-secure AD public-key encryption scheme.
(c) Describe the AD KEM corresponding to the construction in part (a) of the previous exercise
and prove that it is 1CCA secure.
(d) Describe the AD KEM corresponding to the construction in part (b) of the previous exercise
and prove that it is 1CCA secure.
517
(e) Describe the AD KEM corresponding to the construction in part (c) of the previous exercise
and prove that it is 1CCA secure.
12.19 (AD-only CCA security). Consider the notion of AD-only CCA security discussed in
Section 12.7.1.
(a) Suppose that in the previous exercise, we modify the attack game so that the restriction on
decryption queries is ˆd ̸= d (rather than (ˆckem, ˆd) ̸= ( ckem,d)). This deﬁnes the notion of
AD-only 1CCA-secure KEM. (Clearly, any AD KEM that is CCA secure is also AD-only
CCA secure.)
Assume Ea is an AD-only 1CCA-secure AD KEM. Assume Es is a semantically secure cipher.
Suppose we modify the hybrid public-key encryption scheme E in Exercise 12.5 so that is
supports associated data, where the associated data is simply passed through to the AD
KEM. Show that the resulting scheme is AD-only 1CCA-secure.
Discussion: In contrast to part (b) of the previous exercise, we can use a semantically secure
symmetric cipher, instead of a 1CCA-secure cipher, leading to a somewhat simpler scheme
with more compact ciphertexts.
(b) In contrast to Exercise 12.4, show that if Eis AD-only CCA secure, then E2 is also AD-only
CCA secure.
12.20 (An AD-only CCA secure scheme). This exercise explores a particular AD-only CCA
secure scheme EGS that has some nice properties that will prove useful later. The scheme makes
use of the following components:
• a cyclic group G of prime order q generated by g∈G;
• a symmetric cipher Es = (Es,Ds) deﬁned over (K,M,C);
• a hash function HK: G ×G →K;
• a hash function HG : G ×D→ G;
• an eﬃcient algorithm ODDH that on input ( gα,gβ,gγ) that outputs accept if γ = αβ, and
otherwise outputs reject.
The encryption scheme EGS = (G,E,D ) has message space Mand associated data space Dand is
deﬁned as follows:
• the key generation algorithm runs as follows:
G() := α∈Zq, u←gα
output (pk,sk) ←(u,α)
• the encryption algorithm runs as follows:
E(pk := u,m,d ) := β ←R Zq, v←gβ, w←uβ, k←H(v,w), c←R Es(k,m),
u←HG(v,d), w←uβ
output (v,c,w )
• the decryption algorithm runs as follows:
518
D(sk := α, (v,c,w ), d) := u:= HG(v,d)
if ODDH(u,v,w ) = reject
then output reject
else w←vα, k←H(v,w), m←Ds(k,c)
output m
Your task is to prove that EGS is AD-only CCA secure under the CDH assumption for G, and
assuming that Es is semantically secure, and modeling HKand HG as random oracles. You should
show that this holds even if in processing a decryption query ((ˆ v,ˆc,ˆw), ˆd), the adversary is given
more information than just ˆm, namely, ˆw := ˆvα (from which ˆm can certainly be computed). In
particular, show that the advantage of any adversary Ain this strengthened AD-only CCA attack
game is bounded by
2 ·CDHadv[Bcdh,G] + Qe ·SSadv[Bs,Es],
where Qe is a bound on the number of encryption queries made byA, and Bcdh and Bs are elementary
wrappers around A.
Hint: Let (u∗,v∗) be a random pair in G ×G. Set the public key u := u∗. For a random oracle
query (ˆv, ˆd) to HG made explicitly by the adversary, respond with ˆu:= ugρ for randomly generated
ρ∈Zq. For encryption queries, set the random oracle HG on input (v,d) to u:= gσ for randomly
generated σ ∈Zq, and set v := v∗gτ for randomly generated τ ∈Zq. With this, you can arrange
to process all decryption queries without knowing the secret key, to process all encryption queries
without knowing loggv, and to ensure that the adversary learns nothing about encrypted messages
unless he can solve the instance ( u∗,v∗) of the CDH problem (which will be evident by inspecting
the adversary’s queries to the random oracle HK) or break the semantic security of Es.
Discussion: This scheme is the same as EEG, except for the additional group element wplaced in
the ciphertext by the encryption algorithm, and the additional testODDH(u,v,w ) = reject performed
by the decryption algorithm. As we will see in Section 22.3.3, this scheme has nice properties that
are convenient in building a simple and secure threshold decryption scheme, where the decryption
key is never stored in one location and decryption is performed via a distributed computation.
Obviously, the need for the algorithm ODDH limits the applicability of this encryption scheme. We
will see later in Chapter 15 how we can eﬃciently implement ODDH in some settings. We will
also see later in Chapter 20 how we can eliminate the need for algorithm ODDH altogether (see
Exercise 20.19).
12.21 (Baby Bleichenbacher attack). Consider an RSA public key ( n,e), where n is an RSA
modulus, and e is an encryption exponent. For x ∈Zn, consider the predicate Px : Zn →{0,1}
deﬁned as:
Px(r) :=



y←x·r∈Zn
treat y as an integer in the interval [0 ,n)
if y >n/2, output 1
else, output 0



(a) Show that by querying the predicate Px at about log 2 n points, it is possible to learn the
value of x.
(b) Suppose an attacker obtains an RSA public key and an element c∈Zn. It wants to compute
the eth root of c in Zn. To do so, the attacker can query an oracle that takes z ∈Z as
input, and outputs 1 when [ z1/e mod n] > n/2, and outputs 0 otherwise. Here [ z1/e mod n]
519
is an integer w in the interval [0 ,n) such that we ≡zmod n. Use part (a) to show how the
adversary can recover the eth root of c.
12.22 (OAEP is CPA-secure for any trapdoor function). Let T = (G,F,I ) be a trapdoor
function deﬁned over (X,Y) where X= 08 ×{0,1}t−8. Consider the OAEP padding scheme from
Fig. 12.7, omitting the associated data input d, and let EOAEP be the public key encryption scheme
that results from coupling T with OAEP, as in (12.41). Show that EOAEP is CPA secure in the
random oracle model.
12.23 (A counter-example to the CCA-security of OAEP). Let T0 = (G,F0,I0) be a one-
way trapdoor permutation deﬁned over R:= {0,1}h. Suppose, T0 is xor-homomorphic in the
following sense: there is an eﬃcient algorithm C that for all pk output by G and all r,∆ ∈R,
we have C(F0(pk,r)) = F0(pk,r ⊕∆). Next, if t >2h+ 16, let T = (G,F,I ) be the trapdoor
permutation deﬁned over 08 ×{0,1}t−8 as follows:
F
(
pk, (00 ∥r∥z)
)
= 00 ∥F0(pk,r) ∥z.
Notice that from F
(
pk, (00 ∥r∥z)
)
it is easy to recover z, but not the entire preimage. Consider
the public-key encryption EOAEP obtained by coupling this T with OAEP as in (12.41). Show a
CCA attack on this scheme that has advantage 1 in winning the CCA game. Your attack shows
that for some one-way trapdoor functions, the scheme EOAEP may not be CCA-secure.
12.24 (RSA is partial one-way). Consider an RSA public key ( n,e), where n is an RSA
modulus, and e is an encryption exponent. Suppose n is a t-bit integer where t is even, and let T
be an integer that is a little bit smaller than 2 (t/2). Let xbe a random integer in the interval [0 ,n)
and y:= (xe mod n) ∈Zn. Suppose Ais an algorithm so that
Pr
[
A(n,e,y ) = z and 0 ≤x−zT <T
]
>ϵ.
The fact that the integer zT is so close to x means that z reveals half of the most signiﬁcant bits
of x. Hence, Ais an RSA partial one-way adversary for the most signiﬁcant bits.
(a) Construct an algorithm Bthat takes (n,e,y ) as input, and outputs xwith probability ϵ2. For
this, you should determine a more precise value for the parameter T.
Hint: Algorithm Bworks by choosing a random r ∈Zn and running z0 ←A(n,e,y ) and
z1 ←A(n,e, y·re). If Aoutputs valid z0 and z1 both times — an event that happens with
probability ϵ2 (explain why) — then
x≡z0T + ∆0 (mod n)
x·r≡z1T + ∆1 (mod n)
where 0 ≤∆0,∆1 <T . Show an eﬃcient algorithm that given suchr,z0,z1, outputs x,∆0,∆1,
with high probability. Your algorithm Bshould make use of an algorithm for ﬁnding shortest
vectors in 2-dimensional lattices (see, for example, [153]). If you get stuck, see [66].
Discussion: This result shows that if RSA is one-way, then an adversary cannot even
compute the most signiﬁcant bits of a preimage.
(b) Show that a similar result holds if an algorithm A′outputs more than half the least signiﬁcant
bits of x.
520
12.25 (Simpliﬁed Cramer-Shoup decryption). Consider the following simpliﬁed version ESCS
of the Cramer-Shoup encryption scheme (presented in Section 12.5):
• the key generation algorithm runs as follows:
G() := α,δ,δ1,δ2 ←R Zq, u←gα, h←gδ, h1 ←gδ1, h2 ←gδ2
pk ←(u,h,h 1,h2), sk ←(α,δ,δ1,δ2)
output (pk,sk);
for a given secret key sk = ( α,δ,δ1,δ2) ∈Z4
q and a ciphertext ( v,w,z ′,c) ∈G3 ×C, the
decryption algorithm runs as follows:
D(sk, (v,w,z ′,c) ) := ρ←H′(v,w)
if vα = w and vδ1+ρδ2 = z′
then z←vδ, k←H(v,z), m←Ds(k,c)
else m←reject
output m.
Encryption is the same as in ECS.
Show that ⏐⏐1CCAadv[A,ECS] −1CCAadv[A,ESCS]
⏐⏐≤2Qd/q,
for every adversary Athat makes at most Qd decryption queries. Conclude that ESCS is CCA
secure under the same assumption as in Theorem 12.9.
12.26 (Stronger properties for projective hash functions). We can strengthen Attack
Games 12.4 and 12.5, allowing the adversary to choose the values ( v,w) (and ρ) adaptively.
(a) Consider a variant of Attack Game 12.4 in which the adversary ﬁrst submits u ∈G to the
challenger (which deﬁnesLu), obtaining the auxiliary informationh; then the adversary makes
some number of evaluation queries; at some point, the adversary submits ( v,w) ∈G2 \Lu to
the challenger, obtaining zb; ﬁnally, the adversary continues making evaluation queries, and
outputs a bit, as usual. Show that Lemma 12.6 still holds for this variant.
(b) Consider a variant of Attack Game 12.5 in which the adversary ﬁrst submits u ∈G to the
challenger (which deﬁnes Lu), obtaining the auxiliary information (h1,h2); then the adversary
makes some number of evaluation queries; at some point, the adversary submits ( v,w) ∈
G2 \Lu and ρ ∈Zq to the challenger, obtaining z; ﬁnally, the adversary continues making
evaluation queries, and outputs a list of tuples, as usual. Show that Lemma 12.8 still holds
for this variant.
12.27 (Multiplicative Cramer-Shoup encryption). Consider the following multiplicative ver-
sion of the Cramer-Shoup encryption scheme (presented in Section 12.5) that supports associated
data (see Section 12.7) coming from a set D. Let G be a cyclic group of prime order q with gener-
ator g ∈G. Let H′: G3 ×D→ Zq be a hash function. The encryption scheme EMCS = (G,E,D )
is deﬁned over ( G,D,G4) as follows. Key generation is exactly as in ECS. For a given public key
pk = (u,h,h 1,h2) ∈G4 message m∈G, and associated data d∈D, the encryption algorithm runs
as follows:
E(pk,m,d ) := β ←R Zq, v←gβ, w←uβ, e←hβ ·m
ρ←H′(v,w,e,d ), z′←(h1hρ
2)β, output ( v,w,e,z ′).
521
For a given secret key sk = (σ,τ,σ 1,τ1,σ2,τ2) ∈Z6
q and a ciphertext ( v,w,e,z ′) ∈G4, and associ-
ated data d∈D, the decryption algorithm runs as follows:
D(sk, (v,w,e,z ′,d) ) := ρ←H′(v,w,e,d )
if vσ1+ρσ2wτ1+ρτ2 = z′
then output e/(vσwτ)
else output reject.
Show that EMCS is CCA secure, provided H′ is collision resistant and the DDH assumption holds
for G.
Hint: Part (b) of the previous exercise may be helpful.
Note: This scheme can be simpliﬁed, without sacriﬁcing security, along the same lines discussed
in Exercise 12.25, where the secret key is ( α,δ,δ1,δ2) ∈Z4
q, with h = gδ, h1 = gδ1, h2 = gδ2, and
where the decryption algorithm tests if vα = w and vδ1+ρδ2 = z′, and if so outputs e/vδ.
12.28 (Non-adaptive CCA security and Cramer-Shoup lite). One can deﬁne a weaker
notion of CCA security, corresponding to a variant of the CCA attack game in which the adversary
must make all of its decryption queries before making any of its encryption queries. Moreover, just
as we did for ordinary CCA security, it suﬃces to assume that the adversary makes just a single
encryption query. Let us call the corresponding security notion non-adaptive 1CCA security.
Now consider the following simpliﬁed version of the encryption scheme in the previous exercise.
Again, G is a cyclic group of prime order q with generator g∈G. The encryption scheme EMCSL =
(G,E,D ) is deﬁned over ( G,G4) as follows. The key generation algorithm runs as follows:
G() := α←R Zq, u←gα
for i= 0,1: σi,τi ←R Zq, hi ←gσiuτi
pk ←(u,h0,h1), sk ←(σ0,τ0,σ1,τ1)
output (pk,sk).
For a given public key pk = (u,h0,h1) ∈G3 and message m∈G, the encryption algorithm runs as
follows:
E(pk,m) := β ←R Zq, v←gβ, w←uβ, e←hβ
0 ·m
z′←hβ
1 , output ( v,w,z ′,e).
for a given secret key sk = (σ0,τ0,σ1,τ1) ∈Z4
q and a ciphertext ( v,w,z ′,e) ∈G4, the decryption
algorithm runs as follows:
D(sk, (v,w,z ′,e) ) := if vσ1wτ1 = z′
then output e/(vσ0wτ0)
else output reject.
(a) Show that EMCSL is non-adaptive 1CCA secure, provided the DDH assumption holds for G.
(b) Show that EMCSL is not CCA secure.
522
Note: This scheme can also be simpliﬁed along the same lines discussed in Exercise 12.25, and the
same results hold.
12.29 (Generalizing universal projective hash functions). This exercise develops a construc-
tion for universal projective hash functions that generalizes the one presented in Section 12.5.1. Let
G be a cyclic group of prime order q generated by g∈G. Let G1×n be the set of row vectors with
entries in G and Zn×1
q the set of column vectors with entries in Zq. For u= (u1,...,u n) ∈G1×n
and β ∈Zq, deﬁne uβ = (uβ
1 ,...,u β
n) ∈G1×n. For u= (u1,...,u n) ∈G1×n and v= (v1,...,v n) ∈
G1×n, deﬁne u·v = ( u1v1,...,u nvn) ∈ G1×n. Finally, for v = ( v1,...,v n) ∈ G1×n and
σ= (σ1,...,σ n) ∈Zn×1
q , deﬁne vσ = vσ1
1 ···vσn
n ∈G.
Now let u1,..., uk ∈G1×n be ﬁxed throughout the remainder of the exercise, and deﬁne L⊆ G1×n
to be the set of all elements of G1×n that can be written as uβ1
1 ···uβk
k for some β1,...,β k ∈Zq.
(a) Show how to eﬃciently compute vσ, given β1,...,β k ∈Zq such that v= uβ1
1 ···uβk
k , along
with h1,...,h k ∈G, where hi = uσ
i for i= 1,...,k .
(b) Suppose that σ∈Zn×1
q is chosen uniformly at random. Show that for each v∈G1×n\L, the
random variable vσ is uniformly distributed over G, independently of the random variable
(uσ
1 ,..., uσ
k).
12.30 (A universal projective hash function for EMCS). Consider the encryption scheme
EMCS from Exercise 12.27. Let the public key pk = (u,h,h 1,h2), message m, and associated data
d be ﬁxed. Deﬁne L⊆ G4 to be the set of possible outputs of the encryption algorithm on these
inputs:
L:= {(v,w,e,z ′) : v= gβ,w = uβ,e = hβ ·m,z′= (h1hρ
2)β),ρ = H′(v,w,e,d ) for some β ∈Zq }.
Design a universal projective hash function for Lwith outputs in G. The algorithm to evaluate
the function on ( v,w,e,z ′) ∈L takes as input the corresponding β value, along with whatever
auxiliary information is provided to facilitate computation of the function on L. For inputs not in
L, the output of the function should be uniformly distributed overG, independently of the auxiliary
information.
Hint: Use the result of the previous exercise.
12.31 (Interactive hash Diﬃe-Hellman). Let G be a cyclic group of prime order q generated
by g∈G. Let H : G2 →K be a hash function. We say that theInteractive Hash Diﬃe-Hellman
(IHDH) assumption holds for ( G,H) if it is infeasible for an eﬃcient adversary to distinguish
between the following two experiments. In Experiment 0, the challenger computes
α,β ←R Zq, u ←gα,v ←gβ,w ←gαβ, k ←H(v,w)
and sends ( u,v,k ) to the adversary. After that, the adversary is allowed to make a series queries.
Each query is of the form ˜v∈G2. Upon receiving such a query, the challenger computes
˜w←˜vβ, ˜k←H(˜v, ˜w)
and sends ˜k to the adversary. Experiment 1 is exactly the same as Experiment 0, except that the
challenger computes k←R K.
523
(a) Show that if H is modeled as a random oracle and the ICDH assumption holds for G, then
the IHDH assumption holds for ( G,H).
(b) Prove that the ElGamal public-key encryption scheme EEG is CCA secure if the IHDH as-
sumption holds for ( G,H) and Es is 1CCA secure.
12.32 (The twin CDH problem). In Section 12.4, we saw that the basic ElGamal encryption
scheme could not be proved secure under the ordinary CDH assumption, even in the random
oracle model. To analyze the scheme, we had to introduce a new, stronger assumption, called the
interactive CDH (ICDH) assumption (see Deﬁnition 12.4). In this exercise and the next, we show
how to avoid this stronger assumption with just a slightly more involved encryption scheme.
Let G be a cyclic group of prime order q generated by g∈G. The Twin CDH (2CDH) problem
is this: given
gα1,gα2,gβ
compute the pair
(gα1β,gα2β).
A tuple of the form
(gα1,gα2,gβ,gα1β,gα2β)
is called Twin DH (2DH) tuple. The interactive Twin CDH (I2CDH) assumption is this:
it is hard to solve a random instance ( gα1,gα2,gβ) of the 2DH problem, given access to an oracle
that recognizes 2DH-tuples of the form ( gα1,gα2,·,·,·).
(a) Flesh out the details of the I2CDH assumption by giving an attack game analogous to Attack
Game 12.3. In particular, you should deﬁne an analogous advantage I2CDH adv[A,G] for an
adversary Ain this attack game.
(b) Using the trapdoor test in Exercise 10.15, show that the CDH assumption implies the I2CDH
assumption. In particular, show that for every I2CDH adversary A, there exists a CDH
adversary B(where Bis an elementary wrapper around A), such that
I2CDHadv[A,G] ≤CDHadv[B,G] + Qro
q ,
where Qro is an upper bound on the number of oracle queries made by A.
12.33 (Twin CDH encryption). The Twin CDH encryption scheme, E2cdh = (G,E,D ), is
a public-key encryption scheme whose CCA security (in the random oracle model) is based on the
I2CDH assumption (see previous exercise). Let G be a cyclic group of prime order q generated by
g∈G. We also need a symmetric cipher Es = (Es,Ds), deﬁned over (K,M,C), and a hash function
H : G3 →K. The algorithms G, E, and D are deﬁned as follows:
G() := α1 ←R Zq, α2 ←R Zq, u1 ←gα1, u2 ←gα2
pk ←(u1,u2), sk ←(α1,α2)
output (pk,sk);
E(pk,m) := β ←R Zq, v←gβ, w1 ←uβ
1 , w2 ←uβ
2
k←H(v,w1,w2), c←R Es(k,m)
output (v,c);
D( sk, (v,c) ) := w1 ←vα1, w2 ←vα2, k←H(v,w1,w2), m←Ds(k,c)
output m.
524
The message space is Mand the ciphertext space is G ×C.
(a) Suppose that we model the hash function H as a random oracle. Show that E2cdh is CCA
secure under the I2CDH assumption, also assuming that Es is 1CCA secure. In particular,
show that for every 1CCA adversaryAattacking E2cdh, there exist an I2CDH adversaryBi2cdh
and a 1CCA adversary Bs, where Bi2cdh and Bs are elementary wrappers around A, such that
1CCAroadv[A,E2cdh] ≤2 ·I2CDHadv[Bi2cdh,G] + 1CCAadv[Bs,Es].
(b) Now use the result of part (b) of the previous exercise to show that E2cdh is secure in the
random oracle model under the ordinary CDH assumption for G (along with the assumption
that Es is 1CCA secure). In particular, show that for every 1CCA adversary Aattacking
E2cdh, there exist a CDH adversary Bcdh and a 1CCA adversary Bs, where Bcdh and Bs are
elementary wrappers around A, such that
1CCAroadv[A,E2cdh] ≤2 ·CDHadv[Bcdh,G] + 2Qro
q + 1CCAadv[Bs,Es],
where Qro is a bound on the number of random oracle queries made by A.
Discussion: Compared to the ElGamal encryption scheme, EEG, which we analyzed in Sec-
tion 12.4, this scheme achieves CCA security under the CDH assumption, rather than the stronger
ICDH assumption. Also, compared to the instantiation of the Fujisaki-Okamoto transformation
with ElGamal, EEG
FO , which we analyzed in Section 12.6.2, the reduction to CDH here is much
tighter, as we do not need to multiply CDH adv[Bcdh,G] by a factor of Qro as in (12.40). This tight
reduction even extends to the more general multi-key CCA setting, as explored in the next exercise.
12.34 (Multi-key CCA security of Twin CDH). Consider a slight modiﬁcation of the public-
key encryption scheme E2cdh from the previous exercise. This new scheme, which we call xE2cdh, is
exactly the same as E2cdh, except that instead of deriving the symmetric key as k = H(v,w1,w2),
we derive it as k = H(u1,u2,v,w 1,w2). Consider the security of xE2cdh in the multi-key CCA
attack game, discussed above in Exercise 12.6. In that attack game, suppose Qte is a bound on
the total number of encryptions. Also, let Qro be a bound on the total number of random oracle
queries. Let Abe an adversary that attacks the multi-key CCA security of xE2cdh. Show that A’s
advantage is at most
2 ·ϵcdh + 2Qro
q + Qte ·ϵs,
where ϵcdh is that advantage of a CDH adversary Bcdh attacking G and ϵs is the advantage of a
1CCA adversary Bs attacking Es (where both Bcdh and Bs are elementary wrappers around A).
Hint: Use the random self reduction for CDH (see Exercise 10.5).
525
Chapter 13
Digital signatures
In this chapter and the next we develop the concept of a digital signature. Although there are some
parallels between physical world signatures and digital signatures, the two are quite diﬀerent. We
motivate digital signatures with three examples.
Example 1: Software distribution. Suppose a software company, SoftAreUs, releases a soft-
ware update for its product. Customers download the software update ﬁle U by some means, say
from a public distribution site or from a peer-to-peer network. Before installing U on their machine,
customers want to verify that U really is from SoftAreUs. To facilitate this, SoftAreUs appends a
short tag to U, called a signature. Only SoftAreUs can generate a signature on U, but anyone in
the world can verify it. Note that there are no secrecy issues here — the update ﬁle U is available
in the clear to everyone. A MAC system is of no use in this setting because SoftAreUs does not
maintain a shared secret key with each of its customers. Some software distribution systems use
collision resistant hashing, but that requires an online read-only server that every customer uses to
check that the hash of the received ﬁle U matches the hash value on the read-only server.
To provide a clean solution, with no additional security infrastructure, we need a new crypto-
graphic mechanism called a digital signature. The signing process works as follows:
• First, SoftAreUs generates a secret signing key sk along with some corresponding public key
denoted pk. SoftAreUs keeps the secret key sk to itself. The public key pk is hard-coded into
all copies of the software sold by SoftAreUs and is used to verify signatures issued using sk.
• To sign a software update ﬁle U, SoftAreUs runs a signing algorithm S that takes (sk,U) as
input. The algorithm outputs a short signature σ. SoftAreUs then ships the pair ( U,σ) to
all its customers.
• A customer Bob, given the update (U,σ) and the public keypk, checks validity of this message-
signature pair using a signature veriﬁcation algorithm V that takes (pk,U,σ ) as input. The
algorithm outputs either accept or reject depending on whether the signature is valid or not.
Recall that Bob obtains pk from the pre-installed software system from SoftAreUs.
This mechanism is widely used in practice in a variety of software update systems. For security we
must require that an adversary, who has pk, cannot generate a valid signature on a fake update
ﬁle. We will make this precise in the next section.
526
We emphasize that a digital signature σ is a function of the data U being signed. This is very
diﬀerent from signatures in the physical world where the signature is always the same no matter
what document is being signed.
Example 2: Authenticated email. As a second motivating example, suppose Bob receives an
email claiming to be from his friend Alice. Bob wants to verify that the email really is from Alice.
A MAC system would do the job, but requires that Alice and Bob have a shared secret key. What
if they never met before and do not share a secret key? Digital signatures provide a simple solution.
First, Alice generates a public/secret key pair ( pk,sk). For now, we assume Alice places pk in a
public read-only directory. We will discuss how to get rid of this directory in just a minute.
When sending an email m to Bob, Alice generates a signature σ on m derived using her secret
key. She then sends ( m,σ) to Bob. Bob receives ( m,σ) and veriﬁes that m is from Alice in two
steps. First, Bob retrieves Alice’s public key pk. Second, Bob runs the signature veriﬁcation
algorithm on the triple ( pk,m,σ ). If the algorithm outputs accept then Bob is assured that the
message came from Alice. More precisely, Bob is assured that the message was sent by someone
who knows Alice’s secret key. Normally this would only be Alice, but if Alice’s key is stolen then
the message could have come from the thief.
As a more concrete example of this, the domain keys identiﬁed mail (DKIM) system is an email-
signing system that is widely used on the Internet. An organization that uses DKIM generates a
public/secret key pair ( pk,sk) and uses sk to sign every outgoing email from the organization.
The organization places the public key pk in the DNS records associated with the organization, so
that anyone can read pk. An email recipient veriﬁes the signature on every incoming DKIM email
to ensure that the email source is the claimed organization. If the signature is valid the email is
delivered, otherwise it is dropped. DKIM is widely used as a mechanism to make it harder for
spammers to send spam email that pretends to be from a reputable source.
Example 3: Certiﬁcates. As a third motivating example for digital signatures, we consider
their most widely used application. In Chapter 11 and in the authenticated email system above, we
assumed public keys are obtained from a read-only public directory. In practice, however, there is no
public directory. Instead, Alice’s public key pk is certiﬁed by some third party called a certiﬁcate
authority or CA for short. We will see how this process works in more detail in Section 13.8. For
now, we brieﬂy explain how signatures are used in the certiﬁcation process.
To generate a certiﬁed public key, Alice ﬁrst generates a public/private key pair ( pk,sk) for
some public-key cryptosystem, such as a public-key encryption scheme or a signature scheme. Next,
Alice presents her public key pk to the CA. The CA then veriﬁes that Alice is who she claims to
be, and once the CA is convinced that it is speaking with Alice, the CA constructs a statement m
saying “public key pk belongs to Alice.” Finally, the CA signs the message m using its own secret
key skCA and sends the pair Cert := (m,σCA) back to Alice. This pair Cert is called a certiﬁcate
for pk. When Bob needs Alice’s public key, he ﬁrst obtains Alice’s certiﬁcate from Alice and veriﬁes
the CA’s signature in the certiﬁcate. If the signature is valid, Bob has some conﬁdence that pk
is Alice’s public key. The main purpose of the CA’s digital signature is to prove to Bob that the
statement m was issued by the CA. Of course, to verify the CA’s signature, Bob needs the CA’s
public key pkCA. Typically, CA public keys come pre-installed with an operating system or a Web
browser. In other words, we simply assume that the CA’s public key is already available on Bob’s
machine.
527
Of course, the above can be generalized so that the CA’s certiﬁcate for Alice associates several
public keys with her identity, such as public keys for both encryption and signatures.
Non-repudiation. An interesting property of the authenticated email system above is that Bob
now has evidence that the message mis from Alice. He could show the pair ( m,σ) to a judge who
could also verify Alice’s signature. Thus, for example, if msays that Alice agrees to sell her car to
Bob, then Alice is (in some sense) committed to this transaction. Bob can use Alice’s signature as
proof that Alice agreed to sell her car to Bob — the signature binds Alice to the message m. This
property provided by digital signatures is called non-repudiation.
Unfortunately, things are not quite that simple. Alice can repudiate the signature by claiming
that the public key pk is not hers and therefore the signature was not issued by her. Or she
can claim that her secret key sk was stolen and the signature was issued by the thief. After all,
computers are compromised and keys are stolen all the time. Even worse, Alice could deliberately
leak her secret key right after generating it thereby invalidating all her signatures. The judge at
this point has no idea who to believe.
These issues are partially the reason why digital signatures are not often used for legal purposes.
Digital signatures are primarily a cryptographic tool used for authenticating data in computer sys-
tems. They are a useful building block for higher level mechanisms such as key-exchange protocols,
but have little to do with the legal system. Several legislative eﬀorts in the U.S. and Europe at-
tempt to clarify the process of digitally signing a document. In the U.S., for example, electronically
signing a document does not require a cryptographic digital signature. We discuss the legal aspects
of digital signatures in Section 13.9.
Non-repudiation does not come up in the context of MACs because MACs are non-binding.
To see why, suppose Alice and Bob share a secret key and Alice sends a message to Bob with an
attached MAC tag. Bob cannot use the tag to convince a judge that the message is from Alice
since Bob could have just as easily generated the tag himself using the MAC key. Hence Alice can
easily deny ever sending the message. The asymmetry of a signature system — the signer has sk
while the veriﬁer has pk — makes it harder (though not impossible) for Alice to deny sending a
signed message.
13.1 Deﬁnition of a digital signature
Now that we have an intuitive feel for how digital signature schemes work, we can deﬁne them
more precisely. Functionally, a digital signature is similar to a MAC. The main diﬀerence is that in
a MAC, both the signing and veriﬁcation algorithms use the same secret key, while in a signature
scheme, the signing algorithm uses one key, sk, while the veriﬁcation algorithm uses another, pk.
Deﬁnition 13.1. A signature scheme S= (G,S,V ) is a triple of eﬃcient algorithms, G,S and
V, where G is called a key generation algorithm , S is called a signing algorithm , and V is
called a veriﬁcation algorithm. Algorithm S is used to generate signatures and algorithm V is
used to verify signatures.
• Gis a probabilistic algorithm that takes no input. It outputs a pair (pk,sk), where sk is called
a secret signing key and pk is called a public veriﬁcation key.
• S is a probabilistic algorithm that is invoked as σ ←R S(sk,m), where sk is a secret key (as
output by G) and m is a message. The algorithm outputs a signature σ.
528
Challenger Adversary A
(pk,sk) ←
R
G() pk
mi
σi ←S(sk,mi)
(m,σ)
Figure 13.1: Signature attack game (Attack Game 13.1)
• V is a deterministic algorithm invoked as V(pk,m,σ ). It outputs either accept or reject.
• We require that a signature generated by S is always accepted by V. That is, for all (pk,sk)
output by G and all messages m, we have
Pr[V(pk, m, S(sk, m) ) = accept] = 1.
As usual, we say that messages lie in a ﬁnite message space M, and signatures lie in some ﬁnite
signature space Σ. We say that S= (G,S,V ) is deﬁned over (M,Σ).
13.1.1 Secure signatures
The deﬁnition of a secure signature scheme is similar to the deﬁnition of a secure MAC. We give
the adversary the power to mount a chosen message attack, namely the attacker can request the
signature on any message of his choice. Even with such power, the adversary should not be able
to create an existential forgery, namely the attacker cannot output a valid message-signature
pair ( m,σ) for some new message m. Here “new” means a message that the adversary did not
previously request a signature for.
More precisely, we deﬁne secure signatures using an attack game between a challenger and an
adversary A. The game is described below and in Fig. 13.1.
Attack Game 13.1 (Signature security). For a given signature scheme S= (G,S,V ), deﬁned
over (M,Σ), and a given adversary A, the attack game runs as follows:
• The challenger runs (pk,sk) ←R G() and sends pk to A.
• Aqueries the challenger several times. For i= 1,2,..., the ith signing query is a
message mi ∈M. Given mi, the challenger computes σi ←R S(sk,mi), and then
gives σi to A.
• Eventually Aoutputs a candidate forgery pair ( m,σ) ∈M× Σ.
We say that the adversary wins the game if the following two conditions hold:
• V(pk,m,σ ) = accept, and
529
• m is new, namely m̸∈{m1,m2,... }.
We deﬁne A’s advantage with respect to S, denoted SIGadv[A,S], as the probability that Awins
the game. Finally, we say that Ais a Q-query adversary if Aissues at most Q signing queries.
2
Deﬁnition 13.2 (secure signature). We say that a signature scheme Sis secure if for all eﬃcient
adversaries A, the quantity SIGadv[A,S] is negligible.
In case the adversary wins Attack Game 13.1, the pair (m,σ) it outputs is called an existential
forgery. Systems that satisfy Deﬁnition 13.2 are said to be existentially unforgeable under a
chosen message attack.
Veriﬁcation queries. In our discussion of MACs we proved Theorem 6.1, which showed that
tag veriﬁcation queries do not help the adversary forge MACs. In the case of digital signatures,
veriﬁcation queries are a non-issue — the adversary can always verify message-signature pairs for
himself. Hence, there is no need for an analogue to Theorem 6.1 for digital signatures.
Security against multi-key attacks. In real systems there are many users, and each one of
them can have a signature key pair ( pki,ski) for i = 1,...,n . Can a chosen message attack on
pk1 help the adversary forge signatures for pk2? If that were possible then our deﬁnition of secure
signature would be inadequate since it would not model real-world attacks. Just as we did for other
security primitives, one can generalize the notion of a secure signatures to the multi-key setting,
and prove that a secure signature is also secure in the multi-key settings. See Exercise 13.2. We
proved a similar fact for a secure MAC system in Exercise 6.3.
Strongly unforgeable signatures Our deﬁnition of existential forgery is a little diﬀerent than
the deﬁnition of secure MACs. Here we only require that the adversary cannot forge a signature
on a new message m. We do not preclude the adversary from producing a new signature on m
from some other signature on m. That is, a signature scheme is secure even if the adversary can
transform a valid pair ( m,σ) into a new valid pair ( m,σ′).
In contrast, for MAC security we insisted that given a message-tag pair ( m,t) the adversary
cannot create a new valid tag t′̸= tfor m. This was necessary for proving security of the encrypt-
then-MAC construction in Section 9.4.1. It was also needed for proving that MAC veriﬁcation
queries do not help the adversary (see Theorem 6.1 and Exercise 6.7).
One can similarly strengthen Deﬁnition 13.2 to require this more stringent notion of existential
unforgeability. We capture this in the following modiﬁed attack game.
Attack Game 13.2. For a given signature scheme S= (G,S,V ), and a given adversary A, the
game is identical to Attack Game 13.1, except that the second bullet in the winning condition is
changed to:
• (m,σ) is new, namely ( m,σ) ̸∈
{
(m1,σ1), (m2,σ2),...
}
We deﬁne A’s advantage with respect to S, denoted stSIGadv[A,S], as the probability that Awins
the game. 2
Deﬁnition 13.3. We say that a signature scheme Sis strongly secure if for all eﬃcient adver-
saries A, the quantity stSIGadv[A,S] is negligible.
530
Strong security ensures that for a secure signature scheme, the adversary cannot create a new
signature on a previously signed message, as we required for MACs. There are a few speciﬁc
situations that require signatures satisfying this stronger security notion, such as [56, 32] and
a signcryption construction described in Section 13.7. However, most often Deﬁnition 13.2 is
suﬃcient. At any rate, any secure signature scheme S= (G,S,V ) can be converted into a strongly
secure signature scheme S′= (G′,S′,V ′). See Exercise 14.10.
13.1.1.1 Strong binding
Deﬁnition 13.2 ensures that generating a valid message-signature pair is diﬃcult without the secret
key. The deﬁnition, however, does not capture several additional desirable properties for a signature
scheme. Speciﬁcally, it does not preclude a signer from misbehaving in the following ways.
Message confusion: what was signed? Deﬁnition 13.2 does not preclude the signer from
obtaining two distinct messages mand m′and a signature σ, so that σ is a valid signature for both
m and m′. The message m might say “Alice owes Bob ten dollars” while m′says “Alice owes Bob
one dollar.” Since σ is a valid signature for both messages, a judge cannot tell what message Alice
actually signed. If a signer can make this happen, we say the scheme is vulnerable to message
confusion. See Exercise 13.3 for such a scheme.
In common applications of digital signatures the signer is bound to every message that has a
valid signature by the signer. In the example above, Alice would be bound to both messages, and
owe Bob eleven dollars. Consequently, in this setting, there is no harm if the signer produces two
distinct messages with the same signature. However, in other settings, this may be undesirable.
The constructions given in this chapter and the next are not vulnerable to message confusion.
Indeed, for these constructions, a signature is a binding commitment to the message, which means
that the signer cannot produce a public key, a signature, and two distinct messages so that the
signature is valid for both messages.
Signer confusion: who signed? Let S= (G,S,V ) be a signature scheme and let ( m,σ) be
a valid message-signature pair with respect to some public key pk. Suppose an attacker who sees
(pk,m,σ ), can generate a new public key pk′, where pk′̸= pk, such that ( m,σ) is also valid with
respect to the public key pk′. This can cause confusion over who signed m: was it pk or was it pk′?
Both can claim ownership of ( m,σ).
A signature scheme that can be attacked in this way is said to be vulnerable to signer confu-
sion. Exercise 13.4 gives an example of a signature scheme that is vulnerable to signer confusion.
In this example the attacker not only generates pk′, but also the corresponding secret key sk′.
Signer confusion can lead to some undesirable consequences. For example, suppose ( m,σ) is
a signed homework solution set submitted by a student Alice. After the submission deadline, an
attacker Molly, who did not submit a solution set, can use signer confusion to claim that the
homework submission (m,σ) is hers. To do so, Molly generates a public key pk′ such that (m,σ)
is a valid message-signature pair for the key pk′. Because the assignment is properly signed with
respect to both public keys, pk and pk′, the Professor cannot tell who submitted the assignment
(assuming the homework m does not identify Alice). In practice, signer confusion has been used
to attack certain key exchange protocols.
531
Strongly binding signatures. In Exercise 13.5 we deﬁne the concept of a strongly binding
signature scheme which prevents both message confusion and signer confusion. The exercise shows
that, if needed, any signature scheme can be made strongly binding by having the signer append a
collision resistant hash of ( pk,m) to the signature (Exercise 13.5(c)).
13.1.2 Mathematical details
As usual, we give a more mathematically precise deﬁnition of a signature, using the terminology
deﬁned in Section 2.3. This section may be safely skipped on ﬁrst reading.
Deﬁnition 13.4 (Signature). A signature scheme is a triple of eﬃcient algorithms (G,S,V ),
along with two families of spaces with system parameterization P:
M = {Mλ,Λ}λ,Λ, and Σ = {Σλ,Λ}λ,Λ,
As usual, λ∈Z≥1 is a security parameter and Λ ∈Supp(P(λ)) is a system parameter. We require
that
1. M and Σ are eﬃciently recognizable.
2. Algorithm G is an eﬃcient probabilistic algorithm that on input λ,Λ, where λ ∈Z≥1, Λ ∈
Supp(P(λ)), outputs a pair (pk,sk), where pk and sk are bit strings whose lengths are always
bounded by a polynomial in λ.
3. Algorithm S is an eﬃcient probabilistic algorithm that on input λ,Λ,sk,m, where λ ∈Z≥1,
Λ ∈Supp(P(λ)), (pk,sk) ∈Supp(G(λ,Λ)) for some pk, and m ∈Mλ,Λ, always outputs an
element of Σλ,Λ.
4. Algorithm V is an eﬃcient deterministic algorithm that on input λ,Λ,pk,m,σ , where λ ∈
Z≥1, Λ ∈Supp(P(λ)), (pk,sk) ∈Supp(G(λ,Λ)) for some sk, m∈Mλ,Λ, and σ∈Σλ,Λ, and
outputs either accept or reject.
In deﬁning security, we parameterize Attack Game 13.1 by the security parameter λ which is
given to both the adversary and the challenger. The advantage SIGadv[A,S] is then a function of λ.
Deﬁnition 13.2 should be read as saying that SIG adv[A,S](λ) is a negligible function. Similarly for
Deﬁnition 13.3.
13.2 Extending the message space with collision resistant hashing
Suppose we are given a secure digital signature scheme with a small message space, say M=
{0,1}256. We show how to extend the message space to much larger messages using a collision
resistant hash function. We presented a similar construction for MACs in Fig. 8.1. LetS= (G,S,V )
be a signature scheme deﬁned over (M,Σ) and let H : M′→M be a hash function, where the set
M′is much larger than M. Deﬁne a new signature scheme S′= (G,S′,V ′) over (M′,Σ) as
S′(sk, m) := S(sk, H(m)) and V′(pk, m, σ) := V(pk, H(m), σ) (13.1)
The new scheme signs much larger message than the original scheme. This approach is often called
the hash-and-sign paradigm. As a concrete example, suppose we take H to be SHA256. Then
532
any signature scheme capable of signing 256-bit messages can be securely extended to a signature
scheme capable of signing arbitrary long messages. Hence, from now on it suﬃces to focus on
building signature schemes for short 256-bit messages.
The following simple theorem shows that this construction is secure. Its proof is essentially
identical to the proof of Theorem 8.1.
Theorem 13.1. Suppose the signature scheme S is secure and the hash function H is collision
resistant. Then the derived signature scheme S′= (G,S′,V ′) deﬁned in (13.1) is a secure signature.
In particular, suppose Ais a signature adversary attacking S′(as in Attack Game 13.1). Then
there exist an eﬃcient signature adversary BS and an eﬃcient collision ﬁnder BH, which are
elementary wrappers around A, such that
SIGadv[A,S′] ≤SIGadv[BS,S] + CRadv[BH,H]
13.2.1 Extending the message space using TCR functions
We brieﬂy show that collision resistance is not necessary for extending the message space of a
signature scheme. A second pre-image resistant (SPR) hash function is suﬃcient. Recall that in
Section 8.11.2 we used SPR hash functions to build target collision resistant (TCR) hash functions.
We then used a TCR hash function to extend the message space of a MAC. We can do the same
here to extend the message space of a signature scheme.
Let H be a TCR hash function deﬁned over ( KH,M,T). Let S= ( G,S,V ) be a signature
scheme for short messages in KH×T. We build a new signature scheme S′= (G,S′,V ′) for signing
messages in Mas follows:
S′(sk, m) := V′(pk, m,(σ,r) ) :=
r←R KH h←H(r,m) (13.2)
h←H(r,m) Output V(pk, (r,h), σ)
σ←S
(
sk, (r,h)
)
Output (σ,r)
The signing procedure chooses a random TCR key r, includes ras part of the message being signed,
and outputs r as part of the ﬁnal signature. As a result, signatures produced by this scheme are
longer than signatures produced by extending the domain using a collision resistant hash, as above.
Using the TCR construction from Fig. 8.15, the length of ris logarithmic in the size of the message
being signed. This extra logarithmic size key must be included in every signature. Exercise 13.7
proposes a way to get shorter signatures.
The beneﬁt of the TCR construction is that security only relies on H being TCR, which is a
much weaker property than collision resistance and hence more likely to hold for H. For example,
the function SHA256 may eventually be broken as a collision-resistant hash, but the function
H(r,m) := SHA256(r∥m) may still be secure as a TCR.
The following theorem proves security of the construction in (13.2) above. The theorem and its
proof are almost identical to the same theorem and proof applied to MAC systems (Theorem 8.14).
Note that the concrete bound in the theorem below has an extra factor of Q that does not appear
in Theorem 13.1 above. The reason for this extra Q factor is the same as in the proof for MAC
systems (Theorem 8.14).
533
Theorem 13.2. Suppose S= (G,S,V ) is a secure signature scheme and the hash function H is
TCR. Then the derived signature scheme S′= (G,S′,V ′) deﬁned in (13.2) is secure.
In particular, for every signature adversary Aattacking S′(as in Attack Game 13.1) that issues
at most Q signing queries, there exist an eﬃcient signature adversary BS and an eﬃcient TCR
adversary BH, which are elementary wrappers around A, such that
SIGadv[A,S′] ≤SIGadv[BS,S] + Q·TCRadv[BH,H].
13.3 Signatures from trapdoor permutations: the full domain
hash
We now turn to constructing signature schemes. Secure signature schemes can be built from many
cryptographic primitives.
• Signature schemes from hash functions: In Chapter 14 we will construct signature schemes
that require nothing more than a collision resistant hash function. These signature schemes
can have fast signing and veriﬁcation algorithms, but the resulting signatures are several
killobytes long, much longer than signatures generated by an algebraic signature scheme. As
such, they can be useful for signing software distribution packages. These packages tend to
be quite large and the signature, even a relatively long one, has little impact on the overall
package size. An important beneﬁt of hash-based schemes is that they remain secure against
an adversary who has access to a quantum computer, and are thus said to be post-quantum
secure (we discuss quantum computing attacks in Sections 4.3.4 and 16.5).
• Signature schemes from a trapdoor permutation: This is the topic of this chapter. As we
will see in a moment, a trapdoor permutation gives a simple and direct construction for a
signature scheme.
• Signature schemes from discrete log: In Chapters 15 and 19 we will construct signature
schemes using a ﬁnite cyclic group where the discrete log problem is diﬃcult. Signatures and
public keys in these schemes are short, only a few tens of bytes. These signature schemes have
many useful properties, and are frequently used in networking and ﬁnancial applications.
The signature schemes presented in this chapter are proven secure in the random oracle model. We
will present practical non-random-oracle constructions in Chapters 14 and 15.
Now, let us build our ﬁrst signature scheme.
The full domain hash signature scheme. We begin with a simple construction based on
a trapdoor permutation. We then present a concrete signature scheme from the only trapdoor
permutation we have, namely RSA. Recall that a trapdoor permutation scheme deﬁned over Xis a
triple of algorithms T = (G,F,I ), where Ggenerates a public key/secret key pair (pk,sk), F(pk,·)
evaluates a permutation on X in the forward direction, and I(sk,·) evaluates the permutation in
the reverse direction. See Section 10.2 for details.
We show that a trapdoor permutation T gives a simple signature scheme. The only other
ingredient we need is a hash function H that maps messages in Mto elements in X. This function
will be modeled as a random oracle in the security analysis. The signature scheme, called full
domain hash (FDH), denoted SFDH, works as follows:
534
• The key generation algorithm for SFDH is the key generation algorithm G of the trapdoor
permutation scheme T. It outputs a pair ( pk,sk).
• The signature on mis simply the inverse of H(m) with respect to the function F(pk,·). That
is, to sign a message m∈M using sk, the signing algorithm S runs as follows:
S(sk,m) := y←H(m), σ ←I(sk,y)
output σ.
• To verify a signature σ on a message m the veriﬁcation algorithm V checks that F(pk,σ) is
equal to H(m). More precisely, V works as follows:
V(pk,m,σ ) := y←F(pk,σ)
if y= H(m) output accept; otherwise, output reject.
We will analyze SFDH by modeling the hash function H as a random oracle. Recall that in the
random oracle model (see Section 8.10), the function H is modeled as a random function Ochosen
at random from the set of all functions Funs[M,X]. More precisely, in the random oracle version of
Attack Game 13.1, the challenger chooses Oat random. In any computation where the challenger
would normally evaluate H, it evaluates Oinstead. In addition, the adversary is allowed to ask the
challenger for the value of the function Oat any point of its choosing. The adversary may make
any number of such “random oracle queries” at any time of its choosing. We use SIG roadv[A,SFDH]
to denote A’s advantage against SFDH in the random oracle version of Attack Game 13.1.
Theorem 13.3. Let T = (G,F,I ) be a one-way trapdoor permutation deﬁned over X. Let H :
M→X be a hash function. Then the derived FDH signature scheme SFDH is a secure signature
scheme when H is modeled as a random oracle.
In particular, let Abe an eﬃcient adversary attacking SFDH in the random oracle version of
Attack Game 13.1. Moreover, assume that Aissues at most Qro random oracle queries and Qs
signing queries. Then there exists an eﬃcient inverting adversary Bthat attacks T as in Attack
Game 10.2, where Bis an elementary wrapper around A, such that
SIGroadv[A,SFDH] ≤(Qro + 1) ·OWadv[B,T] (13.3)
An overview of the proof of security for SFDH. We defer the full proof of Theorem 13.3
to Section 13.4.2. For now, we sketch the main ideas. To forge a signature on a message m, an
adversary has to compute σ= I(sk,y), where y= H(m). With H modeled as a random oracle, the
value yis essentially just a random point in X, and so this should be hard to do, assuming T is one
way. Unfortunately, this argument does not deal with the fact that in a chosen message attack, the
adversary can get arbitrary messages signed before producing its forgery. Again, sinceH is modeled
as a random oracle, this eﬀectively means that to break the signature scheme, the adversary must
win the following game: after seeing several random points y1,y2,... in X (corresponding to the
hash outputs on various messages), the adversary can ask to see preimages of some of the yi’s
(corresponding to the signing queries), and then turn around and produce the preimage of one of
the remaining yi’s. It turns out that winning this game is not too much easier than breaking the
one-wayness of T in the usual sense. This will be proved below in Lemma 13.5 using a kind of
“guessing argument”: in the reduction, we will have to guess in advance at which of the random
points the adversary will invert F(pk,·). This is where the factor Qro + 1 in (13.3) comes from.
535
Unique signatures. The SFDH scheme is a unique signature scheme: for a given public key,
every message m has a unique signature σ that will be accepted as valid for m by the veriﬁcation
algorithm. This means that if SFDH is secure, it must also be strongly secure in the sense of
Deﬁnition 13.3.
The importance of hashing. The hash function H is crucial to the security of SFDH. Without
ﬁrst hashing the message, the system is trivially insecure. To see why, suppose we incorrectly deﬁne
the signature on m ∈X as σ := I(sk,m). That is, we apply I without ﬁrst hashing m. Then to
forge a signature, the adversary simply chooses a random σ ∈X and computes m ←F(pk,σ).
The pair (m,σ) is an existential forgery. Note that this forgery is created without using the chosen
message attack. Of course this m is likely to be gibberish, but is a valid existential forgery.
This attack shows that the hash function H plays a central role in ensuring that SFDH is secure.
Unfortunately, we can only prove security whenH is modeled as a random oracle. We cannot prove
security of SFDH, when H is a concrete hash function, using standard assumptions about T and H.
13.3.1 Signatures based on the RSA trapdoor permutation
We instantiate the SFDH construction with the only trapdoor permutation at our disposal, namely
RSA. We obtain the RSA full domain hash signature scheme, denoted SRSA-FDH. Recall that
parameters for RSA are generated using algorithm RSAGen( ℓ,e) which outputs a pair ( pk,sk)
where pk = ( n,e). Here n is a product of two ℓ-bit primes. The RSA trapdoor permutation
F(pk,·) : Zn →Zn is deﬁned as F(pk,x) := xe.
For each public key pk = ( n,e), the SRSA-FDH system needs a hash function H that maps
messages in Mto Zn. This is a problem — the output space of H depends on nwhich is diﬀerent
for every public key. Since hash functions generally have a ﬁxed output space, it is preferable
that the range of H be ﬁxed and independent of n. To do so, we deﬁne the range of H to be
Y:= {1,..., 22ℓ−2}which, when embedded in Zn, covers a large fraction of Zn, for all the RSA
moduli n output by RSAGen(ℓ,e).
We describe the signature scheme SRSA-FDH using a hash function H deﬁned over (M,Y). We
chose Yas above so that |Y|≥ n/4 for all n output by RSAGen( ℓ,e). This is necessary for the
proof of security. Because an RSA modulus n is large, at least 2048 bits, the hash function H
must produce a large output, approximately 2048 bits long. One cannot simply use SHA256. We
described appropriate long-output hash functions in Section 8.10.2.
For a given hash function H : M→Y , the SRSA-FDH signature scheme works as follows:
• the key generation algorithm G uses parameters ℓ and e and runs as follows:
G() := ( n,d) ←R RSAGen(ℓ,e), pk ←(n,e), sk ←(n,d)
output (pk,sk);
• for a given secret key sk = (n,d), and message m∈M, algorithm S runs as follows:
S(sk,m) := y←H(m) ∈Y, σ ←yd ∈Zn
output σ;
• for a given public key pk = (n,e) the veriﬁcation algorithm runs as follows:
V(pk,m,σ ) := y←σe ∈Zn
if y= H(m) output accept; otherwise, output reject.
536
Signing and veriﬁcation speed. Recall that typically the public key exponent eis small, often
e = 3 or e = 65537, while the secret key exponent d is as large as n. Consequently, signature
generation, which uses a d exponentiation, is much slower than signature veriﬁcation. In fact,
RSA has the fastest signature veriﬁcation algorithm among all the standardized signature schemes.
This makes RSA very attractive for applications where a signature is generated oﬄine, but needs
to be quickly veriﬁed online. Certiﬁcates used in a public key infrastructure are a good example
where fast veriﬁcation is attractive. We discuss ways to speed-up the RSA signing procedure in
Chapter 16.
Signature size. One downside of RSA is that the signatures are much longer than in other
signature schemes, such as the ones presented in Chapter 19. To ensure that factoring the RSA
modulus n is suﬃciently diﬃcult, the size of n must be at least 2048 bits (256 bytes). As a result,
RSA signatures are 256 bytes, which is considerably longer than in other schemes. This causes
diﬃculties in heavily congested or low bandwidth networks as well as in applications where space
is at a premium. For example, at one point the post oﬃce looked into printing digital signatures
on postage stamps. The signatures were intended to authenticate the recipient’s address and were
to be encoded as a two dimensional bar code on the stamp. RSA signatures were quickly ruled
out because there is not enough space on a postage stamp. We will discuss short signatures in
Section 15.5.
The importance of hashing. We showed above that SFDH is insecure without ﬁrst hashing the
message. In particular, consider the unhashed RSA system where a signature on m ∈Zn is
deﬁned as σ:= md. We showed that this system is insecure since anyone can create an existential
forgery (m,σ). Recall, however, that this attack typically forges a signature on a message m that
is likely to be gibberish.
We can greatly strengthen the attack on this unhashed RSA using the random self-reducibility
property of RSA (see Exercise 10.28). In particular, we show that an attacker can obtain the
signature on any message m of his choice by issuing a single signing query for a random ˆ m∈Z∗
n.
Let (n,e) be an RSA public key and let m∈Zn be some message. As the reader should verify, we
may assume that m∈Z∗
n. To obtain the signature on m the attacker does the following:
r←R Z∗
n, ˆm←m·re
Request the signature on ˆm and obtain ˆσ
Output σ←ˆσ/r
Indeed, if ˆσe = ˆm then σ∈Zn is a valid signature on m since
σe = (ˆσ/r)e = ˆσe/re = ˆm/re = m. (13.4)
The attack shows that by fooling the user into signing a random message ˆ m the adversary can
obtain the signature on a message m of his choice. We say that unhashed RSA signatures are
universally forgeable and thus should never be used.
Surprisingly, the fact that an attacker can convert a signature on a random message into a
signature on a chosen message turns out to play a central role in the construction of so called blind
signatures. Blind signatures are used in protocols for anonymous electronic cash and anonymous
electronic voting. In both applications blind signatures are the main ingredient for ensuring privacy
(see Exercise 13.15).
537
Security of RSA full domain hash. Recall that the security proof for the general full domain
hash SFDH (Theorem 13.3) was very loose: an adversary Awith advantage ϵin attacking SFDH gives
an adversary Bwith advantage ϵ/(Qro + Qs + 1) in attacking the underlying trapdoor permutation.
Can we do better? Indeed, we can: using the random self-reducibility property of RSA, we can
prove security with a much tighter bound, as shown in Theorem 13.4 below. In particular, the factor
Qro + Qs + 1 is replaced by (approximately) Qs. This is signiﬁcant, because in a typical attack, the
number of signing queries Qs is likely to be much smaller than the number of random oracle queries
Qro. Indeed, on the one hand, Qro represents the number of times an attacker evaluates the hash
function H. These computations can be done by the attacker “oﬀ line,” and the attacker is only
bounded by his own computing resources. On the other hand, each signing query requires that an
honest user sign a message. Concretely, a conservative bound on Qro could perhaps be as large
as 2 128, while Qs could perhaps be reasonably bounded by 2 40. We thus obtain a much tighter
reduction for SRSA-FDH than for SFDH with a general trapdoor permutation. However, even for
SRSA-FDH the reduction is not tight due to the Qs factor. We will address that later in Section 13.5.
As in the proof of SFDH, our security proof for SRSA-FDH models the hash function H : M→Y
as a random oracle. The proof requires that Yis a large subset of Zn (we speciﬁcally assume that
|Y|≥ n/4, but any constant fraction would do). In what follows, we use 2 .72 as an upper bound
on the base of the natural logarithm e≈2.718 (not to be confused with the RSA public exponent
e).
Theorem 13.4. Let H : M → Ybe a hash function, where Y = {1,..., 22ℓ−2}. If the RSA
assumption holds for (ℓ,e), then SRSA-FDH with parameters (ℓ,e) is a secure signature scheme,
when H is modeled as a random oracle.
In particular, let Abe an eﬃcient adversary attacking SRSA-FDH in the random oracle version
of Attack Game 13.1. Moreover, assume that Aissues at most Qs signing queries. Then there
exists an eﬃcient RSA adversary Bas in Attack Game 10.3, where and Bis an elementary
wrapper around A, such that
SIGroadv[A,SRSA-FDH] ≤2.72 ·(Qs + 1) ·RSAadv[B,ℓ,e ] (13.5)
We defer the proof of Theorem 13.4 to Section 13.4.2.
13.4 Security analysis of full domain hash
The goal of this section is to analyze the security of the the full domain hash signature scheme;
speciﬁcally, we prove Theorems 13.3 and 13.4. We begin with a tool that will be helpful, and is
interesting and useful in its own right.
13.4.1 Repeated one-way functions: a useful lemma
Let f be a one-way function over ( X,Y). Brieﬂy, this means that given y ←f(x) for a random
x∈X, it is diﬃcult to ﬁnd a pre-image of y. This notion was presented in Deﬁnition 8.6.
Consider the following, seemingly easier, problem: we give the adversary
(
f(x1),...,f (xt)
)
and
allow the adversary to request some, but not all, of the xi’s. To win, the adversary must produce
one of the remaining xi’s. We refer to this as the t-repeated one-way problem. More precisely,
the problem is deﬁned using the following game.
538
Attack Game 13.3 ( t-repeated one-way problem). For a given positive integer tand a given
adversary A, the game runs as follows:
• The challenger computes
x1,...,x t ←R X, y 1 ←f(x1),...,y t ←f(xt)
and sends (y1,...,y t) to the adversary.
• Amakes a sequence of reveal queries. Each reveal query consists of an index j ∈{1,...,t }.
Given j, the challenger sends xj to A.
• Eventually, Athe adversary outputs (ν,x), where ν ∈{1,...,t }and x∈X.
We say that Awins the game if index ν is not among A’s reveal queries, and f(x) = yν. We deﬁne
A’s advantage, denoted rOWadv[A,f,t ], as the probability that Awins the game. 2
The following lemma shows that the repeated one-way problem is equivalent to the standard
one-way problem given in Deﬁnition 8.6. That is, winning in Attack Game 13.3 is not much easier
than inverting f.
Lemma 13.5. For every t-repeated one-way adversary Athere exists a standard one-way adversary
B, where Bis an elementary wrapper around A, such that
rOWadv[A,f,t ] ≤t·OWadv[B,f]. (13.6)
Proof idea. The proof is a kind of “guessing argument”, somewhat similar to what we did, for
example, in the proof of Theorem 6.1. We want to use Ato build an adversary Bthat breaks the
one-wayness of f. So Bstarts with y∗ ∈Y and wants to ﬁnd a preimage of y∗ under f, using A
as a subroutine. The ﬁrst thing that Bdoes is make a guess ω at the value of the index ν that A
will ultimately choose. Our adversary Bthen prepares values y1,...,y t ∈Y as follows: for i̸= ω,
it sets yi ←f(xi) for random xi ∈X; it also sets yω ←y∗. It then sends ( y1,...,y t) to A, as in
Attack Game 13.3. If B’s guess was correct (which happens with probability 1 /t), it will be able
to respond to all of A’s queries, and A’s ﬁnal output will provide the preimage of y that Bwas
looking for. 2
Proof. In more detail, our adversary Bis given y∗:= f(x∗) for a random x∗∈X, and then plays
the role of challenger to Aas in Attack Game 13.3 as follows:
539
Initialize:
x1,...,x t ←R X
y1 ←f(x1),...,y t ←f(xt)
ω←R {1,...,t }, y ω ←y∗ / / Plug y∗ at position ω
Send (y1,...,y t) to A
/ / Bnow knows pre-images for all yi’s other than yω
Upon receiving a query j ∈{1,...,t }from A:
if j ̸= ω
then send xj to A
else output fail and stop
When Aoutputs a pair ( ν,x):
if ν = ω
then output x and stop
else output fail and stop
Now we argue that the inequality (13.6) holds.
Deﬁne Game 0 to be the game played between Aand the challenger in Attack Game 13.3, and
let W0 be the event that Awins the game.
Now deﬁne a new Game 1, which is the same as Game 0, except that the challenger chooses
ω ∈{1,...,t }at random. Also, we say that Awins Game 1 if it wins as in Game 0 with output
(ν,x) such that ν = ω. Deﬁne W1 to be the event that Awins Game 1.
We can think of Games 0 and 1 as operating on the same underlying probability space. Really,
the two games are exactly the same: all that changes is the winning condition. Moreover, as ω is
independent of everything else, we have
Pr[W1] = Pr[W0 ∧ν = ω] = Pr[W0] ·Pr[ν = ω|W0] = (1/t) ·Pr[W0].
Moreover, it is clear that OW adv[B,f] = Pr[W1]. Adversary Bis really just playing Game 1 — it
only aborts when it is clear that it will not win Game 1 anyway, and it wins Game 1 if and only if
it succeeds in ﬁnding a preimage of y∗. 2
Application to trapdoor functions. Lemma 13.5 applies equally well to trapdoor functions.
If T = (G,F,I ) is a trapdoor function scheme deﬁned over ( X,Y), then T is one way in the sense
of Deﬁnition 10.3 if and only if f := F(pk,·) is one way in the sense of Deﬁnition 8.6. Indeed, for
any adversary, the respective advantages in the corresponding attack games are equal. Technically,
with f := F(pk,·), the public key pk is viewed as a “system parameter” deﬁning f.
A tighter reduction for RSA. For a general one-way function f, the concrete bound in
Lemma 13.5 is quite poor: if adversary Ahas advantage ϵ in winning the t-repeated one-way
game, then the lemma constructs a one-way attacker with advantage only ϵ/t.
When f is derived from the RSA function we can obtain a tighter reduction using the random
self-reducibility property of RSA. We replace the factor t by a factor of (about) Q, where Q is the
number of reveal queries from A. This Q is usually much smaller than t.
We ﬁrst restate Attack Game 13.3 as it applies to the RSA function. We slightly tweak the game
and require that the images y1,...,y t given to Alie in a certain large subset of Zn denoted Y. For
540
RSA parameters ℓand e, we set Y:= {1,2,..., 22ℓ−2}so that for all ngenerated by RSAGen(ℓ,e),
we have |Y|≥ n/4.
Attack Game 13.4 ( t-repeated RSA). For given RSA parameters ℓ and e, a given positive
integer t, and a given adversary A, the game runs as follows:
• The challenger computes
(n,d) ←R RSAGen(ℓ,e)
y1,...,y t ←R Y / / Recall that Y:= {1,2,..., 22ℓ−2}
and sends (n,e) and (y1,...,y t) to A.
• Amakes a sequence of reveal queries. Each reveal query consists of an index j ∈{1,...,t }.
Given j, the challenger sends xj := yd
j ∈Zn to A.
• Eventually the adversary outputs (ν,x), where ν ∈{1,...,t }and x∈Zn.
We say that Awins the game if index ν is not among A’s reveal queries, and xe = yν. We deﬁne
A’s advantage, denoted rRSAadv[A,ℓ,e,t ], as the probability that Awins the game. 2
We show that the t-repeated RSA problem is equivalent to the basic RSA problem, but with a
tighter concrete bound than in Lemma 13.5. In particular, the factor oftis replaced by 2.72·(Q+1).
The constant 2.72 is an upper on the base of the natural logarithm e≈2.718.
Lemma 13.6. Let ℓ and e be RSA parameters. For every t-repeated RSA adversary Athat makes
at most Q reveal queries, there exists a standard RSA adversary B, where B is an elementary
wrapper around A, such that
rRSAadv[A,ℓ,e,t ] ≤2.72 ·(Q+ 1) ·RSAadv[B,ℓ,e ]. (13.7)
Proof idea. The proof is similar to that of Lemma 13.5. In that proof, we plugged the challenge
instance y∗ of the one-way attack game at a random position among the yi’s, and using A, we
succeed if Adoes not issue a reveal query at the plugged position, and its output inverts at the
plugged position. Now, using the random self-reducibility property for RSA, we take the challenge
y∗, and “spread it around,” plugging related, randomized versions of y∗at many randomly chosen
positions. We succeed if A’s reveal queries avoid the plugged positions, but its output inverts at
one of them. By increasing the number of plugged positions, the chance of hitting one at the output
stage increases (which is good), but the chance of avoiding them during a reveal query decreases
(which is bad). Using a clever strategy for sampling the set of plugged positions, we can optimize
the success probability to get the desired result. 2
Proof. We describe an adversary Bthat is given ( n,e) and a random y∗∈Zn, and then attempts
to compute an eth root of y∗.
We ﬁrst deal with an annoying corner case. It may happen (albeit with very small probability)
that y∗ /∈Z∗
n. However, in this case, it is easy to compute the eth root of y∗: if y∗ = 0, the eth
root is 0; otherwise, gcd( y∗,n) gives us the prime factorization of n, which allows us to compute
the decryption exponent d, and hence the eth root of y∗.
So from now on, we assume y∗ ∈Z∗
n. Adversary Buses Ato compute an eth root of y∗ as
shown in Fig. 13.2. First, Bgenerates t random values y1,...,y t ∈Y and sends them to A. For
541
Initialize: / / Generate random y1,...,y t ∈Y
Ω ←∅
for i= 1,...,t :
ﬂip a biased coin ci ∈{0,1}such that Pr[ci = 1] = 1/(Q+ 1)
if ci = 1 then Ω ←Ω ∪{i}
(1) repeat
xi ←R Zn, y i ←xe
i ·yci
∗ / / So yi = xe
i or yi = xe
i ·y∗
until yi ∈Y
Send (n,e) and (y1,...,y t) to A
/ / Bnow knows pre-images for all yi where i̸∈Ω
Upon receiving a reveal query j ∈{1,...,t }from A:
if j ̸∈Ω
then send xj to A
else output fail and stop
When Aoutputs a pair ( ν,x):
if ν ∈Ω
(2) then ˜ x←x/xν, output ˜x
else output fail and stop
Figure 13.2: Algorithm Bin the proof of Lemma 13.6
each i = 1,...,t , either yi = xe
i, in which case Bknows an eth root of yi and can respond to a
reveal query for i, or yi = xe
i ·y∗ in which case Bdoes not know an eth root of yi. Here, Ω is the
set of indices i for which Bdoes not know an eth root of yi.
If Breaches the line marked (2) and x is an eth root of yν, we have
˜xe = (x/xν)e = xe/xe
ν = yν/xe
ν = (xe
ν ·y∗)/xe
ν = y∗,
and so B’s output ˜x is an eth root of y∗.
Actually, we have ignored another corner case. Namely, it may happen (again, with very small
probability) that the value xν computed above does not lie in Z∗
n. However, if that happens, it
must be the case that xν ̸= 0 (since 0 /∈Y), and as in the other corner case, we can use xν to factor
n and compute the decryption exponent.
Let us analyze the repeat/until loop at the line marked (1) for a ﬁxed i = 1 ,...,t . Since
y∗∈Z∗
n, each candidate value for yi generated in the loop body is uniformly distributed over Zn.
Since |Y|≥ n/4, the probability that each candidate yi lies in Yat at least 1 /4. Therefore, the
expected number of loop iterations is at most 4. Moreover, when the loop terminates, the ﬁnal
value of yi is uniformly distributed over Y.
We now argue that (13.7) holds. The basic structure of the argument is the same as in
Lemma 13.5. Deﬁne Game 0 to be the game played between Aand the challenger in Attack
Game 13.4, and let W0 be the event that Awins the game.
Now deﬁne a new Game 1, which is the same as Game 0, except that the challenger generates
a set of indices Ω ⊆{1,...,t }, as follows: each i = 1 ,...,t is independently added to Ω with
542
probability 1/(Q+ 1). Let Rbe the set of reveal queries made by A. We say that Awins Game 1
if it wins as in Game 0 with output ( ν,x), and in addition, R∩Ω = ∅and ν ∈Ω. Deﬁne W1 to be
the event that Awins Game 1. We have
Pr[W1] = Pr
[
W0 and R∩Ω = ∅and ν ∈Ω
]
= Pr[W0] ·Pr
[
R∩Ω = ∅and ν ∈Ω
⏐⏐ W0
]
.
Moreover, it is not hard to see that
RSAadv[B,ℓ,e ] ≥Pr[W1].
Indeed, when B’s input y∗lies in Z∗
n, adversary Bis essentially just playing Game 1: the distributions
of (y1,...,y t,Ω) are identical in both games. The condition R∩Ω = ∅corresponds to the condition
that Bdoes not abort in processing one of A’s reveal queries. The condition ν ∈Ω corresponds
to the condition that Bdoes not abort at A’s output stage. When B’s input y∗lies outside of Z∗
n,
adversary Balways wins.
Since Ω is independent of A’s view, it suﬃces to prove the following:
Claim. Let Ω be a randomly generated subset of{1,...,t }, as above. Let R⊆{ 1,...,t }
be a ﬁxed set of at most Qindices, and let ν ∈{1,...,t }be a ﬁxed index not in R. Let
X be the event that R∩Ω = ∅and ν ∈Ω. Then we have
Pr[X] ≥ 1
2.72 ·(Q+ 1).
The claim is trivially true if Q= 0; otherwise, we have:
Pr[X] = Pr[R∩Ω = ∅] ·Pr[ν ∈Ω] ≥
(
1 − 1
Q+ 1
)Q
· 1
Q+ 1 ≥ 1
2.72 ·(Q+ 1).
Here, we have made use of the inequality
(
1 −1/(n+ 1)
)n ≥1/e which holds for all n≥1. That
proves the claim and concludes the proof of the lemma. 2
13.4.2 Proofs of Theorems 13.3 and 13.4
Armed with Lemma 13.5, the proof of Theorem 13.3 is quite straightforward.
Proof of Theorem 13.3. Let Abe an adversary attacking SFDH as in the theorem statement.
Using A, we wish to construct an adversary Bthat breaks the one-wayness of T with advantage as
in (13.3).
We ﬁrst make a couple of simplifying assumptions about A. First, when Aoutputs its forgery
on a particular message, it has previously queried the random oracle on that message. Second,
Anever makes the same random oracle query twice, that is, all of its random oracle queries are
distinct. Third, Anever makes the same signing query twice, that is, all of its signing queries are
distinct. If Adoes not already satisfy these properties, we can always convert it to an adversary
A′that does, increasing the number of random oracle queries by at most 1.
So from now on, let us work with the more convenient adversary A′, which makes at most
t:= Qro + 1 random oracle queries, and whose advantage in breaking the signature scheme SFDH is
the same as that of A. From A′, we construct an adversary B′ that wins the t-repeated one-way
attack game against f := F(pk,·), where t:= Qro + 1, with the same advantage that A′ wins the
signature game. After we have B′, the theorem follows immediately from Lemma 13.5.
Adversary B′works as follows:
543
• It obtains (y1,...,y t) from its own t-repeated one-way challenger.
• It responds to the ith random oracle query from A′with yi.
• If A′asks to sign a particular message ˆm:
– if the random oracle has already been queried at ˆ m, and if this was the jth random
oracle query, B′makes a reveal query at position j to obtain xj, and forwards xj to A′;
– otherwise, if the random oracle has not been queried at ˆm, then B′generates a random
ˆx∈X, computes ˆy ←F(pk,ˆx), and forwards ˆx to A′; moreover, if A′ ever queries the
random oracle at ˆm in the future, B′will respond to that query with the value ˆy.
• Finally, when A′outputs its candidate forgery (m,σ), then by assumption, the random oracle
query was already queried at m; if this was query number ν, then B′outputs (ν,σ).
Clearly, B′ simulates the signature attack game perfectly for A′, and wins its attack game
precisely when A′wins its game.
Proof of Theorem 13.4. This is almost identical to the proof of Theorem 13.3. The only
diﬀerence is that we use Lemma 13.6 instead of Lemma 13.5. In the application of Lemma 13.6,
the number of reveal queries Q in Attack Game 13.4 is bounded by Qs.
13.5 An RSA-based signature scheme with a tight security proof
Theorem 13.4 shows that SRSA-FDH is a secure signature scheme in the random oracle model, but
with a relatively loose security reduction. In particular, let Abe an adversary attacking SRSA-FDH
that issues at most Qs signing queries and succeeds in breaking SRSA-FDH with probability ϵ. Then
Acan be used to break the RSA assumption with probability about ϵ/Qs. It is unlikely that
SRSA-FDH has a tighter security reduction to the RSA assumption.
Surprisingly, a small modiﬁcation to SRSA-FDH gives a signature scheme that has a tight reduc-
tion to the RSA assumption in the random oracle model. The only diﬀerence is that instead of
computing an eth root of H(m), the signing algorithm computes an eth root of H(b,m) for some
random bit b ∈{0,1}. The signature includes the eth root along with the bit b. We call this
modiﬁed signature scheme S′
RSA-FDH.
We describe S′
RSA-FDH using the notation of Section 13.3.1. Let M′ := {0,1}×M . We will
need a hash function H : M′→Y. Furthermore, we will need a PRF F deﬁned over (K,M,{0,1}).
The S′
RSA-FDH signature scheme is deﬁned as follows:
• The key generation algorithm G uses ﬁxed RSA parameters ℓ and e, and runs as follows:
G() := k←R K, (n,d) ←R RSAGen(ℓ,e)
pk ←(n,e), sk ←(k,n,d )
output (pk,sk).
• For a given secret key sk = (k,n,d ) and m∈M, the signing algorithm S runs as follows:
S(sk,m) := b←F(k,m) ∈{0,1}
y←H(b,m) ∈Y, σ ←yd ∈Zn
output (b,σ).
544
• For a given public key pk = (n,e) and signature ( b,σ), the veriﬁcation algorithm does:
V
(
pk, m, (b,σ)
):= y←H(b,m)
if y= σe output accept; otherwise, output reject.
Security. The S′
RSA-FDH system can be shown to be secure under the RSA assumption, when H
is modeled as a random oracle. The security proof uses the random self reduction of RSA to obtain
a tight reduction to the RSA problem. The point is that the factor 2 .72(Qs + 1) in Theorem 13.4
is replaced by a factor of 2 in the theorem below.
Theorem 13.7. Let H : M′→Y be a hash function. Assume that the RSA assumption holds for
(ℓ,e), and F is a secure PRF. Then S′
RSA-FDH is a secure signature scheme when H is modeled as
a random oracle.
In particular, let Abe an eﬃcient adversary attacking S′
RSA-FDH. Then there exist an eﬃcient
RSA adversary Band a PRF adversary BF, where Band BF are elementary wrappers around A,
such that
SIGroadv[A,S′
RSA-FDH] ≤2 ·RSAadv[B,ℓ,e ] + PRFadv[F,BF]
Proof idea. Suppose the PRF F is a random function f : M→{ 0,1}. We build an algorithm Bthat
uses an existential forger Ato break the RSA assumption. Let ( n,d) ←R RSAGen(ℓ,e),x∗ ←R Zn,
and y∗←xe
∗∈Zn. Algorithm Bis given n,y∗and its goal is to output x∗. First Bsends the public
key pk = (n,e) to A. Now Aissues random oracle queries and signing queries. To obtain a tight
reduction, Bmust properly answer all signing queries from A. In other words, Bmust be able to
sign every message in M. But this seems impossible — if Balready knows the signature on all
messages, how can an existential forgery from Apossibly help Bsolve the challenge ( n,y∗)? The
signature produced by Aseems to give Bno new information.
The solution comes from the extra bit in the signature. Recall that in S′
RSA-FDH every message
m∈M has two valid signatures, namely σ0 = (0, H(m,0)d) and σ1 = (1, H(m,1)d). Algorithm B
sets things up so that it knows exactly one of these signatures for every message. In particular,
Bwill know the signature ( b,H(b,m)) where b ←f(m). The forger Awill output an existential
forgery (m,(b,σ)) where, with probability 1 /2, (b,σ) is the signature on m that Bdoes not know.
We will use the random self reduction of RSA to ensure that any such signature enables Bto solve
the original challenge. For this to work, Amust not know which of the two signatures Bknows.
Otherwise, a malicious Acould always output a signature forgery that is of no use to B. This is
the purpose of the PRF.
To implement this idea, Bresponds to random oracle queries and signing queries as follows. We
let Odenote the random oracle implementing H.
• upon receiving a random oracle query ( b,m) ∈M′from Ado:
if b= f(m) then c←0 else c←1
repeat until y∈Y
x←R Zn, y ←xe ·yc
∗∈Zn / / So y= xe or y= xe ·y∗
send y to A / / This deﬁnes O(b,m) := y
Observe that in either case O(b,m) is a uniform value in Y as required. In particular, A
learns nothing about the value of f(m).
545
When b= f(m) the random oracle value O(b,m) is a random value y for which Bknows an
eth root, namely x. When b̸= f(m) then O(b,m) is a random value y for which Bdoes not
know an eth root. In fact, an eth root of y = xe ·y∗ will solve the original challenge — if σ
is an eth root of y then x∗= σ/x∈Zn is an eth root of y∗, since:
xe
∗= σe/xe = y/xe = (xe ·y∗)/xe = y∗. (13.8)
In eﬀect, Buses the random self reduction of RSA to map the original challenge y∗ to a
random challenge y. It then maps O(b,m) to this random y.
• Upon receiving a signing query m∈M from A, respond as follows. First, compute b←f(m)
and let y←O(b,m) ∈Y. By construction, Bdeﬁned O(b,m) = xe for some random x∈Zn
chosen by B. Hence, Bhas an eth root x for this y. It sends Athe signature (b,x).
So far, Bsimulates the challenger perfectly. Its responses to A’s oracle queries are uniform and
random in Yand all its responses to signing queries are valid. Therefore, Aproduces an existential
forgery (b,σ) on some message m. Then σe = O(b,m). Now, if b ̸= f(m) then O(b,m) = xe ·y∗
and hence x∗= σ/x as in (13.8).
In summary, assuming b ̸= f(m), algorithm Bobtains a solution to the challenge y∗. But, by
construction of O, the adversary learns no information about the function f. In particular, f(m)
is a random bit, and is independent of the adversary’s view. Therefore, b ̸= f(m) happens with
probability 1/2. This is the source of the factor of 2 in Theorem 13.7. 2
So what does this mean? The S′
RSA-FDH system is a minor modiﬁcation of SRSA-FDH. Signa-
tures include one additional bit which leads to a tighter reduction to the RSA assumption.
Despite this tighter reduction, S′
RSA-FDH has not gained much adoption in practice. Most
practitioners do not view the extra complexity as a worthwhile tradeoﬀ against the tighter reduction,
especially since this reduction is ultimately heuristic, as it models H as a random oracle. It is not
clear that S′
RSA-FDH is any more secure than SRSA-FDH for any particular instantiation of H. This is
an open question. Conversely, Exercise 13.9 shows that for every instantiation of H, the signature
scheme S′
RSA-FDH is no less secure than SRSA-FDH.
Another diﬀerence is that while SRSA-FDH is a unique signature scheme — every message has a
unique valid signature — the signature scheme SRSA-FDH is not unique. Most messages have two
valid signatures.
13.6 Case study: PKCS1 signatures
The most widely deployed standard for RSA signatures is known as PKCS1 version 1.5 mode 1.
This RSA signing method is commonly used for signing X.509 certiﬁcates. Let n be a t-bit RSA
modulus. The standard requires that t is a multiple of 8. Let e be the encryption exponent (or
signature veriﬁcation exponent). To sign a message m, the standard speciﬁes the following steps:
• Hash m to an h-bit hash value using a collision resistant hash function H, where h is also
required to be a multiple of 8. The standard requires that h<t −88.
• Let D∈{0,1}t be the binary string shown in Fig. 13.3. The string starts with the two bytes
00 01. It then contains a padding sequence of FF-bytes that ends with a single 00 byte. Next
546
00 01 FF FF FF ... FF FF 00 DI H(m)
16 bits
t bits
D:
Figure 13.3: PKCS1 signatures: the quantity D signed by RSA
a short DigestInfo (DI) ﬁeld is appended that encodes the name of the hash function H used
to hash m. For example, when SHA256 is used the DigestInfo ﬁeld is a ﬁxed 19-byte string.
Finally, H(m) is appended. The length of the padding sequence of FF-bytes is such that D
is exactly t bits.
• View Das a t-bit integer, which we further interpret as an element of Zn, and output the eth
root of D as the signature σ.
To verify the signature, ﬁrst compute σe ∈Zn, and then interpret this as a t-bit string D. Finally,
verify that D contains all the ﬁelds shown in Fig. 13.3, and no other ﬁelds.
The reason for prepending the ﬁxed PKCS1 pad to the hash value prior to signing is to avoid
a chosen message attack due to Desmedt and Odlyzko [51]. The attack is based on the following
idea. Suppose PKCS1 directly signed a 256-bit message digest with RSA, without ﬁrst expanding
it to a long string as in Fig. 13.3. Further, suppose the attacker ﬁnds three messages m1,m2,m3
such that
H(m1) = p1, H (m2) = p2, H (m3) = p1 ·p2, (13.9)
where H(m1),H(m2),H(m3) are viewed as integers in the interval [0 ,2256). The attacker can
request the signatures on m1 and m2 and from them deduce the signature on m3 by multiplying
the two given signatures. Hence, the attacker obtains an existential forgery by issuing two chosen
message queries. The attack of Desmedt and Odlyzko extends this basic idea so that the attack
succeeds with high probability using many chosen message queries. The reason for the padding in
Fig. 13.3 is so that the numbers for which an eth root is computed are much longer than 256 bits.
As a result, it is much less likely that an attacker can ﬁnd messages satisfying a condition such as
(13.9).
Security. PKCS1 is an example of a partial domain hash signature. The message mis hashed
into an h-bit string that is mapped into a ﬁxed intervalI inside of Zn. The interval has size |I|= 2h.
Typically, the hash size h is 160 or 256 bits, and the modulus size t is at least 2048 bits. Hence, I
is a tiny subset of Zn.
Unfortunately, the proof of Theorem 13.4 requires that the output of the hash function H be
uniformly distributed over a large subset Yof Zn. This was necessary for the proof of Lemma 13.6.
The set Yhad to be large so that we could pick a random y∈Y for which we knew an eth root.
When hashing into a tiny subset I of Zn the proof of Lemma 13.6 breaks down. The problem
is that we cannot pick a random y ∈I so that an eth root of y is known. More precisely, the
obstruction to the proof is the following problem:
(∗) given an RSA modulus n, output a pair ( y,x) where y is uniformly
distributed in a subset I ⊆Zn and x is an eth root of y.
547
A solution to this problem will enable us to prove security of PKCS1 under the assumption that
computing eth roots is hard in the interval I. Problem ( ∗) is currently open. The best known
algorithm [46] solves the problem for e= 2 whenever |I|≥ n2/3. However, typically in PKCS1, |I|
is far smaller than n2/3 (and for RSA we use e> 2).
In summary, although PKCS1 v1.5 is a widely used standard for signing using RSA, we cannot
prove it secure under the standard RSA assumption. An updated version of PKCS1 known as
PKCS1 v2.1 includes an additional RSA-based signature method called PSS, discussed in the
chapter notes.
13.6.1 Bleichenbacher’s attack on PKCS1 signatures
Implementing cryptography is not easy. In this section, we give a clever attack on a once-popular
implementation of PKCS1 that illustrates its fragility. Let pk = (n,3) be an RSA public key for
the PKCS1 signature scheme: n is a t-bit RSA modulus and the signature veriﬁcation exponent is
3. We assume t≥2048.
When signing a message musing PKCS1 the signer forms the block D shown in Fig. 13.3, and
then, treating D as an integer, computes the cube root of D modulo n as the signature σ.
Consider the following erroneous implementation of the veriﬁcation algorithm. To verify a message-
signature pair (m,σ), with SHA256 as the hash function, the veriﬁer does:
1. compute σe ∈Zn, and then interpret this as a t-bit string D
2. parse D from left to right as follows:
(a) reject if the top most 2 bytes are not 00 01
(b) skip over all FF-bytes until reaching a 00 byte and skip over it too
(c) reject if the next bytes are not the DigestInfo ﬁeld for the SHA256 function
(d) read the following 32 bytes (256 bits), compare them to the hash value SHA256( m), and
reject if not equal
3. if all the checks above pass successfully, accept the signature
While this procedure appears to correctly verify the signature, it ignores one very crucial step: it
does not check that Dcontains nothing to the right of the hash value. In particular, this veriﬁcation
procedure accepts a t-bit block D∗that looks as follows:
D∗:= 00 01 FF ... FF 00 DI hash more bits J
Here J is some sequence of bits chosen by the attacker. The attacker shortened the variable length
padding block of FF’s to make room for the quantity J, so that the total length of D∗is still tbits.
This minor-looking oversight leads to a complete break of the signature scheme. An attacker
can generate a valid signature on any message m of its choice, as we now proceed to demonstrate.
Let w ∈Z be the largest multiple of eight smaller than t/3 −3. To forge the signature on m,
the attacker ﬁrst computes H(m) = SHA256(m) and constructs the block D, as in Fig. 13.3, but
where D is only w bits long (note that w≈t/3). To make D this short, simply make the variable
length padding block suﬃciently short. Next, viewing D as an integer, the attacker computes:
s←
3√
D·2t−w ∈R, x ←⌈ s ⌉ ∈Z, output x.
548
Here, the cube root s of D·2t−w is computed over the real numbers and rounded up to the next
integer x.
We show that x, when viewed as an element of Zn, will be accepted as a valid signature on m.
Since 0 ≤x−s< 1, we obtain
0 ≤x3 −(D·2t−w) = x3 −s3 = (x−s)(x2 + xs+ s2) <3(s+ 1)2.
Observe that s3 = D·2t−w <2t, because the leading bits of D are zero. Moreover, for s≥3, we
have that (s+ 1)2 ≤2s2 <2 ·2(2/3)t, and therefore
0 ≤x3 −(D·2t−w) <3(s+ 1)2 <6 ·2(2/3)t <2t−[(t/3)−3] <2t−w.
In other words, x3 = (D·2t−w) + J where 0 ≤J <2t−w.
It follows that if we treat x as an element of Zn, it will be accepted as a signature on m.
Indeed, x3 will be strictly less than n, so the computation of x3 mod nwill not wrap around at all.
Moreover, when the veriﬁer interprets x3 as a t-bit string D∗, the w most signiﬁcant bits of D∗are
equal to D, ensuring that x will be accepted as a signature on m with respect to the public key
(n,3).
This attack applies to RSA public keys that use a small public exponent, such as e= 3. When
it was originally discovered, it was shown to work well against several popular PKCS1 implemen-
tations. The attack exploits a bug in the implementation of PKCS1 that is easily mitigated: the
veriﬁer must reject the signature if D is not the correct length, or there are bits in D to the right
of the hash value. Nevertheless, it is a good illustration of the diﬃculty of correctly implementing
cryptographic primitives. A simple misunderstanding in reading the PKCS1 speciﬁcation resulted
in a devastating attack on its implementation.
13.7 Signcryption: combining signatures and encryption
A signcryption scheme lets a sender, Alice, send an encrypted message to a recipient, Bob, so that
(1) only Bob can read the message, and (2) Bob is convinced that the message came from Alice.
Signcryption schemes are needed in messaging systems that provide end-to-end security, but where
Bob may be oﬄine at the time that Alice sends the message. Because Bob is oﬄine, Alice cannot
interact with Bob to establish a shared session key. Instead, she encrypts the message intended
for Bob, and Bob receives and decrypts it at a later time. The ciphertext she sends to Bob must
convince Bob that the message is from Alice.
Since anyone can generate public-private key pairs, signcryption only makes sense in an envi-
ronment where every identity is publicly bound to one or more public keys. More precisely, Bob can
tell what public keys are bound to Alice’s identity, and an attacker cannot cause Bob to associate
an incorrect public key to Alice. If this were not the case, that is, if an attacker can generate a
public-private key pair and convince Bob that this public key belongs to Alice, then the goals of
signcryption cannot be achieved: the attacker could send a message on behalf of Alice, and Bob
could not tell the diﬀerence; similarly, the attacker could decrypt messages that Bob thinks he is
sending to Alice.
To capture this requirement on public keys and identities, we assign to every user X of the
system a unique identity idX. Moreover, we assume that any other user can fetch the public key
pkX that is bound to the identity idX. So, Alice can obtain a public key bound to Bob, and she
549
can be reasonably conﬁdent that only Bob knows the corresponding private key. Abstractly, one
can think of a public directory that maintains a mapping from identities to public keys. Anyone
can read the directory, but only the user with identity idX can update the record associated with
idX (in today’s technology, Facebook user proﬁles serve as such a global directory). In Section 13.8
we will see that certiﬁcates are another way to reliably bind public keys to identities.
We will denote the sender’s identity by idS and the recipient’s identity by idR. We denote the
sender’s public-private key pair by pkS and skS and the recipients key pair by pkR and skR. To
encrypt a message m intended for a speciﬁc recipient, the sender needs its own identity idS and
secret key skS as well as the recipients identity idR and public key pkR. To decrypt an incoming
ciphertext, the recipient needs the sender’s identity idS and public key pkS as well as its own
identity idR and secret key skR. With this in place we can deﬁne the syntax for signcryption.
Deﬁnition 13.5. A signcryption scheme SC= (G,E,D ) is a triple of eﬃcient algorithms, G,E
and D, where G is called a key generation algorithm , E is called an encryption algorithm,
and D is called a decryption algorithm.
• Gis a probabilistic algorithm that takes no input. It outputs a pair (pk,sk), where sk is called
a secret key and pk is called a public key.
• E is a probabilistic algorithm that is invoked as c←RE
(
skS,idS,pkR,idR, m
)
, where sk S and
idS are the secret key and identity of the sender, pk R and id R are the public key and identity
of the recipient, and m is a message. The algorithm outputs a ciphertext c.
• Dis a deterministic algorithm invoked as D
(
pkS,idS,skR,idR, c
)
. It outputs either a message
m or a special symbol reject.
• We require that a ciphertext generated by E is always accepted by D. That is, for all possible
outputs (pkS,skS) and (pkR,skR) of G, all identities id S,idR, and all messages m
Pr
[
D
(
pkS,idS,skR,idR, E(skS,idS,pkR,idR, m)
)
= m
]
= 1.
As usual, we say that messages lie in a ﬁnite message space M, ciphertexts lie in some ﬁnite
ciphertext space C, and identities lie in some ﬁnite identity space I. We say that SC =
(G,E,D ) is deﬁned over (M,C,I).
We can think of signcryption as the public-key analogue of authenticated encryption for sym-
metric ciphers. Authenticated encryption is designed to achieve the same conﬁdentiality and au-
thenticity goals as signcryption, but assuming the sender and recipient have already established a
shared secret key. Signcryption is intended for a non-interactive setting where no shared secret key
is available. With this analogy in mind we can consider two signcryption constructions, similar to
the ones in Chapter 9:
• The signcryption analogue of encrypt-then-MAC is encrypt-then-sign: ﬁrst encrypt the mes-
sage with the recipient’s public encryption key and then sign the resulting ciphertext with
the sender’s secret signing key.
• The signcryption analogue of MAC-then-encrypt is sign-then-encrypt: ﬁrst sign the message
with the sender’s secret signing key and then encrypt the message-signature pair with the
recipient’s public encryption key.
550
Which of these is secure? Is one method better than the other? To answer these questions we must
ﬁrst formally deﬁne what it means for a signcryption scheme to be secure, and then analyze these
and other signcryption schemes.
We begin in Section 13.7.1 with a formal deﬁnition of security for signcryption. Admittedly,
our deﬁnition of secure signcryption is a bit lengthy, and it may not be immediately clear that
it captures the “right” properties. In Section 13.7.2, we discuss how this deﬁnition can be used
to derive more intuitive security properties of signcryption in a multi-user setting. It is precisely
these implications that give us conﬁdence that the basic deﬁnition in Section 13.7.1 is suﬃciently
strong. In Sections 13.7.3 and 13.7.4 we turn to the problem of constructing secure signcryption
schemes. Finally, in Section 13.7.5, we investigate some additional desirable security properties for
signcryption, called forward-secrecy and non-repudiation, and show how to achieve them.
13.7.1 Secure signcryption
We begin with the basic security requirements for a signcryption scheme. As we did for au-
thenticated encryption, we deﬁne secure signcryption using two games. One game captures data
conﬁdentiality: an adversary who does not have Alice’s or Bob’s secret key cannot break semantic
security for a set of challenge ciphertexts from Alice to Bob. The other game captures data au-
thenticity: an adversary who does not have Alice’s or Bob’s secret key cannot make Bob accept a
ciphertext that was not generated by Alice with the intent of sending it to Bob.
In both games the adversary is active. In addition to asking Alice to encrypt messages intended
for Bob, and asking Bob to decrypt messages supposedly coming from Alice, the adversary is free
to ask Alice to encrypt messages intended for any other user of the adversary’s choosing, and to
ask Bob to decrypt messages supposedly coming from any other user of the adversary’s choosing.
Moreover, the attack game reﬂects the fact that while Alice may be sending messages to Bob, she
may also be receiving messages from other users. Therefore, the adversary is free to ask Alice to
decrypt messages supposedly coming from any other user of the adversary’s choosing. Similarly,
modeling the fact that Bob may also be playing the role of sender, the adversary is free to ask Bob
to encrypt messages intended for any other user of the adversary’s choosing.
Ciphertext integrity. We start with the data authenticity game, which is an adaptation of the
ciphertext integrity game used in the deﬁnition of authenticated encryption (Attack Game 9.1).
Attack Game 13.5 (ciphertext integrity). For a given signcryption scheme SC = (G,E,D )
deﬁned over (M,C,I), and a given adversary A, the attack game runs as follows:
• The adversary chooses two distinct identities idS (the sender identity) and idR (the receiver
identity), and gives these to the challenger. The challenger runs Gtwice to obtain (pkS,skS)
and (pkR,skR) and gives pkS and pkR to A.
• Aissues a sequence of queries to the challenger. Each query is one of the following types:
S →R encryption query: a message m.
The challenger computes c←R E(skS,idS,pkR,idR,m), and gives c to A.
X →Y encryption query: a tuple (idX,idY,pkY,m), where idX ∈{idS,idR}and (idX,idY) ̸=
(idS,idR). The challenger responds to Awith c, computed as follows:
551
if idX = idS then c←R E(skS,idS,pkY,idY,m),
if idX = idR then c←R E(skR,idR,pkY,idY,m).
X →Y decryption query: a tuple (idX,idY,pkX,ˆc), where idY ∈{idS,idR}and (idX,idY) ̸=
(idS,idR). The challenger responds to Awith ˆm, computed as follows:
if idY = idS then ˆm←D(pkX,idX,skS,idS,ˆc),
if idY = idR then ˆm←D(pkX,idX,skR,idR,ˆc).
• Finally, Aoutputs a candidate ciphertext forgery c′∈C, where c′is not among the responses
to an S →R encryption query.
We say that Awins the game if its candidate ciphertext forgery c′is a valid ciphertext from idS
to idR, that is, D(pkS,idS,skR,idR,c′) ̸= reject. We deﬁne A’s advantage, denoted SCIadv[A,SC],
as the probability that Awins the game. 2
Deﬁnition 13.6. We say that SC= (G,E,D ) provides signcryption ciphertext integrity , or
SCI for short, if for every eﬃcient adversary A, the value SCIadv[A,SC] is negligible.
Security against a chosen ciphertext attack. Next, we deﬁne the data conﬁdentiality game,
which is an adaptation of the game used to deﬁne chosen ciphertext security (Attack Game 12.1).
Note that in this game, the syntax of the X →Y encryption and decryption queries are exactly
the same as in Attack Game 13.5.
Attack Game 13.6 (CCA security). For a given signcryption scheme SC= (G,E,D ), deﬁned
over (M,C,I), and for a given adversary A, we deﬁne two experiments.
Experiment b (b= 0,1):
• The adversary chooses two distinct identities idS (the sender identity) and idR (the receiver
identity), and gives these to the challenger. The challenger runs Gtwice to obtain (pkS,skS)
and (pkR,skR) and gives pkS and pkR to A.
• Aissues a sequence of queries to the challenger. Each query is one of the following types:
S →R encryption query: a pair of equal-length messages ( m0,m1).
The challenger computes c←R E(skS,idS,pkR,idR,mb), and gives c to A.
S →R decryption query: a ciphertext ˆc, where ˆc is not among the outputs of any previous
S →R encryption query.
The challenger computes ˆm←R D(pkS,idS,skR,idR,ˆc), and gives ˆm to A.
The remaining two query types are the same as in the ciphertext integrity game:
X →Y encryption query: a tuple (idX,idY,pkY,m), where idX ∈{idS,idR}and (idX,idY) ̸=
(idS,idR). The challenger responds to Awith c, computed as follows:
if idX = idS then c←R E(skS,idS,pkY,idY,m),
if idX = idR then c←R E(skR,idR,pkY,idY,m).
X →Y decryption query: a tuple (idX,idY,pkX,ˆc), where idY ∈{idS,idR}and (idX,idY) ̸=
(idS,idR). The challenger responds to Awith ˆm, computed as follows:
552
if idY = idS then ˆm←D(pkX,idX,skS,idS,ˆc),
if idY = idR then ˆm←D(pkX,idX,skR,idR,ˆc).
• At the end of the game, the adversary outputs a bit ˆb∈{0,1}.
Let Wb be the event that Aoutputs 1 in Experiment b and deﬁne A’s advantage as
SCCAadv[A,SC] :=
⏐⏐Pr[W0] −Pr[W1]
⏐⏐. 2
Deﬁnition 13.7 (CCA Security). A signcryption scheme SC is called semantically secure
against a chosen ciphertext attack , or simply CCA secure, if for all eﬃcient adversaries A,
the value SCCAadv[A,SC] is negligible.
Finally, we deﬁne a secure signcryption scheme as one that is both CCA secure and has cipher-
text integrity.
Deﬁnition 13.8. We say that a signcryption scheme SC= (G,E,D ) is secure if SC is (1) CCA
secure, and (2) provides signcryption ciphertext integrity.
From two users to multiple users. While this security deﬁnition focuses on just two honest
users, it actually implies a strong security property in a multi-user setting. We will ﬂesh this out
below in Section 13.7.2.
Replay attacks. One thing the deﬁnition does not prevent is a “replay” attack: an attacker can
record a valid ciphertext c from Alice to Bob and at a later time, say a week later, resend the
same c to Bob. Bob receives the replayed ciphertext c and, because it is a valid ciphertext, he
might mistakenly believe that Alice sent him the same message again. For example, if the message
from Alice is “please transfer $10 to Charlie,” then Bob might incorrectly transfer another $10 to
Charlie.
Signcryption is not designed to prevent replay attacks. Higher level protocols that use sign-
cryption must themselves take measures to counter-act them. We will discuss replay attacks and
how to prevent them when we discuss authenticated key exchange in Chapter 21.
Statically vs adaptively chosen user IDs. Our deﬁnition of secure signcryption is subject to
a rather subtle criticism, related to the manner in which user IDs are chosen. While we leave it to
the adversary to choose the user IDs of the sender and receiver (that is, idS and idR), this choice
is “static” in the sense that it is made at the very beginning of the game. A more robust deﬁnition
would allow a more “adaptive” strategy, in which the adversary gets to choose these IDs after seeing
one or both of the public keys, or even after seeing the response to one or more X →Y queries. For
most realistic schemes (including all of those discussed here), this distinction makes no diﬀerence,
but it is possible to dream up contrived schemes where it does (see Exercise 13.18). We have
presented the deﬁnition with statically chosen IDs mainly for the sake of simplicity (and because,
arguably, honest users choose their IDs in a manner that is not so much under an adversary’s
control).
553
13.7.2 Signcryption as an abstract interface
Our deﬁnition of secure signcryption may seem a bit technical, and it is perhaps useful to discuss
how this deﬁnition can be applied. As we did in Sections 9.3 and 12.2.4, we do so by describing
signcryption as an abstract interface. However, unlike in those two sections, it makes more sense
here to explicitly model a system consisting of many users who are trying to send messages to one
another over an insecure network.
The setting is as follows. We have a system of many users: some are “honest” and some are
“corrupt.” The honest users are assumed to follow the speciﬁed communication protocol correctly,
while the corrupt users may do anything they like to try and subvert the protocol. The corrupt users
may collude with each other, and may also attempt to subvert communications by eavesdropping
on and tampering with network communication. In fact, we can just assume there is a single
attacker who orchestrates the behavior of all the corrupt users and completely controls the network.
Moreover, this attacker may have some knowledge of or inﬂuence over messages sent by honest users,
and may have some knowledge of messages received by honest users.
To start with, we assume that each honest user somehow registers with the system by providing
a user ID and a public key. We do not worry about the details of this registration process, except
that we require each honest user to have a unique ID and to generate its public key using the key
generation algorithm of the signcryption scheme (and, of course, keep the corresponding secret key
to itself).
We require that the corrupt users also register with the system. While we insist that all users
(honest and corrupt) have unique IDs, we do not make any requirements on how the corrupt users
generate their public keys: they may use the prescribed key generation algorithm, or they may
do something else entirely, including computing their public key as some function of one or more
honest users’ public keys. In fact, we may even allow the corrupt users to register with the system
after it has been running for a while, choosing their public keys (and even their user IDs) in some
way that depends in some malicious way on everything that has happened so far (including all
network traﬃc).
We model the communication interface as a collection of in-boxes and out-boxes.
For each honest user id S and each registered user (honest or corrupt) idR ̸= idS, we have an
out-box denoted Out(idS,idR). If idR belongs to an honest user, we say that the out-box is safe;
otherwise, we say that it is unsafe. From time to time, user idS may want to send a message to
user idR, and he does so by dropping the message in the out-box Out(idS,idR).
For each registered user (honest or corrupt) idS and each honest user id R ̸= idS, we have an in-
box denoted In(idS,idR). If idS belongs to an honest user, we say that the in-box is safe; otherwise,
we say that it is unsafe. From time to time, a message may appear in the in-box In(idS,idR),
which user idR may then retrieve.
That is the abstract interface. We now describe the real implementation.
First, consider an out-box Out(idS,idR) associated with an honest user idS. The user idR may
or may not be honest. When user idS user drops a message in the out-box, the message is encrypted
using the secret key associated with user idS and the public key associated with user idR (along
with the given user IDs). The resulting ciphertext is sent out to the network.
In a properly functioning network, if user idR is an honest user, this ciphertext will eventually
be presented to the matching in-box In(idS,idR).
Now consider an in-box In(idS,idR) associated with an honest user idR. The user idS may or
may not be honest. Whenever the network presents a ciphertext to this in-box, it is decrypted
554
using the public key of idS and the secret key idR (along with the given user IDs). If the ciphertext
is not rejected, the resulting message is placed in the in-box for later consumption by user idR.
We now describe an ideal implementation of this interface.
Here is what happens when an honest user drops a message in an out-box Out(idS,idR). If the
out-box is safe (i.e., user idR is an honest user), instead of encrypting the given message, a dummy
message is encrypted. This dummy message has nothing to do with the real message (except that
it should be of the same length), and the resulting ciphertext just serves as a “handle”. Otherwise,
if the out-box is unsafe, the real message is encrypted as in the real implementation.
Here is what happens when the network presents a ciphertext to an in-box In(idS,idR). If the
in-box is safe (i.e., user idS is an honest user), the ideal implementation checks if this ciphertext
was previously generated as a handle by the matching out-box Out(idS,idR), and if so, copies
the corresponding message directly from the out-box to the in-box; otherwise, the ciphertext is
discarded. If the in-box is unsafe, the ciphertext is decrypted as in the real implementation.
We hope that it is intuitively clear that this ideal implementation provides all the security one
could possibly hope for. In this ideal implementation, messages magically “jump” from honest
senders to honest receivers: the attacker cannot tamper with or glean any information about
these messages, even if honest users interact with corrupt users. At worst, an attacker reorders
or duplicates messages by reordering or duplicating the corresponding handles (indeed, as already
mentioned, our deﬁnition of secure signcryption does not rule out “replay” attacks). Typically, this
is an issue that a higher level protocol can easily deal with.
We now argue informally that if the signcryption scheme is secure, as in Deﬁnition 13.8, then
the real world implementation is indistinguishable from the ideal implementation. The argument
proceeds in three steps. We start with the real implementation, and in each step, we make a slight
modiﬁcation.
• First, we modify the behavior of the safe in-boxes. Whenever the network presents a cipher-
text to the in-box that came from the matching out-box, the corresponding message is copied
directly from the out-box to the in-box.
The correctness property of the signcryption scheme ensures that this modiﬁcation behaves
exactly the same as the real implementation.
• Second, we modify the behavior of the safe in-boxes again. Whenever the network presents
a ciphertext to the in-box that did not come from the matching out-box, the ciphertext is
discarded.
The ciphertext integrity property ensures that this modiﬁcation is indistinguishable from
the ﬁrst. To reduce from the multi-user setting to the two-user setting, one must employ a
“guessing argument”.
• Third, we modify the behavior of the safe out-boxes, so that dummy messages are encrypted
in place of the real messages.
The CCA security property ensures that this modiﬁcation is indistinguishable from the second.
To reduce from the multi-user setting to the two-user setting, one must employ a “hybrid
argument”.
Just as in Sections 9.3 and 12.2.4, we have ignored the possibility that the ciphertexts generated
in a safe out-box are not unique. If we are going to view these ciphertexts as handles in the ideal
555
implementation, uniqueness is an essential property. However, just as in those cases, the CCA
security property implies that these ciphertexts are unique with overwhelming probability.
13.7.3 Constructions: encrypt-then-sign and sign-then-encrypt
We begin by analyzing the two most natural constructions. Both are a combination of a CCA-
secure public-key encryption scheme and a secure signature scheme. Getting these combinations
right is a little tricky and small variations can be insecure. We explore some insecure variations in
Exercises 13.16 and 13.17.
Let E= (GENC,E,D ) be a public-key encryption scheme with associated data (see Section 12.7).
Recall that this means that E is invoked as c←R E(pk,m,d ), and D is invoked as m←R D(sk,c,d ),
where dis the “associated data”. Also, let S= (GSIG,S,V ) be a signature scheme. Deﬁne algorithm
G as:
G() := ( pkENC, skENC) ←R GENC(), (pkSIG, skSIG) ←R GSIG()
output pk := (pkENC, pkSIG) and sk := (skENC, skSIG)
In what follows we use the shorthand E(pk,m,d ) to mean E(pkENC,m,d ) and S(sk,m) to mean
S(skSIG,m), for some message m. We use a similar shorthand for V(pk,m,σ ) and D(sk,c,d ). We
next deﬁne two natural signcryption schemes, each of which has a message spaceMand an identity
space I.
Encrypt-then-sign. The scheme SCEtS = (G, EEtS, DEtS) is deﬁned as
EEtS(skS,idS,pkR,idR, m) := c←R E
(
pkR, m, idS
)
, σ ←R S
(
skS, (c,idR)
)
output (c,σ);
DEtS
(
pkS,idS,skR,idR, (c,σ)
) := if V(pkS, (c,idR), σ) = reject, output reject
otherwise, output D(skR, c, idS).
Here the encryption scheme Eis assumed to be deﬁned over (M, I, C), so that Iis the associated
data space for E. The signature scheme Sis assumed to be deﬁned over ( C×I , Σ).
Sign-then-encrypt. The scheme SCStE = (G, EStE, DStE) is deﬁned as
EStE(skS,idS,pkR,idR, m) := σ←R S
(
skS, (m,idR)
)
, c ←R E
(
pkR, (m,σ), idS
)
output c;
DEtS
(
pkS,idS,skR,idR, c
) := if D(skR,c, idS) = reject, output reject, otherwise:
(m,σ) ←D(skR, c, idS)
if V(pkS, (m,idR), σ) = reject, output reject
otherwise, output m.
Here the encryption schemeEis assumed to be deﬁned over (M×Σ, I, C), where Iis the associated
data space. The signature scheme Sis assumed to be deﬁned over (M×I, Σ). Moreover, we shall
assume that the signatures are bit strings whose length only depends on the message being signed
(this technical requirement will be required in the security analysis).
The following two theorems show that both schemes are secure signcryption schemes. Notice
that the corresponding symmetric constructions analyzed in Section 9.4 were not both secure.
556
Encrypt-then-MAC provides authenticated encryption while MAC-then-encrypt might not. In the
signcryption setting, both constructions are secure. The reason sign-then-encrypt is secure is that
we are starting from a CCA-secure public-key system E, where as MAC-then-encrypt was built
from a CPA-secure cipher. In fact, we know by Exercise 9.15 that MAC-then-encrypt, where the
encryption scheme is CCA secure, provides authenticated encryption. Therefore, it should not be
too surprising that sign-then-encrypt is secure.
Unlike the encrypt-then-MAC construction, the encrypt-then-sign method requires a CCA-
secure encryption scheme for security, rather than just a CPA-secure encryption scheme. We
already touched on this issue back in Section 12.2.2 as one of the motivations for studying CCA-
secure public-key encryption.
The encrypt-then-sign method requires astrongly securesignature scheme for security, as deﬁned
in Deﬁnition 13.3. Without this, the scheme can be vulnerable to a CCA attack: if an adversary,
given a challenge ciphertext ( c,σ), can produce a new valid signature σ′ on the same data, then
the adversary can win the CCA attack game by asking for a decryption of ( c,σ′). To prevent this,
we require that the signature scheme is strongly secure. This is perhaps to be expected, as in the
symmetric setting, the encrypt-then-MAC construction requires a secure MAC, and our deﬁnition
of a secure MAC is the direct analogue of our deﬁnition of a strongly secure signature scheme.
In contrast, sign-then-encrypt requires just a secure signature scheme — the scheme need not be
strongly secure.
We now present the security theorems for both schemes.
Theorem 13.8. SCEtS is a secure signcryption scheme assuming E is a CCA-secure public-key
encryption scheme with associated data and Sis a strongly secure signature scheme.
In particular, for every ciphertext integrity adversary Aci that attacks SCEtS as in Attack
Game 13.5 there exists a strong signature adversary Bsig that attacks Sas in Attack Game 13.2,
where Bsig is an elementary wrapper around Aci, such that
SCIadv[Aci,SCEtS] = stSIGadv[Bsig,S].
In addition, for every CCA adversary Acca that attacks SCEtS as in Attack Game 13.6 there
exists a CCA adversary Bcca that attacks Eas in Deﬁnition 12.7, and a strong signature adver-
sary B′
sig that attacks Sas in Attack Game 13.2, where Bcca and B′
sig are elementary wrappers
around Acca, such that
SCCAadv[Acca,SCEtS] ≤CCAadadv[Bcca,E] + stSIGadv[B′
sig,S].
Proof sketch. We have to prove both ciphertext integrity and security against chosen ciphertext
attack. Both proofs make essential use of the placement of the identiﬁers idS and idR as deﬁned
in the encryption and decryption algorithms. We start with ciphertext integrity.
Proving ciphertext integrity. We begin by constructing adversary Bsig that interacts with a
signature challenger for S, while playing the role of challenger to Aci in Attack Game 13.5. Bsig
ﬁrst obtains a signature public key pk∗
SIG from its own challenger.
Next, Aci supplies two identities idS and idR. Bsig then uses GENC and GSIG to generate two
public-key encryption key-pairs (pkENC,S,skENC,S) and (pkENC,R,skENC,R), and one signature key-pair
(pkSIG,R,skSIG,R). It sends to Aci the two public keys
pkS := (pkENC,S, pk∗
SIG) and pkR := (pkENC,R, pkSIG,R).
557
Note that Bsig knows all the corresponding secret keys, except for the secret key corresponding to
pk∗
SIG, which is the challenge signature public key that Bsig is trying to attack.
Aci then issues several encryption and decryption queries.
To process an encryption query, Bsig begins by encrypting the given message m using the
encryption algorithm E with the appropriate public key. This generates a ciphertext c. Next, Bsig
must generate an appropriate signature σ. For an S →R encryption query, Bsig obtains a signature
σ under pk∗
SIG on the message ( c,idR) by using its own signature challenger. For an X → Y
encryption query with idX = idS, Bsig obtains a signature σ under pk∗
SIG on the message ( c,idY),
again, by using its own signature challenger. For an X →Y encryption query with idX = idR, Bsig
generates σ by signing the message ( c,idY) directly, using the secret key skSIG,R. In any case, Bsig
responds to the encryption query with the ciphertext/signature pair ( c,σ).
Bsig answers decryption queries from Aci by simply running algorithm DEtS on the given data
in the query. Indeed, Bsig has all the required keys to do so.
Eventually, Aci outputs a valid ciphertext forgery ( c′,σ′), where σ′ is a valid signature on the
message (c′,idR). We argue that the message-signature pair
(
(c′,idR), σ′)
is a strong existential
forgery for the signature scheme S. The only way this can fail is if Bsig had previously asked its
challenger for a signature on ( c′,idR) and the challenger responded with σ′. Observe that the only
reason Bsig would ask for a signature on ( c′,idR) is as part of responding to an S →R encryption
query from Aci. This is where we make essential use of the fact that the identity idR is included in
the data being signed. We conclude that the signature from the challenger cannot be σ′ because
the ciphertext forgery ( c′,σ′) must be diﬀerent from all the S →R ciphertexts generated by Bsig.
It follows that
(
(c′,idR), σ′)
is a valid strong existential forgery on S, as required.
Proving chosen ciphertext security. Next, we sketch the proof of CCA security. It is convenient
to modify the attack game slightly. Let Game 0 be the original signcryption CCA game between a
SCEtS challenger and an adversary Acca. We then deﬁne Game 1, which is the same as Game 0, ex-
cept that we add a “special rejection rule” in the challenger’s logic for processing S →R decryption
queries. Namely, given an S →R decryption query (ˆc,ˆσ), where ˆσ is a valid signature on (ˆc,idR),
and ˆc is the ﬁrst component of a response to a previous S →R encryption query, the challenger
returns reject without further processing.
It is not diﬃcult to see that Games 0 and 1 proceed identically, unless the challenger rejects
a ciphertext (ˆc,ˆσ) in Game 1 that would not be rejected in Game 0. However, if (ˆ c,ˆσ) is such
a ciphertext, then
(
(ˆc,idR),ˆσ
)
is a strong existential forgery for S. Therefore, we can construct
an adversary B′
sig whose advantage in strong existential forgery game against S is equal to the
probability that such a ciphertext gets rejected in Game 1.
We now construct an adversary Bcca whose CCA advantage is the same as Acca’s advantage in
Game 1. As usual, Bcca interacts with its own CCA challenger, while playing the role of challenger
to Acca in Game 1.
Adversary Bcca ﬁrst obtains an encryption public key pk∗
ENC from its own challenger.
Next, Acca supplies two identities idS and idR. Bcca then runs the key-generation algorithm for
the signature scheme twice and the key-generation algorithm for the encryption scheme once, and
sends to Acca the two public keys
pkS := (pkENC,S, pkSIG,S) and pkR := (pk∗
ENC, pkSIG,R),
where it knows all the corresponding secret keys, except for the secret key corresponding to pk∗
ENC.
Acca then issues several encryption and decryption queries.
558
Processing encryption queries. Adversary Bcca answers an S →R encryption query for message
pair (m0,m1) by issuing an encryption query for (m0,m1) to its challenger, relative to the associated
data idS. It gets back a ciphertext c, signs (c,idR) to get σ, and sends ( c,σ) to Acca as a response
to the query.
To answer an X →Y encryption query, Bcca runs algorithm EEtS on the given data in the query.
Indeed, Bcca has all the required keys to do so.
Processing decryption queries. Consider ﬁrst an S →R decryption query (ˆc,ˆσ). Our adversary
Bcca uses the following steps:
1. return reject if ˆσ is an invalid signature on (ˆc,idR) under pkSIG,S;
2. return reject if ˆc is the ﬁrst component of any response to an S →R encryption query (this
is the special rejection rule we introduced in Game 1);
3. ask the CCA challenger to decrypt ˆ c using the associated data idS, and return the result
(note that because of the logic of Steps 1 and 2, Bcca has not issued an encryption query to
its own challenger corresponding to (ˆc,idS)).
The logic for processing an X →Y decryption query ( idX,idY,pkX,(ˆc,ˆσ)) with idY = idR is
similar:
1. return reject if ˆσ is an invalid signature on (ˆc,idR) under pkX;
2. ask the CCA challenger to decrypt ˆ c using the associated data idX, and return the result
(note that because idX ̸= idS, Bcca has not issued an encryption query to its own challenger
corresponding to (ˆc,idX)).
For other decryption queries, we have all the keys necessary to perform the decryption directly.
Finishing up. Eventually, Acca outputs a guess ˆb ∈{0,1}. This guess gives Bcca the same
advantage against its CCA challenger that Acca has in Game 1. 2
Theorem 13.9. SCStE is a secure signcryption scheme assuming E is a CCA-secure public-key
encryption scheme with associated data and Sis a secure signature scheme.
In particular, for every ciphertext integrity adversary Aci that attacks SCEtS as in Attack
Game 13.5 there exists a signature adversary Bsig that attacks Sas in Attack Game 13.1, and
a CCA adversary Bcca that attacks Eas in Deﬁnition 12.7, where Bsig and B′
cca are elementary
wrappers around Aci, such that
SCIadv[Aci,SCEtS] ≤SIGadv[Bsig,S] + CCAadadv[B′
cca,E]
In addition, for every CCA adversary Acca that attacks SCEtS as in Attack Game 13.6 there
exists a CCA adversary Bcca that attacks Eas in Deﬁnition 12.7, where Bcca is an elementary
wrapper around Acca, such that
SCCAadv[Acca,SCEtS] = CCAadadv[Bcca,E]
Proof idea. CCA security for the signcryption scheme follows almost immediately from the CCA
security of E. The reader can easily ﬁll in the details.
559
Proving CI for the signcryption scheme is slightly trickier. Let Game 0 be the original CI attack
game. We modify Game 0 so that for each S →R encryption query, instead of computing
c←R E(pkR,(m,σ),idS)
where
σ←R S(skS,(m,idR)),
the challenger instead computes
c←R E(pkR,(m,dummy),idS).
Call this Game 1. Under CCA security for E, the adversary’s advantage in breaking CI in Game 0
must be negligibly close to the corresponding advantage in Game 1. However, in Game 1, since the
challenger never signs any message of the form ( ·,idR), breaking CI in Game 1 is tantamount to
forging a signature on just such a message.
In proving both security properties, we need to make use of the technical requirement that
signatures are bit strings whose length only depends on the message being signed. 2
13.7.4 A construction based on Diﬃe-Hellman key exchange
Our next signcryption construction does not use signatures at all. Instead, we use a non-interactive
variant of the Diﬃe-Hellman key exchange protocol from Section 10.4.1. The protocol uses a group
G of prime order q with generator g ∈G. This variant is said to be non-interactive because once
every party publishes its contribution to the protocol — gα for some random α ∈Zq — no more
interaction is needed to establish a shared key between any pair of parties. For example, once
Alice publishes gα and Bob publishes gβ, their shared secret is derived from gαβ. The signcryption
scheme we describe can be built from any non-interactive key exchange, but here we present it
concretely using Diﬃe-Hellman key exchange.
The signcryption scheme SCDH is built from three ingredients:
• a symmetric cipher E= (Es,Ds) deﬁned over (K,M,C),
• a group G of prime order q with generator g∈G, and
• a hash function H : G3 ×I2 →K.
Given these ingredients, the system SCDH is deﬁned over (M,C,I) and works as follows:
• The key generation algorithm G runs as follows:
α←R Zq, h ←gα.
The public key is pk := h, and the secret key is sk := α. We use hX to denote the public key
associated with identity idX and use αX to denote the associated secret key.
• E
(
αS,idS,hR,idR, m
)
works by ﬁrst deriving the Diﬃe-Hellman secret between users S and
R, namely hSR := gαS·αR, and then encrypting the message m using the symmetric cipher
with a key derived from hSR. More precisely, encryption works as follows, where hS := gαS:
hSR ←(hR)αS = gαS·αR, k ←H
(
hS,hR,hSR, idS,idR
)
, output c←R Es(k,m).
560
• D
(
hS,idS,αR,idR, c
)
works as follows, where hR := gαR:
hSR ←(hS)αR = gαS·αR, k ←H
(
hS,hR,hSR, idS,idR
)
, output Ds(k,c).
It is easy to verify that SCDH is correct. To state the security theorem we must ﬁrst introduce
a new assumption, called the double-interactive CDH assumption . The assumption is related to,
but a little stronger than, the interactive CDH assumption introduced in Section 12.4.
Intuitively, the double-interactive CDH assumption states that given a random instance (gα,gβ)
of the DH problem, it is hard to compute gαβ, even when given access to a DH-decision oracle that
recognizes DH-triples of the form ( gα,·,·) or of the form ( ·,gβ,·). More formally, this assumption
is deﬁned in terms of the following attack game.
Attack Game 13.7 (Double-Interactive Computational Diﬃe-Hellman). Let G be a cyclic
group of prime order q generated by g ∈G. For a given adversary A, the attack game runs as
follows.
• The challenger computes
α,β ←R Zq, u ←gα, v ←gβ, w ←gαβ
and gives (u,v) to the adversary.
• The adversary makes a sequence of queries to the challenger. Each query is one of the following
types:
α-query: given (˜v, ˜w) ∈G2, the challenger tests if ˜vα = ˜w;
β-query: given (˜u, ˜w) ∈G2, the challenger tests if ˜uβ = ˜w.
In either case, if equality holds the challenger sends “yes” to the adversary, and otherwise,
sends “no” to the adversary.
• Finally, the adversary outputs some ˆw∈G.
We deﬁne A’s advantage in solving the double-interactive computational Diﬃe-Hellman
problem, denoted I2CDHadv[A,G], as the probability that ˆw= w. 2
Deﬁnition 13.9 (Double-Interactive computational Diﬃe-Hellman assumption). We say
that the double-interactive computational Diﬃe-Hellman (I2CDH) assumption holds for G
if for all eﬃcient adversaries Athe quantity I2CDHadv[A,G] is negligible.
The following theorem shows SCDH is a secure signcryption scheme where security is deﬁned as
in the previous section (Deﬁnition 13.8).
Theorem 13.10. SCDH is a secure signcryption scheme assuming Eis an AE-secure cipher, the
I2CDH assumption holds for G, and the hash function H is modeled as a random oracle.
In particular, for every ciphertext integrity adversary Aci that attacks SCDH as in the random
oracle variant of Attack Game 13.5, there exists a ciphertext integrity adversary Bci that attacks
Eas in Attack Game 9.1, and an I2CDH adversary Bdh for G, where Bci and Bdh are elementary
wrappers around Aci, such that
SCIadv[Aci,SCDH] ≤CIadv[Bci,E] + I2CDHadv[Bdh,G]
561
In addition, for every CCA adversary Acca that attacks SCDH as in the random oracle variant
of Attack Game 13.6, there exists a CCA adversary Bcca that attacks Eas in Attack Game 9.2,
and an I2CDH adversary B′
dh for G, where Bcca and B′
iidh are elementary wrappers around Aci,
such that
SCCAadv[Acca,SCDH] ≤CCAadv[Bcca,E] + 2·I2CDHadv[B′
dh,G]
The proof of Theorem 13.10 follows from the analysis of Diﬃe-Hellman as a non-interactive key
exchange scheme (Exercise 21.12).
13.7.5 Additional desirable properties: forward secrecy and non-repudiation
So far we looked at three signcryption schemes: SCDH presented in the previous section and the two
schemes presented in Section 13.7.3. All three schemes satisfy the signcryption security deﬁnition
(Deﬁnition 13.8). However, there are signiﬁcant diﬀerences between SCDH and the two schemes in
Section 13.7.3. One diﬀerence between SCDH and the others is a simple inter-operability issue: it
requires all users of the system to use the same group G for generating their keys. This may be
acceptable in some settings but not in others, and is inherent to how SCDH operates.
There are two other, more fundamental, diﬀerences that are worth examining further. We
explore these diﬀerences by deﬁning two new signcryption properties: (1) forward secrecy, and (2)
non-repudiation.
13.7.5.1 Property I: forward secrecy (security in case of a sender corruption)
Suppose Alice encrypts a message to Bob and sends the resulting ciphertext cto Bob. A week later
the adversary corrupts Alice and steals her secret key. Bob’s key remains intact and only known
to Bob. One might reasonably expect that the adversary should not be able to decrypt c using
Alice’s secret key. We refer to this property as sender corruption forward secrecy or simply forward
secrecy.
Let us deﬁne more precisely what it means for a signcryption scheme to provide sender corrup-
tion forward secrecy. The goal is to ensure that CCA security is maintained even if the adversary
obtains the sender’s secret key. To do so we make a small tweak to the CCA security game (Attack
Game 13.6).
Attack Game 13.8 (CCA security with sender corruption forward secrecy). The game
is identical to Attack Game 13.6 except that we change the setup step as follows: in addition to
giving the adversary the public keys pkS and pkR, the challenger gives the adversary the sender’s
secret key skS. The corresponding advantage is denoted SCCA ′adv[A,SC]. 2
Deﬁnition 13.10. A signcryption scheme SCis said to provide forward secrecyif for all eﬃcient
adversaries A, the value SCCA′adv[A,SC] is negligible.
Forward secrecy for sign-then-encrypt. The sign-then-encrypt construction provides forward
secrecy: the secret key skS is only used for signing messages and does not help to decrypt anything.
Indeed, from the concrete security bound given in Theorem 13.9, one can see that the bound on
the SCCA advantage does not depend at all on the security of the signature scheme.
562
Forward secrecy for encrypt-then-sign. One might be tempted to say the same thing for
encrypt-then-sign; however, this is not quite true in general. Observe that in the concrete security
bound in Theorem 13.8, the bound on the SCCA advantage depends on the security of both the
signature scheme and the encryption scheme. Indeed, as we already discussed in relation to the
need for a strongly secure signature scheme, if the adversary obtains a ciphertext ( c,σ) in response
to an S →R encryption query, and could compute a valid signature σ′̸= σ on (c,idR), then by the
rules of the CCA attack game, the adversary would be free to submit (c,σ′) as an S →R decryption
query, completely breaking CCA security.
Now, without the sender’s signing key, this attack would be infeasible. But with the signing
key, it is easy if the signature algorithm is probabilistic (we will see such signature schemes later):
the adversary can use the sender’s signing key to generate a diﬀerent signature on an inner S →R
ciphertext and obtain a “new” encrypt-then-sign ciphertext that it can submit to the decryption
oracle.
However, all is not lost. There are a couple of ways to salvage the forward secrecy property of
encrypt-then-sign. One way is to salvage the situation is to employ a signature scheme that has
unique signatures (i.e., for every public key and message, there is at most one valid signature — full
domain hash is such a scheme). Then the above attack becomes impossible, even with the signing
key. See also Exercise 13.19, which discusses a modiﬁcation of encrypt-then-sign which achieves
forward secrecy more generically.
Another way to salvage the situation is to weaken the security deﬁnition slightly, by simply not
allowing the adversary to submit a decryption query for the ciphertext ( c,σ′) in the attack game.
Is this reasonable? Arguably, it is, as anyone can easily tell that the ( c,σ) and ( c,σ′) decrypt
to the same thing if σ and σ′ are both valid signatures on c. Indeed, such a restriction on the
adversary corresponds to the notion of gCCA security discussed in Exercise 12.2, and is actually
quite acceptable for most applications.
Forward secrecy for SCDH. The SCDH signcryption system is not forward secure: given the
secret key of the sender, the adversary can decrypt any ciphertext generated by the sender. Fortu-
nately, we can enhance SCDH to provide forward secrecy against sender corruptions.
Enhanced SCDH. Using the notation of Section 13.7.4, the enhanced SCDH signcryption system,
denoted SC′
DH, is deﬁned over ( M, G ×C, I) and works as follows:
• The key generation algorithm Gis as in SCDH. We use hX to denote the public key associated
with identity idX and use αX to denote the associated secret key.
• E
(
αS,idS,hR,idR, m
)
works as follows, where hS := gαS:
β ←R Zq, v ←gβ,
hSR ←(hR)αS, w ←(hR)β,
k←H
(
v,w,h S,hR,hSR, idS, idR
)
, c ←Es(k,m)
output (v,c).
• D
(
hS,idS,αR,idR, (v,c)
)
works as follows, where hR := gαR:
hSR ←(hS)αR, w ←vαR, k ←H
(
v,w,h S,hR,hSR, idS, idR
)
, output Ds(k,c).
563
In this scheme, the symmetric encryption key is derived from the long term secret key hSR = gαS·αR
along with an ephemeral secret keyw= gβ·αR. The ephemeral secret key ensures CCA security even
when the attacker knows the sender’s secret key αS. The long term secret key ensures ciphertext
integrity, as before.
The following theorem proves security of SC′
DH in this stronger signcryption security model.
Interestingly, the proof of CCA security for SC′
DH only relies on the simpler interactive Diﬃe-
Hellman assumption from Section 12.4, not the double-interactive assumption I2CDH that we used
in proving CCA-security for SCDH.
Theorem 13.11. SC′
DH is a secure signcryption scheme that provides forward secrecy assumingEis
an AE-secure cipher, the I2CDH assumption (Deﬁnition 13.9) holds in G, and the hash function H
is modeled as a random oracle.
In particular, for every ciphertext integrity adversary Aci that attacks SC′
DH as in the random
oracle variant of Attack Game 13.5, there exists a ciphertext integrity adversary Bci that attacks
Eas in Attack Game 9.1, and an I2CDH adversary Bdh for G, where Bs and Bdh are elementary
wrappers around Aci, such that
SCIadv[Aci,SCDH] ≤CIadv[Bci,E] + I2CDHadv[Bdh,G].
In addition, for every CCA adversary Acca that attacks SCDH as in the random oracle variant
of Attack Game 13.6, there exists a 1CCA adversary B1cca that attacks Eas in Deﬁnition 9.6,
and an ICDH adversary B′
dh for G, where Bs and B′
dh are elementary wrappers around Aci, such
that
SCCA′adv[Acca,SCDH] ≤1CCAadv[B1cca,E] + 2·ICDHadv[B′
dh,G].
Proof idea. The proof of ciphertext integrity is very similar to the proof in Theorem 13.10. The
proof of CCA security with forward secrecy, where the adversary is given the sender’s secret key, is
almost identical to the proof of ElGamal CCA security (Theorem 12.4), together with the random
self reduction for CDH (see Exercise 10.5); as such, the ICDH assumption is suﬃcient for the proof.
2
13.7.5.2 Property II: non-repudiation (security in case of a recipient corruption)
Suppose Alice encrypts a message mto Bob and obtains the ciphertext c. The question is, does c,
together with Bob’s secret key, provide Bob with enough evidence to convince a third party that
Alice actually sent the message mto Bob? We call this property non-repudiation. We explained
at the beginning of the chapter that such evidence is inherently limited in its persuasive powers:
Alice can simply claim that her secret key was stolen from her and that someone else produced c, or
she can deliberately leak her secret key in order to repudiate c. Nevertheless, since non-repudiation
may be required in some situations, we deﬁne it and show how to construct signcryption schemes
that provide it.
Non-repudiation is also useful as a partial defense against a compromise of Bob’s secret key. If
the signcryption scheme does not provide non-repudiation, then an attacker can use Bob’s compro-
mised secret key to send messages to Bob pretending to be from Alice. This attack is called key
compromise impersonation or KCI. Non-repudiation ensures that Bob’s key cannot be used to
impersonate Alice and therefore a KCI attack is not possible.
564
Deﬁning non-repudiation. We deﬁne non-repudiation by slightly tweaking the ciphertext in-
tegrity game (Attack Game 13.5). The goal is to ensure that ciphertext integrity is maintained
even if the adversary obtains the recipient’s secret key. The modiﬁed game is as follows:
Attack Game 13.9 (Ciphertext integrity with non-repudiation). The game is identical
to Attack Game 13.5 except that we change the setup step as follows: in addition to giving the
adversary the public keys pkS and pkR, the challenger gives the adversary the receiver’s secret key
skR. The corresponding advantage is denoted SCI ′adv[A,SC]. 2
Deﬁnition 13.11. A signcryption scheme SC is said to provide non-repudiation, if for all
eﬃcient adversaries A, the value SCI′adv[A,SC] is negligible.
Non-repudiation for encrypt-then-sign. The encrypt-then-sign construction provides non-
repudiation: the secret key skR is only used to decrypt ciphertexts and does not help in signing
anything. Indeed, in the concrete security bound given in Theorem 13.8, one can see that the
bound on SCI advantage does not depend at all on the security of the signature scheme.
Non-repudiation for sign-then-encrypt. The same argument cannot be made for the sign-
then-encrypt construction. Observe that in the concrete security bound given in Theorem 13.9,
the bound on the SCCI advantage depends on both the security of the encryption scheme and the
signature scheme. In fact, it is easy to see that this scheme cannot provide non-repudiation as we
have deﬁned it. Indeed, given the decryption key, one can always decrypt a ciphertext encrypting
(m,σ) and then simply re-encrypt it, obtaining a diﬀerent, but still valid, ciphertext.
Although sign-then-encrypt does not satisfy our deﬁnition of non-repudiation, it does satisfy
a weaker notion that corresponds to plaintext integrity, rather than ciphertext integrity. Roughly
speaking, this property corresponds to a modiﬁcation of Attack Game 13.9 in which the winning
condition is changed: to win the game, its candidate forgery ˆcmust decrypt to a message that was
never submitted as an S →R encryption query. We leave it to the reader to ﬂesh out the details of
this deﬁnition, and to show that sign-then-encrypt satisﬁes this weaker notion of non-repudiation.
See also Exercise 9.15.
Non-repudiation for SCDH. The SCDH scheme does not provide non-repudiation, in a very
strong sense: the recipient can encrypt any message just as well as the sender. The same is true
for SC′
DH. Because of this property, both these schemes provide complete deniability — the sender
can always claim (correctly) that any ciphertext it generated could have been generated by the
receiver. In real-world settings this deniability property may be considered a feature rather than a
bug.
Summary. Forward secrecy is clearly a desirable property in real-world systems. Non-
repudiation, in the context of signcryption, is not always needed. In situations where forward
secrecy is desirable, but non-repudiation is not, the SC′
DH scheme is a very eﬃcient solution. In
situations where both properties are needed, encrypt-then-sign is a safer option than sign-then-
encrypt, despite only providing a slightly weaker notion of CCA security, as discussed above.
Exercise 13.19 is a variation of encrypt-then-sign that is also an attractive option to ensure both
forward secrecy and non-repudiation.
565
13.8 Certiﬁcates and the public-key infrastructure
We next turn to one of the central applications of digital signatures, namely, their use in certiﬁcates
and public-key infrastructure. In its simplest form, a certiﬁcate is a blob of data that binds a public-
key to an identity. This binding is asserted by a third party called a certiﬁcate authority, or
simply a CA. We ﬁrst discuss the mechanics of how certiﬁcates are issued and then discuss some
real-world complications in managing certiﬁcates — speciﬁcally, how to cope with misbehaving
CAs and how to revoke certiﬁcates.
Obtaining a certiﬁcate. Say Alice wishes to obtain a certiﬁcate for her domain alice.com. She
sends a certiﬁcate signing request (CSR) to the CA, that contains Alice’s identity, her email
address, and the public key that she wishes to bind to her domain.
Once the CA receives the CSR, it checks that Alice is who she claims to be. In some cases this
check is as naive as sending a challenge email to Alice’s address and verifying that she can read the
email. In other cases this is done by requiring notarized documents proving Alice’s identity. We
emphasize that certifying Alice’s real-world identity is the primary service that the CA provides. If
all the checks succeed, the CA assembles the relevant data into a certiﬁcate structure, and signs it
using the CA’s secret signing key. The resulting signed blob is a certiﬁcate that binds the public key
in the CSR to Alice’s identity. Some CAs issue certiﬁcates for free, while others require payment
from Alice to issue a certiﬁcate.
The resulting signed certiﬁcate can be sent to anyone that needs to communicate securely with
Alice. Anyone who has the CA’s veriﬁcation key can verify the certiﬁcate and gain some conﬁdence
that the certiﬁed public key belongs to Alice.
X.509 certiﬁcates. Certiﬁcates are formatted according to a standard called X.509. Fig. 13.4
gives an example X.509 certiﬁcate that binds a public key to an entity identiﬁed in the subject
ﬁeld. Here the entity happens to be Facebook Inc., and its public key is an (elliptic-curve) ElGamal
public key, shown on the right side of the ﬁgure. The certiﬁcate was issued by a CA called DigiCert
Inc., who used its RSA signing key to sign the certiﬁcate using the PKCS1 standard with SHA256
as the hash function. A portion of the CA’s signature is shown on the bottom right of the ﬁgure.
To verify this certiﬁcate one would need the public key for DigiCert Inc.
Every X.509 certiﬁcate has a serial number that plays a role in certiﬁcate revocation, as ex-
plained in Section 13.8.2 below. Certiﬁcates also have a validity window: a time when the certiﬁcate
becomes active, and a time when the certiﬁcate expires. A certiﬁcate is considered invalid outside
of its validity window, and should be rejected by the veriﬁer. The validity window is typically one
or two years, but can be longer or shorter. For example, the certiﬁcate in Fig. 13.4 has a validity
window of about seventeen months. The reason for limiting certiﬁcate lifetime is to ensure that if
the private key is stolen by an attacker, that attacker can only abuse the key for a limited period
of time. The longer the validity window, the longer an attacker can abuse a stolen secret key. We
discuss this further in Section 13.8.2 where we discuss certiﬁcate revocation.
A certiﬁcate issued by a CA can be veriﬁed by anyone who has that CA’s public key. If there
were only one CA in the world then everyone could store a copy of that CA’s public key and use
it to verify all certiﬁcates. However, a single global CA would not work well. First, every country
wants to run a CA for local businesses in its region. Second, to keep the price of certiﬁcates low, it
is best to enable multiple CAs to compete for the business of issuing certiﬁcates. Currently there
are thousands of active CAs issuing certiﬁcates.
566
…
Figure 13.4: An example X.509 certiﬁcate
Certiﬁcate chains. Since there are multiple CAs issuing certiﬁcates, and new ones can appear at
any time, the challenge is to distribute CA public keys to end-users who need to verify certiﬁcates.
The solution, called a certiﬁcate chain, is to allow one CA to certify the public key of another
CA. This process can repeat recursively, resulting in a chain of certiﬁcates where every certiﬁcate
in the chain certiﬁes the public key of the next CA in the chain.
The public key of top level CAs, called root CAs , are pre-installed on all clients that need
to verify certiﬁcates. There are several hundred such root CAs that ship with every standard
operating system. A root CA can issue a certiﬁcate to an intermediate CA, and an intermediate
CA can issue a certiﬁcate to another intermediate CA. Continuing this way we obtain a chain of
certiﬁcates starting from the root and containing one or more intermediate CAs. Finally, the CA at
the bottom of the chain issues a client certiﬁcate for the end identity, such as Facebook in Fig. 13.4.
The certiﬁcate chain for the Facebook certiﬁcate is shown in Fig. 13.5. The root CA is DigiCert
Inc., but its secret key is kept oﬄine to reduce the risk of theft. The root secret key is only used for
one thing: to issue a certiﬁcate for an intermediate CA, that is also owned by DigiCert Inc. That
intermediate CA then uses its secret key to issue client certiﬁcates to customers like Facebook. If
the intermediate CA’s secret key is lost or stolen, the corresponding certiﬁcate can be revoked, and
the root CA can issue a new certiﬁcate for the intermediate CA.
To verify this certiﬁcate chain of length three, the veriﬁer needs a local trusted copy of the
public key of the root CA. That public key lets the veriﬁer check validity of the certiﬁcate issued
to the intermediate CA. If valid, it has some assurance that the intermediate CA can be trusted.
The veriﬁer then checks validity of the certiﬁcate issued to Facebook by the intermediate CA. If
valid, the veriﬁer has some assurance that it has the correct public key for Facebook.
Certiﬁcate chains and basic constraints. X.509 certiﬁcates contain many ﬁelds and we only
scratched the surface in our discussion above. In the context of certiﬁcate chains we mention
two ﬁelds that play an important security role. In Fig. 13.5 we saw that the certiﬁcate chain
issued to Facebook has length three. What is to prevent Facebook from behaving like a CA and
generating a certiﬁcate chain of length four for another identity, say alice.com? This certiﬁcate
chain, unbeknownst to Alice, would enable Facebook to impersonatealice.comand even eavesdrop
567
Figure 13.5: An example certiﬁcate chain
on traﬃc to alice.com by acting as a “man in the middle,” similar to what we saw in Section 10.7.
The reason Facebook cannot issue certiﬁcates is because of a basic constraint ﬁeld that every
CA must embed in the certiﬁcates that it issues. This ﬁeld, called the “CA” ﬁeld, is set to true if
the entity being certiﬁed is allowed to act as a CA, and is set to false otherwise. For a certiﬁcate
chain of length ℓ to be valid, it must be the case that the top ℓ−1 certiﬁcates in the chain have
their CA basic constraint set to true. If not, the chain must be rejected by the veriﬁer. Facebook’s
certiﬁcate has its CA ﬁeld set to “false,” preventing Facebook from acting as an intermediate CA.
Certiﬁcate validation includes many other such subtle checks, and is generally quite tricky to
implement correctly. Many systems that implement custom certiﬁcate validation were found to be
insecure [72], making them vulnerable to impersonation and man-in-the-middle attacks.
13.8.1 Coping with malicious or negligent certiﬁcate authorities
By now it should be clear that CAs have a lot of power. Any CA can issue a rogue certiﬁcate
and bind the wrong public key to Facebook. If left unchecked, a rogue certiﬁcate would enable an
adversary to mount a man-in-the-middle attack on traﬃc to Facebook and eavesdrop on all traﬃc
between Facebook and unsuspecting users. We will discuss these attacks in detail in Chapter 21
after we discuss the TLS session setup mechanism. Several commercial tools make this quite easy
to do in practice.
There are currently thousands of intermediate CAs operating on the Internet and all are trusted
to issue certiﬁcates. Due to the large number of CAs, it is not surprising that wrong certiﬁcates
are routinely discovered. Here is a small sample of incidents:
• Diginotar was a Dutch certiﬁcate authority that was hacked in 2011. The attacker obtained
a Diginotar signed certiﬁcate for *.google.com, and for many other domains, letting the
attacker mount a man-in-the-middle attack on all these domains. In response, major Web
browser vendors revoked trust in all certiﬁcates issued by the Diginotar CA, causing Diginotar
to declare bankruptcy in Sep. 2011.
• India NIC in 2013 erroneously issued certiﬁcates for several Google and Yahoo domains [101].
This intermediate CA was certiﬁed by India CCA, a root CA trusted by Microsoft Windows.
As a result, the Chrome browser no longer trusts certiﬁcates issued by India NIC. Further-
more, following this incident, the India CCA root CA is only trusted to issue certiﬁcates for
domains ending in .in, such as google.co.in.
• Verisign in 2001 erroneously issued a Microsoft code-signing certiﬁcate to an individual mas-
querading as a Microsoft employee [113]. This certiﬁcate enabled that individual to distribute
code that legitimately looked like it was written by Microsoft. In response, Microsoft issued
a Windows software patch that revoked trust in this certiﬁcate.
568
As we can see, many of these events are due to an erroneous process at the CA. Any time a certiﬁcate
is issued that binds a wrong public key to a domain, that certiﬁcate enables a man-in-the-middle
attack on the target domain. The end result is that the attacker can inspect and modify traﬃc to
and from the victim domain.
The question then is how to identify and contain misbehaving CAs. We discuss two ideas below.
Certiﬁcate pinning. The reader must be wondering how the incidents mentioned above were
discovered in the ﬁrst place. The answer is a mechanism called certiﬁcate pinning , which is
now widely supported by Web browsers. The basic idea is that browsers are pre-conﬁgured to
know that the only CA authorized to issue certiﬁcates for the domain facebook.com is “DigiCert
SHA2 High Assurance Server CA,” as shown in Fig. 13.5. If a browser ever sees a certiﬁcate for
facebook.com that is issued by a diﬀerent CA, it does two things: ﬁrst, it treats the certiﬁcate
as invalid and closes the connection, and second, it optionally alerts an administrator at Facebook
that a rogue certiﬁcate was discovered. The incident discussed above, involving India NIC, was
discovered thanks to a certiﬁcate pin for gmail.com. Browsers in India alerted Google to the
existence of a rogue certiﬁcate chain for gmail.com. Google then took action to revoke the chain
and launch an investigation. The signatures in the rogue chain provide irrefutable evidence that
something went wrong at the issuing CA.
In more detail, certiﬁcate pinning works as follows. Every browser maintains a pinning database,
where, roughly speaking, every row in the database is a tuple of the form
(domain, hash0, hash1, ...).
Each hash i is the output of a hash function (so for SHA256, a 32-byte string). The data for
each record is provided by the domain owner. Facebook, for example, provides the hashes for the
facebook.com domain.
When the browser connects to a domain using HTTPS, that domain sends its certiﬁcate chain
to the browser. If the domain is in the pinning database, the browser computes the hash of each
certiﬁcate in the chain. Let S be the resulting set of hash values. Let T be the set of hash values
in the pinning record for this domain. If the intersection of S and T is empty, the certiﬁcate chain
is rejected, and the browser optionally sends an alert to the domain administrator indicating that
a rogue certiﬁcate chain was encountered.
To see how this works, consider again the example chain in Fig. 13.5. The pinning record for
the domain facebook.com is just a single hash, namely the hash of the certiﬁcate for “DigiCert
SHA2 High Assurance Server CA.” In other words, the set T contains a single hash value. If the
browser encounters a certiﬁcate chain for facebook.com where none of the certiﬁcates in the chain
hash to the pinned value, the certiﬁcate chain is rejected. More generally, domains that purchase
certiﬁcates from multiple CAs include the hash of all those CA certiﬁcates in their pinning record.
Why does Facebook write the hash of its CA certiﬁcate in the Facebook pinning record? Why
not write the hash of the Facebook certiﬁcate from Fig. 13.4 in the pinning record? In fact,
writing the CA certiﬁcate in the pinning record seems insecure; it makes it possible for DigiCert to
issue a rogue certiﬁcate for facebook.com that will be accepted by browsers, despite the pinning
record. If instead, Facebook wrote the Facebook certiﬁcate in Fig. 13.4 as the only hash value in
the pinning record, then DigiCert would be unable to issue a rogue certiﬁcate for facebook.com.
The only certiﬁcate for facebook.com that browsers would accept would be the certiﬁcate in
Fig. 13.4. However, there is enormous risk in doing so. If Facebook somehow lost its own secret
569
key, then no browser in the world will be able to connect to facebook.com. Pinning the CA
certiﬁcate lets Facebook recover from key loss by simply asking DigiCert to issue a new certiﬁcate
for facebook.com. Thus, the risk of bringing down the site outweighs the security risk of DigiCert
issuing a rogue certiﬁcate. While losing the secret key may not be a concern for a large site like
Facebook, it is a signiﬁcant concern for smaller sites who use certiﬁcate pinning.
Finally we mention that there are two mechanisms for creating a pinning record: static and
dynamic. Static pins are maintained by the browser vendor and shipped with the browser. Dynamic
pins allow a domain to declare its own pins via an HTTP header, sent from the server to the browser,
as follows:
Public-Key-Pins: pin-sha256="hash"; max-age=expireTime
[; report-uri="reportURI"] [; includeSubDomains]
Here pin-sha256 is the hash value to pin to, max-age indicates when the browser will forget the
pin, and report-uri is an optional address where pin validation failures should be reported. The
HTTP header is accepted by the browser only if it is sent over an encrypted HTTPS session.
The header is ignored when sent over unencrypted HTTP. This prevents a network attacker from
injecting invalid pins.
Certiﬁcate transparency. A completely diﬀerent approach to coping with misbehaving CAs
is based on public certiﬁcate logs. Suppose there existed a public certiﬁcate log that contained
a list of all the certiﬁcates ever issued. Then a company, like Facebook, could monitor the log
and learn when someone issues a rogue certiﬁcate for facebook.com. This idea, called certiﬁcate
transparency, is compelling, but is not easy to implement. How do we ensure that every certiﬁcate
ever issued is on the log? How do we ensure that the log is append-only so that a rogue certiﬁcate
cannot be removed from the log? How do we ensure that everyone in the world sees the same
version of the log?
Certiﬁcate transparency provides answers to all these questions. Here, we just sketch the
architecture. When a CA decides to support certiﬁcate transparency, it chooses one of the public
certiﬁcate logs and augments its certiﬁcate issuance procedure as follows: (1) before signing a new
certiﬁcate, the CA sends the certiﬁcate data to the log, (2) the log signs the certiﬁcate data and
sends back the signature, called a signed certiﬁcate timestamp (SCT), (3) the CA adds the
SCT as an extension to the certiﬁcate data and signs the resulting structure, to obtain the ﬁnal
issued certiﬁcate. The SCT is embedded as an extension in the newly issued certiﬁcate.
The SCT is a promise by the certiﬁcate log to post the certiﬁcate to its log within a certain
time period, say one day. At noon every day, the certiﬁcate log appends all the new certiﬁcates it
received during that day to the log. It then computes a hash of the entire log and signs the hash
along with the current timestamp. The log data and the signature are made publicly available for
download by anyone.
The next piece of the architecture is a set of auditors that run all over the world and ensure
that the certiﬁcate logs are behaving honestly — they are posting to the log as required, and they
never remove data from the log. Every day the auditors download all the latest logs and their
signatures, and check that no certiﬁcates were removed from the logs. If they ﬁnd that a certiﬁcate
on some day t is missing from the log on day t+ 1, then the log signatures from days t and t+ 1
are evidence that the certiﬁcate log is misbehaving.
Moreover, every auditor crawls the Internet looking for certiﬁcates. For each certiﬁcate that
contains an SCT extension, the auditor does an inclusion check: it veriﬁes that the certiﬁcate
570
appears on the latest version of the log that the SCT points to. If not, then the signed SCT along
with the signed log, are evidence that the certiﬁcate log is misbehaving. This process ensures that
all deployed certiﬁcates with an SCT extension must appear on one of the logs; otherwise one of the
certiﬁcate logs is caught misbehaving. Anyone can run the auditor protocol. In particular, every
Web browser can optionally function as an auditor and run the inclusion check before choosing
to trust a certiﬁcate. If the inclusion check fails, the browser notiﬁes the browser vendor who
can launch an investigation into the practices of the certiﬁcate log in question. We note that by
using a data structure, called a Merkle hash tree, the inclusion check can be done very eﬃciently,
without having to download the entire log. We discuss Merkle hash trees and their applications in
Section 8.9.
Unfortunately, auditing is not enough. A devious certiﬁcate log can misbehave in a way that
will not be caught by the auditing process above. Suppose that a CA issues a rogue certiﬁcate for
facebook.com and writes it to a certiﬁcate log, as required. Now, the certiﬁcate log creates two
signed versions of the log: one with the rogue certiﬁcate and one without. Whenever an auditor
downloads the log, it is given the version of the log with the rogue certiﬁcate. To the auditor, all
seems well. However, when Facebook reads the log to look for rogue facebook.com certiﬁcates,
it is given the version without the rogue certiﬁcate. This prevents Facebook from discovering the
rogue certiﬁcate, even though all the auditors believe that the certiﬁcate log is behaving honestly.
The architecture mitigates this attack in two ways. First, every certiﬁcate must be written to at
least two logs, so that both certiﬁcate logs must be corrupt for the attack to succeed. Second, there
is a broadcast mechanism in which the daily hash of all the logs is broadcast to all entities in the
system. A log that does not match the broadcast hash is simply ignored.
The ﬁnal piece of the architecture is mandating certiﬁcate transparency on all CAs. At some
point in the future, browser vendors could decide to reject all certiﬁcates that do not have a valid
SCT from a trusted certiﬁcate log. This will eﬀectively force universal adoption of certiﬁcate
transparency by all CAs. At that point, if a rogue certiﬁcate is issued, it will be discovered on one
of the certiﬁcate logs and revoked. We note that many of the large CAs already support certiﬁcate
transparency.
13.8.2 Certiﬁcate revocation
We next look at the question of revoking certiﬁcates. The goal of certiﬁcate revocation is to ensure
that, after a certiﬁcate is revoked, all clients treat that certiﬁcate as invalid.
There are many reasons why a certiﬁcate may need to be revoked. The certiﬁcate could have
been issued in error, as discussed in the previous subsection. The private key corresponding to
the certiﬁcate may have been stolen, in which case the certiﬁcate owner will want to revoke the
certiﬁcate so it cannot be abused. This happens all the time; sites get hacked and their secrets are
stolen. One well-publicized example is the heartbleed event. Heartbleed is a bug in the OpenSSL
library that was introduced in 2012. The bug was publicly discovered and ﬁxed in 2014, but during
those two years, from 2012 to 2014, a remote attacker could have easily extracted the secret key
from every server that used OpenSSL, by simply sending a particular malformed request to the
server. When the vulnerability was discovered in 2014, thousands of certiﬁcates had to be revoked
because of concern that the corresponding secret keys were compromised.
Given the need to revoke certiﬁcates, we next describe a few techniques to do so.
571
Short-lived certiﬁcates. Recall that every certiﬁcate has a validity period and the certiﬁcate
is no longer valid after its expiration date. Usually, when an entity like Facebook buys a one-year
certiﬁcate, the CA issues a certiﬁcate that expires a year after it was issued. Imagine that instead,
the CA generated 365 certiﬁcates, where each one is valid for exactly one day during that year.
All 365 certiﬁcates are for the same public key; the only diﬀerence is the validity window. These
certiﬁcates are called short-lived certiﬁcates because each is valid for only one day.
The CA keeps all these certiﬁcates to itself, and releases each one at most a week before it
becomes valid. So, the certiﬁcate to be used on January 28 is made available on January 21, but no
sooner. Every day Facebook connects to a public site provided by the CA and fetches the certiﬁcate
to be used a week later. This is a simple process to automate, and if anything goes wrong, there is
an entire week to ﬁx the problem.
Now, when Facebook needs to revoke its certiﬁcate, it simply instructs the CA to stop releasing
short-lived certiﬁcates for its domain. This eﬀectively makes the stolen private key useless after
at most one week. If faster revocation is needed, the CA can be told to release each short-lived
certiﬁcate only an hour before it becomes valid, in which case the secret key becomes useless at
most 25 hours after it is revoked.
The use of short-lived certiﬁcates is the simplest and most practical technique for certiﬁcate
revocation available, yet it is not widely used. The next two techniques are more cumbersome, but
are the ones most often used by CAs.
Certiﬁcate revocation lists (CRLs). A very diﬀerent approach is to have the CA collect all
certiﬁcate revocation requests from all its customers, and on a weekly basis issue a signed list of
all certiﬁcates that were revoked during that week. This list, called a certiﬁcate revocation list
(CRL), contains the serial numbers of all the certiﬁcates that were revoked during that week. The
list is signed by the CA.
Every certiﬁcate includes a special extension ﬁeld called CRL Distribution Points, as shown
in Fig. 13.6. This ﬁeld instructs the veriﬁer where to obtain the CRL from the issuing CA. The
CA must run a public server that serves this list to anyone who asks for it.
When a client needs to validate a certiﬁcate, it is expected to download the CRL from the
CRL distribution point, and reject the certiﬁcate if its serial number appears in the CRL. For
performance reasons, the CRL has a validity period of, say one week, and the client can cache the
CRL for that period. As a result, it may take a week from the time a revocation request is issued
until all clients learn that the certiﬁcate has been revoked.
There are two signiﬁcant diﬃculties with this approach. First, what should the client do if
the CRL server does not respond to a CRL download request? If the client were to accept the
certiﬁcate, then this opens up a very serious attack. An attacker can cause the client to accept a
revoked certiﬁcate by simply blocking its connection to the CRL server. Clearly the safe thing to
do is to reject the certiﬁcate; however, this is also problematic. It means that if the CRL server
run by Facebook’s CA were to accidentally crash, then no one could connect to Facebook until the
CA ﬁxes the CRL server. As you can imagine, this does not go over well with Facebook.
A second diﬃculty with CRLs is that they force the client to download a large list of revoked
certiﬁcates that the client does not need. The client is only interested in learning the validity
status of a single certiﬁcate: the one it is trying to validate. The client does not need, and is not
interested in, the status of other certiﬁcates. This ineﬃciency is addressed by a better mechanism
called OCSP, which we discuss next.
572
…
Figure 13.6: The CRL and OCSP ﬁelds in the certiﬁcate from Fig. 13.4.
The online certiﬁcate status protocol (OCSP). A client that needs to validate a certiﬁcate
can use the OCSP protocol to query the CA about the status of that speciﬁc certiﬁcate. To make
this work, the CA includes an OCSP extension ﬁeld in the certiﬁcate, as shown in Fig. 13.6. This
ﬁeld tells the client where to send its OCSP query. In addition, the CA must setup a server, called
an OCSP responder, that responds to OCSP queries from clients.
When the client needs to validate a certiﬁcate, it sends the certiﬁcate’s serial number to the
OCSP responder. Roughly speaking, the responder sends back a signed message saying “valid” or
“invalid”. If “invalid” the client rejects the certiﬁcate. OCSP responses can be cached for, say a
week, and consequently revocation only takes eﬀect a week after a request is issued.
As with CRLs, it is not clear what the client should do when the OCSP responder simply does
not respond. Moreover, OCSP introduces yet another problem. Because a client, such as a Web
browser, sends to the CA the serial number of every certiﬁcate it encounters, the CA can eﬀectively
learn what web sites the user is visiting. This is a breach of user privacy. The problem can be
partially mitigated by an extension to OCSP, called OCSP stapling, but this extension is rarely
used.
13.9 Case study: legal aspects of digital signatures
While cryptographers say that a signature scheme is secure if it existentially unforgeable under
a chosen message attack, the legal standard for what constitutes a valid digital signature on an
electronic document is quite diﬀerent. The legal deﬁnition tries to capture the notion of intent:
a signature is valid if the signer “intended” to sign the document. Here we brieﬂy review a few
legislative eﬀorts that try to articulate this notion. This discussion shows that a cryptographic
digital signature is very diﬀerent from a legally binding electronic signature.
Electronic signatures in the United States. On June 30, 2000, the U.S. Congress enacted
the Electronic Signatures in Global and National Commerce Act, known as E-SIGN. The goal of
E-SIGN is to facilitate the use of electronic signatures in interstate and foreign commerce.
The U.S. statute of frauds requires that contracts for the sale of goods in excess of $500 be
signed. To be enforceable under U.S. law, E-SIGN requires that an electronic signature possess
three elements: (1) a symbol or sound, (2) attached to or logically associated with an electronic
record, and (3) made with the intent to sign the electronic record. Here we only discuss the
ﬁrst element. The U.S. deﬁnition of electronic signatures recognizes that there are many diﬀerent
573
methods by which one can sign an electronic record. Examples of electronic signatures that qualify
under E-SIGN include:
1. a name typed at the end of an e-mail message by the sender,
2. a digitized image of a handwritten signature that is attached to an electronic document,
3. a secret password or PIN to identify the sender to the recipient,
4. a mouse click, such as on an “I accept” button,
5. a sound, such as the sound created by pressing ‘9’ on a phone,
6. a cryptographic digital signature.
Clearly, the ﬁrst ﬁve examples are easily forgeable and thus provide little means of identifying
the signatory. However, recall that under U.S. law, signing a paper contract with an ‘X’ constitutes
a binding signature, as long as one can establish intent of the signatory to sign the contract. Hence,
the ﬁrst ﬁve examples should be treated as the legal equivalent of signing with an ‘X’.
United nations treaty on electronic signatures. In November 2005 the United Nations
adopted its convention on the use of electronic communications in international contracts. The
signature requirements of the 2005 U.N. convention go beyond those required under E-SIGN. In
particular, the convention focuses on the issue of security, by requiring the use of a method that
(1) identiﬁes the signer, and (2) is reliable. In particular, the convention observes that there is
a big diﬀerence between an electronic signature that merely satisﬁes the basic requirements of
applicable U.S. law (e.g., a mouse click) and a trustworthy electronic signature. Thus, under the
U.N. convention a mouse click qualiﬁes as a digital signature only if it allows the proponent to
ultimately prove “who” clicked, and to establish the intention behind the click.
European Community framework for electronic signatures. in December 1999, the Euro-
pean Parliament adopted the Electronic Signatures Directive. The directive addresses three forms
of electronic signatures. The ﬁrst can be as simple as signing an e-mail message with a person’s
name or using a PIN-code. The second is called the “advanced electronic signature” (AES). The
directive is technology neutral but, in practice, AES refers mainly to a cryptographic digital signa-
ture based on a public key infrastructure (PKI). An AES is considered to be more secure, and thus
enjoys greater legal acceptability. An electronic signature qualiﬁes as an AES if it is: (1) uniquely
linked to the signatory, (2) capable of identifying the signatory, (3) created using means that the
signatory can maintain under his sole control, and (4) is linked to the data to which it relates in
such a manner that any subsequent change of the data is detectable.
13.10 A fun application: forward secure signatures
To be written.
13.11 Notes
Citations to the literature to be added.
574
13.12 Exercises
13.1 (Exercising the deﬁnition). Let ( G,S,V ) be a secure signature scheme with message
space {0,1}n. Generate two signing/veriﬁcation key pairs (pk 0,sk0) ←R G() and (pk 1,sk1) ←R G().
Which of the following are secure signature schemes? Show an attack or prove security.
(a) Accept one valid: S1
(
(sk0,sk1), m
):=
(
S(sk0,m), S(sk1,m)
)
. Verify:
V1
(
(pk0,pk1), m,(σ0,σ1)
)
= ‘accept’ ⇐⇒
[
V(pk0,m,σ 0) = ‘accept’ or V(pk1,m,σ 1) = ‘accept’
]
(b) Sign halves: S2
(
(sk0,sk1), (mL,mR)
):=
(
S(sk0,mL), S(sk1,mR)
)
V2
(
(pk0,pk1), (mL,mR), (σ0,σ1)
)
= ‘accept’ ⇐⇒
V(pk0,mL, σ0) = V(pk1,mR, σ1) = ‘accept’
(c) Sign with randomness: for m∈{0,1}n do
S3
(
sk0, m
):=
[
choose random r←{0,1}n, output
(
r, S(sk0, m⊕r), S(sk0, r)
) ]
.
V3
(
pk0, m, (r,σ0,σ1)
)
= ‘accept’ ⇐⇒ V(pk0, m⊕r, σ0) = V(pk0, r, σ1) = ‘accept’
13.2 (Multi-key signature security). Just as we did for secure MACs in Exercise 6.3, show
that security in the single-key signature setting implies security in the multi-key signature setting.
(a) Show how to extend Attack Game 13.1 so that an attacker can submit signing queries with
respect to several signing keys. This is analogous to the multi-key generalization described
in Exercise 6.3.
(b) Show that every eﬃcient adversary Athat wins your multi-key attack game with probabilityϵ
can be transformed into an eﬃcient adversary Bthat wins Attack Game 13.1 with probability
ϵ/Q, where Q is the number of signature keys. The proof uses the same “plug-and-pray”
technique as in Exercise 6.3.
13.3 (Non-binding signatures). In Section 13.1.1.1 we mentioned that secure signature schemes
can be non-binding: for a given (pk,sk), the signer can ﬁnd two distinct messages m0 and m1 where
the same signature σ is valid for both messages with respect to pk. Give an example of a secure
signature scheme that is non-binding.
Hint: Consider using the hash-and-sign paradigm of Section 13.2, but with the collision resistant
hash functions discussed in Exercise 10.30.
13.4 (A signer confusion attack on RSA). Let us show thatSRSA-FDH is vulnerable to a signer
confusion attack, as discussed in Section 13.1.1.1. Let ( n,e) be Alice’s public key and σ ∈Zn be
a signature on some message m. Then σe = H(m) in Zn. Show that an eﬃcient adversary can
construct a new public key pk′= (n′,e′), along with the corresponding secret key, such that ( m,σ)
is valid message-signature pair with respect to pk′.
Hint: We show in Section 16.1.2.2 that for some primes p, the discrete-log problem in Z∗
p can be
solved eﬃciently. For example, when p = 2ℓ + 1 is prime, and ℓ is poly-bounded, the discrete-log
575
problem in Z∗
p is easy. Show that by forming n′as a product of two such primes, the adversary can
come up with an e′such that σ(e′) = H(m) in Zn′.
13.5 (Strongly binding signatures). In this exercise we explore a general defense against mes-
sage and signer confusion discussed in Section 13.1.1.1. To this end, we deﬁne the concept of a
strongly binding signature scheme . For a given signature scheme S = ( G,S,V ), a strong
binding adversary Atakes no input and outputs ( pk,m, pk′,m′,σ). We say that Adefeats strong
binding if
(pk,m) ̸= (pk′,m′) and V(pk,m,σ ) = V(pk′,m′,σ) = accept.
Deﬁne A’s advantage with respect to S, denoted sbSIGadv[A,S], as the probability that Adefeats
strong binding. We say that Sis strongly binding if sbSIGadv[A,S] is negligible for all eﬃcient
adversaries A.
(a) Let us augment Sso that the signer always attaches the public key pk to the message prior
to signing, and the veriﬁer does the same. That is, deﬁne S′= (G,S′,V ′) where
S′(sk,m) := S
(
sk,(pk,m)
)
and V′(pk,m,σ ) := V
(
pk,(pk,m),σ
)
.
Here we are assuming for simplicity that pk can be easily derived from sk. Show that if we
apply this augmentation to SRSA-FDH from Section 13.3.1 and model the hash function H as
a random oracle, then the augmented scheme is still secure and becomes strongly binding.
(b) Give an example of a secure signature scheme that is not strongly binding even after the
augmentation from part (a) is applied.
(c) Let us look at a stronger augmentation that makes every signature scheme strongly binding.
For a hash function H′′, deﬁne S′′= (G,S′′,V ′′) where
• S′′(sk,m) :=
{
h←H′′(pk,m), σ ←R S(sk,m), output (h,σ)
}
, and
• V′′(
pk,m, (h,σ)
)
accepts if h= H′′(pk,m) and V(pk,m,σ ) = accept.
Show that (i) if Sis secure, then so is S′′, and (ii) if H′′ is collision resistant, then S′′ is
strongly binding.
Discussion: this augmentation results in a slightly longer signature, whereas the augmen-
tation from part (a) does not aﬀect signature length.
13.6 (Derandomizing signatures). Let S= (G,S,V ) be a secure signature scheme deﬁned over
(M,Σ), where the signing algorithm S is probabilistic. In particular, algorithm S uses randomness
chosen from a space R. We let S(sk,m; r) denote the execution of algorithm S with randomness
r. Let F be a secure PRF deﬁned over ( K,M,R). Show that the following signature scheme
S′= (G′,S′,V ) is secure:
G′() :=
{
(pk,sk) ←R G(), k ←R K, sk′:= (sk,k), output (pk,sk′)
}
;
S′(sk′,m) := {r←F(k,m), σ ←S(sk,m; r), output σ}.
Now the signing algorithm for S′is deterministic.
13.7 (Extending the domain using enhanced TCR). In Exercise 8.27 we deﬁned the notion
of an enhanced-TCR. Show how to use an enhanced-TCR to eﬃciently extend the domain of a
576
signature. In particular, let H be an enhanced-TCR deﬁned over (KH,M,X) and let S= (G,S,V )
be a secure signature scheme with message spaceX. Show that S′= (G,S′,V ′) is a secure signature
scheme:
S′(pk,m) :=
{
r←R KH, σ←S
(
sk,H(r,m)
)
, output (σ,r)
}
;
V′(
pk,m, (σ,r)
):= {accept if σ= V(pk,H(r,m))}.
The beneﬁt over the construction in Section 13.2.1 is that r is not part of the message given to the
signing procedure.
13.8 (Selective security). Selective security is a weak notion of signature security, where the
adversary has to commit ahead of time to the message m for which it will forge a signature. Let
(G,S,V ) be a signature scheme deﬁned over ( M,Σ). The selective security game begins with the
adversary sending a message m ∈M to the challenger. The challenger runs ( pk,sk) ←R G() and
sends pk to the adversary. The adversary then issues a sequence of signing queries m1,...,m Q, as
in Attack Game 13.1, where m ̸= mi for all i = 1,...,Q . The adversary wins if it can produce
a valid signature on m, and the scheme ( G,S,V ) is selectively secure if no eﬃcient adversary
can win this game with non-negligible probability. Note that unlike Attack Game 13.1, here the
adversary has to commit to the message m before it even sees the public key pk.
Now, for a hash function H : M′ →M, deﬁne a new signature scheme ( G,S′,V ′) as in (13.1).
Show that if ( G,S,V ) is selectively secure, and H is modeled as a random oracle, then ( G,S′,V ′)
is existentially unforgeable. In particular, for every existential forgery adversary Aagainst S′ =
(G,S′,V ′) there exists a selective forgery adversary Bagainst S= (G,S,V ) such that
SIGroadv[A,S′] ≤Qro ·SELadv[B,S] + Qs/|M|,
where Amakes at most Qro queries to H and at most Qs signing queries. Here SEL adv[B,S] is B’s
advantage in winning the selective security game against S.
13.9 (FDH variant). Show that the signature scheme S′
RSA-FDH (deﬁned in Section 13.5) is no
less secure than the signature scheme SRSA-FDH (deﬁned in Section 13.3.1). You should show that
if Ais an adversary that succeeds with probability ϵ in breaking S′
RSA-FDH (which has message
space M), then there exists an adversary B(whose running time is roughly the same as that of
A) that succeeds with probability ϵin breaking SRSA-FDH (with message space M′= {0,1}×M).
This should hold for any hash function H.
13.10 (Probabilistic full domain hash). Consider the following signature schemeS= (G,S,V )
with message space M, and using a hash function H : M×R→ Zn:
G() := {(n,d) ←R RSAGen(ℓ,e), pk := (n,e), sk := (n,d), output (pk,sk)};
S(sk,m) :=
{
r←R R, y ←H(m,r), σ ←yd ∈Zn, output (σ,r)
}
;
V
(
pk,m, (σ,r)
):= {y←H(m,r), accept if y= σe and reject otherwise}.
Show that this signature is secure if the RSA assumption holds for ( ℓ,e), the quantity 1 /|R|is
negligible, and H is modeled as a random oracle. Moreover, the reduction to inverting RSA is
tight.
Discussion: While S′
RSA-FDH, from Section 13.5, also has a tight reduction, the construction here
does not use a PRF. The cost is that signatures are longer because r is included in the signature.
577
13.11 (Batch RSA). Let us show how to speed up signature generation in SRSA-FDH.
(a) Let n= pq such that neither 3 nor 5 divide ( p−1)(q−1). We are given p,q and y1,y2 ∈Zn.
Show how to compute both x1 := y1/3
1 ∈Zn and x2 := y1/5
2 ∈Zn by just computing the
15th root of t := (y1)5(y2)3 ∈Zn and doing a bit of extra arithmetic. In other words, show
that given t1/15 ∈Zn, it is possible to compute both x1 and x2 using a constant number of
arithmetic operations in Zn.
(b) Describe an algorithm for computing a 15th root in Zn using a single exponentiation, for n
as in part (a).
(c) Explain how to use parts (a) and (b) to speed up the SRSA-FDH signature algorithm. Specif-
ically, show that the signer can sign two messages at once using about the same work as
signing a single message. The ﬁrst message will be signed under the public key ( n,3) and the
other under the public key ( n,5). This method generalizes to fast RSA signature generation
in larger batches.
13.12 (Shortening RSA signatures). Let us see how to shorten an SRSA-FDH signature by
about one third when the public key is ( n,e) with e = 3. We will need the following fact: for
every x ∈Zn there exist integers 0 ≤r < n1/3 and 0 ≤s ≤n2/3 such that x = ( r/smod n).
These r and s can be eﬃciently found. We will use this fact to shorten an SRSA-FDH signature.
Let pk = (n,3) be an SRSA-FDH public key. To sign a message m the signer does: (i) compute the
SRSA-FDH signature σ:= H(m)1/3 ∈Zn, (ii) ﬁnd integers r and sso that σ= (r/smod n) where r
and s satisfy the bounds from the fact, and (iii) output the shortened signature ˆ σ:= s. Note that
ˆσ is two thirds the length of σ.
(a) Show that the veriﬁer can recover σ from ˆσ, n, and m. Hint: ﬁrst show how to compute the
integer r. Then σ= (r/smod n).
(b) Once the veriﬁer recovers σ from ˆσ it can run the standard SRSA-FDH veriﬁcation algorithm.
Show that using ˆσas the signature does not harm security by showing that an adversary that
breaks security of the shortened scheme can be used to break security of SRSA-FDH.
13.13 (Signature with message recovery). Let T = (G,F,I ) be a one-way trapdoor permu-
tation deﬁned over X:= {0,1}n. Let R:= {0,1}ℓ and U:= {0,1}n−ℓ, for some 0 <ℓ<n . Let H
be a hash function deﬁned over ( M×U , R), and let W be a hash function deﬁned over ( R,U).
Consider the following signature scheme S= (G,S,V ) deﬁned over (M×U , X) where
S
(
sk, (m0,m1)
):=
{
h←H(m0,m1), σ ←I
(
sk, h∥(W(h) ⊕m1)
)
, output σ
}
(a) Explain how the veriﬁcation algorithm works.
(b) Show that the scheme is secure assuming T is one-way, 1/|R|is negligible, and H and W are
modeled as random oracles.
(c) Show that just given ( m0,σ), where σ is a valid signature on the message ( m0,m1), it is
possible to recover m1. A signature scheme that has this property is called a signature
with message recovery. It lets the signer send shorter transmissions: the signer need only
transmit (m0,σ) and the recipient can recover m1 by itself. This can somewhat mitigate the
cost of long signatures with RSA.
578
(d) Can the technique of Section 13.5 be used to provide a tight security reduction for this
construction?
13.14 (An insecure signature with message recovery). Let T = ( G,F,I ) be a one-way
trapdoor permutation deﬁned over X:= {0,1}n. Let H be a hash function deﬁned over ( M0,X).
Consider the following signature scheme S= (G,S,V ) deﬁned over (M0 ×X, X) where
S
(
sk, (m0,m1)
):=
{
σ←I(sk, H(m0) ⊕m1), output σ
}
V
(
pk, (m0,m1), σ
):=
{
y←F(pk,σ), accept if y= H(m0) ⊕m1 and reject otherwise
}
(a) Show that given ( m0,σ), where σ is a valid signature on the message ( m0,m1), it is possible
to recover m1.
(b) Show that this signature scheme is insecure, even when T is one-way and H is modeled as a
random oracle.
13.15 (Blind signatures). At the end of Section 13.3.1 we mentioned the RSA signatures can
be adapted to give blind signatures. A blind signature scheme lets one party, Alice, obtain a
signature on a message m from Bob, so that Bob learns nothing about m. Blind signatures are
used in e-cash systems and anonymous voting systems.
Let (n,d) ←R RSAGen(ℓ,e) and set ( n,e) as Bob’s RSA public key and ( n,d) as his corresponding
private key. As usual, let H : M→ Zn be a hash function. Alice wants Bob to sign a message
m∈M. They engage in the following three-message protocol:
(1) Alice chooses r←R Zn, sets m′←H(m) ·re ∈Zn, and sends m′to Bob,
(2) Bob computes σ′←(m′)d ∈Zn and sends σ′to Alice,
(3) Alice computes the signature σ on m as σ←σ′/r∈Zn.
Equation (13.4) shows that σ is a valid signature on m.
(a) Show that Bob sees a random message m′ in Zn that is independent of m. Therefore, he
learns nothing about m.
(b) We say that a blind signature protocol is secure if the adversary, given a public key and the
ability to request Q blind signatures on messages of his choice, cannot produce Q+ 1 valid
message-signature pairs. Write out the precise deﬁnition of security.
(c) Show that security of the RSA blind signature scheme follows from an assumption called the
one more RSA or 1MRSA assumption, if we model H as a random oracle.
The 1MRSA assumption is the RSA analogue of the 1MDH assumption deﬁned in Sec-
tion 11.6.3. It is deﬁned using the following game. The challenger runs (n,d) ←R RSAGen(ℓ,e)
and sends ( n,e) to the adversary. Next, the adversary makes a sequence of queries to the
challenger, each of which can be one of the following types:
Challenge query: the challenger chooses v←R Zn, and sends v to the adversary.
RSA solve query: the adversary submits ˆv ∈Zn to the challenger, who computes
and sends ˆw←ˆv1/e ∈Zn to the adversary.
579
At the end of the game, the adversary outputs a list of distinct pairs, each of the form ( i,w),
where iis a positive integer bounded by the number of challenge queries, andw∈Zn. We call
such a pair (i,w) correctif w= ve in Zn and vis the challenger’s response to the ith challenge
query. We say the adversary wins the game if the number of correct pairs exceeds the number
of RSA solve queries. The 1MRSA assumption says that every eﬃcient challenger wins this
game with at most negligible probability. In particular, an eﬃcient adversary who sees the
e-th root of a number of challenges cannot produce the e-th root of any other challenge.
13.16 (Insecure signcryption). Let E = ( GE,E,D ) be a CCA-secure public-key encryption
scheme with associated data and let S= (GS,S,V ) be a strongly secure signature scheme. Deﬁne
algorithm G as in Section 13.7.3. Show that the following encrypt-then-sign signcryption scheme
(G,E′,D′) is insecure:
E′(skS,idS,pkR,idR, m) := c←R E
(
pkR, m, idR
)
, σ ←R S
(
skS, (c,idS)
)
output (c,σ)
D′(
pkS,idS,skR,idR, (c,σ)
) := if V(pkS, (c,idS), σ) = reject, output reject
otherwise, output D(skR, c, idR)
13.17 (The iMessage attack). Let E = ( GE,E,D ) be a CCA-secure public-key encryption
scheme and let S = ( GS,S,V ) be a strongly secure signature scheme. Let ( Esym,Dsym) be a
symmetric cipher with key space Kthat implements deterministic counter mode. Deﬁne algorithm
G as in Section 13.7.3. Consider the following encrypt-then-sign signcryption scheme ( G,E′,D′):
E′(skS,idS,pkR,idR, m) := k←R K, c 1 ←Esym
(
k, (idS,m)
)
, c 0 ←R E(pkR, k)
σ←R S
(
skS, (c0,c1,idR)
)
output (c0,c1,σ)
D′(
pkS,idS,skR,idR, (c0,c1,σ)
) := if V(pkS, (c0,c1,idR), σ) = reject, output reject
k←D(skR, c0), (id,m) ←Dsym(k,c1)
if id ̸= idS output reject
otherwise, output m
Because the symmetric ciphertext c1 is part of the data being signed by the sender, the designers
assumed that there is no need to use an AE cipher and that deterministic counter mode is suﬃcient.
Show that this system is an insecure signcryption scheme by giving a CCA attack. At one point, a
variant of this scheme was used by Apple’s iMessage system and this lead to a signiﬁcant breach of
iMessage [69]. Because every plaintext message mincluded a checksum (CRC), an adversary could
decrypt arbitrary encrypted messages using a chopchop-like attack (Exercise 9.5).
13.18 (Signcryption: statically vs adaptively chosen user IDs). In the discussion following
Deﬁnition 13.8, we brieﬂy discussed the possibility of a more robust security deﬁnition in which
the adversary is allowed to choose the sender and receiver user IDs adaptively, after seeing one or
both of the public keys, or even after seeing the response to one or more X →Y queries.
(a) Work out the details of this more robust deﬁnition, deﬁning corresponding SCI and SCCA
attack games.
(b) Give an example of a signcryption scheme that satisﬁes Deﬁnition 13.8 but does not satisfy
your more robust deﬁnition. To this end, you should start with a scheme that satisﬁes Deﬁ-
nition 13.8, and then “sabotage” the scheme somehow so that it still satisﬁes Deﬁnition 13.8,
580
but no longer satisﬁes your more robust deﬁnition. You may make use of any other standard
cryptographic primitives, as convenient.
13.19 (Signcryption: encrypt-and-sign-then-sign). In this exercise, we develop a varia-
tion on encrypt-then-sign called encrypt-and-sign-then-sign. As does the scheme SCEtS, this new
scheme, denoted SCEaStS, makes use of a public-key encryption scheme with associated data
E= (GENC,E,D ), and a signature scheme S= (GSIG,S,V ). Key generation for SCEaStS is identical
to that in SCEtS. However, SCEaStS makes use of another signature scheme S′= (G′
SIG,S′,V ′). The
encryption algorithm EEaStS(skS,idS,pkR,idR,m) runs as follows:
(pk′,sk′) ←R G′
SIG, c←R E(pkR,m, pk′), σ←R S(skS,pk′),
σ′←R S′(sk′,(c,σ,idS,idR)), output ( pk′,c,σ,σ ′)
The decryption algorithm DEaStS(pkS,idS,skR,idR,(pk′,c,σ,σ ′)) runs as follows:
if V(pkS,pk′,σ) = reject or V′(pk′,(c,σ,idS,idR),σ′) = reject
then output reject
else output D(skR,c, pk′)
Here, the value ephemeral public veriﬁcation key pk′is used as associated data for the encryption
scheme E.
Your task is to show that SCEaStS is a secure signcryption scheme that provides both forward
secrecy and non-repudiation, under the following assumptions:
(i) Eis CCA secure;
(ii) Sis secure (not necessarily strongly secure);
(iii) S′ is strongly secure — in fact, it is suﬃcient to assume that S′ is strongly secure against
an adversary that makes at most one signing query in Attack Game 13.2 (we will see very
eﬃcient signature schemes that achieve this level of security in the next chapter).
Discussion: Note that we have to run the key generation algorithm S′ every time we encrypt,
thereby generating an ephemeral signing key that is only used to sign a single message. The fact
that we only need security against 1-query adversaries means that it is possible to very eﬃciently
implement S′under reasonable assumptions. This is the topic of the next chapter.
Another feature is that in algorithm EEaStS, we can run algorithms E and S in parallel; more-
over, we can even run algorithms G′
SIG and S before algorithm EEaStS is invoked (as discussed in
Section 14.5.1). Similarly, in algorithm DEaStS, we can run algorithms V, V′, and D in parallel.
13.20 (Veriﬁable random functions). A veriﬁable random function (VRF) is a PRF, with
the additional property that anyone can verify that the PRF value at a given point is computed
correctly. Speciﬁcally, a VRF deﬁned over ( X,Y) is a triple of eﬃcient algorithms ( G,F,V ),
where algorithm G outputs a public key pk and a secret key sk. Algorithm F is invoked as
(y,π) ←F(sk,x) where x ∈X , y ∈Y, and where π is called a validity proof. Algorithm V is
invoked as V(pk,x,y,π ), and outputs either accept or reject. We say that yis the value of the VRF
at the point x, and πis the validity proof for y. The VRF must satisfy the following two properties:
581
• Correctness: for all ( pk,sk) output by G, and all x ∈ X, if ( y,π) ← F(sk,x) then
V(pk,x,y,π ) = accept.
• Uniqueness: for all pk and every x∈X, only a single y ∈Y can have a valid proof π. More
precisely, if V(pk,x,y,π ) = V(pk,x,y ′,π′) = accept then y= y′. This ensures that even with
the secret key, an adversary cannot lie about the value of the VRF at the point x.
VRF security is deﬁned using two experiments, analogous to the characterization of a PRF given
in Exercise 4.7. In both experiments, the challenger generates ( pk,sk) using G, and gives pk to the
adversary. The adversary then makes a number of function queries and a single test query (with
any number of function queries before and after the test query). In a function query, the adversary
submits x ∈X and obtains ( y,π) ←F(sk,x). In the test query, the adversary submits ˜ x ∈X :
in one experiment, he obtains ˜y, where (˜y,˜π) ←F(sk,˜x); in the other experiment, he obtains a
random ˜y ∈Y. The test point ˜x is not allowed among the function queries. The VRF is secure if
the adversary cannot distinguish the two experiments.
(a) Show that a secure VRF (G,F,V ) deﬁned over (X,Y) can be constructed from a unique signa-
ture scheme (G,S′,V ′) with message space X(unique signatures were deﬁned in Section 13.3).
Try deﬁning F(sk,x) as follows: compute σ ←S′(sk,x), and then output y := H(σ) as the
value of the VRF at x and π := σ as the validity proof for y. Here H is a hash function
that maps signatures to elements of Y. Explain how the VRF algorithm V works, and prove
security of the construction when H is modeled as a random oracle.
(b) Given a secure VRF ( G,F,V ) deﬁned over ( X,Y), where |Y| is super-poly, show how to
construct a secure signature scheme with message space X.
Discussion: Another VRF scheme is presented in Exercise 20.16. To see why VRFs are useful,
let’s see how they can be used to convince a veriﬁer that a ciphertext in a symmetric cipher is
decrypted correctly. Let ( G,F,V ) be a secure VRF deﬁned over ( X,Y) where Y:= {0,1}n, for
some n. Consider the symmetric cipher (E,D) with message space Ywhere encryption is deﬁned as
E(sk,m) :=
{
r←R X, (y,π) ←F(sk,r), c ←m⊕y, output (r,c)
}
.
D
(
sk, (r,c)
)
is deﬁned analogously. Now, let ( r,c) be a ciphertext and let mbe its alleged decryp-
tion. Using the VRF property, it is easy to convince anyone that m is the correct decryption of
(r,c), without revealing anything else. Simply give the veriﬁer the proof π that m⊕c is the value
of the VRF at the point r.
582
Chapter 14
Fast hash-based signatures
In the previous chapter we presented a number of signature schemes built from a trapdoor permuta-
tion like RSA. In this chapter we return to more basic primitives, and construct signature schemes
from one-way and collision resistant hash functions. The resulting signatures, called hash-based
signatures, can be much faster to generate and verify than RSA signatures. An important feature
of hash-based signatures is that, with suitable parameters, they are secure against an adversary
who has access to a quantum computer. The RSA trapdoor permutation is insecure against such
attacks, as explained in Section 16.5. The post-quantum security of hash-based signatures drives
much of the interest in these schemes. We will therefore use post-quantum security parameters to
evaluate their performance.
The drawback of hash-based signature schemes is that the signatures themselves are much longer
than RSA signatures. As such, they are well suited for applications like signing a software update
where signature size is not important because the data being signed is quite large to begin with.
They are not ideal for signing Web certiﬁcates where short signatures are needed to reduce network
traﬃc.
We begin by constructing hash-based one-time signatures, where a key pair ( pk,sk) can be
used to securely sign a single message. Security can break down completely if ( pk,sk) is used to
sign multiple messages. More generally, we deﬁne a q-time signature, where a key pair ( pk,sk)
can be used to securely sign q messages, for some small q. In our context, q is typically rather
small, say less than a hundred.
Deﬁnition 14.1. We say that a signature system S is a secure q-time signature if for all
eﬃcient signature adversaries Athat issue at most q signature queries, the value SIGadv[A,S]
deﬁned in Attack Game 13.1 is negligible. When q = 1 we say that S is a secure one-time
signature.
We shall ﬁrst construct fast one-time signatures from one-way functions and then describe their
many applications. In particular, we show how to construct a regular (many-time) signature scheme
from a one-time signature. When using one-time signatures, one typically attaches the public-key
to the signature. Therefore, we will usually aim to minimize the combined length of the public-key
and the signature.
As we did in Section 13.1.1, we can deﬁne a stronger notion of security, where it is hard to come
up with a signature on a new message, and it is also hard to come up with a new signature on a
previously signed message.
583
Deﬁnition 14.2. We say that a signature system Sis a strongly secure q-time signature if for
all eﬃcient signature adversaries Athat issue at most q signature queries, the value stSIGadv[A,S]
deﬁned in Attack Game 13.2 is negligible. When q = 1 we say that Sis a strongly secure one-
time signature.
We shall explore this stronger notion in the exercises.
14.1 Basic Lamport signatures
In Section 8.11 we deﬁned the notion of a one-way function. Let f be such a one-way function
deﬁned over (X,Y). We can use f to construct a simple one-time signature for signing a one-bit
message m∈{0,1}. Simply choose two random values x0 and x1 in Xand set
pk :=
(
f(x0), f(x1)
)
; sk := (x0,x1)
Write pk = (y0,y1). To sign a one bit message m ∈{0,1}output the signature S(sk,m) := xm.
Concretely, the signature on the message ‘0’ is x0 and the signature on the message ‘1’ is x1. To
verify a signature σ on m simply check that f(σ) = ym. We call this system S1bit.
If f is a one-way function then an adversary cannot recover x0 or x1 from the public-key pk.
Hence, just given pk, the adversary cannot forge a signature on either one of the two messages
in {0,1}. Similarly, the signature on a message m ∈{0,1}does not help the adversary forge a
signature on the complementary message m⊕1. Therefore, this simple signature scheme is a secure
one-time signature, as summarized in the following theorem.
Theorem 14.1. Let f be a one-way function over (X,Y). Then S1bit is a secure one-time signature
for messages in {0,1}.
Proof. Let Abe a one-time signature adversary that attacks S1bit. The adversary asks for the
signature on a message b ∈ {0,1}and outputs the signature on the message 1 −b. Then by
Lemma 13.5, using t= 2, there exists an algorithm Bfor inverting f that satisﬁes:
SIGadv[A,S1bit] ≤2 ·OWadv[B,f] 2
Basic Lamport signatures. Extending the idea above lets us build a one-time signature for 256-
bit messages, which is suﬃcient for signing an arbitrary long message as discussed in Section 13.2.
More generally, to sign a v-bit message we simply repeat the one-time one-bit signature above
v times. The resulting signature system, called the basic Lamport signature system SL =
(G,S,V ), is deﬁned as follows (see Fig. 14.1):
• Algorithm G outputs a public-key pk ∈Y2v and secret key sk ∈X2v as follows:
choose 2v random values:
( x1,0, ..., x v,0
x1,1, ..., x v,1
)
←R X2v
for i= 1,...,v and j = 0,1 do: yi,j ←f(xi,j)
output:
sk :=
( x1,0, ..., x v,0
x1,1, ..., x v,1
)
∈X2v and pk :=
( y1,0, ..., y v,0
y1,1, ..., y v,1
)
∈Y2v
584
x1,0 x2,0 x3,0 x4,0 x5,0 x6,0 x7,0 x8,0 x9,0
x1,1 x2,1 x3,1 x4,1 x5,1 x6,1 x7,1 x8,1 x9,1
sk : ∈X18
Lamport signature on a message m= 010011100 consists of all shaded squares
Figure 14.1: Lamport signatures: an example
• Algorithm S(sk,m), where m= m1 ...m v ∈{0,1}v, outputs the signature:
σ:= (x1,m1, x2,m2, ..., xv,mv) ∈Yv
• Algorithm V(pk,m,σ ) where m∈{0,1}v and σ= (σ1,...,σ v) ∈Xv outputs
{
accept if f(σi) = yi,mi for all i= 1,...,v
reject otherwise
Signature generation takes no work at all. The signer simply reveals certain values already in its
possession. Verifying a signature takes v evaluations of the function f.
The proof of security for this system follows from Theorem 14.2 below where we prove security
of a more general system. Alternatively, one can view this v-bit system as v independent instances
of the one-bit system discussed in Theorem 14.1. Security of the v-bit system is then an immediate
corollary of multi-key security discussed in Exercise 13.2.
Shrinking the secret-key. Because the secret key is just a sequence of random elements in X,
it can be generated using a secure PRG. The signer keeps the short PRG seed as the secret key and
nothing else. It evaluates the PRG when signing a message and outputs the appropriate elements
as the signature. This shrinks the size of the secret key to a single PRG seed, but at the cost of
slightly increasing the work to sign messages. If ultra fast signing is needed, this optimization can
be ignored.
Shrinking the public-key. The size of the public-key in the basic Lamport scheme is quite
large, but can be made short at the cost of increasing the signature length. We do so using a
generic transformation described in Exercise 14.1 that shows that the public-key in every signature
scheme can be made short.
14.1.1 Shrinking the signature using an enhanced TCR
The length of a Lamport signature is linear in the length v of the message being signed. So far
we assumed v = 256 bits which is the output length of SHA256. We can reduce v using the ideas
developed in Exercise 8.27, where we showed how an enhanced TCR hash function can be used in
place of a collision resistant hash function. This lets us halve the hash length v without hurting
security. Shrinking v this way will approximately halve the size of the Lamport signature.
585
For completeness, we brieﬂy present the resulting signature scheme ( G,S′,V ′), which we call
randomized Lamport. Let Hetcr be an enhanced TCR function deﬁned over ( R, M, {0,1}v).
Here Ris a nonce space and Mis (possibly much larger) message space. Algorithm Gis unchanged
from the basic Lamport scheme SL = (G,S,V ). Algorithm S′and V′work as follows:
• S′(sk,M): given M ∈M as input, do:
r←R R, m ←Hetcr(r,M), σ ′←S(sk,m), output σ:= (r,σ′).
• V′(
pk, M, (r,σ′)
)
: given M ∈M and (r,σ′) as input, do:
m←Hetcr(r,M), output V(pk,m,σ ′).
The same argument as in Exercise 8.27 shows that this construction is secure as long as the
basic Lamport signature scheme is secure and Hetcr is an enhanced TCR. Moreover, suppose we
want the adversary to make at least 2128 evaluations of Hetcr to win the enhanced TCR game with
advantage 1/2. Then part (b) of Exercise 8.27 shows that it suﬃces to take v = 130 instead of
v= 256. This approximately halves the size of the Lamport signature. The signature includes the
random nonce r, but this nonce can be short, only about the size of a single element in X.
Post-quantum security. In Section 4.3.4 we discussed quantum exhaustive search attacks.
These attacks show that a quantum adversary can win the enhanced TCR game for a v-bit hash
function in time 2 v/2. Therefore, for post-quantum security we must use v= 256 even when using
an enhanced TCR. For this reason, we will evaluate all the schemes in this chapter using v= 256.
Of course, if one is only concerned with classical adversaries then v= 130 is suﬃcient.
14.2 A general Lamport framework
Our description of the basic Lamport signature, while simple, is not optimal. We can further shrink
the signature size by quite a lot. To do so, we ﬁrst develop a general framework for Lamport-like
signatures. This framework reduces the security of Lamport signatures to an elegant combinatorial
property that will let us build better one-time and q-time signatures.
As in the previous section, let f be a one-way function over ( X,Y). We wish to sign messages
in M:= {0,1}v for some ﬁxed v. As usual, this lets us sign arbitrary length messages by ﬁrst
hashing the given message using a collision resistant function or an enhanced TCR. The general
Lamport framework works as follows:
• A secret key is nrandom values x1,...,x n ∈X for some nthat will be determined later. The
public-key consists of the n hashes yi := f(xi) for i= 1,...,n .
• To sign a message m∈M we use a special function P that maps mto a subset of {1,...,n }.
We will see examples of such P in just a minute. To sign mwe ﬁrst compute P(m) to obtain
a subset s←P(m) ⊆{1,...,n }. The signature is just the subset of preimages σ:= {xi}i∈s.
• To verify a signature σ on a message m the veriﬁer checks that σ contains the pre-images of
all public-key values {yi}i∈P(m).
586
As in the basic Lamport scheme, the signer need not store a large secret key sk := (x1,...,x n).
Instead, he keeps a single PRF key sk = (k) and generates the xi as xi ←F(k,i). All we need
is a secure PRF deﬁned over ( K, {1,...,n }, X). In more detail, the generalized Lamport system
SP = (G,S,V ) works as follows:
Algorithm G():
k←R K
for i= 1,...,n :
xi ←F(k,i) ∈X
yi ←f(xi) ∈Y
output:
pk = (y1,...,y n)
sk = (k)
Algorithm S(sk,m):
s←P(m) ⊆{1,...,n }
let s:= {s1,...,s ℓ}
for j = 1,...,ℓ :
σj ←F(k,sj)
output:
σ←(σ1,...,σ ℓ)
Algorithm V(pk,m,σ ):
let P(m) = {s1,...,s ℓ}
let σ:= (σ1,...,σ u)
if ℓ= u and f(σi) = ysi
for all i= 1,...,ℓ
then output accept
otherwise output reject
Now that we understand the general framework, the question is how to choose the function P.
Speciﬁcally, for what functions P is this a secure one-time signature scheme? The adversary sees
the signature on a single message m ∈M of his choice, and wants to forge a signature on some
other message m′∈M. Clearly, if the set P(m′) is contained in the set P(m) then the signature
on m also gives a signature on m′. Hence, for security we must insist that it be diﬃcult for the
adversary to ﬁnd distinct messages m and m′ such that P(m) contains P(m′). For now we focus
on functions where such containment is not possible, no matter how powerful the adversary is.
Deﬁnition 14.3. We say that a function P from Mto subsets of {1,...,n }is containment free
if for all distinct messages m,m′∈M the set P(m) is not contained in the set P(m′).
Containment free functions are easy to build: take P to be an injective function that always
outputs subsets of a ﬁxed size ℓ. Clearly a subset of size ℓ cannot contain another subset of size ℓ
and hence such a P is containment free. The basic Lamport system SL of Section 14.1 is a special
case of this general framework. It uses n = 2 v and a containment free function P that always
outputs subsets of size v.
The following theorem shows that every containment free P gives a secure one-time signature
system. Security of the basic Lamport signature system follows as a special case.
Theorem 14.2. Suppose f is a one-way hash over (X,Y) and F is a secure PRF deﬁned over
(K, {1,...,n }, X). Let P be a containment free function fromMto subsets of {1,...,n }. Then SP
is a secure one-time signature for messages in M.
In particular, suppose Ais a signature adversary attacking SP that issues at most one signature
query. Then there exist an eﬃcient adversary Bf attacking the one-wayness of f, and a PRF
adversary BF, where Bf and BF are elementary wrappers around A, such that
SIGadv[A,SP] ≤n·OWadv[Bf,f] + PRFadv[BF,F] (14.1)
Proof idea. The proof shows that Acan be used to solve the repeated one-way problem for f as
deﬁned in Section 13.4.1. We construct an adversary Bthat uses Ato win the repeated one-way
587
game. We then use Lemma 13.5 with t = n to convert Binto an algorithm for breaking the
one-wayness of f. This is the source of the factor of n in (14.1).
The repeated one-way game starts with the repeated one-way challenger C giving Ba list of n
elements y1,...,y n ∈Y. Bneeds to invert one of them. It runs Aand does the following:
• Bsends ( y1,...,y n) as the public-key to A. Since F is a secure PRF, this public-key is
indistinguishable from a public-key generated by G().
• Arequests the signature on some message m1. Our Brequests from C the preimages of all
the yi where i∈P(m1), and sends these pre-images as the signature to A.
• Finally, Aoutputs a forgery σ for some message m ̸= m1. Since P is containment free we
know that P(m) \P(m1) is not empty and hence there exists some j in P(m) \P(m1). If σ is
a valid signature on m then σ contains a pre-image xj ∈X of yj ∈Y. Our Boutputs (j,xj)
as its solution to the repeated one-way problem.
Since j ̸∈P(m1) we know that Bnever requested a pre-image for yj. Hence (j,xj) is a valid solution
to the repeated one-way problem. The theorem now follows from Lemma 13.5. 2
q-time signatures. The general containment free framework presented here directly extends to
give q-time signatures for small q. The only diﬀerence is that the function P must satisfy a stronger
property called q-containment freeness. We explore this extension in Exercise 14.5.
14.2.1 An explicit containment free function
When using a one-time signature we often aim to minimize the total combined length of the public-
key and the signature. In the general Lamport framework, this amounts to setting n to be the
smallest value for which there is an eﬃciently computable function from M:= {0,1}v to subsets
of {1,...,n }that is containment free. One can show (using Sperner’s theorem) that the smallest
possible n is about nmin := v+ (log2 v)/2. Recall that the basic Lamport system uses n = 2v,
which is almost twice as big as this lower bound.
We present an eﬃcient containment free function Popt that uses n:= v+ 1 +⌈log2 v⌉, which is
close to the optimal value of n. For simplicity, let us assume that v is a power of 2. Recall that the
weight of a bit string m∈{0,1}v is the number of bits in m that are set to 1. The function Popt
is deﬁned as follows:
input: m∈{0,1}v
output: Popt(m) ⊆{1,...,n }
Popt(m) := c←v−weight(m) / / c∈[0,v] is the number of 0s in m
encode c as a binary string in {0,1}(log2 v)+1
m′←m∥c ∈{0,1}n / / c is called a checksum
output the set {i s.t. m′
i = 1}⊆{ 1,...,n } / / here m′= m′
1 ...m ′
n
Fig. 14.2 gives an example. The function is clearly injective: if Popt(m0) = Popt(m1) then m0 = m1.
The following lemma shows that it is also containment free.
Lemma 14.3. For every distinct m0,m1 ∈{0,1}v we have that Popt(m0) ̸⊆Popt(m1).
588
x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12sk :
For m= 01001100 we have checksum = 0101. The signature consists of all shaded squares.
Figure 14.2: Optimized Lamport signatures: an example
Proof. Let m0,m1 be distinct messages and let c0,c1 be the checksums for m0,m1 respectively
as deﬁned in algorithm Popt. Suppose Popt(m0) ⊆Popt(m1). Then clearly m0 contains fewer 1
bits than m1 implying that c0 > c1. But if c0 > c1 then there must exist some bit in the binary
representations of c0 and c1 that is 0 in c1 but 1 in c0. This bit implies that Popt(m0) ̸⊆Popt(m1)
as required. 2
Fig. 14.2 shows the resulting optimized Lamport system in action. Since Popt is containment
free, Theorem 14.2 shows that the resulting signature system is a secure one-time signature system.
Concrete parameters. The public-key length is |pk| = v + 1 + log2 v elements in Y. The
expected length of a signature for a random messages m∈{0,1}v is about |pk|/2 ≈v/2 elements
in X. Thus, in the optimized Lamport system, both the public-key and the signature are about
half the length of those in the basic Lamport system SL. Using Exercise 14.1, the total combined
size of the public-key and the signature is v elements in X∪Y plus one hash.
Concretely, for post-quantum security one typically takesX= Y= {0,1}256 and v= 256. With
these parameters, the combined size of the public-key and the signature is about 8.5 KB. In the
next two sections we show how to greatly reduce this size.
14.3 Winternitz one-time signatures
We next present a beautiful generalization of the Lamport framework that dramatically shrinks
the signature and public-key size. But there is no free lunch. This improvement comes at the cost
of more work to generate and verify signatures. We begin by deﬁning the notion of a hash chain.
Hash chains. Let f : X→X be a function. For a non-negative integer j we let f(j)(x) denote
the jth iterate of f, namely f(j)(x) := f(f(f(···(x) ···))) where f is repeated j times. For
example,
f(0)(x) := x ; f(1)(x) := f(x) ; f(2)(x) := f(f(x)) ; f(3)(x) := f(f(f(x)))
and so on. For x ∈X the sequence f(0)(x),f(1)(x),...,f (d)(x) is called a hash chain of length
d+ 1. The value x is called the base of the chain and y:= f(d)(x) is called its top.
The Winternitz scheme. We wish to sign messages in M:= {0,1}v for some ﬁxed v using a
one-way function f deﬁned over (X,X). The scheme operates as follows (see also Fig. 14.3):
589
x1
f
f
y1
f
x2
f
f
y2
f
x3
f
f
y3
f
x4
f
f
y4
f
x5
f
f
y5
f
x6
f
f
y6
f
x7
f
f
y7
f
x8
f
f
y8
f
x9
f
f
y9
f
sk →
H
−→ pky4
x7
The secret key sk is used to derive x1,...,x 9 ∈X. The public key pk is the hash of y1,...,y 9 ∈X.
The shaded circles represent the signature on a message m where P(m) = (2,1,2,3,2,1,0,2,1).
This P(m) describes a cut through the rectangle illustrated by the thin line.
Figure 14.3: Winternitz signatures with n= 9 and d= 3
• Fix parameters n and d that will be determined later. A secret key is n random values
x1,...,x n ←R X. The public-key consists of the n hashes yi := f(d)(xi) for i= 1,...,n .
As before, we can compress the secret key by generating ( x1,...,x n) using a PRG deﬁned
over (S,Xn). Then the actual secret key is just a short seed in S. Similarly, we can compress
the public-key by only publishing a short collision resistant hash of the vector ( y1,...,y n), as
shown in Fig. 14.3.
• To sign a message m∈M we use a special function P that maps mto a vector sof length n.
Every component of sis a number in {0,...,d }. More precisely, let In
d := ({0,...,d })n. Then
P is a function P : M→ In
d.
To sign m we ﬁrst compute s←P(m). Let s= (s1,...,s n) ∈In
d. Then the signature is the
vector σ :=
(
f(s1)(x1),...,f (sn)(xn)
)
∈Xn, as illustrated by the shaded circles in Fig. 14.3.
The signature corresponds to a cut through the rectangle in the ﬁgure, represented by the
thin line through the shaded circles.
• To verify a signature σ = ( σ1,...,σ n) ∈ Xn on a message m ﬁrst compute P(m) =
(s1,...,s n) ∈In
d. Next, compute the vector
ˆy:=
(
f(d−s1)(σ1),...,f (d−sn)(σn)
)
∈Xn.
The signature σ is valid only if ˆy is equal to the public-key vector ( y1,...,y n).
In more detail, the Winternitz scheme Swin, parameterized by n and d, works as follows. Here
we use a PRG Gprg deﬁned over (S,Xn) and a collision resistant hash function H : Xn →T .
590
Algorithm G():
k←R S
(x1,...,x n) ←Gprg(k)
for i= 1,...,n :
yi ←f(d)(xi)
output:
pk := H(y1,...,y n)
sk := (k)
Algorithm S(sk,m):
(x1,...,x n) ←Gprg(k)
s←P(m) ∈In
d
let s= (s1,...,s n)
for i= 1,...,n :
σi ←f(si)(xi)
output:
σ←(σ1,...,σ n)
Algorithm V(pk,m,σ ):
let P(m) = (s1,...,s n)
let σ= (σ1,...,σ n)
for i= 1,...,n :
ˆyi ←f(d−si)(σi)
ˆy←(ˆy1,..., ˆyn)
if H(ˆy) = pk output accept
otherwise output reject
This scheme is a generalization of the general Lamport framework. Speciﬁcally, when d= 1 the
scheme is equivalent to the Lamport framework.
Security. For what functionsP is this system a secure one-time signature? Suppose the adversary
ﬁnds two messages m,m′∈M such that every entry in the vector P(m′) is greater than or equal
to the corresponding entry in P(m). We say that P(m′) dominates P(m). Any such pair can be
used to forge signatures: given a signature on mthe adversary can compute everything needed for
a signature on m′. For example, the signature on the message min Fig. 14.3 can be used to derive
a signature on a message m′where P(m′) = (2,2,2,3,2,2,2,2,2).
Hence, at a minimum we must insist that it be diﬃcult for the adversary to ﬁnd distinct
messages m and m′such that P(m′) dominates P(m). This motivates the following deﬁnition:
Deﬁnition 14.4. Let s,s′ be vectors in Id
n. We say that s′ dominates s if s′
i ≥ si for all
i= 1,...,n . We say that a function P : M→ Id
n is domination free if for all distinct messages
m,m′∈M the vector P(m′) does not dominate P(m).
Visually, P is domination free if for every pair of messages m,m′in M, the cuts corresponding
to P(m) and P(m′) cross at at least one point. We will construct such a function P after we prove
security of the signature scheme.
The security analysis of Winternitz requires that f : X →Xbe a strong one-way function in
the following sense: we say that f is one-way on d iterates if for all j = 1,...,d , it is hard to
ﬁnd an f-inverse of f(j)(x), where x←R X. We capture this property in the following game:
Attack Game 14.1 (One-way on d iterates). For a given function f : X →Xand a given
adversary A, the attack game runs as follows:
• The adversary chooses j ∈{1,...,d }and sends j to the challenger.
• The challenger computes x←R Xand y←f(j)(x), and sends y to A.
• The adversary outputs x′∈X.
We say Awins the game if f(x′) = y. We deﬁne the adversary’s advantage iOW adv[A,f,d ] to be
the probability that it wins. 2
Deﬁnition 14.5. For an integer d >0, we say that f : X →X is one-way on d iterates if
iOWadv[A,f,d ] is negligible for all eﬃcient adversaries A.
591
Exercise 14.16 shows that a one-way function f need not be one-way on d iterates, even when
d= 2. Nevertheless, standard cryptographic functions such as SHA256 are believed to be one-way
on d iterates for reasonable values of d, say d≤106. This strong one-way property holds if f is a
random oracle and |X|is large, as discussed in Exercise 14.16.
Armed with the deﬁnition of one-way on iterates and domination free functions, we can now
state the security of Winternitz signatures.
Theorem 14.4. Let f be a one-way function on diterates deﬁned over (X,X). Let Gprg be a secure
PRG over (S,Xn), let H be collision resistant over (Xn,T), and let P : M→ Id
n be domination
free. Then the Winternitz scheme Swin is a secure one-time signature for messages in M.
In particular, suppose Ais a signature adversary attacking Swin that issues at most one signature
query. Then there exist eﬃcient adversaries Bf,BG,BH, where all three are elementary wrappers
around A, such that
SIGadv[A,Swin] ≤nd·iOWadv[Bf,f,d ] + PRGadv[BG,Gprg] + CRadv[BH,H] (14.2)
Proof idea. The proof proceeds along the same lines as the proof of Theorem 14.2. The main
diﬀerence is that we need to generalize Lemma 13.5 so that it applies to iterates of a one-way
function. We explore this generalization in Exercise 14.15. The bound in Exercise 14.15 is the
source of the factor nd in (14.2). The rest of the proof is essentially as in Theorem 14.2. 2
14.3.1 A domination free function for Winternitz signatures
It remains to provide a domination free function P : M→ Id
n for parameters n and d. When
|M|= 2v we describe a construction that satisﬁes
n≈v/log2(d+ 1). (14.3)
Taking, for example, d= 15 gives n≈(v/4) + 1. Since a Winternitz signature contains nelements
in X, this leads to a fourfold reduction in combined signature and public-key length compared to
the optimized Lamport signature (Section 14.2.1). That signature corresponds to setting d= 1.
To be fair, this reduction in length comes at the expense of veriﬁcation time. When d = 15
signature veriﬁcation requires 8 n evaluations of the one-way function on average. Note that 8 n
is approximately 2 v. In comparison, optimized Lamport requires only about v/2 evaluations on
average. Hence, veriﬁcation is about four times slower.
Concretely, when v = 256 and X = {0,1}256, the function P described below provides the
following combined signature and public-key size for diﬀerent values of d:
d: 1 3 15 1023
minimum n: 265 133 67 28
combined size (KB): 8.5 4.2 2.1 0.9
A domination free function P. We describe the function P : {0,1}v →In0
d as a generalization
of the containment free function Popt from Section 14.2.1. Fix d and let n0 be the smallest integer
such that 2v ≤(d+ 1)n0. If we treat the message m∈{0,1}v as an integer in [0 ,2v), we can write
m in base ( d+ 1) and obtain a vector of digits ( s1,...,s n0) in In0
d (possibly with leading zeros).
When d+1 is a power of two this is done by simply partitioning m∈{0,1}v into consecutive blocks
of log2(d+ 1) bits.
592
Next, set n1 := ⌈logd+1(dn0)⌉+ 1 and n := n0 + n1. One can verify that indeed n is about
v/log2(d+ 1) as promised in (14.3). Now, using d,n0,n1, the function P works as follows:
input: m∈{0,1}v
output: ( s1,...,s n) ∈Id
n
P(m) := write m as an n0-digit number in base ( d+ 1): (s1,...,s n0) ∈In0
d
c←dn0 −(s1 + ··· + sn0) / / c∈[0,dn0] is called a checksum
write c as an n1-digit number in base ( d+ 1): c= (c1,...,c n1) ∈In1
d
m′←( s1,...,s n0, c1,...,c n1 ) ∈In
d
output m′
When d= 1 this function is equivalent to the function Popt. The following lemma shows that it is
domination free. This completes our description of Winternitz signatures.
Lemma 14.5. For every distinct m0,m1 ∈{0,1}v we have that P(m0) does not dominate P(m1).
Proof. Let m0,m1 be distinct messages and let c0,c1 be the checksums for m0,m1 respectively,
as deﬁned in algorithm P. Because P is injective, P(m0) ̸= P(m1). Suppose P(m1) dominates
P(m0). Then clearly c1 < c0. But if c1 < c0 there must exist some digit in their ( d+ 1)-ary
representations that is smaller in c1 than in c0. This digit implies that P(m1) does not dominate
P(m0), as required. 2
14.4 HORS: short Lamport signatures
Our ﬁnal Lamport variation shows how to shrink the signature without increasing veriﬁcation time.
This expands the public-key, but we then show how to shrink the public-key using a Merkle tree
(Section 8.9).
Let Sets[n,ℓ] denote the set of all subsets of {1,...,n }of size ℓ. This set contains
(n
ℓ
)
elements.
Suppose we had an injective and eﬃciently computable functionPhors : {0,1}v →Sets[n,ℓ] for some
parameters n and ℓ. Such a function is containment free, and can therefore be used in the general
Lamport one-time signature framework (Section 14.2) to sign messages in {0,1}v. The resulting
signature scheme is called hash to obtain a random subset or simply HORS.
We show in Exercise 14.3 how to construct the function Phors for every choice of suﬃciently
large parameters n and ℓ. Exercise 14.4 gives another approach.
Concrete parameters. Because the function Phors is injective, it must be the case that its range
is at least as large as its domain. In other words, we must choose the parameters n and ℓ so that(n
ℓ
)
≥2v. When v= 256 some viable options for n and ℓ that satisfy
(n
ℓ
)
≥2v are as follows:
pk size: n 512 1024 2048 8192
min signature size: ℓ 58 44 36 27
In particular, when the public-key contains n= 1024 elements of Y, the signature need only contain
ℓ = 44 elements of X. This is far shorter than the optimized Lamport signature (Section 14.2.1)
and veriﬁcation time is much faster than with Winternitz signatures of comparable size. This comes
at the expense of a large public-key, which we address next.
593
y1
x1
y2
x2
y3
x3
y4
x4
y5
x5
y6
x6
y7
x7
y8
x8
y9
x9
y10
x10
y11
x11
y12
x12
y13
x13
y14
x14
y15
x15
y16
x16
y17 y18 y19 y20 y21 y22 y23 y24
y25 y26 y27 y28
y29 y30
y31
sk →
pk
The HORST system with n= 16 and ℓ= 4. When Phors(M) = {3,7,8,11}the signature is the set
of shaded nodes. The secret key sk is a short PRF key from which x1,...,x 16 ∈X are derived.
Figure 14.4: HORST signature: an example
14.4.1 Shrinking the public-key using a Merkle tree
For many applications of one-time signatures one wants to minimize the total combined size of the
public-key and the signature. As speciﬁed above, the public-key consists of n elements in Yand
the signature consists of ℓ elements in X. This can be reduced signiﬁcantly using the Merkle Tree
technique of Section 8.9. Let H be a hash function from ˆY2 to ˆYand let us assume that Yis a
subset of ˆY. At key generation time, algorithm G places all the y1,...,y n ∈Y at the leaves of a
Merkle tree and sets the public-key pk to be the root of the Merkle tree after iteratively hashing
the leaves using H. The public-key pk is then a single element in ˆY. Signatures produced by this
method include the t pre-image values in Xplus proofs that the corresponding y values are in the
Merkle tree.
This signature scheme is called HORS tree, or simply HORST. An example of the system in
action is shown in Fig. 14.4.
In Section 8.9 we showed that ℓproofs in a Merkle tree with nleaves require at most ℓlog2(n/ℓ)
tree nodes. Hence, the total combined length of the signature and public-key is ℓ elements in X
and 1 + ℓlog2(n/ℓ) elements in ˆY. Since ℓlog2(n/ℓ) is often smaller than n this Merkle technique
results in signiﬁcant savings over the HORS method.
Concretely, the combined public-key and signature size is only a small improvement over the
Lamport scheme (Section 14.2.1). The improvement becomes substantial when we consider q-time
signatures for small q. Exercise 14.6 shows how HORST gives an eﬃcient q-time signature scheme.
594
14.5 Applications of one-time signatures
One-time signatures constructed from one-way functions can be much faster than RSA signatures.
Their speed makes them useful for several applications. We give two examples here.
14.5.1 Online/oﬄine signatures from one-time signatures
Let us see how to speed up signature generation in all (many-time) signature schemes. The idea is
to split up the signing algorithm S in to two phases. The bulk of the signing work is done before
the message to be signed is known. We call this the oﬄine phase. Then, once a message m is
given we quickly output a signature on m. We call this the online phase. Our goal is minimize
the work in the online phase.
Using one-time signatures, we can easily modify any signature system so that the online work
is fast. The idea is as follows: in the oﬄine phase we generate an ephemeral key pair ( pk1,sk1)
for the one-time signature system and sign pk1 using the long-term signing key. Then, when the
message mis given, we quickly sign musing the one-time signature. Thus, the online signing work
is just the time to sign m using a one-time system.
More precisely, let S∞ = ( G∞,S∞,V∞) be a long-term signature system such as SRSA-FDH.
Let S1 = (G1,S1,V1) be a fast one-time signature system. Deﬁne a hybrid signature system S=
(G,S,V ) as follows:
• G runs G∞ to obtain a key pair ( pk∞,sk∞).
• S(sk∞,m) works as follows:
1. ( pk1,sk1) ←R G1() / / Generate a one-time key pair
2. σ0 ←R S∞(sk∞, pk1) / / Sign the one-time public-key
3. σ1 ←R S1(sk1, m) / / Once m is known, sign m using the one-time system
4. output σ:= (pk1,σ0,σ1)
• V(pk∞,m,σ ) parses σ as σ:= (pk1,σ0,σ1) and outputs accept only if:
V∞(pk∞, pk1,σ0) = accept and V1(pk1, m,σ1) = accept
The bulk of the signing work, Steps 1 and 2, takes place before the message m is known. Step 3
used to sign m is as fast as generating a one-time signature.
A real-world application for online/oﬄine signatures comes up in the context of web authenti-
cation at a large web site. Users who want to login to the site are ﬁrst redirected to a login server.
The login server asks for a username/password and then, after successful authentication, signs a
special token that is sent to the user’s web browser. This signed token then gives the user access
to systems at the web site (perhaps only for a bounded amount of time).
At a large site the login server must sign hundreds of millions of tokens per day. But demand
for these signed tokens is not uniform. It peaks at some hours of the day and ebbs at other times.
During low usage times the login server can spend the time to generate many pairs ( pk1,σ0). Then
at peak times, the server can use these pairs to quickly sign actual tokens. Overall, the online/oﬄine
mechanism allows the login server to balance out demand for computing cycles throughout the day.
595
14.5.2 Authenticating streamed data with one-time signatures
Consider a radio transmission streamed over the Internet. The signal is sent as a stream of packets
over the network. The radio station wants to authenticate the stream so that each recipient can
verify that the transmission is from the station. This prevents intermediaries from messing with
the broadcast, for example, replacing the station’s ads with their own ads.
Recipients want to play the packets as they are received. One option is for the radio station
to sign every packet using its long term signing key, but this will be quite slow. Can we can do
better? Again, we can speed things up using one-time signatures. The idea is to amortize the cost
of a single expensive RSA signature over many packets.
Let S∞= (G∞,S∞,V∞) be a long-term signature system such asSRSA-FDH. Let S1 = (G1,S1,V1)
be a fast one-time signature system. The radio station already has a long term key pair ( pk0,sk0)
generated using G∞. It generates a chain of one-time key pairs {(pki,ski)}for i= 1,...,ℓ . Then
key ski will be used to authenticate both pki+1 and packet number i. More precisely, the station
does the following:
input: ( pk0,sk0) and packets m0,m1,...
(pk1,sk1) ←R G1()
σ0 ←R S∞(sk, (m0,pk1)) / / sign the ﬁrst one-time key using the long-term key
send (m0,pk1,σ0)
For i= 1,2,... do:
(pki+1,ski+1) ←R G1()
σi ←R S1(ski, (mi,pki+1) ) / / sign key pk i+1 using sk i
Send (mi,pki+1,σi)
The recipient veriﬁes this stream by using the public-key in packet ito verify packet i+ 1, starting
with the ﬁrst packet. Overall, the station signs the ﬁrst one-time key using the slow long-term
signature and signs the remaining keys using a fast one-time signature. Thus, the cost of the slow
signatures is amortizes across many packets. Note also that no buﬀering of packets at either the
sender or the receiver is needed.
Of course, this approach adds additional network traﬃc to send the sequence of public keys to
the recipients. It should only be used in settings where the additional traﬃc is cheaper than signing
every packet with the long-term key.
14.6 From one-time signatures to many-time signatures
We now turn to constructing a many-time signature scheme from a one-time signature. This
will show that a many-time signature scheme can be built with nothing more than one-way and
collision resistant functions. The resulting scheme is post-quantum secure. Here we focus on
building stateless signatures. That is, the signer does not maintain any internal state between
invocations of the signing algorithm. Stateless signatures are much easier to use than stateful ones,
especially in a distributed environment where many machines issue signatures using the same secret
key.
596
14.6.1 Indexed signatures
We will need a simple variation of q-time signatures. A q-indexed signature is a q-time signature
scheme S= (S,G,V ) where the message space is M′:= {1,...,q }×M. We also require that the
signing algorithm S be deterministic. We show in Exercise 13.6 that the signing algorithm of any
signature scheme can be easily de-randomized using a secure PRF, so this does not limit the choice
of signature scheme.
Security of a q-indexed signature is deﬁned using the standard signature attack game (Attack
Game 13.1) with one restriction — the adversary can issue up to q signature queries for messages
(ui,mi), but u1,...,u q must all be distinct. In other words, once the adversary issues a signature
query for ( u,m) no other signature query can use the same u. As usual, the adversary wins this
game if it is able to produce an existential forgery for S, namely a valid message-signature pair
((u,m), σ) for some new message ( u,m). We let iSIG adv[A,S] denote A’s advantage in winning
this game.
Deﬁnition 14.6. A q-indexed signature system is a signature system S= (G,S,V ) where the
message space is M′= {1,...,q }×M and the signing algorithm S is deterministic. We say that
Sis a secure q-indexed signature if for all eﬃcient q-query signature adversaries A, the quantity
iSIGadv[A,S] is negligible.
Any one-time signature gives a q-indexed signature. Let S1 = (G1,S1,V1) be a one-time signa-
ture. The derived q-indexed signature S= (G,S,V ) works by generating qone-time public/private
key pairs and signing a message ( u,m) using key number u. More precisely, algorithms ( G,S,V )
work as follows:
Algorithm G():
For i= 1,...,q :
(pki,ski) ←R G1()
Output:
pk = (pk1,..., pkq)
sk = (sk1,..., skq)
S
(
sk, (u,m)
):= S1(sku, m)
V
(
sk, (u,m),σ
):= V1(pku, m, σ) (14.4)
Security of this construction follows immediately from the security of the underlying one-time
signature. The proof of security uses the same “plug-and-pray” argument as in Exercise 13.2.
Shrinking the public-key. The size of the public-key in the brute-force construction (14.4) is
linear in q. This can be greatly reduced using the Merkle tree approach we used in Fig. 14.4 to
shrink a HORS public-key. Place the q one-time public keys at the leaves of a Merkle tree and
compute the corresponding hash at the root. This single hash value at the root is the public key
for the q-indexed scheme. Exercise 14.19 shows how to eﬃciently compute it.
A signature on a message (u,m) contains the Merkle proof needed to authenticate the one-time
public key pku, along with the one-time signature on m using sku. Signature size is then
T + t·log2 q (14.5)
597
pk′ pk′′
pk0
m1 m2 m3 m4
Figure 14.5: Using a 2-indexed signature to sign four messages
where T is the combined length of a single one-time signature and a single one-time public-key,
and t is the output size of the hash function H used in the Merkle tree.
14.6.2 A many-time signature scheme from an indexed signature
Let S= (Gq,Sq,Vq) be a q-indexed signature. We build a many-time signature system SMerkle =
(G,S,V ). The system uses an implicit q-ary tree of depth d. Internal tree nodes contain public-keys
generated by Gq(). Messages to be signed are placed at the leaves of this tree. Each leaf is used to
sign at most one message enabling us, in principal, to sign up to qd messages.
Let (pk0,sk0) ←R Gq(). To keep things simple for now, let us assume q= 2 so that the key sk0 is
only good for signing two messages. We let pk0 be the public-key. Fig. 14.5 shows how to amplify
this system to sign four messages. First we generate two more key pairs ( pk′,sk′), (pk′′,sk′′) and
sign pk′and pk′′with sk0:
σ′←Sq
(
sk0, (1,pk′)
)
and σ′′←Sq
(
sk0, (2,pk′′)
)
The pairs ( pk′,σ′) and ( pk′′,σ′′) prove that pk′ and pk′′ were certiﬁed by sk0. Now, sk′ and sk′′
can each sign two messages giving a total of four messages that can be signed. For example, the
signature on m2 is: (
(pk′,σ′), Sq
(
sk′, (2,m2)
))
To verify the signature, ﬁrst check that pk′ is properly signed with respect to the public-key pk0.
Second, check that m2 is properly signed with respect to pk′. If both sub-signatures verify then
the signature is said to be valid.
We can repeat this process to obtain greater ampliﬁcation — pk′ and pk′′ can each sign two
new public-keys to obtain a total of four certiﬁed public-keys. Each of these in turn can sign two
messages, thus enabling us to sign a total of eight messages. By repeating this process d times we
increase the number of messages that can be signed to 2 d.
Fig. 14.6 illustrates this idea (for q = 2) using a tree of depth d = 3. The public-key pk0 is
generated by Gq() and lives at the root of the tree. The secret key is sk0. To sign a message mdo:
1. First, pick a random leaf. In Fig. 14.6 we use leaf (2 ,1,1) — namely we go right from the
root and then left twice.
2. Next, generate two public/private key pairs ( pk1,sk1) and ( pk2,sk2) using Gq() for internal
nodes on the path. Every node on the path signs its child and the location of the child. The
last node pk2 signs the message, as shown on the right of Fig. 14.6.
598
m
pk2
pk1
pk0
σ1 ←S(sk0, (2,pk1) )
σ2 ←S(sk1, (1,pk2) )
σ3 ←S(sk2, (1,m) )
Figure 14.6: Merkle signatures with a tree of depth d= 3
The ﬁnal signature is
(
(2,1,1), (pk1,σ1), (pk2,σ2), σ3
)
which includes the intermediate public-
keys and signatures as well as the location of the leaf (2,1,1). To verify this signature simply check
that all sub-signatures in this tuple are valid.
The key management problem. For this system to be secure it is essential that once the signer
generates a public/private key pair for an internal node, that same key pair is used for all future
signatures — we cannot ever generate a new key pair for that internal node. To see why, consider
an internal node just below the root. Suppose that when signing message m the signer generates
a key pair ( pk1,sk1) for the left child and then signs (1 ,pk1) with sk0, as required. Later, when
signing message m′̸= mthe signer generates a new pair ( pk′
1,sk′
1) for that node and signs (1 ,pk′
1)
with sk0. An observer in this case sees signatures for both (1 ,pk1) and (1 ,pk′
1) under sk0, which
can completely compromise security of the underlying q-indexed signature. In fact, when building a
2-indexed signature from Lamport one-time signatures, such usage will result in an insecure system.
Hence, key pairs in this tree, once generated, must be kept forever.
For exactly the same reason, every leaf node can only be used to sign a single message — using
a leaf to sign two distinct messages would completely compromise security of the q-indexed private
key at the parent of that leaf.
To make the signature stateless, we make the signer pick a random leaf for every message and
hope that he never picks the same leaf twice. For this to work, the number of leaf nodes must be
large, say 2 160, so that the probability of a collision after issuing many signatures is small. But
then the number of internal nodes is large and we cannot possibly store all internal key pairs in the
tree. Again, since the signature is stateless we cannot generate internal key pairs “on the ﬂy” and
then store them for future invocations of the signing algorithm. Fortunately, this key management
problem has a simple and elegant solution.
Generating internal keys using a PRF. To address the key management problem raised
in the previous paragraph, our plan to is to generate all internal key pairs using a secure PRF.
Consider a q-ary tree of depth d. Every node in the tree is identiﬁed by the path from the root
to that node. That is, a node v at depth e is identiﬁed by a vector ( a1,...,a e) ∈{1,...,q }e.
This vector indicates that v is child number ae of its parent, the parent is child number ae−1 of its
parent, and so on all the way to the root. We refer to ( a1,...,a e) as the ID of node v.
599
Let F be a PRF that maps node IDs in {1,...,q }≤d to bit strings in {0,1}w for some w.
The output of F will be used as the random bits given to algorithm Gq. Therefore, we need w
to be greater than the maximum number of random bits consumed by Gq. We will write Gq(r),
where r ∈{0,1}w, to denote the output of Gq using random bits r. Clearly once the bits r are
speciﬁed, algorithm Gq(r) is deterministic. The PRF F assigns a public/private key pair to every
internal node in the q-ary tree. The key pair at node ⃗ a:= (a1,...,a e) ∈{1,...,q }≤d is simply
(pk⃗ a,sk⃗ a) := Gq(F(k,⃗ a)) where k is the PRF secret key and 1 ≤e≤d.
Recall that we required the q-indexed signing algorithm to be deterministic. Hence, signing
the same message twice with the same private key always results in the same signature. This
is needed so that every time we sign an internal node, the resulting signature is identical to the
one obtained during prior invocations of the signing algorithm. If this were not the case, then an
observer who sees multiple Merkle signatures would obtain more than q distinct signatures for a
particular internal public-key.
14.6.3 The complete Merkle stateless signature system
Let (Gq,Sq,Vq) be a q-indexed signature and letF be a PRF deﬁned over (K, {1,...,q }≤d, {0,1}w).
We use F to assign key pairs to internal tree nodes as discussed in the preceding paragraph. The
Merkle signature SMerkle = (G,S,V ) system works as follows. To generate ( pk,sk) algorithm G
does:
Algorithm G() : k←R K / / Pick a random PRF key
(pk0,sk0) ←R Gq()
output sk ←(k,sk0) and pk ←pk0
The signature generation and signature veriﬁcation algorithms are described in Fig. 14.7.
Security. Next we turn to proving that this construction is secure assuming the underlying q-
indexed signature is secure. Suppose we use a tree of depth dand use the system SMerkle to generate
a total of Q signatures. We show that the signature is secure as long as Q2/(2qd) is negligible.
Theorem 14.6. Let d,q be poly-bounded integers such that qd is super-poly. Let Sq be a secure
q-indexed signature. Then the derived Merkle signature SMerkle is a secure signature.
In particular, suppose Ais a Q-query signature adversary attacking SMerkle. Then there exist
an eﬃcient q-query adversary Band a PRF adversary BF, where Band BF are elementary
wrappers around A, such that
SIGadv[A,S] ≤PRFadv[BF,F] + Qd·iSIGadv[B,Sq] + Q2
2qd
Proof idea. As usual, we ﬁrst replace the PRFF with a random function. Now the Merkle signature
system SMerkle contains qd independent instances of the Sq system. The adversary Aissues at most
Q queries for SMerkle signatures. Each signature uses d instances of Sq. Hence, throughout the
game Ainteracts with at most Qd instances of Sq. Let ℓ:= Qd.
We construct adversary Bto break Sq using a basic “plug-and-pray” argument. Bis given a Sq
public-key pk and its goal is to forge a pk signature. It starts by generating ℓ= Qdpublic/private
key pairs of Sq denoted pk0,..., pkℓ−1. It then replaces one of these public-keys by the challenge
600
Algorithm S(sk,m): where sk = (k,sk0)
/ / Choose a random leaf node:
⃗ a:= (a1,...,a d) ←R (
{1,...,q }
)d
/ / Sign public-keys along path to leaf
For i= 1 to d−1:
ri ←F(k, (a1,...,a i) )
(pki,ski) ←Gq(ri)
σi ←R Sq(ski−1, (ai,pki) )
/ / Sign m using leaf key:
σd ←R Sq(skd−1, (ad,m) )
/ / Output signature:
σ←
(
⃗ a,(pk1,σ1), ..., (pkd−1,σd−1), σd
)
output σ
Algorithm V(pk0,m,σ ):
/ / Parse signature components:
σ←
(
⃗ a,(pk1,σ1), ..., (pkd−1,σd−1), σd
)
/ / Verify public-keys along path to leaf:
for i= 1 to d−1:
if Vq(pki−1, (ai,pki), σi) = reject:
output reject and stop
/ / Verify signature on m:
if Vq(pkd−1, (ad,m), σd) = reject:
output reject and stop
output accept
Figure 14.7: The Merkle signing and veriﬁcation algorithms
pk. Now Bknows the private keys for all ℓ instances of Sq except for one. It has a signing oracle
that it can query to generate up to q (indexed) signatures for this pk.
Next, Bassigns pk0 to the root of the q-ary tree and sends pk0 to Aas the SMerkle public-key
to attack. Adversary Aissues signature queries m1,...,m q for SMerkle. For the ith query mi,
adversary Bpicks a random leaf vi and assigns public-keys in pk0,..., pkℓ−1 to internal nodes on
the path from this leaf to the root. Bdoes this assignment consistently, namely, once some public-
key pki is assigned to an internal node this assignment will remain in eﬀect for the remainder of
the game.
Next, Buses the secret keys at its disposal to generate the necessary Sq signatures to obtain a
valid SMerkle signature for mi. This requires generating signatures with respect to all the public-keys
on the path from the leaf vi to the root. It sends the resulting SMerkle signature to A.
In the event that one of the public-keys on the path from the leaf vi to the root is pk, our B
generates the required pk signature by issuing a signature query to its challenger. This works ﬁne
as long as Bnever queries its challenger for pk signatures on distinct messages ( u, ˆm0) and (u, ˆm1)
that have the same u. Such queries are not allowed in the q-indexed attack game. Observe that
this failure event can only happen if two messages mi,mj from Ahappen to get mapped to the
same leaf node. Since there are qd leaves and each of the Qmessages is assigned to a random leaf,
this happens with probability at most Q2/(2qd).
Now, suppose all queries from Aare mapped to distinct leaves. Then we just said that B
correctly answers all signature queries from A. Eventually, Aproduces a SMerkle signature forgery
(m,σ), where σ is a vector containing d signatures. This σ uses some leaf v. Visualize the path
from v to the root of the tree. Similarly, for each of the Q signatures given to A, visualize the
601
m1 m m2 m3
u
Figure 14.8: Merkle signatures: proof of security
corresponding Q paths to the root, as shown in Fig. 14.8. Let u be the lowest tree node at which
the path from v intersects one of these Q paths. pku is the public key at that node. Suppose the
leaf v is a descendant of the ith child of u.
The main point is that σ must contain an existential forgery for the public-key pku. This is
because throughout the interaction with A, adversary Bnever generated a signature with index i
with respect to pku. If this node u happens to be the node to which pk is assigned then Bjust
obtained a forgery on pk that lets it win the q-indexed forgery game. Since pk is placed randomly
in one of the ℓ= Qd key pairs, this happens with probability 1 /Qd, as required. 2
14.6.4 Nonce-based Merkle signatures
Up until now we only considered stateless signatures — the signer did not maintain state between
invocations of the signing algorithm. Several signature systems, including Merkle signatures, be-
come more eﬃcient when the signing algorithm is allowed to maintain state. We observed a similar
phenomenon in Section 7.5 where stateful MACs were occasionally more eﬃcient than their stateless
counterparts.
A nonce-based signature is a tuple of three algorithms ( G,S,V ) as in the case of stateless
signatures. Algorithms G and V have the same inputs and outputs as in the stateless case. The
signing algorithm S, however, takes an additional input N called a nonce that lies in some nonce-
space N . The system remains secure as long as algorithm S is never activated twice using the
same nonce N . That is, the system is existentially unforgeable, as long as the adversary does not
obtain two signatures S(sk,m, N ) and S(sk,m′,N ′) where N = N ′.
Stateless signatures are preferable to nonce-based ones, especially in an environment where
multiple entities can issue signatures for a particular private key. For example, a heavily loaded
certiﬁcate authority is often implemented using several machines, each of which issues signatures
using the authority’s private key. A nonce-based signature in these settings would be harder to use
since all these machines would have to somehow synchronize their state to ensure that the signing
602
algorithm is never called twice with the same nonce. While this is certainly feasible, one typically
prefers stateless signatures so that synchronization is a non-issue.
Nonce-based Merkle signatures. When nonce-based signatures are adequate, the nonce can
greatly improve the eﬃciency of the Merkle signature system. Recall that the stateless Merkle
signing algorithm chose a random leaf in the q-ary tree and signed the message using that leaf. The
number of leaves had to be suﬃciently large so that the probability of choosing the same leaf twice
is negligible. In the nonce-based settings, we can simply make the nonce indicate what leaf to use.
The uniqueness of the nonce ensures that every signature uses a diﬀerent leaf. This lets us greatly
shrink the Merkle signing tree leading to much shorter and more eﬃcient signatures.
Speciﬁcally, the nonce-based Merkle signing algorithm takes as input a tuple ( sk,m, N ), where
N is a nonce, and outputs a signature. It signs m using leaf number N . The only modiﬁcation to
Fig. 14.7 is that leaf number N is used instead of a random leaf. The nonce space N is simply the
integers between 1 and the number of leaves in the tree, namely N := {1,...,q d}. The veriﬁcation
algorithm is unchanged from Fig. 14.7.
If we wish to support 240 signatures per public key, it suﬃces to choose qand dso that qd ≥240.
This gives much shorter signatures than in the stateless scheme where we needed qd to be much
larger than 240 to ensure that no two messages are ever randomly assigned to the same leaf.
Comparing signature sizes. Stateless Merkle signatures are much longer than nonce-based
ones. Consider, for example, nonce-based Merkle signatures supporting 2 40 signatures per public
key. Using q = 1024 and d = 4, and using the q-indexed signature from Section 14.6.1, a nonce-
based signature contains only four one-time signatures plus 40 hashes: 10 hashes for each of the
Merkle trees used in the q-indexed signature. Using the 2.1 KB Winternitz signature scheme, this
comes to about 9.6 KB per signature. Stateless signatures, where qd = 2160, n= 1024, and d= 16
are four times longer. In comparison, RSA signatures are far shorter, only 256 bytes per signature,
but are under threat from progress in quantum computing.
We conclude by pointing out that in the nonce-based settings, the extreme parameters q= 240
and d = 1 can be quite useful for signing software updates. This setup corresponds to a very
wide tree of depth 1. Key generation is slow, but signature veriﬁcation is super fast: only a single
one-time signature veriﬁcation plus 40 hash operations for the Merkle tree, as explained in (14.5).
Signature generation can also be done eﬃciently: if the nonce is a counter, counting from 1 to q,
then an eﬃcient Merkle tree traversal algorithm can be used to quickly generate the Merkle
tree nodes needed for each signature. See Exercise 14.20.
14.7 A fun application: the TESLA broadcast MAC
Alice is the head engineer at the Galileo project, aglobal navigation satellite system, or GNSS.
GNSS is the general name for navigation systems that use a constellation of satellites. Prominent
examples include the American GPS, the European Galileo, the Russian Glonass, and the Chinese
Beidou. Alice’s group is in charge of managing dozens of satellites that form the backbone of the
Galileo system. Each satellite broadcasts a short message to earth every second, and these messages
are processed by GNSS receivers in phones, cars, airplanes, ships, and many other devices.
Alice is worried about a spooﬁng attack on Galileo. A spoofer is a device that jams the true
signal from the satellites, and transmits a fake signal in its place. When the spoofer is active,
603
every receiver close by is fooled into reporting the wrong location. A GPS spoofer is believed to
have been the cause of a 2017 maritime incident in which twenty ships in the Black Sea reported
that their navigation equipment placed them 32km away from their true location — they knew the
system was wrong because it placed the ships near an inland airport.
Alice wants to secure Galileo against spooﬁng attacks. She knows that there are many other
possible attacks on GNSS, but she is speciﬁcally concerned about spooﬁng.
Alice’s ﬁrst idea — after reading the last two chapters — is to use digital signatures: every
message from the satellites will be signed using a secret key in the satellite, and all GNSS receivers
will have the corresponding public veriﬁcation key. This prevents spooﬁng because GNSS receivers
will ignore incoming messages that are not properly signed.
However, there is a problem. The data rate in GNSS systems is extremely low; only 20 bits per
second can be used for data authentication. At this slow data rate, sending a long 2048 bit RSA
signature is out of the question. In Chapters 19 and 15 we will construct shorter digital signatures,
where each signature can be as short as 384 bits. However, at 20 bits per second, even a 384 bit
signature will take twenty seconds to transmit. This means that, at best, location data can be
veriﬁed twenty seconds after it is transmitted. Alice would like to do better.
The problem we are trying to solve is called broadcast authentication: a single source, a
satellite, is broadcasting data that needs to be veriﬁed by many recipients. Ideally, Alice would like
to use a MAC to solve this problem because a MAC tag is much shorter than a digital signature.
However, to use a MAC, the sender and all the recipients need the MAC key, and this is insecure.
If all the recipients have the MAC key, then the spooﬁng device can also have it, and this key lets
the spoofer compute a valid tag for its attack messages (see also Exercise 6.4).
A broadcast authentication system should be secure against an adversary who controls many
recipients, possibly all the recipients except one. Moreover, the adversary can mount a chosen
message attack on the source. The adversary’s goal is to produce a new message-tag pair that will
be accepted by a recipient that the adversary does not control. The system is secure if no eﬃcient
adversary can do so with non-negligible probability.
Timed broadcast authentication. Let us see a clever way to use a secure MAC for broadcast
authentication. The approach, called TESLA, is based on hash chains developed in Section 14.3.
First, let us specify the problem more precisely.
The satellite needs to broadcast a stream of messages m1,m2,...,m n ∈M, one message every
ﬁxed time period, say every T seconds for some small value of T. Messages are not known ahead
of time: every message is known only shortly before it is transmitted. The TESLA system can be
used whenever the following two assumptions hold:
• Synchronized clocks: The satellite and all the receivers have synchronized clocks, and the
clock skew between the satellite and the receivers is at most T/2 seconds. Moreover, every
message mi, for i= 1,...,n , contains a data ﬁeld that indicates the time slot for which the
message is intended. A recipient will reject a message that is received outside of its indicated
time slot.
• Delayed authentication: The system can function properly even if an incoming message can
only be validated T seconds after it is received. During the T seconds gap, the receiver cannot
tell if an incoming message is authentic or forged. Exercise 14.21 explores an alternate design
604
that supports immediate authentication at the cost of a moderate increase to the broadcast
size.
Both assumptions hold for GNSS receivers, although there is some debate as to whether a receiver
should display an updated location during the T seconds gap when the received data has not yet
been authenticated.
With this setup, we are ready to describe the TESLA system.
The TESLA broadcast authentication system. Let (S,V ) be a secure MAC deﬁned over
(K,M,T), and let f : K→K be a function. The broadcaster sets up the scheme by creating a
hash chain of MAC keys. It chooses a random key k0 ←R Kand computes
k1 := f(k0), k2 := f(k1), ..., k n = f(kn−1)
for some n that will be determined later. Pictorially this chain of keys can be described as:
k0
f−−−−−→k1
f−−−−−→k2
f−−−−−→ ··· f−−−−−→kn−1
f−−−−−→kn
For now, let us assume that all the recipients somehow have kn, the last key in the chain. We will
discuss how to distribute kn at the end of the section.
We assume that f is one-way on niterates as in Deﬁnition 14.5. This means that an adversary
that is given ki ∈K, for some i> 0, cannot compute ki−1. In fact, we will need something slightly
stronger: we need that for all i= 0,...,n −1, the MAC system (S,V ) is secure with respect to the
key ki (as in Attack Game 6.1) even if the adversary is given ki+1 = f(ki).
To explain how the TESLA system works, let us ﬁrst see how the satellite authenticates the
ﬁrst message m1. The ﬁrst two broadcasts that the satellite sends are deﬁned as follows:
• message 1: use key kn−1 to compute t1 := S(kn−1,m1) and broadcast ( m1,t1).
• message 2: use key kn−2 to compute t2 := S(kn−2,m2) and broadcast ( m2,t2,kn−1).
Notice that the tag t1 in message 1 is computed using the key kn−1. This key is included in the
body of message 2.
A recipient ﬁrst receives message 1, but cannot validate the tag t1 because it does not yet have
the key kn−1. Instead, the recipient waits. Message 2, which arrives T seconds later, includes kn−1.
Now the recipient can verify m1 by checking that (i) the key kn−1 is correct, namely f(kn−1) = kn,
and (ii) m1 is valid, namely V(kn−1,m1,t1) = accept. If so, then m1 is accepted. We see that there
is a delay of T seconds between the time that the message m1 is received, and the time when it is
marked as authentic.
Why is this secure? Suppose the attacker wants to replace message m1 with a spoofed message
m′
1 ̸= m1 that has the same time slot number as m1. Then:
• Before message 2 is broadcast, the attacker does not have the key kn−1, and cannot compute
a valid tag for m′
1. In fact, because of our security assumption about the MAC system ( S,V ),
the attacker cannot create any new valid pair ( m′
1,t′
1) with respect to the key kn−1.
• Once message 2 is broadcast, the attacker obtains kn−1, and can now compute a valid tag
for m′
1 as t′
1 := S(kn−1,m′
1). It can then broadcast ( m′
1,t′
1). However, this is too late: the
broadcast (m′
1,t′
1) will be rejected by the recipients because at this point the recipients are
expecting a message for time slot number two. They reject messages for a lower time slot.
605
As a result, the attacker cannot replace m1 with a forgery m′
1.
The remaining messages are authenticated in the same way. In particular, the sequence of
broadcasts from left to right is as follows: (one broadcast every T seconds)
m1
t1 −→
m2
t2
kn−1
−→
m3
t3
kn−2
−→ ··· −→
mn−1
tn−1
k2
−→
mn
tn
k1
−→ k0 (14.6)
where ti := S(kn−i,mi) for i = 1,...,n . When a recipient receives the broadcast ( mi,ti,kn−i+1),
for i = 2,...,n , it checks that (i) f(kn−i+1) = kn−i+2, and (ii) V(kn−i+1,mi−1,ti−1) = accept. If
so, it accepts the message mi−1 as authentic. Again, message mi−1 is accepted T seconds after it
is received. The security argument is the same as the argument for the message m1.
Data size in each broadcast. Let’s ignore the ﬁrst and last broadcasts (which are special).
The authentication data in all other broadcasts is a pair ( t,k) ∈T ×K. In our settings, the tag t
can be as short as 32 bits because a forgery probability of 1 /232 is acceptable. However, the key k
needs to be at least 128 bits to ensure that the function f is one-way. Together, this is 160 bits per
broadcast, which is much shorter than 384 bits when using a digital signature. Assuming a rate of
20 bits per second, the system will take eight seconds to broadcast one ( t,k) pair, resulting in an
authentication delay of eight seconds for received messages. If the keys are reduced to 100 bits and
the tags are reduced to 20 bits, then the delay is only six seconds, at the cost of reduced security.
How to distribute the top of the chain. We still need to explain how to securely distribute
kn to all the recipients. If the hash chain of keys is very long, say long enough to support several
months or even years worth of broadcasts, then the top of the chain can be distributed by an oﬄine
process. For example, kn can be posted on a trusted web site and all the recipients get it from
there. We will not go into more detail here, but note that in GNSS, the distribution of kn is a
complicated process.
Remark 14.1 (a word of caution). Our description of the hash chain is a bit oversimpliﬁed.
In practice one needs to defend against pre-processing attacks such as the ones discussed in Sec-
tion 18.3.1.3. To do so, the hash chain function f must take two additional inputs. First, there is a
random nonce N generated when the chain is ﬁrst created. This “randomizes” the function f and
makes it harder to carry out an oﬄine analysis of f. This nonce is distributed to recipients along
with the key kn. Second, the function takes its position in the chain as input. In other words, for
i> 0, the key ki is computed as ki := f(ki−1,N ,i). 2
14.8 Notes
Citations to the literature to be added.
14.9 Exercises
14.1 (Shortening the public-key). Let ( G,S,V ) be a signature scheme, and suppose that
algorithm G generates public-keys in some set X. We show a generic transformation that gives a
new signatures scheme ( G′,S′,V ′) where the public-key is short, only 16 bytes.
606
(a) Let H be hash function deﬁned over ( X,Y). Algorithm G′ now works as follows: it runs
algorithm G to obtain pk and sk, and outputs
pk′:= H(pk), sk′:= (pk,sk).
Explain how algorithms S′and V′work.
(b) Prove that ( G′,S′,V ′) is a secure signature scheme, assuming ( G,S,V ) is secure, and H sat-
isﬁes the following collision resistance property, which is a variation of 2nd-preimage collision
resistance: namely, given ( pk,sk) as generated by G, it is hard ﬁnd pk∗ ̸= pk such that
H(pk∗) = H(pk).
Note: If H is modeled as a random oracle, then |Y| ≈2128 is large enough to ensure
reasonable (non-quantum) security.
(c) Show that when this transformation is applied to the basic Lamport signature scheme SL
discussed in Section 14.1, the signature size need only be twice as long as in SL.
14.2 (Attacking Lamport multi-key security). In our description of the various Lamport
signature schemes there is a ﬁxed one-way function f : X →Ythat all users in the system use.
This can cause a problem.
(a) Consider the multi-key signature game from Exercise 13.2 played against the basic Lamport
signature scheme. Show that after seeing ≈|Y|1/2 public keys, and one signature under each
of these keys, an adversary can forge the signature for one of the given public keys with
probability 1/2. This gives the adversary advantage 1 /2 in winning the multi-key security
game.
(b) When Y:= {0,1}256 the attack from part (a) is not a concern. However, when the range
is smaller, say Y:= {0,1}128, this can lead to a real-world attack. A simple solution is to
expand the domain of f to R×X and modify the key generation algorithm to include a fresh
random nonce r∈R in the public and secret keys. The r associated with a key pair ( pk,sk)
will always be prepended to the input of f when operating with pk or sk. Explain why this
prevents the attack from part (a) when |R|= |Y|.
14.3 (An injective mapping to ℓ-size subsets). Recall that Sets[ n,ℓ] is the set of all ℓ-size
subsets of {1,...,n }. In Section 14.4 we needed an injective mapping Phors : {0,1}v →Sets[n,ℓ]
where 2v ≤
(n
ℓ
)
, that is eﬃciently computable. The following algorithm provides such a mapping.
In fact, it injectively maps any integer in
[
0,
(n
ℓ
))
to an element of Sets[ n,ℓ].
input: 0 ≤m<
(n
ℓ
)
output: s⊆{1,...,n }where |s|= ℓ
s←∅, t ←ℓ
for k= n down to 1 until t= 0:
if m<
(k−1
t−1
)
:
s←s∪{k}, t ←t−1
else:
m←m−
(k−1
t−1
)
output s
607
Prove that the function computed by this algorithm always outputs a set in Sets[n,ℓ] and is injective.
Hint: Use the identity
(k
t
)
=
(k−1
t−1
)
+
(k−1
t
)
. This identity corresponds to a partition of Sets[ k,t]
into two types of sets: sets that contain the element k and sets that do not.
Discussion: The n×ℓbinomial coeﬃcients used in the algorithm can be pre-computed so that the
online running time is quite fast. If that table is too large to store, the algorithm can pre-compute
a single value, namely
(n−1
ℓ−1
)
, and quickly derive from it the n binomial coeﬃcients needed for a
run of the algorithm. For example,
(n−2
ℓ−2
)
=
(n−1
ℓ−1
)
·ℓ−1
n−1 , when n,ℓ > 1. This takes one integer
multiplication and one integer division per iteration.
14.4 (Another injective mapping to ℓ-size subsets). Let us see another injective function
Phors : {0,1}v → Sets[n,ℓ] that is designed for the case when the input is uniform in {0,1}v.
Suppose that n= 2t and v = t(ℓ+ c) for some c≥0. This lets us treat an element of {0,1}v as a
sequence of ℓ+ c elements in {1,...,n }. For a random m∈{0,1}v deﬁne Phors(x) as:
parse m as a sequence u1,u2,...,u ℓ+c ∈{1,...,n }
i←0 , s ←∅
repeat:
i←i+ 1, s ←s∪{ui}
until |s|= t or i= ℓ+ c
if |s|= t output the set s; otherwise output fail.
(a) Show that for m ←R {0,1}v, if Phors(m) ̸= fail then Phors(m) is uniformly distributed in
Sets[n,ℓ].
(b) Show that for m←R {0,1}v, the probability that Phors(m) = fail is bounded by et−1 ·(t/n)c+1,
where e≈2.71.
Discussion: We can assume that the inputmto Phors is uniform because mis typically the output
of a random oracle applied to the message to be signed plus a random nonce (as in Section 14.1.1).
The function Phors built here is more eﬃcient that the one in Exercise 14.3, but has a failure
probability which can occasionally force a re-try with a fresh nonce.
14.5 (Lamport q-time stateless signatures). Let P be a function mapping Mto subsets
of {1,...,n }. We say that P is q-containment free if for every x,y1,...,y q ∈ M, we have
P(x) ⊈ P(y1) ∪···∪ P(yq) whenever x /∈{y1,...,y q}.
(a) Generalize Theorem 14.2 to show that if the function P is q-containment free then the general
Lamport framework (Section 14.2) is a q-time secure signature scheme.
(b) Show that if P is q-containment free then n= Ω(q2v), where |M|= 2v. This shows that the
public-key or the signature size must grow quadratically in q.
14.6 (q-time HORST stateless signatures). Let P : R×M→ Sets[n,ℓ] be a function. Let A
be an adversary that takes as input sets s1,...,s q in Sets[n,ℓ] and outputs a pair ( r,x) ∈R×M
such that P(r,x) ⊆s1 ∪···∪ sq.
(a) Show that if P is modeled as a random oracle, and Amakes at most Qro queries to P then
Asucceeds with probability at most Qro ·
(qℓ
ℓ
)
/
(n
ℓ
)
. Therefore, for a given q, one can choose
the parameters n,ℓ so that a bounded adversary succeeds with only negligible probability.
608
(b) Explain how to use the function P as an enhanced TCR in the HORST system. Use part (a)
to show that the resulting signature scheme is q-time secure when n,ℓ are chosen so that
Qro ·
(qℓ
ℓ
)
/
(n
ℓ
)
is negligible.
(c) Continuing with part (b) and setting n := 2048, what is the smallest value of ℓ needed if
we want the adversary’s advantage in defeating the HORST 2-time signature to be at most
Qro/2256? What is the smallest ℓ for a 3-time signature under the same conditions?
Discussion: Assuming X = ˆY, the resulting combined size of a 2-time HORST signature
and public-key is 347·log2(|X|) bits. The 3-time HORST combined size is 433 ·log2(|X|) bits.
This is much shorter than the corresponding sizes for 2-time and 3-time Lamport signatures
from Exercise 14.5 using the same n and X.
14.7 (Insecure two-time signatures). Let S= (G,S,V ) be a secure (many-time) signature
scheme. Show how to construct from Sa new signature scheme S′ that is one-time secure, but
two-time insecure: if the signer uses a single signing key to sign two messages, then the secret key
is revealed publicly.
Hint: Try embedding in the public-key an encryption of the secret key under some symmetric
key k. Every signature must include a share of k, otherwise the signature is rejected.
14.8 (Lamport is strongly secure). Prove that the general Lamport framework in Section 14.2
gives is a strongly secure one-time signature scheme in the sense of Deﬁnition 14.2, assuming the
one-way function f is also 2nd-preimage collision resistant (as in Deﬁnition 8.6).
14.9 (Winternitz is strongly secure). As in the precious exercise, one can also show that
the Winternitz construction in Section 14.3 gives a strongly secure one-time signature, under an
appropriate assumption on the function f. State the assumption and prove the result.
14.10 (A many-time strongly secure signature). Consider the online-oﬄine signature con-
struction in Section 14.5.1. Suppose we modify the signing algorithm so that σ1 is computed as
σ1 ←R S1(sk1,(m,σ0)), and modify the veriﬁcation algorithm accordingly. Prove that this modiﬁed
scheme is strongly secure (in the sense of Deﬁnition 13.3) assuming that S∞ is secure and S1 is
strongly one-time secure.
14.11 (A strongly secure one-time signature from discrete log (I)). Let G be a cyclic
group of prime order q generated by g ∈G. Let H : M→ Zq be a hash function. We deﬁne a
signature scheme (G,S,V ) with message space Mas follows.
• The key generation algorithm G computes α,β ←R Zq, u ←gα ∈G, v ←gβ ∈G, and
outputs the public-key pk := (u,v) ∈G2 and the secret key sk := (α,β) ∈Z2
q.
• Given a secret key sk = (α,β) ∈Z2
q and a message m∈M, the signing algorithm S computes
c←H(m) ∈Zq and outputs the signature σ←cα+ β ∈Zq.
• Given a public-key pk = ( u,v) ∈ G2, a message m ∈ M, and a signature σ ∈ Zq, the
veriﬁcation algorithm V computes c←H(m) and accepts if gσ = v·uc.
Discussion: Notice that for every message and public key there is a unique signature that will be
accepted by the veriﬁer. Moreover, signing is quite fast, only one multiplication and one addition
in Zq. As such, this scheme is particularly well suited for online/oﬄine signatures as discussed in
Section 14.5.1.
609
(a) Show that when H is modeled as a random oracle, an adversary that breaks the strong one-
time security of the scheme (as in Deﬁnition 14.2) can be used to solve the discrete logarithm
problem in G.
(b) The construction from part (a) is trivially not two-time secure: signing two distinct messages
with the same key reveals the secret key by solving a linear system in the variables α,β.
Generalize the scheme to obtain a strong two-time secure signature scheme. The public key
pk is in G3, the signature is in Zq, and the hash function H is H : M→ Z2
q. Prove security of
the scheme based on the diﬃculty of the discrete-log problem in G, assuming H is modeled
as a random oracle.
14.12 (A strongly secure one-time signature from discrete log (II)). Like the previous
exercise, this exercise develops a strongly secure one-time signature based on the discrete logarithm
assumption. However, unlike in the previous exercise, this scheme does not rely on the random
oracle model. The cost is that signatures are twice as long.
Let G be a cyclic group of prime order q generated by g ∈G. Let h ∈G be a random group
element, which we view as a system parameter. We can deﬁne a signature scheme ( G,S,V ) with
message space Zq as follows.
• The key generation algorithm G computes
α,β,γ,δ ←R Zq, u←gαhβ ∈G, v←gγhδ,
and outputs the public-key pk := (u,v) ∈G2 and the secret key sk := (α,β,γ,δ ) ∈Z4
q.
• Given a secret key sk = ( α,β,γ,δ ) ∈Z4
q and a message m ∈Zq, the signing algorithm S
computes
σ←γ+ mα, τ ←δ+ mβ,
and outputs the signature ( σ,τ) ∈Z2
q.
• Given a public-key pk = (u,v) ∈G2, a message m ∈Zq, and a signature ( σ,τ) ∈Z2
q, the
veriﬁcation algorithm V checks if
gσhτ = v·um,
and outputs accept if this holds, and reject otherwise.
(a) Show that an adversary that breaks the strong one-time security of the scheme (as in Deﬁ-
nition 14.2) can be used to ﬁnd two diﬀerent representations of a u, relative to g and h, as
deﬁned in Section 10.6.1. Hence, by Fact 10.3, this adversary can be used to solve the discrete
logarithm problem in G.
Hint: First argue that the information contained in the public key and a single signature is
independent of β. To do this, ﬁx the random choices made by the adversary. Now, condition
on any ﬁxed value of β, and show that (u,v,τ ) is uniformly distributed over G×G×Zq, mis
determined by uand v, and σis determined by the equationgσhτ = v·um. This independence
immediately implies that if we condition on any ﬁxed value of ( u,v,τ ), then β is uniformly
distributed over Zq.
610
(b) Consider the key generation algorithm G0 that is the same as G, but sets β := 0. Show that
(G0,S,V ) is strongly one-time secure.
Hint: Show that for any adversary, its advantage in breaking ( G0,S,V ) is identical to its
advantage in breaking (G,S,V ). Again, use the hint from part (a): the information contained
in the public key and a single signature is independent of β.
Discussion: In this modiﬁed scheme, the public key is just ( u,v) = ( gα,gγhδ), and the
secret key is ( α,γ,δ ). A signature on m is just (σ,τ) = (γ+ mα,δ), which can be computed
using just one multiplication and one addition in Zq, like the scheme in the previous exercise.
Since τ = δ, which does not depend on m, one might be tempted to modify the scheme
further, and just place δ itself in the public key, and leave it out of the signature (and leave
hout of the scheme altogether). However, the proof of security would no longer apply to this
scheme (can you see why?). Moreover, it is not hard to see that this last scheme would be
no more secure than the scheme in the previous exercise, but with no hash function, and the
security of that scheme is not at all clear.
(c) Show that ( G,S,V ) is not two-time secure: given signatures on two distinct messages m0 and
m1 in Zq, the adversary can forge the signature on every message m∈Zq of its choice.
14.13 (From AD-only CCA security to CCA security). Suppose E= (G,E,D ) is a public-
key encryption scheme with message space M, ciphertext space C, and associated data space P.
Suppose S1 = (G1,S1,V1) is a signature scheme with message space C×D , where Dis some ﬁnite
set, and where public keys are contained in the set Pabove. We deﬁne a new public-key encryption
scheme E′= (G,E′,D′) with message space Mand associated data space Das follows:
E′(pk,m,d ) := ( pk1,sk1) ←R G1(), c←R E(pk,m, pk1), σ←R S1(sk1,(c,d))
output (pk1,c,σ );
D′(sk,(pk1,c,σ ),d) := if V1(pk1,(c,d)) = reject
then output reject
else output D(sk,c, pk1).
Show that if Eis AD-only CCA secure (as deﬁned in Section 12.7.1), and S1 is strongly one-time
secure, then E′is CCA secure (as in Deﬁnition 12.7).
14.14 (Online/oﬄine signatures from discrete-log). In Section 14.5.1 we showed that one-
time signatures can be used to improve the online performance of any signature scheme. The
one-time signature scheme in Exercise 14.12 is especially well suited for this application: signatures
are relatively short and signing is fast. In this exercise we show an even better approach. Let G be
a cyclic group of prime order q generated by g ∈G. Let ( G∞,S∞,V∞) be a many-time signature
scheme with message space M∞:= G. Deﬁne the following many-time signature scheme ( G,S,V )
with message space M:= Zq:
G() :=



(pk∞,sk∞) ←R G∞(),
τ ←R Zq, h ←gτ,
sk := (sk∞,τ), pk := (pk∞,h)



611
S(sk,m) :=



oﬄine phase:
ρ←R Zq, u ←gρ,
σ←S∞(sk,u)
online phase:
α←ρ−τm ∈Zq
output (σ,α)



V
(
pk,m, (σ,α)
):=
{ u←gαhm,
output V∞(pk∞,u)
}
Show that (G,S,V ) is secure assuming that (G∞,S∞,V∞) is secure and the discrete-log assumption
holds for G.
Discussion: Note that the online signing phase is only one multiplication and one addition in
Zq. This construction is based on the trapdoor hash Hdl(α,β) := gαhβ discussed in Exercise 10.30,
where τ = Dlogg(h) is the trapdoor.
14.15 (Repeated d-iterates of a one-way function). In the proof of Winternitz’s scheme
(Theorem 14.4) we needed a generalization of Lemma 13.5 that applies to iterated one-way func-
tions. Consider the following generalization of Attack Game 13.3 for given parameters nand dand
adversary A:
• The challenger computes x1,...,x n ←R X and y1 ←f(d)(x1),...,y n ←f(d)(xn). It sends
(y1,...,y n) to A.
• Amakes a sequence of reveal queries (i,j) where 1 ≤i ≤n and 0 ≤j ≤d. The challenger
responds with xi,j := f(j)(xi).
• Eventually, Aoutputs (a,b,x ), where a,b are positive integers and x∈X.
We say that Awins the game if f(b)(x) = ya and there was no reveal query ( a,b′) with b′≤b. Let
riOWadv[A,f,t,d ] be the probability that Awins the game. Prove that for every adversary Ain
this game there exists a (single instance) iterated one-way adversary Bsuch that
riOWadv[A,f,n,d ] ≤nd·iOWadv[B,f,d ]
14.16 (Iterated one-way functions). Let f : X→X be a function.
(a) Suppose f is one-way on d iterates, as in Deﬁnition 14.5. Show that the function f(d) is
one-way.
(b) Let f be a one-way function. Construct a function ˆf using f such that ˆf is one-way, but
ˆf(2)(x) := ˆf( ˆf(x)) is not. By part (a) this ˆf is also not one-way on a 2-iterate.
(c) Suppose f is a one-way permutation. Show that f is one-way on diterates for all bounded d.
(d) Show that if |X| is large and f is a random oracle then f is one-way on d iterates for all
bounded d. In particular, an adversary that makes Qro queries to the random oracle has
advantage at most O(dQro/|X|) in winning the random oracle variant of Attack Game 14.1.
Use Exercise 14.17.
14.17 (Iterations shrink the range). Let f : X →Xbe a random function. Show that for
d≪|X|1/2, the size of the image of f(d) behaves approximately as 2
d+1 |X|.
612
Discussion: This means that inverting f(d) by exhaustive search takes about a factor of (d+ 1)/2
fewer attempts than inverting f. Of course, evaluating f(d) takes d times longer, and therefore
the overall time to invert f(d) by exhaustive search is about the same as the time to invert f.
Exercise 14.18 gives a better algorithm for inverting f(d).
14.18 (Inverting an iterated function). Let f : X→X be a random function, where N := |X|.
Let f(d) be its d-th iterate, for some 0 < d <
√
N/log2 N. Give an algorithm Athat makes Q
queries to H, where 0 ≤Q < N/d, and wins the one-way inversion game (Deﬁnition 8.6) against
f(d) with advantage at least 1
2 dQ/N. In particular, for x←R X, your algorithm Aﬁnds a preimage
of f(d)(x) with probability 1 /2, after only about N/d queries to f. This shows that inverting f(d)
is about d times easier than inverting f.
Hint: On input y ← f(d)(x), try choosing a random x0 ←R X and computing the sequence
x0,f(x0),f(2)(x0),f(3)(x0),... . If the sequence hits y after more than d steps, then a preimage
of y is found. If the sequence loops on itself, choose a new random x0 ←R X and try again. Show
that this approach has the claimed success rate.
Discussion: This method does not generalize to invert a composition of d independent random
functions, h(x) := f1
(
f2(···fd(x) ···)
)
where f1,...,f d : X →X. In fact, one can show that
inverting h is as hard as inverting a random function f : X→X . This observation can be used to
strengthen the iterated hash function in the Winternitz signature scheme.
14.19 (Tree hash). Key generation in the q-indexed signature scheme of Section 14.6.1 requires
building a Merkle tree over q leaves using a hash function H : Y2 →Y . Recall that each leaf
contains the hash of a fresh public key of a one-time signature scheme. Let LeafCalc be a function
that takes as input an integer 1 ≤i ≤q and returns the contents of leaf number i. Suppose that
a call to LeafCalc takes one time unit as does one evaluation of H. Construct an algorithm that
computes the hash value at the Merkle tree root in time O(q) using only enough space needed to
store O(log q) elements of Y. This algorithm is called the treehash algorithm.
14.20 (Merkle tree traversal). Consider a Merkle tree with q leaves, where q is a power of two.
Let H : Y2 →Y be a hash function used to build the Merkle tree. As in the previous exercise, let
LeafCalc be a function that takes as input an integer 1 ≤i≤q and returns the contents hi ∈Y of
leaf number i. Assume that evaluating each of LeafCalc and H takes one time unit. As usual, for
every leaf 1 ≤i≤q there is a set of log 2 q nodes in the Merkle tree that authenticate leaf irelative
to the hash value at the Merkle root. This set of nodes is called the Merkle proof for leaf i. Let
Merkle(i) be a function that outputs the Merkle proof for leaf number ialong with the contents hi
of that leaf. The Merkle tree traversal problem is to compute the q items
Merkle(1), Merkle(2), ... Merkle(q)
sequentially one after the other. Show an algorithm for the Merkle tree traversal problem that runs
in amortized time log2 q per item, and only needs enough space to store log 2 q elements of Y.
Discussion: Merkle tree traversal can speed up the signing algorithm of thenonce-basedq-indexed
signature scheme from Section 14.6.1, where the nonce is a counter that indicates which leaf in the
Merkle tree to use. The counter is incremented after every invocation of the signing algorithm. In
addition to the nonce, the signer maintains the necessary O(log q)-size state needed for the tree
traversal algorithm. Better Merkle tree travesal algorithms [152, 39] run in worst-case time log2 q
per output and use space O(log2 q).
613
14.21 (Broadcast authentication with no delay). In Section 14.7 we presented the TESLA
broadcast authentication system built from a secure MAC system ( S,V ) and a function f. One
down side of this system is delayed authentication: an incoming message can only be veriﬁed
one time slot after it is received. Let’s change the system to enable immediate authentication by
changing the broadcast data in (14.6) to:
t1 −→
m1
t2
kn−1
−→
m2
t3
kn−2
−→ ··· −→
mn−2
tn−1
k2
−→
mn−1
tn
k1
−→
mn
k0
where ti := S(kn−i,mi) for i = 1,...,n . When a recipient receives the broadcast ( mi,ti+1,kn−i),
for i= 1,...,n −1, it checks that (i) f(kn−i) = kn−i+1, and (ii) V(kn−i,mi,ti) = accept. If so, it
immediately accepts the message mi, without any delay. Note, however, that the sender needs to
know the contents of mi one time slot before mi is transmitted; otherwise it cannot compute ti at
the right time.
(a) What is the security property that the MAC system ( S,V ) must satisfy for this construction
to be secure? Write out the explicit security game, and argue that if a MAC system satisﬁes
your deﬁnition then an attacker cannot replace the message mi in time slot i with a fake
message m′
i ̸= mi for all i= 1,...,n .
(b) Show that if H is a hash function deﬁned over (K×M, T) that is modeled as a random oracle,
and f is one way on n iterates, then the derived MAC system IH satisﬁes your deﬁnition
from part (a) assuming 1 /|K|and 1/|T| are negligible.
Discussion: Assuming that Kis large, the best attack on the system from part (b) requires about
|T| queries to the hash function H to succeed with constant probability. In GNSS the attacker
only has a few seconds to mount this attack, so that it may be suﬃcient to set T := {0,1}80. Then
each tag is 80 bits, instead of 32 bits as in Section 14.6. Hence, authentication data is increased
from 160 bits to 208 bits per time slot. The beneﬁt is immediate authentication.
614
Chapter 15
Elliptic curve cryptography and
pairings
In previous chapters we saw many applications of the discrete log, CDH, and DDH assumptions
in a ﬁnite cyclic group G. Our primary example for the group G was the multiplicative group (or
subgroup) of integers modulo a suﬃciently large prime p. This group is problematic for a number of
reasons, most notably because the discrete log problem in this group is not suﬃciently diﬃcult. The
best known algorithm, called the general number ﬁeld sieve (GNFS), discussed in Chapter 16,
runs in time exp( ˜O((log p)1/3)). It was used in 2019 to solve a discrete log problem modulo a general
795-bit prime. This algorithm is the reason why, in practice, we must use a prime p whose size is
at least 2048 bits. High security applications must use even larger primes. Arithmetic modulo such
large primes is slow and greatly increases the cost of deploying cryptosystems that use this group.
Several other families of ﬁnite cyclic groups with an apparent hard discrete log have been
proposed. Of all these proposals, the group of points of an elliptic curve over a prime ﬁnite ﬁeld
is the most suitable for practice, and is widely used on the Internet today. The best known
discrete log algorithm in an elliptic curve group of size q runs in time O(√q). This means that
to provide security comparable to AES-128, it suﬃces to use a group of size q ≈2256 so that the
time to compute discrete log is √q≈2128. The group operation uses a small number of arithmetic
operations modulo a 256-bit prime, which is considerably faster than arithmetic modulo a 2048-bit
prime.
Additional structure. As we will see, certain elliptic curve groups have an additional structure,
called a pairing, that is enormously useful in cryptography. We will see many examples of encryp-
tion and signature schemes built using pairings. These systems exhibit powerful properties that are
beyond what can be built using the multiplicative group of the integers modulo a prime. Some ex-
amples include aggregate signatures, broadcast encryption, functional encryption, and many others.
The bulk of the chapter is devoted to exploring the world of pairing-based cryptography.
15.1 The group of points of an elliptic curve
Elliptic curves come up naturally in several branches of mathematics. Here we will follow their
development as a branch of arithmetic (the study of rational numbers). Our story begins with
Diophantus, a greek mathematician who lived in Alexandria in the third century AD. Diophantus
615
(a) The curve
 (b) Adding P = (−1,−3) and Q= (1,3)
Figure 15.1: The curve y2 = x3 −x+ 9 over the reals (not drawn to scale)
was interested in the following problem: given a bivariate polynomial equation, f(x,y) = 0, ﬁnd
rational points satisfying the equation. A rational point is one where both coordinates are rational,
such as (1/2, 1/3), but not (1,
√
2 ). Diophantus wrote a series of inﬂuential books on this subject,
called the Arithmetica, of which six survived. Fourteen centuries later, Fermat scribbled his famous
conjectures in the margins of a latin translation of the Arithmetica. An insightful short book by
Bashmakova [11] describes Diophantus’ ideas in a modern mathematical language.
Much of the Arithmetica studies integer and rational solutions of quadratic equations. However,
in a few places Diophantus considers problems of higher degree. Problem 24 of book 4, which is
the ﬁrst occurrence of elliptic curves in mathematics, looks at a cubic equation. The problem is
equivalent to the following question: ﬁnd rational points ( x,y) ∈Q2 satisfying the equation
y2 = x3 −x+ 9. (15.1)
Fig. 15.1 shows a plot of this curve over the real numbers. We do not know what compelled
Diophantus to ask this question, but it is a good guess that he would be shocked to learn that the
method he invented to answer it now secures Internet traﬃc for billions of people worldwide.
One can easily verify that the six integer points (0 ,±3), (1,±3), (−1,±3) are on the
curve (15.1). Diophantus wanted to ﬁnd more rational points on this curve.
He proceeded to derive new rational points from the six he already had. Here is one way to do
it, which is slightly diﬀerent from what Diophantus did. Let P := (−1,−3) and Q:= (1,3), both
satisfying (15.1). Let’s look at the line passing through P and Q, as shown in Fig. 15.1b. One can
easily verify that this line is simply y= 3x. It must intersect the curve y2 = x3 −x+ 9 at exactly
three points. To see why, observe that if we substitute 3 x for y in (15.1) we obtain the univariate
cubic equation (3 x)2 = x3 −x+ 9. We already know two rational roots of this cubic equation:
x1 = −1 from the point P, and x2 = 1 from the point Q. It is not diﬃcult to show that a cubic
with rational coeﬃcients that has two rational roots, must also have a third rational root x3. In
our case, this third rational root happens to be x3 = 9. Setting y3 = 3x3 we obtain a new point
on the curve (15.1), namely (9 ,27). For reasons that will become clear in a minute, we denote this
point by −R. We get another point for free, (9 ,−27), which we call R. More generally, for a point
T = (x,y) on the curve, we let −T be the point −T := (x,−y).
This technique for building rational points is called the chord method. It is quite general:
given two distinct rational points U and V on the curve, where U ̸= −V, we can pass a line through
616
them, and this line must intersect the curve at a third rational point W. For example, applying
this to the points P and R gives two new points (−56
25 , 3
125 ) and (−56
25 , −3
125 ).
The chord method was re-discovered several times over the centuries, but it ﬁnally stuck with
the work of Poincar´ e on algebraic curves [130]. Poincar´ e likened the process of constructing a new
rational point from two known rational points to an addition operation in a group. Speciﬁcally, for
distinct points U and V on the curve, with U ̸= −V, let W be the point on the curve obtained by
passing a line through U and V and ﬁnding its third point of intersection with the curve. Then
Poincar´ e deﬁnes the sum ofU and V, denoted U ⊞V, as
U ⊞V := −W. (15.2)
Fig. 15.1b shows this addition rule applied to the points P and Q. Their sum P ⊞Q is the point
R = (9 ,−27). Deﬁning addition as in (15.2) makes this operation associative, when it is well
deﬁned. Recall that associativity means that ( U ⊞V) ⊞W = U ⊞(V ⊞W).
We will show in the next section how to enhance this addition rule so that the set of points
on the curve becomes a group. Some of the most beautiful results in number theory, and some of
the deepest open problems, come from trying to understand the properties of the group of rational
points on elliptic curves [8].
Going back to Diophantus, his approach for ﬁnding rational points on (15.1) is a variation of
the method we just saw. Instead of passing a line through two distinct points, Diophantus chose
to pass a tangent to the curve at one of the known points. Say we pass a tangent at the point
P = (−1,−3). As before, it is not diﬃcult to show that on a cubic curve with rational coeﬃcients,
if ( x1,y1) is a rational point with y1 ̸= 0, then the tangent at ( x1,y1) must intersect the curve
at exactly one more point T, and this point must also be rational. In our case, the tangent at
P = (−1,−3) is the line y = −1
3 x−10
3 . It intersects the curve at the point P and at the point
(19
9 , −109
27 ) which is indeed rational. This method, called the tangent method, is another way
to build a new rational point from a given rational point ( x1,y1), when y1 ̸= 0. As we will see, it
corresponds to adding the point P to itself, namely computing P ⊞P.
15.2 Elliptic curves over ﬁnite ﬁelds
The curve (15.1) is an example of an elliptic curve deﬁned over the rationals. For cryptographic
applications we are mostly interested in elliptic curves over ﬁnite ﬁelds. For simplicity, we only
consider elliptic curves deﬁned over a ﬁnite ﬁeld Fp where p> 3 is a prime.
Deﬁnition 15.1. Let p> 3 be a prime. An elliptic curve E deﬁned over Fp is an equation
y2 = x3 + ax+ b, (15.3)
where a,b ∈Fp satisfy 4a3 + 27b2 ̸= 0. We write E/Fp to denote the fact that E is deﬁned over Fp.
The condition 4a3 + 27b2 ̸= 0 ensures that the equation x3 + ax+ b= 0 does not have a double
root. This is needed to avoid certain degeneracies.
The set of points on the curve. Let E/Fp be an elliptic curve, and let e ≥1. We say that
a point ( x1,y1), where x1,y1 ∈ Fpe, is a point on the curve E if ( x1,y1) satisﬁes the curve
equation (15.3). When e= 1 the point ( x1,y1) is deﬁned over the base ﬁeld Fp. When e >1 the
point is deﬁned over an extension of Fp.
617
The curve includes an additional “special” point Ocalled the point at inﬁnity . Its purpose
will become clear in a minute. We write E(Fpe) to denote the set of all points on the curve E that
are deﬁned over Fpe, including the point O.
For example, consider the curve E : y2 = x3 + 1 deﬁned over F11. Then
E(F11) =
{
O, (−1,0), (0,±1), (2,±3), (5,±4), (7,±5), (9,±2)
}
(15.4)
This curve has 12 points in F11 and we write |E(F11)|= 12.
A classic result of Hasse shows that |E(Fpe)|= pe + 1 −t for some integer t in the interval
|t|≤ 2√pe. This shows that the number of points on E(Fpe) is close to pe + 1. The set E(Fp) in
example (15.4) has exactly p+ 1 points (so that t= 0).
Remark 15.1 (Point counting). A beautiful algorithm due to Schoof [141] can be used to
compute the number of points in E(Fpe) in time polynomial in log( pe). Hence, |E(Fpe)|can be
computed eﬃciently even for a large prime p. Elkies and Atkin showed how to reduce the running
time, and the resulting point counting method is called the Schoof-Elkies-Atkin algorithm, or
simply, the SEA algorithm. 2
The addition law. As we discussed in the previous section, there is a natural group law deﬁned
on the points of an elliptic curve. The group operation is written additively using the symbol “ ⊞”
to denote point addition. We deﬁne the point at inﬁnity Oto be the identity element: for all
P ∈E(Fpe) we deﬁne P ⊞O= O⊞P = P.
Now, let P = (x1,y1) and Q= (x2,y2) be two points in E(Fpe). The sum P ⊞Q= (x3,y3) is
deﬁned using one of the following three rules:
• if x1 ̸= x2 we use the chord method. Let sc := y1−y2
x1−x2
be the slope of the chord through the
points P and Q. Deﬁne
x3 := s2
c −x1 −x2 and y3 := sc(x1 −x3) −y1.
• if x1 = x2 and y1 = y2 (i.e., P = Q), but y1 ̸= 0, we use the tangent method. Let st := 3x2
1+a
2y1
be the slope of the tangent at P. Deﬁne
x3 := s2
t −2x1 and y3 := st(x1 −x3) −y1.
• if x1 = x2 and y1 = −y2 then deﬁne P ⊞Q:= O.
This addition law makes the set E(Fpe) into a group. The identity element is the point at
inﬁnity. Every point O̸= P = (x1,y1) ∈E(Fpe) has an additive inverse, namely −P = (x1,−y1).
Finally, it can be shown that this addition law is associative. The group law is clearly commutative,
P ⊞Q= Q⊞P for all P,Q ∈E(Fpe), making this an abelian group.
As in any group, for a point P ∈E(Fpe) we write 2 P := P ⊞P, 3 P := P ⊞P ⊞P, and more
generally, αP := (α−1)P ⊞P for any positive integer α. Note that αP can be computed using at
most 2 log2 α group operations using the repeated squaring algorithm (Appendix A).
15.2.1 Montgomery and Edwards curves
Equation (15.3) for an elliptic curve is called the Weierstrass form of the curve. There are many
equivalent ways of describing an elliptic curve and some are better suited for computation than the
Weierstrass form. We give two examples.
618
Montgomery curves. A Montgomery curve E/Fp in the variables u and v is written as
Bv2 = u3 + Au2 + u
for some A,B ∈Fp where B(A2−4) ̸= 0. This curve equation can be easily changed into Weierstrass
form via the change of variables u := Bx −A/3 and v := By. The number of points on a
Montgomery curve, |E(Fpe)|, is always divisible by four. Exercise 15.4 explores the computational
beneﬁt of Montgomery curves. They will also come up in the next section.
Some Weierstrass curves deﬁned over Fp cannot be put into Montgomery form over Fp by a
change of variables. For example, if the Weierstrass curve has an odd number of points over Fp,
then it has no Montgomery form over Fp. Such Weierstrass curves cannot beneﬁt from the speedup
associated with Montgomery curves. The curve P256, discussed in the next section, is an example
of such a curve.
Edwards curves. Another way to describe an elliptic curve E/Fp is in Edwards form, which is
x2 + y2 = 1 + dx2y2
where d ∈Fp satisﬁes d ̸= 0,1. Again, this curve can be put into Weierstrass form via a simple
rational change of variable. The beauty of the Edwards form is that the chord and tangent addition
law is extremely easy to describe. For points P = (x1,y1) and Q= (x2,y2) in E(Fpe), we deﬁne
P ⊞Q:=
( x1y2 + x2y1
1 + dx1x2y1y2
, y1y2 −x1x2
1 −dx1x2y1y2
)
.
That’s it. There is no need for three separate rules. The identity is the point O= (0,1) and the
inverse of a point ( x1,y1) is ( −x1,y1). The points ( ±1,0) have order four, which means that the
number of points on an Edwards curve, |E(Fpe)|, is always divisible by four.
The simplicity of the addition law on an Edwards curve makes it easier to resist timing attacks
on the implementation. It also leads to a very fast implementation.
15.3 Elliptic curve cryptography
Let E/Fp be an elliptic curve and let E(Fp) be the group of points on this curve that are deﬁned
over Fp. We know that E(Fp) is a ﬁnite abelian group. Suppose that it also happens to be cyclic,
meaning that E(Fp) is generated by some point P ∈E(Fp). We can now ask about the complexity
of problems like discrete log, computational Diﬃe-Hellman (CDH), and decision Diﬃe-Hellman
(DDH) in the group E(Fp). Once we establish that discrete log, CDH, and DDH are hard in this
group, we can instantiate all the constructions we covered in the previous several chapters using
the group E(Fp). The resulting systems are called elliptic curve cryptosystems.
Let us review the discrete log problem in E(Fp). Let P be a point in E(Fp) of order q, meaning
that qP = O. The discrete log problem in E(Fp) is as follows: We are given P and q, along with
another point Q := αP, for some α ∈Zq. Here αP is the point obtained by adding P to itself α
times using the addition law. We want to compute α, the discrete log of Q base P in E(Fp).
In Chapter 16 we will show that there is a generic discrete log algorithm that computes discrete
log in every cyclic group of order q using at most O(√q) group operations. We want a curve E/Fp,
or a set of curves, where the best known discrete log algorithm in E(Fp) runs in about the same
time as this generic algorithm. That is, there is no known algorithm that can compute discrete log
in E(Fp) much faster than √q time, where q:=
⏐⏐E(Fp)
⏐⏐.
619
Insecure curves. For many elliptic curvesE/Fp over a prime ﬁeldFp, the best known discrete log
algorithm runs in time O(√q), where q:=
⏐⏐E(Fp)
⏐⏐. However, there are several notable exceptions
where discrete log is much easier. Three examples are:
• In Section 16.1.2.2 we will show that if
⏐⏐E(Fp)
⏐⏐is composite, and all its prime factors are less
than some bound qmax, then there is an algorithm to compute discrete log in E(Fp) in time
˜O(√qmax). In particular, if all the prime factors of
⏐⏐E(Fp)
⏐⏐are small, say less than 2 80, then
the discrete log problem in E(Fp) is easy in practice. For this reason we will only use a curve
E/Fp if
⏐⏐E(Fp)
⏐⏐is equal to either q, 4q, or 8q for some prime number q. This will ensure that
the algorithm from Section 16.1.2.2 runs in time O(√q).
• When
⏐⏐E(Fp)
⏐⏐= p, the discrete log problem in E(Fp) is solvable in polynomial time. These
curves are called anomalous curves and should never be used in cryptographic applications.
• Suppose there is a small integer τ >0 such that |E(Fp)|divides pτ −1. Then discrete log
on E(Fp) reduces to discrete log in the ﬁnite ﬁeld Fpτ, as explained in Section 15.4. Discrete
log in Fpτ can be solved using variants of the general number ﬁeld sieve (GNFS) discrete log
algorithm discussed in Section 16.2. For example, if τ is small, say τ = 2, and p is a 256-bit
prime, then discrete log in E(Fp) can be solved eﬃciently: ﬁrst reduce a given discrete log
problem to Fp2, and then apply GNFS in Fp2. Since Fp2 is a ﬁnite ﬁeld of size about 2 512,
the discrete log problem in Fp2 can be solved in a reasonable amount of time (several hours).
To prevent this attack we need to ensure that pτ is suﬃciently large so that GNFS in Fpτ is
infeasible.
To avoid these pitfalls, many implementations use a curve from a ﬁxed collection of curves that
have been vetted and shown to not be vulnerable to the attacks listed above. The three most widely
used curves are called secp256r1, secp256k1, and Curve25519. We discuss these curves in the
next two subsections.
Remark 15.2 (Discrete log in E(Fpe)). Let E/Fp be an elliptic curve deﬁned over the prime
ﬁnite ﬁeld Fp. Asymptotically, discrete log in the group E(Fpe), for e ≥3, is not as hard as one
would like. For e≥2 there is a discrete log algorithm in the group E(Fpe) that runs in conjectured
time ˜O(p2−2/e) [70]. For e= 3 this evaluates to ˜O(p1.33), for e= 4 it evaluates to ˜O(p1.5), and so
on. This is asymptotically faster than the generic discrete log algorithm that, for e= 3 and e= 4,
runs in time O(p1.5) and O(p2), respectively. For this reason, the group E(Fpe) can only be used
if p is suﬃciently large so as to make this algorithm impractical. Taking p≥2256 is suﬃcient. 2
15.3.1 The curves secp256r1 and secp256k1
Two widely used elliptic curves, called secp256r1 and secp256k1, are speciﬁed in a standard
called SEC2, where SEC is an acronym for “standards for eﬃcient cryptography.” Both curves are
deﬁned over a 256-bit prime ﬁeld, hence the “256” in their names. The ‘r’ in secp256r1 signiﬁes that
the curve is a random curve, meaning that it was generated by a certain sampling procedure. The
‘k’ in secp256k1 signiﬁes that the curve is a Koblitz curve as explained below. The curve secp256r1
is widely used in Internet protocols, while secp256k1 is widely used in blockchain systems.
The curve secp256r1. This curve was approved by the U.S. National Institute of Standards
(NIST) for federal government use in a standard published in 1999. The NIST standard refers to
620
this curve as Curve P256. All implementations of TLS 1.3 are required to support this curve for
Diﬃe-Hellman key exchange. It is the only mandatory curve in the TLS 1.3 standard, as discussed
in Section 21.10.
The curve secp256r1 is deﬁned as follows:
• The curve is deﬁned over the prime pr := 2256 −2224 + 2192 + 296 −1. The special structure
of this prime is meant to improve the performance of arithmetic modulo pr.
• The curve has the Weierstrass form y2 = x3 −3x+ b where b∈Fpr written as a 255-bit
number in hexadecimal is:
b:= 5ac635d8 aa3a93e7 b3ebbd55 769886bc 651d06b0 cc53b0f6 3bce3c3e 27d2604b.
• The number of points on this curve is a prime number. Recall that it must be close to pr.
• The standard also speciﬁes a point Gr that generates the entire group E(Fpr), where E is the
curve secp256r1.
How was the odd looking parameter b selected? The reality is that we do not really know. The
standard lists an unexplained constant called a seed S. This seed was provided as input to a
public deterministic algorithm, that generated the parameter b. This process was designed to
select a random curve that resists the known discrete log attacks. The problem is that we do not
know for sure how the seed S was selected. An organization that wants to use secp256r1 might
worry that S was chosen adversarially so that discrete log on the resulting curve is easy. Currently
we do not know how to select such a seed even if we wanted to, so this concern is just an intriguing
speculation. As far as we can tell, secp256r1 is a ﬁne curve to use. It is widely used in Internet
protocols.
The curve secp256k1. The second curve in the SEC2 standard is called secp256k1. This curve
was selected for the digital signature scheme in the Bitcoin blockchain. Subsequent blockchains
inherited the same curve. One reason for this choice is the concern over the choice of seed in the
curve secp256r1 (also known as Curve P256) discussed in the previous paragraph.
The curve secp256k1 is deﬁned as follows:
• The curve is deﬁned over the prime pk := 2256 −232 −29 −28 −27 −26 −24 −1. Again, the
special structure of this prime is meant to improve the performance of arithmetic modulo pk.
• The curve has the Weierstrass form y2 = x3 + 7. Note that there are no unexplained
large constants in the Weierstrass form.
• The number of points on this curve is a prime number. Recall that it must be close to pk.
• The standard also speciﬁes a point Gk that generates the entire group E(Fpk), where E is the
curve y2 = x3 + 7.
621
Properties of secp256k1. The curve secp256k1 has two interesting properties. First, it is a
Koblitz curve, which means that there is a useful map deﬁned on it. To explain what that means,
let E be the curve y2 = x3 + 7, and let p:= pk be the prime over which secp256k1 is deﬁned. Let q
be the size of E(Fp). It so happens that p≡1 (mod 3) and therefore there exists 1 ̸= ω∈Fp such
that ω3 = 1.
Now, consider the map
φ: F2
p →F2
p deﬁned by φ(x,y) := (ωx,y). (15.5)
It is easy to verify that if ( x,y) ∈F2
p is a point in E(Fp) then φ(x,y) = ( ωx,y) is also in E(Fp).
Indeed, if (x,y) satisﬁes y2 = x3 + 7 in Fp then so does ( ωx,y) because ω3 = 1. Let us also deﬁne
φ(O) := Oso that φ is a map from E(Fp) to E(Fp).
The general theory of elliptic curves tells us that the map φ in (15.5) must be a group homo-
morphism. Therefore, since E(Fp) is a cyclic group, there must be a constant λin Zq such that for
all P in E(Fp) we have
φ(P) = λ·P,
where λ·P is the point in E(Fp) obtained by adding P to itself λ times.
We can determine the constant λ as follows. We know that for all non-zero P ∈E(Fp), the
three points P, φ(P), and φ2(P) := φ(φ(P)) all have the same y-coordinate, and therefore they
are colinear — they all lie on the horizontal line through P. By deﬁnition of the addition law, this
means that their sum must be zero. In other words, for all P ∈E(Fp) we have
O= P + φ(P) + φ2(P) = (1 + λ+ λ2) ·P.
We can therefore conclude that 1 + λ+ λ2 = 0 in Zq. Hence, λ must be one of the two non-trivial
cube roots of unity in Zq. Indeed, q≡1 (mod 3) so that a non-trivial cube root exists in Zq. Thus,
we know the value of λ.
How is this map useful? The fact that the map φ is so easy to calculate can be used to speed
up multiplication on the curve secp256k1. Let α∈Zq and suppose that we want to calculate α·P
for some P ∈E(Fp). For most α in Zq it is possible to ﬁnd integers τ0,τ1,τ2 such that
α= τ0 + τ1λ+ τ2λ2 and
⏐⏐τi
⏐⏐≤2q1/3 for i= 0,1,2.
Then to compute αP it suﬃces to compute
αP = τ0 ·P + τ1 ·φ(P) + τ2 ·φ2(P). (15.6)
This equality converts a multiplication byαinto three multiplications where the multipliersτ0,τ1,τ2
are much smaller than α. The expression on the right hand side of (15.6) can computed about two
to three times faster than computing αP directly. This is done using a fast multi-exponentiation
algorithm discussed in Appendix A. The end result is that the map φ can be used to speed up
multiplication on secp256k1.
Finally, we note that the curve secp256k1 has another useful property that is a bit magical. If
E is the curve y2 = x3 + 7 and p:= pk is the prime over which secp256k1 is deﬁned, then
the group E(Fp) has size q and the group E(Fq) has size p.
This property is useful in certain zero knowledge proof systems, but we will not make use of it here.
622
Security of discrete log on secp256r1 and secp256k1. Because the primes pr and pk are
close to 2 256, the number of points on both curves is also close to 2 256. Therefore, computing
discrete log on these curves using a generic discrete log algorithm takes approximately 2 128 group
operations. We assume that no algorithm can compute discrete log much faster than that. The
intent is that discrete log on both curves (as well as CDH and DDH on both curves) should be at
least as hard as breaking AES-128. Consequently, if one is aiming for the level of security provided
by AES-128, then either curve can be used for Diﬃe-Hellman key exchange, public-key encryption,
and digital signatures.
Curves with higher security. Some high-security applications use AES-256 to encrypt plaintext
data. In these cases, one should use an elliptic curve with a higher security parameter. One option
is another curve from SEC2 called secp521r1, whose size is approximately 2 521. It is deﬁned over
the Mersenne prime p = 2521 −1. Discrete log on this curve is believed to require at least 2 256
group operations. This curve is also approved by the U.S. national institute of standards (NIST)
for federal government use.
15.3.2 A security twist
Every elliptic curve E/Fp has a related curve ˜E/Fp called the twist of E. Let c ∈Fp be some
quadratic non-residue in Fp. If E is the curve y2 = x3 + ax+ b then its twist ˜E is the curve
cy2 = x3 +ax+b. It is a simple exercise to show that
⏐⏐E(Fp)
⏐⏐+
⏐⏐˜E(Fp)
⏐⏐= 2p+2. Since the number
of points on E(Fp) is p+ 1−t, it follows that the number of points on ˜E(Fp) must be ˜n:= p+ 1 +t.
We say that a curveE/Fp is twist secure if discrete log is intractable on bothE(Fp) and ˜E(Fp).
For E/Fp to be twist secure we need, at the very least, that both n= |E(Fp)|and ˜n= |˜E(Fp)|are
prime numbers, or are small multiples of large primes.
Why do we need twist security? Consider a system where Bob has a secret key α∈Zq. Under
normal operation, anyone can send Bob a pointP ∈E(Fp) and Bob will respond with the pointαP.
One system that operates this way is the oblivious PRF in Section 11.6.2. Before responding, Bob
had better check that the given point P is in E(Fp); otherwise, the response that Bob sends back
could compromise his secret key α, as discussed in Exercise 15.1 (see also Remark 12.1 where a
similar issue came up). Checking that a point P = (x1,y1) satisﬁes the curve equation is quite
simple and eﬃcient. However some implementations use the optimizations outlined in Exercises 15.2
and 15.4, where Bob is only sent the x-coordinate of P. The y-coordinate is not needed and is
never sent. In this case, checking that the given x1 ∈Fp is valid requires a full exponentiation to
conﬁrm that x3
1 +ax1 +bis a quadratic residue in Fp (see Appendix A.2.3). Suppose Bob skips this
expensive check. Then an attacker could send Bob an x1 ∈Fp that is the x-coordinate of a point ˜P
on the twist ˜E(Fp). Bob would then respond with the x-coordinate of α˜P in ˜E(Fp). If discrete log
in ˜E(Fp) were easy, this response would expose Bob’s secret key α. Hence, if Bob skips the group
membership check, we must ensure, at the very least, that discrete log in ˜E(Fp) is intractable so
that α˜P does not expose α. Twist security is meant to ensure exactly that.
The curves secp256r1 and secp256k1 were not designed to be twist secure. The size of the twist
of secp256r1 is divisible by 34905 = 3 ×5 ×13 ×179. Consequently, discrete log on the twist
is
√
34905 ≈187 times easier than on secp256r1 (see Section 16.1.2.2). Similarly, for secp256k1
the size of the twist is divisible by 3 2 ×132 ×3319 ×22639, and consequently, discrete log on the
twist is ≈218 times easier than on secp256k1. These are important facts to remember, but not a
623
signiﬁcant enough concern to disqualify secp256r1 or secp256k1.
15.3.3 Curve25519
Curve25519 is designed to support an optimized group operation and to be twist secure. The curve
is deﬁned over the prime p := 2255 −19, which is the reason for its name. This p is the largest
prime less than 2 255 and this enables fast arithmetic in Fp.
It is easiest to describe Curve25519 as a Montgomery curve, namely a curve in the form E :
By2 = x3 +Ax2 +xfor some A,B ∈Fp where p> 3. Exercise 15.4 shows that these curves support
a fast multiplication algorithm to compute αP from P where P ∈E(Fp) and α ∈Z. We noted
earlier that the number of points |E(Fp)|on a Montgomery curve is always a multiple of four.
Curve25519 presented as a Montgomery curve is simply
y2 = x3 + 486662 ·x2 + x.
The number of points on this curve is eight times a prime. We say that the curve has cofactor
eight. The curve is generated by a point P = (x1,y1) where x1 = 9. For completeness, we note
that Curve25519 can also be presented as the Edwards curve x2 + y2 = 1 + (121665/121666)x2y2.
Why the constant 486662? When deﬁning a Montgomery curve, the smaller A is, the faster
the group operation becomes, as explained in Exercise 15.4. For the best performance we need
(A−2)/4 to be small [17]. Dan Bernstein, who designed this curve, chose the smallest possible
A so that the curve is secure against the known discrete log attacks. He also made sure that the
order of the curve and the order of its twist are either four times a prime or eight times a prime.
Dan Bernstein writes [18]:
The smallest positive choices for A are 358990, 464586, and 486662. I rejected A =
358990 because one of its primes is slightly smaller than 2 252, raising the question of
how standards and implementations should handle the theoretical possibility of a user’s
secret key matching the prime; discussing this question is more diﬃcult than switching
to another A. I rejected 464586 for the same reason. So I ended up with A= 486662.
This explanation is a bit more satisfying than the unexplained constants in the random curve
secp256r1, and the unexplained prime in secp256k1.
15.4 Pairing based cryptography
Up until now, we used elliptic curves as an eﬃcient group where the discrete log problem and its
variants, CDH and DDH, are believed to be diﬃcult. This group makes it possible to instantiate
eﬃciently many of the schemes described in earlier chapters. We now show that certain elliptic
curves have an additional structure, called a pairing. The pairing enables a world of new schemes
that could not be built from discrete log groups without this additional structure. The resulting
schemes make up an important area called pairing based cryptography.
To present pairing based schemes we focus on the new capabilities enabled by a pairing and
abstract away the details of the elliptic curve group. As in earlier chapters, we will write the
group operation multiplicatively. This is a little diﬀerent from the ﬁrst part of this chapter where
we used additive notation for the group operation to be consistent with traditional elliptic curve
mathematical notation.
624
Deﬁnition 15.2. Let G0,G1,GT be three cyclic groups of prime order q where g0 ∈G0 and g1 ∈G1
are generators. A pairing is an eﬃciently computable function e : G0 ×G1 →GT satisfying the
following properties:
1. bilinear: for all u,u′∈G0 and v,v′∈G1 we have
e(u·u′, v) = e(u,v) ·e(u′,v) and e(u, v·v′) = e(u,v) ·e(u,v′),
2. non-degenerate: gT := e(g0,g1) is a generator of GT.
When G0 = G1 we say that the pairing is a symmetric pairing. We refer to G0 and G1 as the
pairing groups or source groups, and refer to GT as the target group.
Bilinearity implies the following central property of pairings that will be used in all our con-
structions: for all α,β ∈Zq we have
e(gα
0 , gβ
1 ) = e(g0,g1)α·β = e(gβ
0 , gα
1 ). (15.7)
This equality follows from the useful equalities e(gα
0 , gβ
1 ) = e(g0,gβ
1 )α = e(gα
0 ,g1)β which are
themselves a direct consequence of bilinearity. We note that in cyclic groups such as G0 and G1,
the relation (15.7) is equivalent to the bilinearity property stated in Deﬁnition 15.2.
The non-degeneracy requirement in Deﬁnition 15.2 ensures that the pairing e does not always
output 1 ∈GT for all inputs. Such a pairing is bilinear, but not very useful. In addition, we will
want the discrete log problem to be diﬃcult in the groups G0 and G1. Many of the constructions
will require additional problems to be hard, such as CDH and certain variants of DDH.
Direct consequences. A pairing e: G0 ×G1 →GT has a number of consequences for the groups
G0 and G1. First, when G0 = G1 the decision Diﬃe-Hellman (DDH) problem in G0 is easy. To
see why, observe that given a triple ( u,v,w ) = (gα
0 ,gβ
0 ,gγ
0 ) ∈G3, we can test if γ = α·β in Zq by
checking if
e(u,v) = e(g0,w). (15.8)
By (15.7) this equality holds if and only if e(g0,g0)αβ = e(g0,g0)γ, which holds if and only if
γ = α·β. This is precisely when ( u,v,w ) is a DH-triple. Hence, DDH is easy.
One implication of this is that the multiplicative ElGamal encryption scheme from Exercise 11.5
is not semantically secure in a symmetric pairing group, and should not be instantiated in such
a group. Similarly, the full ElGamal encryption scheme EEG from Section 11.5 cannot be proved
secure using the analysis in Section 11.5.2, which is based on DDH. Nevertheless, EEG can still be
proved secure based on CDH in the random oracle model using the analysis in Section 11.5.1.
For an asymmetric pairing, where G0 ̸= G1, it may still be possible that the DDH assumption
holds in both G0 and G1. In fact, for the most eﬃcient asymmetric pairings from elliptic curves,
the DDH assumption is believed to hold in both G0 and G1.
Second, computing discrete log in either G0 or G1 is no harder than computing discrete log in
the target group GT. To see why, suppose we are given u0 = gα
0 ∈G0 and are asked to compute
its discrete log α∈Zq. To do so, we compute u:= e(u0,g1) and set gT := e(g0,g1). Observe that
u ∈GT satisﬁes u = (gT)α, and we can ﬁnd α by computing the discrete log of u base gT in the
group GT. Hence, if the discrete log problem in GT is easy, then so is the discrete log problem in
G0. A similar argument applies to G1. Consequently, for discrete log to be hard in either G0 or
G1, we must choose the groups so that discrete log in the target group GT is also hard.
625
Constructing pairings from elliptic curves. A pairing e : G0 ×G1 →GT is typically con-
structed from an elliptic curve E/Fp. The most natural and eﬃcient pairings from elliptic curves
are asymmetric, where G0 ̸= G1. For this reason we will describe all the pairing-based systems in
this chapter using asymmetric pairings.
A pairing constructed from an elliptic curve E/Fp, has groups G0,G1,GT with the following
properties:
• G0 is an order q subgroup of E(Fp), for some prime q,
• G1 is an order q subgroup of E(Fpd), for some d> 0, where G1 ∩G0 = {O},
• GT is an order q multiplicative subgroup of the ﬁnite ﬁeld Fpd.
The integer d >0 is called the embedding degree of the curve. Clearly d needs to be small so
that the elements in G1 and GT can be represented eﬃciently. If d were large we could not write
down elements in G1 and GT. Elliptic curves where d is small, say d≤16, are said to be pairing
friendly elliptic curves.
Because G0 is deﬁned over the base ﬁeld Fp and G1 is deﬁned over a larger ﬁeld Fpd, the
representation of elements in G0 is typically much shorter than elements in G1. This plays an
important role in choosing what to put in each group. For example, to minimize ciphertext length
we will prefer that group elements that appear in the ciphertext live in G0.
The pairing function e on an elliptic curve comes from an algebraic pairing called the Weil
pairing. This pairing can be eﬃciently evaluated using an algorithm due to Victor Miller, and
called Miller’s algorithm. In practice one uses variants of the Weil pairing called the Tate and Ate
pairings, for which Miller’s algorithm is more eﬃcient. We refer to [67] for more information about
the deﬁnition of these pairings and how to evaluate them.
The most commonly used curves in practice are called bn256 and bls381. Both curves have
embedding degree d = 12 and the group order q is a 256-bit prime. Both curves are deﬁned over
a prime ﬁeld: bn256 is deﬁned over a 256-bits prime ﬁeld, while bls381 is deﬁned over a 381-bits
prime ﬁeld. The curve bls381 is believed to provide better security because discrete log in GT is
harder, but arithmetic in bls381 is slower because of the larger prime.
Pairing performance. Let e: G0 ×G1 →GT be a pairing derived from a pairing friendly elliptic
curve E/Fp. All three groups have prime order q. Computing the pairing takes O(log q) arithmetic
operations in Fp, similar to exponentiation inG0 or G1. However, in practice, computing the pairing
takes longer than an exponentiation (the exact overhead depends on the speciﬁc implementation).
We therefore try to minimize the number of pairing computations in the systems we design. To
give a sense for the relative costs, Table 15.1 gives running times for exponentiation and pairing on
the curve bn256. It shows that a pairing is about ten times slower than an exponentiation in G0.
Many optimizations are possible when implementing a pairing.
• First, when one of the pairing inputs is ﬁxed, it is possible to speed up the pairing computation
signiﬁcantly using pre-computation. Speciﬁcally, suppose one needs to compute many pairings
e(u,vi) for i = 1 ,...,n where u ∈G0 is ﬁxed. Then it is possible to build a table that
depends only on u. Once the table is constructed, computing each pairing is much faster
than computing it without the pre-computed table. This speed-up is analogous to the pre-
computation speed-up for ﬁxed-base exponentiation discussed in Appendix A.2.4.
626
time (milliseconds)
exponentiation: in G0 0.22
in G1 0.44
in GT 0.95
pairing: G0 ×G1 →GT 2.32
Table 15.1: Benchmarks for pairings on the curve bn2561.
• Second, a product of pairings ∏n
i=1 e(ui,vi) ∈GT can be computed faster than computing
the npairings one by one. For example, the ﬁnal step in a pairing computation is raising the
computed value to a ﬁxed large power. This ﬁnal exponentiation can be aggregated across all
the pairings in the product and done only once for the entire product. Other optimizations
are also possible.
Often these two optimizations combine to give a signiﬁcant speed-up [143, 144].
15.5 Signature schemes from pairings
Pairings enable many new advanced encryption and signature schemes, as well as other primitives.
In this section, we present several new signature schemes, based on pairings, with interesting
properties.
In Section 15.5.1, we introduce the BLS signature scheme . One nice feature of BLS signatures
is compactness — a signature is a single short group element. Another nice feature of the scheme is
that it supports signature aggregation, which is a means by which many signatures can be publicly
compressed into a single, shortaggregate signature. This is a new feature for a signature scheme that
we have not seen before. It is discussed in Sections 15.5.2 and 15.5.3. Finally, in Section 15.5.4,
we present two signature schemes based on pairings that can be proved secure without relying
on the random oracle model. Note that except for the hash-based signature schemes presented
in Chapter 14 (speciﬁcally, in Section 14.6), all of the other signatures schemes we have seen so
far rely for their security on the random oracle model. The pairing based schemes we present in
Section 15.5.4 produce much shorter signatures than those produced by the hash-based schemes.
15.5.1 The BLS signature scheme
Let e: G0 ×G1 →GT be a pairing where G0,G1,GT are cyclic groups of prime order q, and where
g0 ∈G0 and g1 ∈G1 are generators. We will also need a hash function H that maps messages in a
ﬁnite set Mto elements in G0.
The BLS signature scheme, denoted SBLS = ( G,S,V ), has message space Mand works as
follows:
• G(): The key generation algorithm runs as follows
α←R Zq, u ←gα
1 ∈G1.
The public key is pk := u, and the secret key is sk := α.
1Benchmarked on a 2.4 GHz Intel i5 520M using the Miracl library. Numbers are from the Miracl documentation.
627
• S(sk,m) : To sign a message m∈M using a secret key sk = α∈Zq, do:
σ←H(m)α ∈G0, output σ.
• V(pk,m,σ ): To verify a signature σ ∈ G0 on a message m ∈ M, using the public key
pk = u∈G1, output accept if
e
(
H(m), u
)
= e(σ, g1).
It is easy to verify that a valid signature is always accepted: for all public keys u ←gα
1 and
messages m∈M, a valid signature σ←H(m)α will be accepted by the veriﬁer because
e
(
H(m), u
)
= e
(
H(m), gα
1
)
= e
(
H(m)α, g1
)
= e(σ,g1).
As presented, signatures live in G0 and public keys live in G1. In the previous section we
mentioned that elements inG0 have a shorter representation compared to elements inG1. Therefore,
as described, the scheme is optimized for short signatures — a signature is a single element in G0.
One can equally optimize for short public keys by deﬁning a dual scheme where the roles of G0
and G1 are reversed: signatures live in G1 and public keys live in G0. The security analysis below
applies equally well to this dual scheme.
Unique signatures. The SBLS scheme is a unique signature scheme: for a given public key,
every message m∈M has a unique signature σ ∈G0 that will be accepted as valid for m by the
veriﬁcation algorithm. This means that if SBLS is secure, it must also be strongly secure in the
sense of Deﬁnition 13.3.
Security. We can prove thatSBLS is secure when H : M→ G0 is modeled as a random oracle. We
will need a minor variation of the computational Diﬃe-Hellman (CDH) assumption which adapts
the standard CDH assumption to the case when two groups are used.
Attack Game 15.1 (co-CDH). For a given adversary A, the attack game runs as follows.
• The challenger computes
α,β ←R Zq, u 0 ←gα
0 , u 1 ←gα
1 , v 0 ←gβ
0 , z 0 ←gαβ
0
and gives the tuple ( u0,u1,v0) to the adversary. Note that α is used twice, once
in G0 and once G1.
• The adversary outputs some ˆz0 ∈G0.
We deﬁne A’s advantage in solving the co-CDH problem for e, denoted coCDHadv[A,e], as
the probability that ˆz0 = z0. 2
Here we used z0 to denote the Diﬃe-Hellman secret z0 := gαβ
0 . More generally, in attack games
in this chapter we will always use the variable z for the unknown value that the adversary is trying
to compute or distinguish from random.
Deﬁnition 15.3 (co-CDH assumption). We say that the co-CDH assumption holds for the
pairing e if for all eﬃcient adversaries Athe quantity coCDHadv[A,e] is negligible.
628
When e is a symmetric pairing we have G0 = G1 and g0 = g1, in which case the co-CDH
assumption is identical to the standard CDH assumption. We can now prove security of SBLS.
Theorem 15.1. Let e: G0 ×G1 →GT be a pairing and let H : M→ G0 be a hash function. Then
the derived BLS signature scheme SBLS is a secure signature scheme assuming co-CDH holds for e,
and H is modeled as a random oracle.
In particular, let Abe an eﬃcient adversary attacking SBLS in the random oracle version of
Attack Game 13.1. Moreover, assume that Aissues at most Qs signing queries. Then there
exists an eﬃcient co-CDH adversary B, where Bis an elementary wrapper around A, such that
SIGroadv[A,SBLS] ≤2.72 ·(Qs + 1) ·coCDHadv[B,e]. (15.9)
Proof idea. The proof mimics the proof of security for the RSA-FDH signature scheme (Theo-
rem 13.4). Adversary Bis given a tuple ( u0 = gα
0 , u1 = gα
1 , v0 = gβ
0 ) where α,β ←R Zq, as in the
co-CDH attack game. It needs to compute z0 := gαβ
0 = vα
0 . The adversary begins by sending the
BLS public key pk := u1 to the signature forger A.
Next, the forger makes a sequence of Qs signing queries, and Qro queries to H. For j = 1,2,...
our adversary Bresponds to hash query number j for H(mj) by choosing ρj ←R Zq and setting
H(mj) := gρj
0 . This enables Bto answer all of A’s signing queries. The signature on mj is simply
σj = uρj
0 because σj := H(mj)α = gρjα
0 = uρj
0 . Our Bcan compute this σj using the given u0 in
the co-CDH challenge. This is the reason we include u0 in the challenge.
Eventually Aoutputs a valid signature forgery (m,σ) where σ= H(m)α. We know that H(m)
was deﬁned either when Aissued an explicit query for H(m), or when Aoutput the ﬁnal forgery
(note that m cannot appear in a signature query from A). Thus, H(m) was deﬁned in one of the
(Qro + 1) queries to H. Suppose that Aissued a query for H(m) in hash query number ν, for some
1 ≤ν ≤Qro + 1. One proof strategy is to have Btry to guess ν at the beginning of the game. B
chooses a random ω in {1,...,Q ro + 1}at the beginning of the game, and responds to hash query
number ω with H(mω) := v0. If Bwas lucky and it chose ω correctly, so that ω = ν, then the
signature σ on m= mω output by Asatisﬁes σ= H(m)α = H(mω)α = vα
0 = z0, which is the value
Bneeds to win the co-CDH game.
This proof strategy for computing z0 works ﬁne, but incurs a factor (Qro + 1) loss in B’s success
probability compared to A’s, because Bneeds to guess ν correctly at the beginning of the game.
This loss can be reduced to 2 .72 ·(Qs + 1), as claimed in (15.9), using exactly the same argument
used in Lemma 13.6 for the RSA function. See Exercise 15.5 for the CDH analogue of Lemma 13.6.
Using Exercise 15.5 the proof of security is identical to the proof of security for RSA-FDH in
Section 13.4.2 and gives the bounds stated in (15.9). 2
15.5.2 Signature aggregation
The BLS signature scheme is remarkably agile. It is quite easy to build a threshold signature
scheme from it (Section 22.2.2), a blind signature scheme (Exercise 15.7), and even generate the
secret key distributively among several parties. In this section we focus on a feature of BLS called
signature aggregation, which is a new capability that we have not seen before.
Signature aggregation refers to the ability to compress signatures on diﬀerent messages,
possibly issued using diﬀerent signing keys, into a single short aggregate signature. The short
aggregate convinces the veriﬁer that all the input messages were properly signed.
629
Deﬁnition 15.4. An aggregate signature scheme SAis a signature scheme with two additional
eﬃcient algorithms A and VA:
• A signature aggregation algorithm A(pk,σ): takes as input two equal length vectors, a vector
of public keys pk= (pk1,..., pkn) and a vector of signatures σ= (σ1,...,σ n). It outputs an
aggregate signature σag.
• A deterministic aggregate veriﬁcation algorithm VA
(
pk,m,σag): takes as input two equal
length vectors, a vector of public keys pk = ( pk1,..., pkn), a vector of messages m =
(m1,...,m n), and an aggregate signature σag. It outputs either accept or reject.
• The scheme is correct if for all pk= (pk1,..., pkn), m= (m1,...,m n), and σ= (σ1,...,σ n),
if V(pki,mi,σi) = accept for i= 1,...,n then Pr
[
VA
(
pk,m,A(pk,σ)
)
= accept
]
= 1.
The length n of all the input vectors is less than some upper-bound parameter N.
The aggregate signature σag output by algorithm Ashould be short. Its length should be about
the same as the length of a single signature, no matter how many signatures are being aggregated.
Notice that anyone can aggregate a given set of signatures using the aggregation algorithm A.
Aggregation requires no knowledge of the secret signing keys and requires no interaction with the
signers. In particular, aggregation can take place long after the signers issued the signatures.
Signature aggregation comes up in many real-world scenarios. For example, in a certiﬁcate chain
containing multiple certiﬁcates issued by diﬀerent authorities, one can aggregate all the signatures
in the chain into a single short aggregate. This shrinks the overall length of the certiﬁcate chain.
Another example is a distributed system where multiple parties periodically sign and publish their
view of the state of the system. These signatures are recorded on a long-term log. The log manager
can compress the log by aggregating all these signatures into a single short aggregate. This eﬃciency
improvement is especially useful in blockchain systems [60].
Before we discuss aggregation security let us ﬁrst look at a simple attempt at aggregating BLS
signatures. Recall that the scheme uses a hash function H : M→ G0. A BLS public key is a group
element pk = gα
1 ∈G1, and a signature is a group element σ ∈G0. Consider the following simple
aggregation procedure A that aggregates n signatures by simply computing their product. More
precisely, the scheme works as follows.
The aggregate signature scheme SABLS = (SBLS,A, VA):
• A
(
pk∈Gn
1 , σ∈Gn
0
):=
{
σag ←σ1 ·σ2 ···σn ∈G0, output σag ∈G0
}
.
• VA
(
pk∈Gn
1 , m∈Mn, σag
)
: accept if
e(σag,g1) = e
(
H(m1),pk1
)
···e
(
H(mn),pkn
)
. (15.10)
To see why the veriﬁcation algorithm accepts an aggregate σag of valid signatures observe that
e(σag,g1) = e(σ1 ···σn, g1) =
n∏
i=1
e(σi,g1) =
n∏
i=1
e
(
H(mi)αi,g1
)
=
=
n∏
i=1
e
(
H(mi),gαi
1
)
=
n∏
i=1
e
(
H(mi),pki
)
so that the equality in (15.10) holds.
630
Remark 15.3. Although we described the aggregation procedure as a one-time process, the BLS
aggregation mechanism can be done incrementally. One can aggregate a number of signatures to
obtain an aggregate. Later aggregate more signatures into the aggregate, and so on. Moreover,
given an aggregate σag and some signature σ that was aggregated into σag, one can remove σ from
the aggregate by computing σag′←σag/σ. 2
Remark 15.4. In some cases we can end up with an aggregate signatureσag that contains multiple
copies of a signature σ. For example, suppose server S1 aggregates signatures from a number of
users and sends the aggregate σag
(1) to server S3. Server S2 aggregates signatures from another set
of users and also sends the aggregate σag
(2) to server S3. Server S3 aggregates the two aggregates
σag
(1) and σag
(2) to obtain the ﬁnal aggregate σag ←σag
(1) ·σag
(2). If Alice sends its signature σ
to both S1 and S2 then σ will be included twice in the ﬁnal aggregate σag. This does not hurt
performance or security, however, a veriﬁer needs to know the multiplicity of every signature in the
aggregate to correctly verify σag using (15.10). 2
Remark 15.5 (Faster veriﬁcation). Verifying an aggregate of n signatures in (15.10) requires
computing n+ 1 pairings. This can be simpliﬁed when all the signed messages in the aggregate are
the same, namely m1 = m2 = ··· = mn = m. This happens when multiple parties sign the same
message m, and there is a need to aggregate all these signatures into a single aggregate σag. In this
case the veriﬁcation equation (15.10) for verifying the aggregate σag simpliﬁes to:
e(σag,g1) ?= e
(
H(m),apk
)
where apk := pk1 ···pkn ∈G1. (15.11)
This requires only two pairings, independent of n. The quantity apk is called an aggregate
public key. If the set of signers in the aggregate is ﬁxed in advance, then this short apk can be
pre-computed, and the veriﬁer does not need to store the keys pk1,..., pkn. 2
15.5.2.1 The rogue public key attack
While the BLS aggregation scheme SABLS works correctly, it is completely insecure as is. To explain
the attack, consider a user Bob whose public key is pkB := uB ∈G1. The adversary wants to make
it seem as if Bob signed some message m ∈M which Bob did not sign. To do so, the adversary
creates a public key pkadv by choosing a random α←R Zq and computing
u←gα
1 , pkadv ←u/uB ∈G1. (15.12)
The private key corresponding to this public key is αadv := α−αB, where αB is Bob’s secret key.
The adversary does not know this secret key, but it can still claim that pkadv is its public key. We
say that pkadv is a rogue public key.
The aggregate public key for pkB and pkadv is apk := pkB ·pkadv = uB ·(u/uB) = u = gα
1 ,
and the adversary knows α ∈Zq. Therefore it can compute σag := H(m)α ∈G0. This σag is a
valid aggregate signature on the message m issued by the public keys pkB and pkadv. Indeed, the
veriﬁcation equation (15.11) holds for σag:
e(σag,g1) = e
(
H(m)α,g1
)
= e
(
H(m),gα
1
)
= e
(
H(m),u
)
= e
(
H(m),apk
)
.
Hence, the adversary created an aggregate signature σag that makes it look as if Bob signed m.
This is a complete break of the scheme.
631
15.5.2.2 Deﬁnition of secure aggregation
Our security deﬁnition must properly model rogue public key attacks as above. In the security
game the adversary is asked to create a valid aggregate signature over a number of public keys,
where the adversary controls all but one of the public keys. Before outputting the aggregate forgery,
the adversary can issue signing queries for the one public key that it does not control.
Attack Game 15.2 (Aggregate signature security). For a given aggregate signature scheme
SA= (G,S,V,A, VA) with message space M, and a given adversary A, the attack game runs as
follows:
• The challenger runs (pk,sk) ←R G() and sends pk to A.
• Aqueries the challenger. For i = 1 ,2,..., the ith signing query is a message
m(i) ∈M. The challenger computes σi ←R S(sk,m(i)), and then gives σi to A.
• Eventually A outputs a candidate aggregate forgery ( pk,m,σag) where pk =
(pk1,..., pkn) and m= (m1,...,m n) ∈Mn.
We say that the adversary wins the game if the following conditions hold:
• VA(pk,m,σag) = accept,
• there is at least one 1 ≤j ≤n such that (1) pkj = pk, and (2) Adid not issue a signing
query for mj, meaning that mj /∈{m(1),m(2),... }.
We deﬁne A’s advantage with respect to SA, denoted ASIG adv[A,SA], as the probability that A
wins the game. 2
Deﬁnition 15.5. We say that an aggregate signature scheme SAis secure if for all eﬃcient
adversaries A, the quantity ASIGadv[A,SA] is negligible.
15.5.2.3 Who signed? Strongly binding aggregation
Some aggregation schemes that are secure against forgery attacks (Deﬁnition 15.5), can be vulner-
able to an issue caused by a group of colluding malicious signers. Let Alice, Bob, Carol, and David
be a group of four signers. Suppose that by working together they can construct four distinct key
pairs (pki,ski) for i= 1,..., 4, a message m, and an aggregate signature σag such that
VA
(
(pk1,pk2), (m,m), σag
)
= VA
(
(pk3,pk4), (m,m), σag
)
= accept.
In other words, by working together the four of them caused a collision: the aggregate signature
σag is a valid signature on m with respect to both pk= (pk1,pk2) and pk′= (pk3,pk4). This can
cause confusion over who signed m: was it Alice and Bob or was it Carol and David? We saw a
similar issue in Section 13.1.1.1 where we discussed signer confusion.
Let us deﬁne a strong form of message and public key binding for signature aggregation.
Attack Game 15.3 (Strong binding). Let SA= ( G,S,V,A, VA) be an aggregate signature
scheme. A strong binding adversary Atakes no input, and outputs ( pk,m,pk′,m′,σag). We say
that Ais successful if
(pk,m) ̸= (pk′,m′) and VA(pk,m,σag) = VA(pk′,m′,σag) = accept.
632
Deﬁne A’s advantage with respect to SA, denoted sbASIGadv[A,SA], as the probability that Ais
successful. 2
Deﬁnition 15.6. We say that an aggregate signature scheme SAis strongly binding if for all
eﬃcient adversaries A, the quantity sbASIGadv[A,SA] is negligible.
The deﬁnition ensures that one cannot ﬁnd an aggregate signature σag that is valid for two
distinct pairs (pk,m) and (pk′,m′).
Exercise 15.11(c) shows that every secure aggregate signature scheme can be made strongly
binding. The cost is a slight increase in the size of the aggregate signature. As we will see, some
secure aggregation schemes are strongly binding without modiﬁcation. For others, if strong binding
is needed, one can apply the augmentation from Exercise 15.11(c).
15.5.3 Secure BLS aggregation
Now that we understand the security requirements let’s see how to enhance the simple aggregation
procedure in (15.10) to properly aggregate BLS signatures. There are two primary methods to
prevent rogue public keys, each designed for a diﬀerent use case. We ﬁrst describe both methods
and then prove their security.
15.5.3.1 Method 1: message augmentation
The simplest approach to securely aggregating BLS signatures is to prepend the signing public
key to every message being signed. Speciﬁcally, the modiﬁed aggregation scheme, denoted SA
(1)
BLS,
is the same as SABLS in (15.10) except that the signing algorithm now uses a hash function H :
G1 ×M→ G0 and is deﬁned as
S(sk,m) := H(pk,m)α where sk = α∈Zq and pk ←gα
1 .
In eﬀect, the message being signed is the pair ( pk,m) ∈G1 ×M. The veriﬁcation and aggre-
gate veriﬁcation algorithms are equally modiﬁed to hash the pairs ( pk,m). Speciﬁcally, aggregate
veriﬁcation works as
VA
(
pk∈Gn
1 , m∈Mn, σag
)
:
accept if e(σag,g1) = e
(
H(pk1,m1),pk1
)
··· e
(
H(pkn,mn),pkn
)
and pki ̸= 1 for all i= 1,...,n .
We prove that this method is secure against forgery attacks in Section 15.5.3.3 below. Exercise 15.10
shows that this method is also strongly binding (Deﬁnition 15.6). The last line in algorithm VA is
needed to ensure strong binding.
15.5.3.2 Method 2: proof of possession of the secret key
A downside of Method 1, where the public key is prepended to every message, is that when ag-
gregating multiple signatures on the same message we cannot take advantage of the veriﬁcation
optimization in Remark 15.5. After the public keys are prepended, the messages being signed are
no longer the same, and this prevents the veriﬁer from taking advantage of the optimized veriﬁca-
tion test in (15.11). Here we show a diﬀerent aggregation method that preserves this optimization,
when the set of public keys is known in advance.
633
Recall that in the rogue public key attack (15.12), the adversary did not know the secret key
corresponding to its rogue public key pkadv. We can therefore prevent the attack by requiring every
signer to prove possession of the secret key corresponding to its public key.
The modiﬁed aggregation scheme, denoted SA
(2)
BLS, is the same as SABLS deﬁned in (15.10)
except that the key generation algorithm also generates a proof π to show that the signer has
possession of the secret key. We attach this proof π to the public key, and it is checked during
aggregate veriﬁcation. In particular, the key generation and aggregate veriﬁcation algorithms use
an auxiliary hash function H′: G1 →G0, and operate as follows:
• G() :=
{
α←R Zq, u ←gα
1 ∈G1, π ←H′(u)α ∈G0
output pk := (u,π) ∈G1 ×G0 and sk := α∈Zq
}
.
• VA
(
pk, m∈Mn, σag
)
: Let pk= (pk1,..., pkn) =
(
(u1,π1),..., (un,πn)
)
be npublic keys,
and let m= (m1,...,m n). Accept if
– valid proofs: e
(
πi,g1
)
= e
(
H′(ui),ui
)
for all i= 1,...,n , and
– valid aggregate: e(σag,g1) = e
(
H(m1),u1
)
···e
(
H(mn),un
)
.
The new term π = H′(u)α ∈G0 in the public key is used to prove that the public key owner is in
possession of the secret key α. This π is a BLS signature on the public key u∈G1, but using the
hash function H′instead of H. The aggregate veriﬁcation algorithm ﬁrst checks that all the terms
π1,...,π n ∈G0 in the given public keys are valid, and then veriﬁes that the aggregate signature σag
is valid exactly as in SABLS.
We prove security of this method against forgery attacks in Section 15.5.3.3 below. When using
this scheme, it is important that the hash function H′ be independent from the hash function H
used to sign messages, otherwise the scheme is insecure (see Exercise 15.8). Of course, in practice,
both H and H′ can be derived from a single hash function like SHA3 using domain separation,
where H(x) := SHA3(0 ∥x) and H′(x) := SHA3(1 ∥x).
Remark 15.6. The proof of possession πdoes not depend on the message being signed. Therefore,
if the same public key pk is used to sign many messages, the veriﬁer need only verify correctness
of π once and record the fact that pk has been validated. There is no need to check π on every
signature veriﬁcation. 2
Remark 15.7. One can slightly modify algorithm G to generate the proof of possession π as
π ←H′(u,id)α, where H′ takes as input u∈G1 along with the identity id of the user who owns
this public key. Algorithm VA is similarly modiﬁed when verifying π. This prevents an attacker
from claiming Alice’s public key pk = (u,π) as its own public key. The attacker does not know
Alice’s secret key, so there is limited harm that can be caused by this public key theft. However,
since including id as an input to H′is cheap, we might as well do it. 2
The beneﬁt of SA
(2)
BLS is that when all the messages are the same, namely m1 = m2 = ··· = mn,
we can take advantage of the veriﬁcation optimization in (15.11) and replace the veriﬁcation of σag
with an equality check that requires only two pairings. Of course, this assumes that all the proofs
of possession have been previously veriﬁed.
To summarize, if one expects to mostly verify aggregate signatures on distinct messages or from
fresh public keys, then SA
(1)
BLS is preferable. If the signatures being aggregated are usually signatures
on the same message, and the set of public keys is known in advance, then SA
(2)
BLS is preferable.
634
Remark 15.8 (aggregating proofs of possession). A veriﬁer often needs to store all the public
keys and signatures it encounters to enable future auditing of its actions. Let
{
pki = (ui,πi)
}n
i=1
be a collection of n public keys. Since the proofs of possession π1,...,π n ∈G0 included in the
public keys are themselves BLS signatures, it is tempting to aggregate all of them into a single value
π:= π1 ···πn ∈G0 as in (15.10) and only store (u1,...,u n,π). This roughly halves the space needed
to store the public keys pk1,..., pkn. Anyone can verify the aggregate proof of possession π by
checking that e(π,g1) = ∏n
i=1 e
(
H′(ui),ui
)
. We would like to claim that if this equality holds, then
an aggregate signature by a subset of the public keys can be trusted. Unfortunately, aggregating
proofs of possession this way invalidates the proof of security for SA
(2)
BLS given in Theorem 15.2
below. In the proof we need every public key to include an explicit proof of possession in non-
aggregated form. Nevertheless, we show in Exercise 15.9 that a slightly modiﬁed argument can
prove that aggregating the proofs of possession π1,...,π n is secure in certain settings. 2
Remark 15.9 (Strong binding). While the scheme SA
(2)
BLS is secure against forgery attacks, it
is not strongly binding in the sense of Deﬁnition 15.6. We explore this in Exercise 15.11 where we
also present a simple ﬁx. Exercise 15.12 gives a very diﬀerent aggregation scheme that is strongly
binding without modiﬁcation, and retains some of the performance beneﬁts of SA
(2)
BLS. 2
15.5.3.3 Proving security of both aggregation methods
Let’s now prove that the two aggregation methods, SA
(1)
BLS and SA
(2)
BLS, presented in the previous
two sections, are secure against forgery attacks as in Deﬁnition 15.5. The two schemes have very
similar security proofs, but the proofs diﬀer crucially at a few points. We prove security of each
scheme, highlighting the places where the proofs diﬀer.
To prove security we need a bit more structure on the pairing e: G0 ×G1 →GT. Speciﬁcally,
let ψ : G1 →G0 be the unique group homomorphism that satisﬁes ψ(g1) = g0. To prove security
we will need ψ to be eﬃciently computable. When e is a symmetric pairing, so that G0 = G1
and g0 = g1, the group homomorphism ψ is the identity function, which is trivially eﬃciently
computable. Other pairing instantiations support an eﬃciently computable group homomorphismψ
from G1 to G0 called the trace map. However, for some optimized pairings, no eﬃciently computable
homomorphism is known from G1 to G0. We will discuss this case in Remark 15.10, after we prove
security.
We are now ready to prove security of SA
(1)
BLS and SA
(2)
BLS.
Theorem 15.2 (Security of SA
(1)
BLS and SA
(2)
BLS). Let e : G0 ×G1 →GT be a pairing and let
ψ: G1 →G0 be an eﬃciently computable group homomorphism satisfying ψ(g1) = g0.
(a) Let H : G1 ×M→ G0 be a hash function. Then the derived SA
(1)
BLS signature aggregation
scheme is secure, assuming co-CDH holds for e, and H is modeled as a random oracle.
(b) Let H : M→ G0 and H′ : G1 →G0 be hash functions. Then the derived SA
(2)
BLS signature
aggregation scheme is secure, assuming co-CDH holds for e, and H and H′ are modeled as
random oracles.
In particular, let A1 be an eﬃcient adversary attacking SA
(1)
BLS, and let A2 be an eﬃcient adver-
sary attacking SA
(2)
BLS, in the random oracle version of Attack Game 15.2. Moreover, assume that
A1 and A2 issue at most Qs signing queries. Then there exist an eﬃcient co-CDH adversary B,
635
where Bis an elementary wrapper around one of A1 or A2, such that for both i= 1,2
ASIGroadv[Ai,SA(i)
BLS] ≤2.72 ·(Qs + 1) ·coCDHadv[B,e]. (15.13)
Proof idea of Theorem 15.2 part (a). First, we sketch the security proof for SA
(1)
BLS. Adversary Bis
given a tuple (u0 = gα
0 , u1 = gα
1 , v0 = gβ
0 ) where α,β ←R Zq, as in the co-CDH attack game. It needs
to compute z0 := gαβ
0 = vα
0 , which is equivalent to ﬁnding a z0 ∈G0 satisfying e(z0,g1) = e(v0,u1).
Adversary Bcomputes z0 by interacting with an aggregate forger A1 for SA
(1)
BLS.
Adversary Bbegins by sending the public key pk := u1 = gα
1 to the aggregate forger A1. Next,
A1 makes a sequence of queries: Qro hash queries to H, and Qs signature queries. Bresponds to
hash queries as in the proof of Theorem 15.1. It ﬁrst chooses a random ωin {1,...,Q ro + 1}. Then
for j = 1,2,... , when A1 issues hash query number j for H(pk(j),m(j)), adversary Bresponds as
follows:
− if j ̸= ω then Bchooses ρj ←R Zq and sets H(pk(j),m(j)) := gρj
0 ,
− if j = ω then Bsets H(pk(ω),m(ω)) := v0.
These responses to hash queries enable Bto respond to A1’s signature queries as in the proof of
Theorem 15.1. Note that if pk(ω) = u1 then Bcannot respond to a signature query for m(ω).
However, if Bguesses ω correctly then A1 will never issue a signature for m(ω), as explained next.
Eventually A1 outputs a valid aggregate forgery:
pk= (˜u1,..., ˜un), m= (m1,...,m n) ∈Mn, σag ∈G0 (15.14)
where σag ∈G0 is a valid aggregate forgery so that
e(σag,g1) = e
(
H(˜u1,m1),˜u1
)
··· e
(
H(˜un,mn),˜un
)
. (15.15)
Moreover, the challenge public key pk = u1 = gα
1 must appear at least once on the right hand side
of (15.15), along with some message m, and A1 never issued a signature query for m.
We know that H(u1,m) was deﬁned either when A1 issued an explicit hash query for H(u1,m),
say at hash query number 1 ≤ν ≤Qro, or it was deﬁned when A1 output the ﬁnal forgery, which
we treat at hash query number ν = Qro + 1. If Bguessed ν correctly at the beginning of the game,
so that ω= ν, then H(u1,m) = H(pk(ω),m(ω)) = v0. Moreover, A1 never issued a signature query
for m(ω), and therefore Bcan correctly answer all of A1’s signature queries.
So, suppose H(u1,m) = v0. Adversary Bneeds to compute the signature σ := H(u1,m)α =
vα
0 = z0. This signature σ is part of the aggregate forgery σag, and Bneeds to extract it from σag.
It does so by dividing out all the signatures in σag until only σ remains. To see how, let us separate
the signatures aggregated into σag into two types. Speciﬁcally, for t= 1,...,n each pair (˜ut,mt) in
(15.14) is one of:
• Type I: ˜ut = u1 and mt = m. In this case, the signature on mt with respect to the public
key ˜ut is σ= z0, which is the signature that Bneeds to compute.
• Type II: ˜ut ̸= u1 or mt ̸= m. In this case Bcan compute itself the signature σt on mt with
respect to the public key ˜ut. To see why, recall that H(˜ut,mt) = gρt
0 and Bknows ρt ∈Zq.
We claim that σt = ψ(˜ut)ρt. Indeed, if ˜ut = gαt
1 for some αt ∈Zq then
σt = ψ(˜ut)ρt = ψ
(
gαt
1
)ρt
= ψ(g1)αt·ρt = gαt·ρt
0 = H(˜ut,mt)αt, (15.16)
and hence σt is the unique signature on mt with respect to ˜ut. Moreover, Bcan compute σt
because ψ is eﬃciently computable.
636
This shows that Bcan compute itself all the signatures that were aggregated into σag, except for
the challenge signature σ.
Let d ≥1 be the number of times that the pair ( u1,m) appears in (15.14), that is, d is the
number pairs of type I. Let s ∈G0 be the product of all the computed signatures σt of type II
above, and note that Bcan eﬃciently compute s. Each σt of type II is a valid signature, and
therefore e(σt,g1) = e
(
H(˜ut,mt),˜ut
)
. Then, by re-ordering terms we can re-write (15.15) as
e(σag,g1) = e
(
H(u1,m),u1
)d ·e(s,g1) = e(v0,u1)d ·e(s,g1) = e(z0,g1)d ·e(s,g1) = e(zd
0s,g1).
This implies that σag = zd
0 ·sand therefore z0 = (σag/s)1/d. Our Boutputs (σag/s)1/d as the required
co-CDH solution.
This proof strategy works ﬁne, but incurs a factor ( Qro + 1) loss in B’s success probability,
because Bneeds to guess ν correctly at the beginning of the game. This loss can be reduced to
2.72 ·(Qs + 1) as claimed in (15.13) using Exercise 15.5, as we did in Theorem 15.1. 2
Proof idea of Theorem 15.2 part (b). Next, we sketch the proof of security for SA
(2)
BLS. Adversary B
is given a tuple ( u0 = gα
0 , u1 = gα
1 , v0 = gβ
0 ) where α,β ←R Zq, as in the co-CDH attack game. It
needs to compute z0 := gαβ
0 = vα
0 by interacting with an aggregate forger A2 for SA
(2)
BLS.
Adversary Bbegins by sending the public keypk := (u1,π) to the forgerA2, where π:= H′(u1)α.
To compute this π, Bdeﬁnes H′(u1) := gτ
0 for τ ←R Zq, and sets π←uτ
0. Then π= H′(u1)α.
Next, the forger makes a sequence of signature queries and hash queries to H and H′.
• H query: Bresponds to H queries as in the proof of Theorem 15.1. First, Bchooses a random
ω in {1,...,Q ro + 1}. Next, for j = 1,2,... it responds to query number j for H(m(j)) by
choosing ρj ←R Zq and setting H(m(j)) := gρj
0 . For query number ω, it behaves diﬀerently
and instead Bsets H(m(ω)) := v0. These responses to hash queries enables Bto respond to
A2’s signature queries as in the proof of Theorem 15.1.
• H′query: Bresponds to a query for H′(u), where u̸= u1, by choosing a random ζ ←R Zq and
setting H′(u) := v0 ·gζ
0. Recall that H′(u1) has already been deﬁned as H′(u1) := gτ
0 .
Eventually A2 outputs a valid aggregate forgery:
pk=
(
(˜u1,˜π1),..., (˜un,˜πn)
)
, m= (m1,...,m n) ∈Mn, σag ∈G0 (15.17)
where
e(˜πt,g1) = e
(
H′(˜ut),˜ut
)
for all t= 1,...,n , and (15.18)
e(σag,g1) = e
(
H(m1),˜u1
)
··· e
(
H(mn),˜un
)
. (15.19)
Moreover, the challenge public key pk = ( u1,π) is one of the public keys in pk. Let m be the
corresponding message in the tuple m, then A2 never issued a signature query for m.
As in the proof of part (a), suppose that H(m) was deﬁned in hash query number ν, where
1 ≤ν ≤Qro + 1, and that Bguessed ν correctly at the beginning of the game, so that ω= ν. Then
H(m) = H(m(ω)) = v0.
Adversary Bnow needs to extract a signature σ := H(m)α = vα
0 = z0 from the aggregate
forgery σag. It does so by dividing out all the signatures in σag until only σremains. For t= 1,...,n
each pair (pkt,mt) =
(
(˜ut,˜πt),mt
)
in (15.17) is one of three types:
637
• type I: mt = mand ˜ut = u1 = gα
1 . In this case, the signature on mt with respect to the public
key (˜ut,˜πt) is σ= z0, which is the signature that Bneeds to compute.
• type II: mt ̸= m. In this case Bcan compute the signature σt on mt with respect to the
public key pkt. To see why, recall that H(mt) = gρt
0 and Bknows ρt ∈Zq. Then Bcomputes
σt := ψ(˜ut)ρt. This σt ∈G0 is the signature on mt under pkt, as shown in (15.16).
• type III: mt = m but ˜ut ̸= u1. We show that in this case Bcan also compute the signature
σt on mt with respect to the public key pkt = (˜ut,˜πt). This is where the proof ˜πt is used, and
is the main diﬀerence from the proof of security for SA
(1)
BLS. Recall that H′(˜ut) = v0 ·gζt
0 and
Bknows ζt ∈Zq. Adversary Bcomputes σt as σt := ˜πt/ψ(˜ut)ζt ∈G0.
We claim that this σt is the signature on mt with respect to pkt. Indeed, suppose ˜ut = gαt
1
for some αt ∈Zq. Then by (15.18) we know that ˜πt = H′(˜ut)αt, and therefore
σt = ˜πt
ψ(˜ut)ζt
= H′(˜ut)αt
ψ(gαt
1 )ζt
=
(
v0 ·gζt
0
)αt
gαt·ζt
0
= vαt
0 = H(m)αt = H(mt)αt.
Hence σt is the unique signature on mt with respect to ˜ut.
This shows that Bcan compute itself all the signatures that were aggregated into σag, except for
the challenge signature σ.
Let d ≥1 be the number of times that the pair ( u1,m) appears in (15.17), that is, d is the
number pairs of type I. Let s ∈G0 be the product of all the computed signatures σt of types II
and III above, and note thatBcan eﬃciently computes. Each σt of type II or III is a valid signature,
and therefore e(σt,g1) = e
(
H(mt),˜ut
)
. Then, by re-ordering terms we can re-write (15.19) as
e(σag,g1) = e
(
H(m),u1
)d ·e(s,g1) = e(v0,u1)d ·e(s,g1) = e(z0,g1)d ·e(s,g1) = e(zd
0s,g1).
From this it follows that σag = zd
0 ·s, and therefore z0 = (σag/s)1/d. Then Boutputs (σag/s)1/d as
the required co-CDH solution.
This proof strategy works ﬁne, but incurs a factor ( Qro + 1) loss in B’s success probability,
because Bneeds to guess ν correctly at the beginning of the game. This loss can be reduced to
2.72 ·(Qs + 1) as claimed in (15.13) using Exercise 15.5, as we did in Theorem 15.1. 2
Remark 15.10. The proof of Theorem 15.2 uses the homomorphism ψ : G1 →G0 to extract a
signature forgery from the adversary. As mentioned in the discussion before the theorem, some
pairing instantiations naturally support the required ψ. For other instantiations, including the
ones used in practice, there is no known eﬃciently computable ψ. We can still prove security,
but we need a stronger version of the co-CDH problem. In particular, we prove security under
the, so called, ψ-co-CDH assumption, which states that the co-CDH problem is diﬃcult even for an
eﬃcient adversary that is given access to an oracle for the functionψ. In the proof of Theorem 15.2,
adversary Bwould call this oracle every time it needs to evaluate ψ. 2
Remark 15.11. Theorem 15.2 shows that an aggregate signature will not verify unless all the
honest signers contributed a valid signature. However, this does not mean that all the inputs
to the aggregation algorithm A were valid signatures. Here is an example. Let ( pk1,m1,σ1)
and (pk2,m2,σ2) be valid signature triples, where σ1,σ2 ∈G0. Then the aggregation algorithm
A
(
(pk1,pk2),(σ1,σ2)
)
outputs the aggregate signature σ = σ1 ·σ2. Clearly the algorithm will
638
output exactly the same σ if the pair ( σ1,σ2) were replaced by the pair ( δσ1,δ−1σ2) for some
δ ∈G0. When δ ̸= 1, the triples ( pk1, m1, δσ1) and ( pk2, m2, δ−1σ2) are not valid signature
triples, yet their aggregation gives a valid aggregate signature. 2
15.5.4 Signature schemes secure without random oracles
To conclude our discussion of pairing-based signatures, we show how to use pairings to construct
signature schemes whose security does not rely on the random oracle model. In particular, when
the message being signed is short, there is no need to hash it before signing or verifying, and this
can be useful in certain settings (e.g., when proving knowledge of a message-signature pair in zero
knowledge, as discussed in Chapter 20).
There are several ways to use pairings to construct signatures schemes based on co-CDH and
related assumptions, without the random oracle model. Here we give two constructions that rely
on a stronger assumption than co-CDH, but whose security proof is short and direct. The ﬁrst
scheme requires the signer to use a PRF. The second scheme requires no symmetric primitives when
signing short messages.
As usual, let e: G0 ×G1 →GT be a pairing where G0,G1,GT are cyclic groups of prime order q
with generators g0 ∈G0 and g1 ∈G1. Let F be a PRF deﬁned over ( K,Zq,Zq). We construct a
signature scheme SG = (G,S,V ) with message space M:= Zq.
The signature scheme SG = (G,S,V ):
• G(): key generation runs as follows
α,β ←R Zq, k ←R K, u1 ←gα
1 , v0 ←gβ
0 , hT ←e(v0,g1) ∈GT.
The public key is pk := (u1,hT) ∈G1 ×GT. The secret key is sk := (α,β,k ) ∈Z2
q ×K.
• S(sk,m) : To sign a message m∈Zq using a secret key sk = (α,β,k ), do:
r←R F(k,m) ∈Zq, w0 ←g(β−r)/(α−m)
0 , output σ←(r,w0) ∈Zq ×G0.
Note that w0 is undeﬁned when m= α. In that case we set w0 ←1 and r←β.
• V(pk,m,σ ): To verify a signature σ = (r,w0) ∈Zq ×G0 on a message m ∈Zq, using the
public key pk = (u1,hT), output accept if
e
(
w0, u1g−m
1
)
·e(g0,g1)r = hT. (15.20)
A valid signature (r,w0) is always accepted because
e
(
w0, u1g−m
1
)
= e
(
g(β−r)/(α−m)
0 , gα−m
1
)
= e
(
g0,g1)β−r = hT ·e(g0,g1)−r,
which is what (15.20) checks for. The PRF used during signing is needed to ensure that the signer
returns the same signature when asked to sign the same message multiple times. This is needed in
the proof of security, and also for security of the scheme (see Exercise 15.15). Veriﬁcation does not
use the PRF.
To prove security we will need an assumption called d-CDH, or power CDH, which is much
stronger than the basic CDH. We deﬁne this assumption next.
639
Attack Game 15.4 ( d-CDH). For a given adversary Aand a positive integer d, the attack game
runs as follows.
• The challenger chooses α←R Zq and h1 ←R G1, and gives the adversary the tuple
(
gα
0 , g(α2)
0 , ..., g(αd)
0 , g α
1 , h 1, h(αd+2)
1
)
. (15.21)
Let z:= e(g0,h1)(αd+1) ∈GT.
• The adversary outputs some ˆz∈GT.
We deﬁne A’s advantage in solving the d-CDH problem for e, denoted PCDHadv[A,e,d ], as
the probability that ˆz= z. 2
Deﬁnition 15.7 ( d-CDH assumption). Let d be a positive integer. We say that the d-CDH
assumption holds for e if for all eﬃcient adversaries Athe quantity PCDHadv[A,e,d ] is negligible.
This assumption is valid as long as d is much smaller than q, the order of the groups. In
Exercise 16.3 we show that, for some q, the data provided in (15.21) can reduce the work to ﬁnd
α by a factor of
√
d, compared to the work to compute discrete log in G0. In our application, d is
about the number of signature queries from the adversary, which is a relatively small number (say,
less than 240). Out of an abundance of caution, one can slightly increase the size of qto compensate
for this
√
d loss. Alternatively, one can choose a pairing e so that Exercise 16.3 does not apply in
the group G0.
The d-CDH assumption, and its variants, are used in many cryptographic constructions. The
following observation plays a central role in its applications, and in particular, in proving security
of the signature scheme SG. Let P(X) := ∑d′
i=0 γi ·Xi ∈Zq[X] be a polynomial of degree d′≤d.
Then using the data in (15.21) we can construct the term gP(α)
0 ∈G0 by observing that:
gP(α)
0 = g
∑d′
i=0 γi·αi
0 =
d′
∏
i=0
(
g(αi)
0
)γi
. (15.22)
All the terms on the right hand side of (15.22) are provided in (15.21), and hence gP(α)
0 can be
calculated using this expression. In other words, we can evaluate a low-degree polynomial P at α,
without knowledge of α.
We next prove security of the signature schemeSG using the d-CDH assumption, where d= Q+1
and Q is the number of signature queries from the adversary.
Theorem 15.3. Let e : G0 ×G1 →GT be a pairing for which d-CDH holds for poly-bounded d.
Let F be a secure PRF over (K,Zq,Zq). Then SG is a strongly secure signature scheme.
In particular, let Abe an adversary attacking SG as in Attack Game 13.2. Moreover, assume
that Aissues at most Q signing queries. Then there exists a (Q+ 1)-CDH adversary B1 and a
PRF adversary B2, where B1 and B2 are elementary wrappers around A, such that
stSIGadv[A,SG] ≤PCDHadv[B1,e, (Q+ 1)] + PRFadv[B2,F] + (1/q). (15.23)
640
Proof idea. Algorithm B1 will use Ato break the ( Q+ 1)-CDH assumption. However, to do so,
algorithm B1 must answer all of A’s signature queries, which means that B1 must know a signature
on most messages in the message space. But then the signature forgery ( m,σ) produced by Ais of
no use to B1 because B1 likely already knows a signature on m.
Here we resolve this obstacle by having B1 give Aa certain public key pk := (u1,hT), where
B1 knows exactly one valid signature (rm,wm) for every message m∈Zq. It will use these known
signatures to answer A’s signature queries. If Arepeatedly asks for a signature on the same
message m′, our B1 must respond with the same signature every time, because it only knows one
signature on m′. This is the reason why we use a PRF to derandomize the signing process; it
ensures that our B1 behaves as a real signer. Eventually, Aoutputs its signature forgery ( m,σ).
We will show that, with overwhelming probability, this σ is a signature on mthat is diﬀerent from
the one that B1 knows for m. Hence, B1 learned a new signature on m that it did not previously
know. We will show that this fresh σ lets Bsolve the given (Q+ 1)-CDH challenge. 2
Proof sketch. We ﬁrst modify the challenger in Attack Game 13.2 by replacing the PRF F by a
random function f : Zq →Zq. We now construct an adversary B1 that plays the role of challenger
to Aand solves the given ( Q+ 1)-CDH instance. For α←R Zq, this B1 takes as input
gα
0 , g(α2)
0 , ..., g(α(Q+1))
0 , g α
1 , h 1, h(α(Q+3))
1 (15.24)
and needs to compute e(g0,h1)(α(Q+2)). Our B1 begins by preparing the following data:
• B1 chooses a random polynomial P(X) = ∑Q+1
i=0 γi·Xi ∈Zq[X] of degree (Q+ 1) by choosing
its coeﬃcients γ0,...,γ Q+1 at random in Zq.
• The quantity P(α) will play the role of β ∈ Zq in the signature scheme. B1 computes
v0 ←gP(α)
0 using (15.22). It sets hT ←e(v0,g1) and sets u1 ←gα
1 taken from (15.24).
Next, B1 runs adversary Agiving it the public key pk = (u1,hT) prepared above. B1 will emulate
the random function f : Zq →Zq internally. Now Aissues a sequence of signature queries. For
j = 1,...,Q adversary B1 responds to a signature query for message mj ∈Zq as follows:
• Let rj := P(mj) ∈Zq. Then the polynomial ˆP(X) := P(X) −rj satisﬁes ˆP(mj) = 0. Hence,
ˆP(X) can be written as ˆP(X) = (X−mj) ·p(X) for some polynomial p∈Zq[X] of degree Q.
Our B1 can eﬃciently calculate the coeﬃcients of p(X) by dividing the polynomial ˆP by
(X−mj).
• B1 computes wj ←gp(α)
0 using (15.22) and sends the signature σj := ( rj,wj) to A. This
implicitly deﬁnes the random function f as f(mj) := rj.
To see that σj is a valid signature on mj, ﬁrst observe that the polynomial p(X) satisﬁes p(X) =
(P(X) −rj)/(X−mj). Then wj = gp(α)
0 = g(P(α)−rj)/(α−mj)
0 , as required.
Second, we need to argue that rj is uniform in Zq and is independent of the adversary’s current
view. This follows directly from the fact that P is a random polynomial of degree Q+1. Evaluating
P at Q+2 distinct points results inQ+2 random and independent values inZq. The public key gives
the adversary one point ( α,P(α)) on P, and every signature reveals one more point ( mj,P(mj))
on P. Since the adversary makes at most Q signing queries, it sees at most ( Q+ 1) points on P,
and therefore all are uniform in Zq and independent of everything else.
641
Eventually A outputs a valid forgery ( m,σ) where σ = ( r,w). We know that w =
g(P(α)−r)/(α−m)
0 . If P(m) = r then B1 aborts and terminates. In this case the forgery ( m,σ)
is the signature that B1 already knew for m, so it learned nothing new from A. We argue that
this failure event happens with probability at most 1 /q, which is the reason for the 1 /q term in
(15.23). There are three cases. (1) If m is one of the signing queries, say m = mj, then clearly
r ̸= rj = P(m); otherwise the forgery is invalid. (2) If m = α then B1 can trivially solve the
(Q+ 1)-CDH challenge. (3) Otherwise, P(m) is point number Q+ 2 on the polynomial P, and is
therefore independent of the adversary’s view, which means that r= P(m) holds with probability
at most 1/q, as claimed.
Now, suppose P(m) ̸= r with w = g(P(α)−r)/(α−m)
0 and m ̸= α. This is information that B1
could not compute itself. It can use (r,w) to calculate the required e(g0,h1)(α(Q+2)) using a sequence
of elementary polynomial manipulations. We show how in Exercise 15.14. 2
15.5.4.1 A diﬀerent construction
We next describe a diﬀerent pairing-based signature scheme that is also secure without random
oracles, but this time without relying on a PRF in the signing algorithm. We obtain a signature
scheme where both signing and veriﬁcation are simple algebraic algorithms that require no sym-
metric primitives, when signing short messages. This can be useful in certain settings (e.g., when
the signing process is a distributed protocol among multiple parties, as discussed in Chapter 23).
The proof of security for this scheme is quite diﬀerent from the previous section, and is the topic
of Exercise 15.16.
The signature scheme SBB = (G,S,V ) with message space M:= Zq:
• G(): key generation runs as follows
α,β ←R Zq, u1 ←gα
1 , v1 ←gβ
1 , h 0 ←R G0, hT ←e(h0,g1) ∈GT.
The public key is pk := (u1,v1,hT) ∈G2
1 ×GT. The secret key is sk := (α,β,h 0) ∈Z2
q ×G0.
• S(sk,m) : To sign a message m∈Zq using a secret key sk = (α,β,h 0), do:
r←R Zq, w0 ←h1/(α+rβ+m)
0 , output σ←(r,w0) ∈Zq ×G0.
If α+ rβ+ m= 0 then repeat this process with a fresh random r←R Zq. Alternatively, choose
a random r←R Zq where r̸= {m−α
β }.
• V(pk,m,σ ): To verify a signature σ = (r,w0) ∈Zq ×G0 on a message m ∈Zq, using the
public key pk = (u1,v1,hT), output accept if
e
(
w0, u1vr
1gm
1
)
= hT. (15.25)
One can verify that a valid signature is always accepted. This is because the term u1vr
1gm
1 in
(15.25) is equal to gα+rβ+m
1 . When paired with w0 = h1/(α+rβ+m)
0 from a valid signature, the result
is e(h0,g1) = hT, which is what (15.25) checks for.
We prove security of this signature scheme using the d-CDH assumption, where d = Q is the
number of signature queries from the adversary.
642
Theorem 15.4. Let e : G0 ×G1 →GT be a pairing for which d-CDH holds for poly-bounded d.
Then SBB is a strongly secure signature scheme.
In particular, let Abe an adversary attacking SBB as in Attack Game 13.2. Moreover, assume
that Aissues at most Q signing queries. Then there exists a Q-CDH adversary B, where Bis
an elementary wrapper around A, such that
stSIGadv[A,SBB] ≤2 ·PCDHadv[B,e,Q ] + (Q/q). (15.26)
Proof idea. We construct Bthat runs Agiving it a certain public key pk := (u1,v1,hT). Our Bwill
know Qvalid signatures for every messagem∈M, and this enables it to answer all ofA’s signature
queries. If Aissues a repeated query for the same message, our Banswers with a diﬀerent random
signature every time. Eventually, Aoutputs its signature forgery ( m,σ). We will show that with
probability at least 1/2, this σ is a signature on mthat Bdid not previously know. Moreover, this
σ lets Bsolve the Q-CDH challenge. The complete proof is the topic of Exercise 15.16. 2
15.6 Advanced encryption schemes from pairings
We now turn to encryption and key exchange schemes from pairings. Using pairings we construct
encryption schemes with new properties that cannot be constructed eﬃciently in cyclic groups
without a pairing. We begin by describing identity based encryption, and in the next section
we discuss its extreme generalization called functional encryption.
15.6.1 Identity based encryption
When Alice wants to send an encrypted message to Bob, she must ﬁrst obtain Bob’s current public
key. She either asks Bob for his public key certiﬁcate, or looks up Bob’s public key in a public key
directory. Either way, there is a communication round trip from Alice before Alice can send Bob
an encrypted message. Identity based encryption, or IBE, is a method to save this initial round
trip. Beyond key exchange, IBE can also be used to construct chosen ciphertext secure public key
encryption, and can be used for searching on encrypted data, as we will see in Section 15.6.4.
In an IBE scheme any string can function as a public key: Bob’s email address is a public key,
the current date is a public key, even the numbers 1, 2, and 3 are all public keys. If Bob uses his
email address as his public key, then Alice can encrypt an email to Bob without an initial round
trip to look up Bob’s public key. Knowledge of Bob’s email address is suﬃcient to send Bob an
encrypted email.
In more detail, in an identity based encryption scheme, or IBE, there is a trusted entity,
that we call Tracy, who generates a master key pair ( mpk,msk). The key mpk is called a master
public key and is known to everyone. Tracy keeps the master secret key msk to herself. When
Bob wants to use his email address as his public key, he must somehow obtain the private key,
which is derived from msk and Bob’s public key.
We refer to Bob’s email address as his (public) identity id and denote the corresponding private
key by skid . Bob obtains skid by contacting the trusted party Tracy. He ﬁrst proves his identity
id to Tracy, by proving that he owns the email address in question. This is done using a protocol
that is outside of the cryptosystem. Once Tracy is convinced that Bob owns the identity id, she
uses msk and id to generate skid , and sends this key to Bob over a secure channel. Now Bob can
use skid to decrypt all ciphertexts encrypted to his identity id. Alice, and everyone else, can send
643
encrypted emails to Bob without ﬁrst looking up his public key. This is assuming Alice and others
already have the global master public key mpk.
Before we explain how to use such a scheme, let us ﬁrst deﬁne IBE more precisely.
Deﬁnition 15.8. An identity based encryption scheme Eid = (S,G,E,D ) is a tuple of four
eﬃcient algorithms: a setup algorithm S, a key generation algorithm G, an encryption
algorithm E, and a decryption algorithm D.
• S is a probabilistic algorithm invoked as (mpk,msk) ←RS(), where mpk is called the master
public key and msk is called the master secret key for the IBE scheme.
• G is a probabilistic algorithm invoked as sk id ←R G(msk,id), where msk is the master secret
key (as output by S), id ∈ID is an identity, and sk id is a secret key for id.
• E is a probabilistic algorithm invoked as c←RE(mpk,id,m).
• D is a deterministic algorithm invoked as m←D(skid ,c). Here m is either a message, or a
special reject value (distinct from all messages).
• As usual, we require that decryption undoes encryption; speciﬁcally, for all possible outputs
(mpk,msk) of S, all identities id ∈ID, and all messages m, we have
Pr
[
D
(
G(msk,id), E(mpk,id,m)
)
= m
]
= 1.
• Identities are assumed to lie in some ﬁnite identity space ID, messages in some ﬁnite
message space M, and ciphertexts in some ﬁnite ciphertext space C. We say that the
IBE scheme Eid is deﬁned over (ID,M,C).
We can think of IBE as a special public key encryption scheme where the messages to be
encrypted are pairs ( id,m) ∈ID×M . The master secret key msk enables one to decrypt all well
formed ciphertexts. However, there are weaker secret keys, such as skid , that can only decrypt
some well formed ciphertexts: skid can decrypt a ciphertext c←R E(mpk,id′,m) only if id = id′.
Another way to think of IBE is as a public key system where exponentially many public keys
have been generated. If the identity space is ID:= {1,2,...,N }for some large N, say N = 2128,
then the N public keys can be written as ( mpk,1), (mpk,2),..., (mpk,N). In a regular public key
system a list of N public keys cannot be compressed, and will take space O(N) to store. In an
IBE, these N public keys can be compressed so that only a short mpk needs to be stored.
If a company decides to use IBE for its internal encrypted email system, most likely an employee
Bob will not use his email address, id, as his IBE public key. The reason is that if Bob’s secret key
skid were stolen, Bob would need to change his email address to ensure that skid cannot decrypt
future incoming emails. This is undesirable.
Instead, Bob could use the identity string (id, today’s date) as his IBE public key. Anyone who
knows Bob’s id, and the current date, can send encrypted emails to Bob without ﬁrst looking up
his public key. In eﬀect, Bob’s public key changes every day. Every morning Bob would talk to
the trusted entity Tracy to obtain his daily secret key. He can then decrypt all incoming emails for
that day, without contacting Tracy. This setup has two interesting properties:
• First, if Bob leaves the company and his access needs to be revoked, Tracy could be instructed
to stop issuing new keys for Bob. This prevents Bob from decrypting messages sent after his
last day in the oﬃce.
644
• Second, Alice can encrypt an email to Bob using the public key (id, today’s date + one year ).
Since Tracy will not release the corresponding secret key for a year, Bob will only get to read
the email a year from today. In eﬀect, Alice can send mail to the future.
With some engineering, these capabilities can also be implemented using regular public key en-
cryption. However, it either requires that Tracy participate in every decryption, or requires an
additional sender round-trip to lookup a daily public key for Bob. Saving a round trip in a key
exchange protocol is quite desirable, as we will see when we discuss real-world key exchange in
Section 21.10.
It is important to point out that using IBE for key management results in a diﬀerent trust model
than a traditional certiﬁcate authority (CA). When using an IBE, the trusted entity Tracy, who
holds msk, can decrypt everyone’s messages. An honest CA in a standard public key system cannot
do that. As such, IBE is not always appropriate. It is well suited for a corporate environment where
operational requirements already mandate a trusted entity that is capable of accessing all data.
IBE is not appropriate in settings that require end-to-end security where only the end points should
have access to the plaintext. Nevertheless, Tracy’s power can be greatly reduced by secret sharing
her msk across multiple parties. All, or most, of the parties would need to collude in order to
inappropriately use msk, as discussed in Exercise 22.13.
IBE has several important applications beyond public key management. We will explore these
in Section 15.6.4.
Secure identity based encryption. We next deﬁne the notion of semantic security for an IBE
scheme. We will discuss several variations on this basic notion, including chosen-ciphertext security,
after the basic deﬁnition.
The basic security deﬁnition considers an adversary who obtains the secret keys for a number
of identities of its choice. The adversary should not be able to break semantic security for some
other identity of its choice for which it does not have the secret key.
Attack Game 15.5 (semantic security). For a given IBE scheme Eid = (S,G,E,D ), deﬁned
over (ID,M,C), and for a given adversary A, we deﬁne two experiments.
Experiment b (b= 0,1):
• The challenger computes (mpk,msk) ←R S(), and sends mpk to the adversary A.
• Athen makes a series of queries to the challenger. Each query can be one of two types:
– Key query: for j = 1,2,..., the jth key query consists of an identity id′
j ∈ID. The
challenger computes the secret key sk′
j ←G(msk,id′
j), and sends sk′
j to A.
– Encryption query: a query consists of a triple ( id,m0,m1) ∈ID×M 2, where m0 and
m1 are messages of the same length. The challenger computes c←R E(mpk,id,mb) and
sends c to A.
We restrict the adversary by requiring that it issue at most one encryption query (id,m0,m1),
but this can be relaxed as explained in Remark 15.12 below. Moreover, we require that the
adversary never issue a key query for the identity id used in its encryption query.
• At the end of the game, the adversary outputs a bit ˆb∈{0,1}.
645
Let Wb be the event that Aoutputs 1 in Experiment b. Deﬁne A’s advantage with respect to Eid
as
SSadv[A,Eid] :=
⏐⏐⏐Pr[W0] −Pr[W1]
⏐⏐⏐. 2
Deﬁnition 15.9 (semantic security). An IBE scheme Eid is semantically secure if for all
eﬃcient adversaries A, the value SSadv[A,Eid] is negligible.
This notion of semantic security is sometimes called adaptively secure IBE , because the
adversary can choose the challenge identity id adaptively, after seeing many secret keys of its
choice.
Remark 15.12. We can allow the adversary in Attack Game 15.5 to issue multiple encryption
queries, but this does not give the adversary more power. An IBE scheme that is secure against an
adversary that can only issue a single encryption query is also secure against an adversary that can
issue multiple encryption queries. The proof of this statement is a simple hybrid argument that is
almost identical to the proof of Theorem 11.1. 2
15.6.2 Related security notions
In addition to semantic security, as in Deﬁnition 15.9, there are a number of related security
requirements for IBE. We describe three requirements here: private IBE, selectively secure IBE,
and chosen ciphertext secure IBE.
15.6.2.1 Private IBE
A private IBE is a stronger requirement for IBE security. In a private IBE, an eavesdropper who
has mpk and intercepts a ciphertext c←R E(mpk,id,m), cannot learn the identity id of the target
recipient. We will see a number of applications for this stronger notion in Section 15.6.4. Private
IBE is sometimes called anonymous IBE. We capture this stronger IBE security property by
enhancing the adversary’s encryption query in Attack Game 15.5.
Attack Game 15.6 (Private IBE). For a given IBE scheme Eid = ( S,G,E,D ) deﬁned over
(ID,M,C), and for a given adversary A, the game proceeds as in Attack Game 15.5. The only
diﬀerence is that in the single encryption query from A, the adversary submits two pairs ( id0,m0)
and (id1,m1) in ID×M , where m0 and m1 are equal length messages. The challenger computes
c←R E(mpk,idb,mb) and sends c to A. As usual, we require that the adversary never issues a key
query for either id0 or id1.
Let PrSSadv[A,Eid] be A’s advantage in winning this private IBE game. 2
To see that this enhanced game captures the privacy property we want, observe that if the
challenge ciphertext reveals the target identity, then the attacker can easily predict the bit b and
win the game. Attack Game 15.5 is a special case of this enhanced game where we require that
id0 = id1 in the encryption query.
Deﬁnition 15.10 (private IBE). An IBE scheme Eid is said to be semantically secure and
private if for all eﬃcient adversaries A, the value PrSSadv[A,Eid] is negligible.
646
15.6.2.2 Selectively secure IBE
Selective security is a weaker notion of IBE security where the adversary is restricted in how it
issues its encryption query. Speciﬁcally, we modify Attack Game 15.5 so that the adversary must
choose the challenge id at the beginning of the game, before receiving mpk from the challenger.
Attack Game 15.7 (selective semantic security). For a given IBE scheme Eid = (S,G,E,D )
deﬁned over (ID,M,C), and for a given adversary A, the game begins with Asending id ∈ID to
the challenger. Next, the challenger computes ( mpk,msk) ←R S() and sends mpk to A. From then
on the game proceeds as in Attack Game 15.5. However, the adversary’s encryption query is just
a pair of equal length messages m0,m1 ∈M, and the challenger’s response is c←R E(mpk,id,mb),
where id is from the adversary’s ﬁrst message at the beginning of the game. Everything else is
unchanged.
For an adversary Aattacking an IBE scheme Eid, we let SelSSadv[A,Eid] denote A’s advantage
in winning this selective security game against Eid. 2
Deﬁnition 15.11 (selective security). An IBE scheme Eid is selectively secure if for all
eﬃcient adversaries A, the value SelSSadv[A,Eid] is negligible.
In Attack Game 15.7 the adversary must choose the challenge identity before it sees the IBE
master public key mpk. Because we restrict the adversary’s power in this way, it can be easier to
construct a selectively secure IBE than an adaptively secure IBE. As we will see, selective security
is suﬃcient for some applications (see Section 15.6.4).
An IBE scheme that is selectively secure can be enhanced into one that is adaptively secure
as in Deﬁnition 15.9. To do so we need a hash function H : ID′ →ID. Let Eid = (S,G,E,D )
be a selectively secure IBE scheme with identity space ID. Construct an enhanced IBE scheme
E′
id = (S,G′,E′,D) with identity space ID′where G′and E′operate as follows:
G′(msk,id) := G
(
msk,H(id)
)
, E ′(skp,id,m) := E
(
skp,H(id),m
)
. (15.27)
Algorithms Sand Dare unchanged. The following theorem shows thatE′
id satisﬁes adaptive security
as in Deﬁnition 15.9. The theorem shows that to construct a semantically secure IBE, in the random
oracle model, it suﬃces to ﬁrst construct a selectively secure scheme and then apply (15.27).
Theorem 15.5. Let Eid = (S,G,E,D ) be a selectively secure IBE where |ID|is super-poly. Then
E′
id = (S,G′,E′,D) is a secure IBE as in Deﬁnition 15.9, when H is modeled as a random oracle.
In particular, let Abe an adversary attacking E′
id as in Attack Game 15.9. Moreover, assume
that Aissues at most Qs key queries and at most Qro queries to H. Then there exists a selective
IBE adversary B, where Bis an elementary wrapper around A, such that
SSroadv[A,E′
id] ≤(Qro + 1) ·SelSSadv[B,Eid] +
(
Qs/|ID|
)
. (15.28)
Proof sketch. The selective IBE adversary Bbegins by choosing a random ω ←R {1,...,Q ro + 1}
and a random idch ←R ID. It sends idch as the challenge identity to its challenger and gets back an
mpk for E′
id. Next, Bplays the role of an IBE challenger to A, and starts by giving it mpk. Now,
Aissues three types of queries: H queries, key queries, and a single encryption query.
• When Aissues query number j for H(id′
j), our Bresponds by (consistently) mapping H(id′
j)
to a random element in ID. The only exception is query number ω where Binstead deﬁnes
H(id′
ω) := idch.
647
• When Aissues key query number j for id′
j ∈ID ′, our Bsets idj ←H(id′
j). If H(id′
j) is
not yet deﬁned then Bdeﬁnes H(id′
j) := idj for a random idj ←R ID. If idj ̸= idch then B
issues a key query to its own selective challenger for idj, and sends the response back to A.
However, if idj = idch then Bis not allowed to query its challenger for this key. In this case
Baborts and terminates. This failure event happens with probability 1 /|ID|per query.
• When Aissues its single encryption query for ( id′,m0,m1), our Bﬁrst issues an internal
query for id ←H(id′). If H(id′) was deﬁned in hash query number ωthen id = idch. Then B
forwards (m0,m1) to its selective IBE challenger, and sends the response c←R E(mpk,id,mb)
to A. Here b∈{0,1}is the challenger’s challenge bit.
Eventually Boutputs the bit ˆb∈{0,1}that Aoutputs when it terminates. By a union bound, the
probability that Baborts as a result of a key query is at most Qs/|ID|, which is the reason for the
additive error term in (15.28). Bcorrectly guesses the H query that Awill use for its encryption
query with probability 1/(Qro + 1), which is the reason for the multiplicative term in (15.28). Note
that an identity that is used in a key query cannot be used in the encryption query, which is why
the multiplicative term is not 1 /(Qro + Qs + 1). 2
15.6.2.3 Chosen-ciphertext secure IBE
The third IBE security deﬁnition we consider is chosen-ciphertext secure IBE. This is a stronger
notion of security where we allow the adversary in Attack Game 15.5 to also issue decryption queries.
A decryption query is a pair ( id′,c′). The challenger responds by computing skid ←R G(msk,id′)
and m′←D(skid ,c′), and sends m′to the adversary. As usual, the adversary can issue a decryption
query for all pairs in ID×C except for the pair ( id,c) associated with the adversary’s encryption
query.
There are a number of generic transformations that transform a semantically secure IBE into
a chosen ciphertext secure IBE [16]. These transformations are similar to the generic method
described in Section 12.6 for transforming a semantically secure public key encryption scheme into
one that is chosen ciphertext secure. We describe one such transformation in Exercise 15.18.
15.6.3 Identity based encryption from pairings
We now turn to constructing IBE schemes. There are a number of constructions for secure IBE using
pairings. Here we give two simple constructions that show diﬀerent ways of using pairings. The ﬁrst
construction gives a simple private IBE in the random oracle model. The second construction has
some performance beneﬁts over the ﬁrst, and does not use random oracles, but is only selectively
secure. It can be made adaptively secure using Theorem 15.5. A third construction is presented in
Exercise 15.19.
We will need the following components:
• As usual, let e : G0 ×G1 →GT be a pairing where G0,G1,GT are cyclic groups of prime
order q with generators g0 ∈G0 and g1 ∈G1,
• a symmetric cipher Es = (Es,Ds) deﬁned over (K,M,C),
• hash functions H0 : ID→ G0 and H1 : G1 ×GT →K.
648
15.6.3.1 Construction 1
Let EBF = (S,G,E,D ) be the following IBE scheme with identity space IDand message space M:
• S(): the setup algorithm runs as follows:
α←R Zq, u 1 ←gα
1 , mpk ←u1, msk ←α, output (mpk,msk).
• G(msk,id): key generation using msk = α runs as:
skid ←H0(id)α ∈G0, output skid .
• E(mpk,id,m): encryption using the public parameters mpk = u1 runs as:
β ←R Zq, w 1 ←gβ
1 , z ←e
(
H0(id), uβ
1
)
∈GT,
k←H1(w1,z), c ←R Es(k,m), output (w1,c).
• D
(
skid , (w1,c)
)
: decryption using secret key skid of ciphertext (w1,c) runs as follows:
z←e(skid ,w1), k ←H1(w1,z), m ←Ds(k,c), output m.
To see that the scheme is correct it suﬃces to show that k computed during encryption is the
same as k obtained during decryption. Since k = H1(w1,z) it suﬃces to show that decryption
recovers the correct z by computing z←e(skid ,w1). This follows from:
e
(
skid ,w1
)
= e
(
H0(id)α, gβ
1
)
= e
(
H0(id), gαβ
1
)
= e
(
H0(id), uβ
1
)
.
The quantity on the right is the value of z used in encryption.
15.6.3.2 The decision-BDH assumption
Security of EBF follows from an assumption called the bilinear Diﬃe-Hellman assumption, or
BDH. Here we will use the decisional version of the assumption. The assumption says that given
random elements gα
0 ,gβ
0 ,gγ
0 ∈G0, plus a few additional terms, the quantity e(g0,g1)αβγ ∈GT is
indistinguishable from a random element in GT.
Attack Game 15.8 (Decision bilinear Diﬃe-Hellman). let e: G0 ×G1 →GT be a pairing
where G0,G1,GT are cyclic groups of prime order q with generators g0 ∈G0 and g1 ∈G1. For a
given adversary A, we deﬁne two experiments.
Experiment b (b= 0,1):
• The challenger computes
α,β,γ,δ ←R Zq, u 0 ←gα
0 , u 1 ←gα
1 , v 0 ←gβ
0 , w 1 ←gγ
1 ,
z(0) ←e(g0,g1)αβγ ∈GT, z (1) ←e(g0,g1)δ ∈GT
and gives (u0,u1,v0,w1,z(b)) to the adversary.
• The adversary outputs a bit ˆb∈{0,1}.
649
Let Wb be the event that Aoutputs 1 in Experiment b. Deﬁne A’s advantage in solving the
decision bilinear Diﬃe-Hellman problem for e as
DBDHadv[A,e] :=
⏐⏐⏐Pr[W0] −Pr[W1]
⏐⏐⏐. 2
Deﬁnition 15.12 (Decision BDH assumption). We say that the decision bilinear Diﬃe-
Hellman (DBDH) assumption holds for the pairing eif for all eﬃcient adversaries Athe quantity
DBDHadv[A,e] is negligible.
15.6.3.3 Security of the IBE scheme EBF
We use decision BDH to prove that EBF is a semantically secure and private IBE in the random
oracle model. We model H0 : ID→ G0 as a random oracle, and model H1 : G1 ×GT →K as a
secure key derivation function (KDF) as in Deﬁnition 11.5.
Theorem 15.6. If decision BDH holds for e, H0 is modeled as a random oracle, H1 is a secure
KDF, and Es is semantically secure, then EBF (from Section 15.6.3.1) is a semantically secure and
private IBE.
In particular, let Abe an adversary attacking EBF as in Attack Game 15.6. Moreover, assume
that Aissues at most Qs key queries. Then there exist a decision BDH adversary Be, a KDF
adversary Bkdf that plays Attack Game 11.3 with respect to H1, and an SS adversary Bs that
plays Attack Game 2.1 with respect to Es, where Be,Bkdf, and Bs are elementary wrappers around
A, such that
SSroadv[A,EBF] ≤2 ·2.72 ·(Qs + 1) ·DBDHadv[Be,e] + 2·KDFadv[Bkdf,H1] + SSadv[Bs,Es]. (15.29)
Proof sketch. Adversary Be is given a decision BDH challenge
(u0 = gα
0 , u 1 = gα
1 , v 0 = gτ
0 , w 1 = gβ
1 , z )
where α,β,τ ←R Zq. It needs to decide if z = e(g0,g1)αβτ or if z is uniform in GT. The adversary
begins by sending the IBE public parameters mpk := u1 to the IBE adversary A.
Next, Amakes a sequence of Qs key queries and Qro queries to H0. For j = 1 ,2,... our
adversary Be responds to hash query number j for H0(id(j)) by choosing ρj ←R Zq and setting
H0(id(j)) := gρj
0 . This enables Be to answer all of A’s key queries. The key for id(j) is simply
skj := H0
(
id(j))α = gρjα
0 = uρj
0 ,
which Be can compute itself using the given u0. This is the reason we include u0 in the decision
BDH assumption.
One exception to the above is that at the beginning of the game Be chooses a random ω in the
range 1 to ( Qro + 1), and responds to hash query number ω with H0(id(ω)) := v0. If Aever issues
a key query for id(ω) then Be cannot answer it, and Be must abort and fail.
At some point Aissues a single encryption query for the pair ( id0,m0) and ( id1,m1), as in
the private IBE security game. Be chooses a random b ←R {0,1}and issues an internal query for
H0(idb). If Be guessed ω correctly then idb = id(ω) and hence H0(idb) = v0. Then Be responds by
650
computing k←H1(w1,z) and c←R Es(k,mb), and sends the challenge ciphertext ( w1,c) to A. The
point is that if z= e(g0,g1)αβτ then
z= e(v0,gαβ
1 ) = e
(
H0(idb),uβ
1
)
,
which is the correct value of z obtained when encrypting mb using randomness β ∈Zq. However, if
z is uniform in GT then k := H1(w1,z) is uniform in K, and independent of the adversary’s view.
Now Acannot tell if it was given an encryption of m0 or m1 thanks to the semantic security of Es.
When eventually Aoutputs a bit ˆb ∈ {0,1}, our Be outputs 1 if b = ˆb and 0 otherwise.
Then by an argument from (2.10), Be’s advantage against decision BDH is half A’s advantage in
distinguishing E(mpk,id0,m0) from E(mpk,id1,m1).
This proof strategy works, but incurs a factor (Qro +1) loss in Be’s success probability compared
to A’s, because Be needs to guess ωcorrectly. This loss can be reduced to 2 .72·(Qs +1), as claimed
in (15.29) using Exercise 15.5. 2
15.6.3.4 Construction 2
We next present a second IBE scheme that has a very diﬀerent proof of security using a technique
called a punctured master key. This scheme avoids hashing identities onto the bilinear group which
can lead to eﬃciency improvements over construction 1. We prove selective IBE security from
decision BDH, without relying on the random oracle model.
The construction uses a pairing e, and a symmetric cipher Es = (Es,Ds) deﬁned over (K,M,C),
as in construction 1. It also uses a hash function H : G2
1 ×GT →K.
Let EBB = (S,G,E,D ) be the following IBE scheme with identity space ID := Zq and message
space M:
• S(): the setup algorithm runs as follows:
α,β ←R Zq, u 0 ←gα
0 , u 1 ←gα
1 , v 0 ←gβ
0 , v 1 ←gβ
1 ,
w0 ←R G0, w ←e(w0,g1) ∈GT,
mpk ←(u1,v1,w), msk ←(u0,v0,w0), output (mpk,msk).
• G(msk,id): key generation for id ∈Zq using msk = (u0,v0,w0) runs as:
γ ←R Zq, a 0 ←w0 ·
(
uid
0 ·v0
)γ , b 0 ←gγ
0 , skid ←(a0,b0) ∈G2
0, output skid .
Notice that skid = (a0,b0) is a multiplicative ElGamal encryption of the master secret value
w0 ∈G0 using the public key ( uid
0 ·v0) ∈G0.
• E(mpk,id,m): encryption using the public parameters mpk = (u1,v1,w) runs as:
ρ←R Zq, x 1 ←gρ
1, y 1 ←
(
uid
1 ·v1
)ρ, z ←wρ,
k←H(x1,y1,z), c ←R Es(k,m), output (x1,y1,c).
• D
(
skid , (x1,y1,c)
)
: to decrypt ( x1,y1,c) using secret key skid = (a0,b0) do:
z←e(a0,x1)/e(b0,y1), k ←H(x1,y1,z), m ←Ds(k,c), output m.
651
To see that the scheme is correct it suﬃces to show that k computed during encryption is the
same as k obtained during decryption. Since k = H(x1,y1,z) it suﬃces to show that decryption
recovers the correct z by computing z←e(a0,x1)/e(b0,y1). This follows from:
e(a0,x1)
e(b0,y1) = e
(
w0 ·
(
uid
0 v0
)γ , gρ
1
)
e
(
gγ
0 ,
(
uid
1 v1
)ρ) = e(w0,g1)ρ ·e
((
uid
0 v0
)γ , gρ
1
)
e
(
gγ
0 ,
(
uid
1 v1
)ρ) = e(w0,g1)ρ = wρ,
which is the value of z used in encryption.
Privacy. Is the scheme EBB a private IBE? When using a symmetric pairing, where G0 = G1,
the scheme is not private, unlike Construction 1. To see why, suppose an eavesdropper intercepts
a ciphertext (x1,y1,c) intended for either id0 or id1. If id is the target identity then
e(g0,y1) = e(uid
0 v0, x1).
This lets the eavesdropper distinguish a ciphertext for id0 from one for id1. One complication
is that u0,v0, needed for the test, are part of the master secret msk, and may not be available
to the adversary. When using a symmetric pairing this is not a problem because u0 = u1 and
v0 = v1 and both u1 and v1 are public as part of mpk. In this case the scheme is clearly not private.
However, when using an asymmetric pairing, where G0 ̸= G1, and there is no eﬃciently computable
homomorphism from G1 to G0, then u0,v0 are secret. In this case there is no known attack on the
privacy of the scheme.
Security. We prove security of the schemeEBB using the same decision BDH used to prove security
of Construction 1. The theorem below proves that the scheme is a selectively secure IBE (as in
Deﬁnition 15.11) without relying on random oracles. The scheme can be made adaptively secure
using Theorem 15.5, by hashing all identities using a random oracle.
Theorem 15.7. If decision BDH holds for e, H is a secure KDF, and Es is a semantically secure
cipher, then EBB is a selectively secure IBE.
In particular, for every selective IBE adversary Athat attacks EBB as in Attack Game 15.7,
there exist a decision BDH adversary Be, a KDF adversary Bkdf that plays Attack Game 11.3
with respect to H, and an SS adversary Bs that plays Attack Game 2.1 with respect to Es, where
Be,Bkdf, and Bs are elementary wrappers around A, such that
SelSSadv[A,EBB] ≤2 ·DBDHadv[Be,e] + 2·KDFadv[Bkdf,H] + SSadv[Bs,Es].
Proof idea. The core of the proof shows that an eﬃcient adversary learns nothing about the quantity
z ∈GT used in the challenge ciphertext. More precisely, suppose a selective IBE adversary Acan
distinguish a challenge ciphertext (x1,y1,c) created using a correctz(i.e., z= wρ), from a ciphertext
created using a random z (i.e., z←R GT). We use Ato construct a Be that attacks decision BDH.
The Be we construct uses a technique called a punctured secret key. It operates as follows.
First, Be is given a decision BDH challenge as input. Next, it runs Aand obtains from Aa challenge
identity id ∈ID that Aintends to attack. Be then prepares a master key pair ( mpk,mskp) and
gives mpk to A. The master secret key mskp is constructed in a special way so that Be knows the
secret key skid′ for all identities id′∈ID, except for id. We say that mskp is a punctured key. It
works correctly for all identities, except for the identity id where it was punctured. Note that the
652
identity id to puncture must be known before Be generates the master key pair ( mpk,mskp). This
is the reason we only prove selective security.
Adversary Be uses mskp to answer all the key queries from A. This works because Acannot ask
for a key for the punctured identity id. Be then uses the given decision BDH instance to generate
the challenge ciphertext for the identity id. If the adversary correctly distinguishes a random z
from a correct z, then Be can answer its decision BDH challenge. 2
Proof sketch. Let’s see how Be works. It is given a decision BDH instance
(
ˆu0 = gα
0 , ˆu1 = gα
1 , ˆv0 = gβ
0 , ˆx1 = gρ
1, z
)
, (15.30)
where α,β,ρ ←R Zq and z = e(g0,g1)αβρ or z ←R GT. Our Be needs to decide whether the given z
is of the former or the latter type. To do so, Be runs Aand obtains a challenge identity id ∈ID
from A. It creates a master public key mpk and a punctured master secret key mskp as follows:
κ←R Zq, u 0 ←ˆu0, u 1 ←ˆu1, v 0 ←u(−id)
0 ·gκ
0 , v 1 ←u(−id)
1 ·gκ
1 ,
w←e(ˆv0,ˆu1) = e(g0,g1)αβ, mpk ←(u1,v1,w), mskp ←(κ,u0,v0,ˆv0).
Be sends mpk to Aand keeps the punctured mskp to itself. We will see why this mskp is a
punctured secret key in the next paragraph. Note that the full (non-punctured) master secret key
corresponding to mpk is msk := ( u0, v0, w0) where w0 := gαβ
0 . This is because w in mpk is
w= e(gαβ
0 ,g1). However, Be does not know msk because it cannot compute w0 = gαβ
0 .
Next, Aissues a sequence of key queries. Suppose query j is for identity idj ̸= id. Our Be
responds by choosing γ ←R Zq and computing:
aj ←ˆv
− κ
idj−id
0 ·(uidj
0 ·v0)γ, b j ←ˆv
− 1
idj−id
0 ·gγ
0 , skj ←(aj,bj) ∈G2
0. (15.31)
Be sends skj to A. To see that skj is a valid secret key for idj set γ′ := γ−β/(idj −id) ∈Zq.
Then a simple calculation shows that
aj = gαβ
0 ·(uidj
0 ·v0)γ′
and bj = gγ′
0 .
Hence, (aj,bj) is computed as a secret key foridj as in the real scheme, using γ′∈Zq as the random
nonce. This shows that mskp can be used to generate a secret key for every identity, except for id
where (15.31) fails because of the division by zero in the exponent.
At some point Aissues its single encryption query for messages m0,m1 ∈M with respect to
the challenge identity id. Note that uid
1 ·v1 = g1. Adversary Be chooses b←R {0,1}and computes:
x1 ←ˆx1 = gρ
1, y 1 ←(ˆx1)κ = (uid
1 ·v1)ρ, k ←H(x1,y1,z), c ←R E(k,mb).
It sends (x1,y1,c) to A. If z = e(g0,g1)αβρ = wρ then this is a valid encryption of mb. If z ←R GT
then k is uniform in K, in the adversary’s view, and hence the adversary cannot tell if it was given
an encryption of m0 or m1 thanks to the semantic security of Es. When eventually Aoutputs a bit
ˆb∈{0,1}, our Be outputs 1 if b= ˆband 0 otherwise. Then Be’s advantage against decision BDH is
half A’s advantage in distinguishing the correct z from a random z, by a standard argument from
(2.10). 2
653
15.6.4 Applications
IBE can be used as a cryptographic primitive to construct other cryptosystems. We give three
examples.
15.6.4.1 Chosen ciphertext security from IBE
A selectively secure IBE scheme can be used to construct a chosen ciphertext secure public key
encryption (PKE) scheme. Let Eid = (G,S,E,D ) be a selectively secure IBE scheme with identity
space IDand message space M. Let S= (Gs,Ss,Vs) be a signature scheme and suppose that all
public keys output by Gs() are contained in ID. We construct a chosen ciphertext secure public
key encryption scheme where the encryption algorithm ﬁrst runs Gs() to generate a signature key
pair (pks,sks). It then IBE encrypts the given message m using pks as the identity, and uses sks
to sign the resulting ciphertext.
In more detail, the chosen ciphertext secure public key encryption schemeE= (Ge,Ee,De) with
message space Mworks as follows:
• Ge(): key generation runs ( mpk,msk) ←R G() and outputs (mpk,msk) as the public key pair.
• Ee(mpk,m): the algorithm runs as
(pks,sks) ←R Gs(), c ←R E(mpk,pks,m), σ ←R Ss(sks,c), output (c, pks, σ).
• De
(
msk,(c,pks,σ)
)
works as follows:
if Vs(pks,c,σ ) = reject, output reject and stop
otherwise, sk ←R G(msk,pks), m ←D(sk,c), output m.
The following theorem shows that a selectively secure IBE is suﬃcient to ensure that E is
CCA secure. Note that every signing key sks in this scheme is only used to sign a single message.
Therefore, a one-time secure signature scheme is suﬃcient for security.
Theorem 15.8. Let Eid be a selectively secure IBE scheme as in Deﬁnition 15.11. Let S be a
strongly secure one-time secure signature scheme. Then Eis a chosen ciphertext secure public key
encryption scheme.
In particular, for every CCA adversary Athat attacks E and issues at most Qe encryption
queries, there exist a selective IBE adversary B1 that attacks Eid, and a strong one-time signature
adversary B2 that attacks S, where B1 and B2 are elementary wrappers around A, such that
CCAadv[A,E] ≤Qe ·SelSSadv[B1,Eid] + Qe ·stSIGadv[B2,S].
Proof idea. Let Abe a CCA adversary that attacks Eas in Attack Game 12.1. By Theorem 12.1
we can assume that Aissues a single encryption query to its challenger.
Let ( c,pks,σ) be the challenge ciphertext given to Ain a CCA game. If Aever issues a
decryption query for a ciphertext ( c′,pks,σ′) where (c′,σ′) ̸= (c,σ), and where the response is not
reject, then we immediately obtain an adversary B2 that attacks the one-time signature scheme S.
Hence, we can modify the CCA challenger to answer all decryption queries for ciphertexts of the
form (·,pks,·) with reject. This only negligibly aﬀects A’s advantage.
654
Now we can use Ato build an adversary B1 that attacks the selectively secure IBE scheme
Eid as in Attack Game 15.7. B1 plays the role of CCA challenger to A. At the beginning of the
IBE game, B1 runs (pks,sks) ←R Gs() and sends pks to its selective IBE challenger as the selected
identity that it intends to attack. It next receives mpk from the IBE challenger and forwards mpk
to Aas the public key to attack.
When Aissues its (single) encryption query for m0,m1, our B1 issues an encryption query using
m0,m1 to its own challenger. It gets back an IBE challenge ciphertext c ←R E(mpk,pks,mb). It
then creates the PKE challenge ciphertext ( c,pks,σ) by computing σ ←R Ss(sks,c) and sends this
ciphertext to A.
Next, B1 responds to all of A’s decryption queries by requesting the appropriate secret key
from the IBE challenger. Note that because decryption queries from Aof the form ( ·,pks,·) are
automatically answered with reject, our B1 will never ask its IBE challenger for a secret key for the
identity pks, as required in the selective IBE game.
Eventually Aoutputs a bit b′ that indicates whether c is an encryption of m0 or m1. Our B1
outputs this band halts. It is easy to see that B1’s advantage in winning the IBE game is the same
as A’s advantage in winning the CCA game against E, as required. 2
Properties of this construction. This CCA construction has a number of interesting proper-
ties. First, when instantiated with Construction 2 above, and the one-time signature from Exer-
cise 14.12, the result is a CCA secure public key encryption scheme without random oracles. The
only other such system we saw was described in Section 12.5. The current construction gives a
completely diﬀerent approach.
Second, an observer can verify the signature σ in a given ciphertext (c,pks,σ). If the signature
veriﬁes then the observer is convinced that the ciphertext is well formed, and decryption will
not output reject. This property makes it possible to construct a simple and eﬃcient PKE that
supports threshold decryption with CCA security, as discussed in Chapter 22. In such a scheme
the decryption key is shared among a number of parties, and a threshold of parties is needed to
decrypt a given ciphertext. The technique in this section enables each party to ensure that the
ciphertext is well formed before outputting its partial decryption. The details are provided in [30].
See also Exercise 22.14.
15.6.4.2 Signatures from IBE
A secure IBE scheme directly gives a secure signature scheme. Let Eid = (S,G,E,D ) be a seman-
tically secure IBE scheme with identity space IDand plaintext space MIBE := {0,1}n, for some n.
We construct a signature scheme for messages m∈ID. The signature on m is the IBE secret key
skm associated with identity m. To verify the signature, check that the key skm correctly decrypts
ciphertexts encrypted for the identity m.
In more detail, the signature scheme S= (G′,S′,V ′) derived from Eid has message space ID
and works as follows:
• G′(): run ( mpk,msk) ←R G() and outputs ( mpk,msk) as the signature key pair.
• S′(msk,m): for m∈ID, output σ←R G(msk,m).
• V′(mpk,m,σ ): choose t←R MIBE, compute c←R E(mpk,m,t ), and accept if D(σ,c) = t.
655
The veriﬁer computes the encryption of a random message t∈MIBE under identity m, and accepts
the signature σ if it correctly decrypts this ciphertext. We typically require that the veriﬁcation
algorithm V′be deterministic, but here V′is randomized. It can be de-randomized by deriving the
required randomness from a random oracle applied to ( m,σ), but we will not pursue that here.
The following theorem proves security of this scheme.
Theorem 15.9. Let Eid be a semantically secure IBE scheme as in Deﬁnition 15.9, where the
message space |MIBE|= {0,1}n is super-poly. Then the derived signature scheme Sis secure.
In particular, for every signature adversary Athat attacks S, there exist an IBE adversary B
that attacks Eid, where Bis an elementary wrapper around A, such that
SIGadv[A,S] ≤SSadv[B,Eid] +
(
1/|MIBE|
)
. (15.32)
Proof sketch. Adversary Binteracts with an IBE challenger for Eid, and plays the role of a signature
challenger to Awith respect to S. The IBE challenger sends Ba master public key mpk, and B
forwards this mpk to Aas the signature public key. Next Aissues a sequence of signing queries.
For each query, our Bresponds by issuing the corresponding key query to its IBE challenger, and
forwards the answer back to A.
Eventually Aoutputs a signature forgery ( m,σ), where Adid not previously issue a signing
query for m. Now Bchooses two random messagest0,t1 ←R MIBE = {0,1}n, and issues an encryption
query to its IBE challenger for identitymand messages t0,t1. It gets back a challenge IBE ciphertext
c←R E(mpk,m,t b) for some b∈{0,1}. Then Bcomputes ˆt←D(σ,c). If ˆt= t1 it outputs ˆb:= 1;
otherwise, it outputs ˆb:= 0. Observe that
• when b= 1, then c←R E(mpk,m,t 1), and Boutputs 1 with probability SIG adv[A,S],
• when b= 0, then c←R E(mpk,m,t 0), and Boutputs 1 with probability 1 /|MIBE|.
Then SSadv[B,Eid] =
⏐⏐SIGadv[A,S] −1/|MIBE|
⏐⏐, from which (15.32) follows. 2
To conclude this section, we note that the BLS signature scheme is closely related to the
signature scheme obtained from applying Theorem 15.9 to the IBE scheme EBF (Construction 1).
15.6.4.3 Searching on public-key encrypted data
Our third and ﬁnal application for IBE shows how a server can test if an encrypted message contains
a certain keyword, without the server learning anything else about the message.
In particular, consider an email server that processes emails on behalf of Alice. Alice wants the
email server to implement the following policy: if the subject line in an incoming email contains
a trigger word such as urgent, boss, or Bob, the server should send an alert to Alice’s phone.
Otherwise, the server should store the email in Alice’s inbox for later processing. Alice can update
the list of trigger words Wat any time by notifying the mail server. In this section we assume that
email senders behave honestly. We will not worry about a spam mailer that tries to incorrectly
trigger text messages to Alice’s phone. Spam ﬁltering is an orthogonal problem.
Now, suppose Alice enables encrypted email by publishing a public key pk that senders can
encrypt to. She keeps the secret decryption key to herself. The problem is that now the email
server can no longer do its job, unless it has Alice’s secret decryption key. Without Alice’s key, the
656
server cannot tell if the subject line contains one of the trigger words. Alice prefers not to give her
decryption key to the mail server.
This raises a natural question: can Alice give the mail server a “weak” key that lets it test if
the subject line contains a trigger words in W, without learning anything else about the email.
IBE provides a solution to this problem. Let Eid = (S,G,E,D ) be an IBE scheme with message
space MIBE, and where the identity space ID contains all english words. Alice generates IBE
parameters (mpk,msk) ←R S() and publishes mpk along with her public key pk. She sends to the
mail server a list of secret IBE keys skw ←R G(msk,w), one key for each word w∈W. Overall, the
server receives |W|keys.
When Bob sends an email to Alice he does the following:
• First, encrypt the email with Alice’s regular public key pk to obtain a ciphertext ct.
• Second, let w1,w2,...,w n ∈ID be the words in the email subject line. Bob treats each word
on the subject line as an IBE identity, and encrypts a random plaintext tusing that identity.
That is, Bob chooses t←R MIBE and computes
ci ←E(mpk,wi,t), for i= 1,...,n.
• Third, Bob sends the email ( t,c1,...,c n, ct) to Alice.
When the server receives an incoming encrypted email (t,c1,...,c n, ct) it tries to decrypt each
of c1,...,c n using the list of secret keys at its disposal. This takes a total of n×|W| decryption
attempts. If for some w∈W and i= 1,...,n it obtains D(skw,ci) = t, then it learns that the email
subject contains the trigger word w∈W, and can forward the (encrypted) email to Alice’s phone.
The reason the sender encrypts a random plaintext t∈MIBE is to ensure that D(skw,ci) ̸= twhen
ci is not encrypted for identity w. We are assuming that the message space MIBE is super-poly.
This way, the server learns what trigger words appear in the subject line, and how many times
each word appears. It learns nothing else about the subject line, other than its length.
For this scheme to properly hide the subject line from an eavesdropper, the IBE scheme Eid
must be a private IBE, as deﬁned in Section 15.6.2.1. Otherwise, the ciphertexts c1,...,c n may
expose the identities (words) under which they were encrypted. Hence, Construction 1 can be used
here, but Construction 2 is more limited.
This approach is called searching on encrypted data because it lets the server search the
subject line for particular words without revealing anything else about the subject line. It is closely
related to a construction we discussed back in Section 6.12 for searching on data encrypted using a
symmetric cipher. Here we show how to do the same for data encrypted using a public key system.
This problem raises a number of additional interesting questions:
• Better privacy: the scheme above reveals to the server what trigger words appeared in the
subject line as well as their frequency. Is there a scheme that reveals nothing to the server
other than the existence of a trigger word? The server should not learn what words appeared
in the subject line.
• Spam ﬁltering: Alice may want the email server to remove incoming spam emails, without
learning anything about an incoming email other than its spam status. Can we run a spam
checker on an encrypted email, without revealing anything about the email other than its
spam status?
657
We will come back to these questions in the next section where we discuss functional encryption.
15.7 The functional encryption paradigm
All the encryption schemes discussed in the book so far have the following property: suppose Alice
receives a ciphertext c that is the encryption of some message m. If Alice has the decryption key,
she learns the entire plaintext m; if not, she learns nothing about m. We refer to this as the
binary nature of ciphers : decryption is all or nothing. A functional encryption scheme relaxes
this binary notion. The goal is to support many “weak” decryption keys, where each key only
reveals speciﬁc partial information about the plaintext m, and nothing else.
To give an example, consider a mail server that processes Alice’s encrypted emails. Every
incoming email is encrypted under Alice’s public key, which we will call mpk. The mail server
needs to discard all spam emails before they reach Alice. The determination if an email is spam is
done using a spam predicate P : M→{ 0,1}. The email is considered to be spam when P(m) = 1,
and is benign otherwise. Alice wants to give the mail server a weak secret key skP that lets it
determine the spam status of an email, but learn nothing else about the email. More precisely, if
c ←R E(mpk,m) then D(skP,c) should output P(m) ∈{0,1}and nothing else. This lets the mail
server do its job, but learn nothing about the email contents beyond its spam status. Alice derives
the weak key skP from her full decryption key, which we will call msk.
Functional encryption. Let us generalize this idea to deﬁne functional encryption more ab-
stractly. A functional encryption scheme is designed for some function F,
F : F×M→Y ,
where F, M, and Yare ﬁnite sets, and Ycontains a special symbol ⊥∈Y . This F is called a
functionality. For every f ∈F we obtain a derived function F(f,·) : M→Y acting on messages
in M. We call f a function identiﬁer and we will sometime use f to denote the function F(f,·).
Every secret key skf is associated with some f ∈F and with the corresponding function F(f,·).
If c is the encryption of a message m∈M then decrypting c using the secret key skf will output
f(m) := F(f,m), whenever F(f,m) ̸= ⊥. This key skf is called a functional key because it
outputs a function of the plaintext m. When F(f,m) = ⊥the decryption algorithm is free to
output whatever it wants.
In more detail, a functional encryption scheme operates as follows. As with IBE, one party runs
the setup algorithm S to generate a master public key mpk and a master secret key msk. Anyone
can use mpk to encrypt a message m∈M by invoking the encryption algorithm asc←R E(mpk,m).
The master secret key msk is used to derive functional keys: for f ∈F there is a key generation
algorithm Gthat is invoked asG(msk,f) and outputs a functional keyskf. Running the decryption
algorithm D(skf,c) outputs F(f,m), whenever F(f,m) ̸= ⊥. The decryption algorithm may also
output reject to indicate that c is an invalid ciphertext. This syntax is captured in the following
deﬁnition.
Deﬁnition 15.13. A functional encryption scheme FE = ( S,G,E,D ) for a functionality
F : F×M→Y , where ⊥∈Y , is a tuple of four eﬃcient algorithms: a setup algorithm S, a
key generation algorithm G, an encryption algorithm E, and a decryption algorithm D.
658
• S is a probabilistic algorithm invoked as (mpk,msk) ←RS(), where mpk is called the master
public key and msk is called the master secret key.
• G is a probabilistic algorithm invoked as sk f ←R G(msk,f), where msk is the master secret
key (as output by S), f ∈F represents the function F(f,·) : M→Y , and sk f is a secret key
for this function.
• E is a probabilistic algorithm invoked as c←RE(mpk,m).
• D is a deterministic algorithm invoked as u ←D(skf,c). Here u is either a quantity in Y,
or a special reject value that is not in Y.
• We require that the scheme is correct, namely that for all possible outputs (mpk,msk) of S,
all messages m∈M, and all function identiﬁers f ∈F, if F(f,m) ̸= ⊥then
Pr
[
D
(
G(msk,f), E(mpk,m)
)
= F(f,m)
]
= 1.
• We say that FE is deﬁned over (F,M,C), where Cis some ﬁnite ciphertext space C.
Let’s see a few examples of simple functionalities.
Example 15.1. A regular public key encryption scheme, as deﬁned in Chapter 11, is a primitive
functional encryption supporting only one function, the identity function I. In particular, regular
public key encryption implements the following functionality:
FPKE : {I}×M→ (M∪{⊥}) where FPKE(I,m) = m for all m∈M. (15.33)
There is only one functional secret key, skI, and it fully decrypts every well formed ciphertext. 2
Example 15.2. An identity based encryption scheme deﬁned over ( ID,M,C) is a functional
encryption scheme that implements the following functionality
Feq : ID× (ID×M ) →(M∪{⊥})
where
Feq
(
id′, (id,m)
)
=
{
m if id = id′
⊥ otherwise. (15.34)
Here there is a functional secret key skid′ for every id′∈ID. This key outputs m for a ciphertext
c ←R E(mpk,(id,m)) whenever id = id′. This captures the requirement that skid′ must properly
decrypt ciphertexts encrypted for the identity id′. When id ̸= id′, the functionality outputs ⊥, and
therefore the decryption algorithm can output anything. Hence, IBE implements the “equality”
functionality. 2
We will see a few more examples in the next subsection.
659
The null functional key skϵ. Recall that a ciphertext c generated by a public key encryption
scheme can leak the length of the encrypted plaintext to an observer. We capture this allowed
leakage by including a special function identiﬁer ϵ ∈F in the functionality F : F×M→Y .
For a public key scheme, we augment (15.33) by deﬁning F(ϵ,m) := len(m). The corresponding
functional key, called the null key, is denoted skϵ. We treat the key skϵ as a public value, and this
lets any observer learn the length of the encrypted plaintext by running D(skϵ,c) on a ciphertext c.
Speciﬁcally, for a regular public-key encryption scheme, the set Fcontains two function identiﬁers,
F:= {I,ϵ}.
For an IBE scheme, the null key skϵ can have one of two roles. In a private IBE scheme, a
ciphertext must not reveal the target identity, but can reveal the plaintext length. In this case,
we augment (15.34) with F
(
ϵ,(id,m)
) := len( m). In a regular IBE (i.e., a non private IBE) a
ciphertext can reveal the target identity to an observer. In this case, we augment (15.34) with
F
(
ϵ,(id,m)
) :=
(
id,len(m)
)
. This means that everyone can learn the target identity id from a
ciphertext c by running D(skϵ,c).
Deﬁning secure functional encryption. The main security requirement for functional encryp-
tion is collusion resistance: an adversary who obtains several functional secret keys should learn
nothing about the contents of a challenge ciphertext, beyond what is revealed by the keys at its
disposal. In particular, suppose the adversary obtains a list of keys skf1,..., skfn that includes
the null key skϵ. The adversary is also given a challenge ciphertext c ←R E(mpk,m), for some
message m. Clearly the adversary can learn f1(m),...,f n(m) about m. We require that the ad-
versary not learn anything else about m. There are several ways to capture this. Here we give a
simple deﬁnition that is an adaptation of the standard deﬁnition of semantic security. We allow the
adversary to choose two messages m0,m1 ∈M such that fi(m0) = fi(m1) for all i= 1,...,n . It is
then given the encryption of either m0 or m1 and should be unable to decide, with non-negligible
advantage, which one it was given.
Attack Game 15.9 (semantic security). For a given functional encryption scheme FE =
(S,G,E,D ) for a functionality F : F×M→Y , where ϵ ∈F, and for a given adversary A, we
deﬁne two experiments.
Experiment b (b= 0,1):
• The challenger computes ( mpk,msk) ←R S(), computes the null key skϵ ←G(msk,ϵ), and
sends mpk and skϵ to the adversary A.
• Athen makes a series of queries to the challenger. Each query can be one of two types:
– Key query: the query consists of a function identiﬁer f ∈F. The challenger computes
the secret key sk′
f ←G(msk,f), and sends sk′
f to A.
– Encryption query: the query consists of a pair (m0,m1) ∈M2. The challenger computes
c←R E(mpk,mb) and sends c to A.
We restrict the adversary to a single encryption query ( m0,m1) ∈M2.
Query restriction: let S ⊆F be the set of function identiﬁers in all the key queries issued by
the adversary, including ϵ. We require that m0,m1 speciﬁed in the encryption query satisfy
f(m0) = f(m1) for all f ∈S. This ensures that the adversary cannot trivially win the game
using the keys at its disposal.
660
• At the end of the game, the adversary outputs a bit ˆb∈{0,1}.
Let Wb be the event that Aoutputs 1 in Experiment b. Deﬁne A’s advantage with respect to FE
as
SSadv[A,FE] :=
⏐⏐⏐Pr[W0] −Pr[W1]
⏐⏐⏐. 2
Deﬁnition 15.14 (semantic security). A function encryption scheme FE is semantically
secure if for all eﬃcient adversaries A, the value SSadv[A,FE] is negligible.
Remark 15.13. While Deﬁnition 15.14 is suﬃcient for simple functionalities F : F×M→Y ,
it can sometimes lead to unsatisfactory conclusions. For example, let π : M → Mbe a one-
way permutation. Consider a public key encryption scheme where a ciphertext c←R E(mpk,m) is
designed to reveal π(m) to an observer. That is, the null key skϵ is such that D(skϵ,c) := π(m).
The only other supported functional key fully decrypts the ciphertext. The scheme can reveal π(m)
to an observer, but should not reveal m. Nevertheless, a trivial scheme where encryption is deﬁned
as E(mpk,m) := m would be considered secure under Deﬁnition 15.14. This is because the only
allowed encryption query is one where m0 = m1, in which case the adversary has no hope of getting
any advantage in winning Attack Game 15.9. This example illustrates a potential diﬃculty with
this deﬁnition. Nevertheless, for simple functionalities that do not involve computational hardness,
Deﬁnition 15.14 is adequate. 2
Robust functional encryption. Suppose Alice is given a secret key skf for one function, and
Bob is given a secret key skg for another function. An adversary might try to create a malicious
ciphertext c that causes these keys to decrypt to inconsistent values. Say, D(skf,c) = a and
D(skg,c) = b, but there is no message m∈M such that f(m) = a and g(m) = b. The adversary
should not be able to construct such a ciphertext c.
To give an example where this could be a problem, consider again the spam ﬁltering settings
above. The mail server is given a secret key skP for a spam predicate P : M→{ 0,1}. Alice has a
full decryption key skI, where I(m) is the identity function, I(m) = m for all m∈M. The mail
server only forwards to Alice emails c for which D(skP,c) = 0, so that Alice only sees non-spam
email. However, suppose the adversary could create a ciphertext c such that D(skP,c) = 0, but
D(skI,c) = m where m is spam, namely P(m) = 1. This encrypted email will reach Alice despite
being spam, letting the adversary evade the spam ﬁlter.
A functional encryption scheme is said to be robust if there are no misleading ciphertexts as
above. This is captured in the following deﬁnition.
Deﬁnition 15.15 (robust functional encryption). Let FE = ( S,G,E,D ) be a functional
encryption scheme deﬁned over (F,M,C). We say that FE is robust if for all possible outputs
(mpk,msk) of S, and every ciphertext c∈C, there is a message m∈M such that for all f ∈F
Pr
[
f(m) = D(skf,c)
]
= 1,
when sk f ←RG(msk,f), D(skf,c) ̸= reject, and f(m) ̸= ⊥.
For the spam ﬁltering application we need the functional encryption scheme to be robust, but
that is not enough. The adversary can submit any ciphertext of its choice to the mail server,
and then observe if it forwards the ciphertext to Alice. This is a type of decryption query in a
chosen ciphertext security game. Therefore, to ensure conﬁdentiality, the functional encryption
661
scheme must also be chosen ciphertext secure. This ensures that an adversary cannot decrypt a
target ciphertext by submitting a sequence of related ciphertexts to the mail server and observing
the server’s behavior. We refer to [120] for a discussion of chosen ciphertext secure functional
encryption and its construction.
15.7.1 Sample functional encryption schemes from pairings
In this section we present a few functionalities that can be implemented eﬃciently using pairings.
Expanding the set of functionalities that can be implemented eﬃciently is an active area of research.
In Exercise 15.21 we describe a simple “brute force” construction that works well for functionalities
F : F×M→Y where the set of functions Fis small. Here we are interested in constructions for
functionalities where Fis large.
15.7.1.1 Functional encryption for low-degree polynomials
Consider a functional encryption scheme where the message space is M:= Zn
q, so that messages
are n-tuples in Zn
q. Let Fd ⊆Zq[X1,...,X n] be the set of n-variate polynomials of total degree at
most d. For f ∈Fd and m∈M, deﬁne the polynomial evaluation functionality Fd as
Fd(f,m) := f(m).
A functional encryption scheme for Fd operates as follows:
• a ciphertext c is an encryption of some n-tuple m∈M,
• a functional secret key skf corresponds to a polynomial f ∈Fd,
• invoking the decryption algorithm as D(skf,c) outputs f(m) ∈Zq, and nothing else.
Suppose some authority holds the master secret key msk. When the authority releases a functional
key skf, for some f ∈Fd, it is eﬀectively giving permission to compute f(m) from any encrypted
tuple c←R E(mpk,m). If the authority releases only a small number of functional keys, then only
partial information can be obtained about an encrypted tuple m∈M.
Linear functional encryption. Consider functional encryption for F1, the set of n-variate linear
polynomials. In this case, the polynomial f is simply a vector in Zn
q, and the function being
computed is an inner product of f and the encrypted plaintext vector m ∈Zn
q. For this reason
functional encryption for F1 is often called inner product functional encryption . If a party
has nfunctional keys corresponding to nlinearly independent vectors, then it can learn the entirety
of mfrom c←R E(mpk,m). If the party has fewer than nfunctional keys, then it only learns partial
information about m.
As an example application, consider a set of points ( xi,yi) ∈Z2
q for i= 1,...,n . These points
could represent medical data, ﬁnancial data, census data, or some other sensitive data. The data
owner wants to make the data available to researchers, but without revealing the data in the
clear. Let x := (x1,...,x n) and y := (y1,...,y n). The x-coordinate values are public, but the
y-coordinate values are private and need to be protected by encryption. Hence, when the data
owner places the data on a public server, it publishes
(
x, E(mpk,y)
)
. (15.35)
662
A researcher wants to study a certain subset of the points, say, corresponding to people in a
certain age group or in a certain geographic region. Speciﬁcally, let S ⊆{1,...,n }be a subset
of the n points. The researcher wants to compute the least squares ﬁt line of the points in S.
Recall that the least squares ﬁt line y = ax+ b is the line that minimizes the squared error
L(a,b) := ∑
i∈S[yi−(axi+ b)]2. This line y= ax+ bis computed using two inner-products with y:
a:= ⟨a,y⟩ and b:= ⟨b,y⟩
where a,b∈Zn
q are explicit vectors derived from xand S. Because xis public, the researcher can
calculate the vectors aand b.
The researcher asks the data owner for permission to carry out the study. If approved, the
data owner sends the functional keys ska and skb, corresponding to the vectors a and b, to the
researcher. The researcher can then use these keys, and the encrypted data, to compute the line
y= ax+bneeded for her study. In fact, these functional keys let the researcher carry out her study
on all such encrypted data sets. She can use ska and skb to calculate the least square ﬁt line from
any published data set that is encrypted as in (15.35). The researcher never needs to talk to the
data owner again. Of course, if the researcher wants to study a diﬀerent subset S′she must again
obtain permission from the data owner.
There are a number of constructions available for this inner product functionality. In fact, one
does not need pairings for this; a variant of ElGamal encryption is suﬃcient. See Exercise 15.22
for the details.
Quadratic functional encryption. For quadratic polynomials, when d = 2, a functional en-
cryption scheme forF2 enables the researcher to compute variance and standard deviation of subsets
of the encrypted data in (15.35). Such schemes can be implemented eﬃciently using an encryption
scheme based on pairings.
It is tempting to try and reduce the quadratic case to the linear case by asking the encryptor
to encrypt all pairwise products. For example, if the data is the column vector x:= (x1,...,x n)T,
then the encryption algorithm could ﬁrst compute the n×n matrix X := x·xT, and encrypt
it using the linear functional encryption scheme discussed above. All quadratic functions on x
can be expressed as linear functions on X. Hence, a linear functional scheme seems to suﬃce.
However, the ciphertext size is now quadratic in n. Moreover, this approach is not robust (as in
Deﬁnition 15.15) — there is no guarantee that the encryptor carried out the expansion from x
to X correctly. As mentioned above, using pairings, it is possible to construct a robust functional
encryption scheme for F2 where ciphertext size is linear in n.
For polynomials of degree three or higher one has to resort to more powerful tools that we will
not discuss here. We refer to [7] for some example constructions.
15.7.1.2 Attribute based encryption
Attribute based encryption (ABE) is an important instance of functional encryption that is deﬁned
by predicates. A predicate is a function P : X →{0,1}that outputs one bit representing true
or false. A predicate family is a set of predicates P :=
{
P : X → {0,1}
}
. An attribute
based encryption scheme or ABE deﬁned over ( P,X,M) is a functional encryption scheme
that operates as follows:
663
• a ciphertext c is an encryption of a pair ( x,m) ∈X×M , where x is called the attribute
and m is called the data,
• a functional secret key skP corresponds to a predicate P ∈P,
• invoking the decryption algorithm as D(skP,c) outputs m if P(x) = 1. When P(x) = 0 the
output of D(skP,c) is undeﬁned and can be anything.
We say that Xis the attribute space and Mis the data space.
Formally, an ABE scheme for a predicate family Pis a functional encryption scheme for the
functionality
F : P× (X×M ) →(M∪{⊥})
deﬁned as
F
(
P, (x,m)
):=
{
m if P(x) = 1, and
⊥ otherwise.
We will deﬁne the null functional key skϵ after we look at a few examples. An ABE scheme is
secure if it is secure as a functional encryption scheme for the functionality F.
Examples. An IBE system is an ABE for the equality predicate family, namely Peq := {Pid′},
where Pid′(id) = 1 if and only if id = id′. We already saw this in (15.34) where we showed that
IBE is a special case of functional encryption.
As another example, let X:= {1,...,N }for some integer N. Consider a hospital database that
contains encrypted pairs ( x,y), where x ∈X is the numerical result of a blood test, and y is the
patient’s name. Let Pleq := {Pt}be the comparison predicate family: for x,t ∈X we set Pt(x) = 1
if and only if x≤t. A functional key skt for a predicate Pt reveals all patients whose test result is
less than a threshold t, but keeps all other patient names hidden. A nurse could be given a key skt
and use it to compile a list of all patients who need to be notiﬁed about the results of their test.
The nurse learns nothing about the names of the remaining patients.
Another interesting predicate family comes from linear algebra. Here the attribute space is
X:= Zn
q. The predicate family Pis derived from vectors in X. A predicate Pv ∈P corresponds
to a vector v ∈X and is deﬁned as: for x ∈X we set Pv(x) = 1 if and only if x is orthogonal
to v. Then for a secret key skv and a ciphertext c ←R E(mpk,(x,m)), running D(skv,c) outputs
m whenever v is orthogonal to x. An ABE scheme for this predicate family is called an inner
product ABE.
Private attribute based encryption. Recall that in the context of IBE we distinguished be-
tween IBE and private IBE. In an IBE, a ciphertext may reveal the intended target identity. In
a private IBE a ciphertext must not reveal the target identity. Similarly, we diﬀerentiate between
ABE and private ABE. In an ABE scheme, deﬁned over ( P,X,M), a ciphertext may reveal the
attribute x ∈X embedded in the ciphertext. In a private ABE scheme the ciphertext must not
reveal x.
Formally, we capture this diﬀerence using the null functional key skϵ. Recall that skϵ is used in
a functional encryption scheme to identify what an observer can learn from an observed ciphertext.
Let c←R E
(
mpk,(x,m)
)
be an ABE ciphertext.
664
• In an ABE scheme, running D(skϵ,c) outputs
(
x, len(m)
)
, where len(m) is the length of the
message m. This means that c may reveal both x and the length of m.
• In a private ABE scheme, running D(skϵ,c) outputs only len( m). This means that c may
reveal the length of m, but nothing else.
This distinction means that in a private ABE scheme, a ciphertext does not reveal the embedded
attribute x. Private ABE is also called predicate encryption.
Constructions for ABE. Let X:= {0,1}n and let Pbe the family of predicates on Xthat are
computed by a monotone boolean formula with at most s boolean gates. Recall that a boolean
formula is a special case of a boolean circuit where the out-degree of every gate is at most one. A
formula with n inputs can have at most depth log 2 n. A monotone formula is a formula that uses
only and and or gates (but no not gates).
There are several eﬃcient ABE schemes for Pusing bilinear maps. The ciphertext size is linear
in n and the private key size is linear in s. An example construction is given in [78].
There is also interest in constructions that go beyond formulas, and support ABE for circuits. It
is not known how to construct such schemes from pairings. However, there are known constructions
from multilinear maps (as deﬁned in Section 15.8), and from lattices (as deﬁned in Chapter 17).
We refer to [34] for an example construction and a survey of the work in the area.
Constructions for private ABE. Private ABE is much harder to construct. There are eﬃcient
pairing-based constructions for inner-product private ABE [98], but not much else.
15.7.2 Variations on functional encryption
There are many fascinating variations on the basic functional encryption paradigm described above.
Here we brieﬂy survey a few variations, just to give a taste for the breadth of this topic.
Secret key functional encryption. This is a functional encryption scheme where the setup
algorithm S outputs only one key sk, and that key is needed for key generation in algorithm G,
as well as for encryption by E. This concept can be useful for securely storing personal data on a
remote server. Alice can encrypt her data using sk before sending it to the server. Later, she wants
the server to publish some function of her data. Alice can send a functional key to the server, and
the server uses it to obtain and publish the result in the clear.
Delegatable functional encryption. Refers to a scheme where a functional key skf, for f ∈F,
can be further restricted. Let h: Y→Y be some function and let f′∈F be the function index for
the composition h(f(m)) : M→Y . We say that the functional encryption scheme is delegatable, if
there is an algorithm that takes as input the functional key skf and mpk, and outputs a restricted
functional key skf′ for the function h(f(·)).
Multi-input functional encryption (MIFE). Refers to a functionality that operates on mul-
tiple inputs, namely F : F×M d →Y . The decryption algorithm takes as input a functional
key skf for f ∈F, and d independent ciphertexts ci ←R E(mpk,mi) for i = 1,...,d . It outputs
D(skf,c1,...,c d) = F(f,m1,...,m d) ∈Y. We refer to [74] for the deﬁnition of security.
665
An interesting two-input MIFE is called order revealing encryption , where the message
space is M:= {0,1,..., 2n}and the functionality implements the “less than” relation:
F(lt,m1,m2) :=
{
1 if m1 <m2,
0 otherwise .
The functional key sklt can be applied to two ciphertexts c1 ←R E(mpk,m1) and c2 ←R E(mpk,m2)
by running D(sklt,c1,c2). The result reveals the relative ordering of m1 and m2 and nothing
else about m1 and m2. This functionality is meaningful in the context of secret key functional
encryption, and can potentially be used to carry out a binary search over an encrypted database
index. Unfortunately, it has been shown to not work well in practice because it reveals too much
information about the underlying data (see e.g., [59]).
Function hiding. Refers to a functional encryption scheme where a functional key skf does not
reveal the function f. This is not always possible, but can be constructed in some settings [38, 22].
15.8 Multilinear maps
While bilinear maps lead to impressive encryption and signature schemes, there are still many
problems that cannot be solved using bilinear maps, most notably eﬃcient functional encryption
for more sophisticated functionalities than the ones mentioned in Section 15.7.1.
A stronger algebraic primitive, called a cryptographic multilinear map, lets us solve these
and many other open problems in cryptography. In this section we deﬁne multilinear maps and
present some of their applications. However, we need to point out that at the time of this writing,
there are no known cryptographic multilinear maps, beyond bilinear maps. This remains one of
the central open problems in cryptography.
To simplify the notation, we will focus onsymmetric multilinear maps, although one can equally
deﬁne asymmetric multilinear maps.
Deﬁnition 15.16 (a multilinear map). Let G and GT be cyclic groups of prime order q where
g ∈G is a generator. A d-way multilinear map , or simply a d-linear map, is an eﬃciently
computable function e: Gd →GT satisfying the following properties:
1. multilinear: if u1,...,u d ∈G and α1,...,α d ∈Zq then
e
(
uα1
1 ,...,u αd
d
)
= e(u1,...,u d)α1···αd and
2. non-degenerate: gT := e(g,...,g ) is a generator of GT.
At the very least, we want the discrete log problem to be hard in the groupG. As usual, for some
applications we will need the n-linear map to satisfy stronger complexity assumptions. A standard
hardness assumption on multilinear maps is called the decision multilinear Diﬃe-Hellman
assumption, or decision MDH, which is a direct generalization of decision BDH.
Attack Game 15.10 (Decision multilinear Diﬃe-Hellman). Let e: Gd →GT be a d-linear
map. For a given adversary A, we deﬁne two experiments:
Experiment b (b= 0,1):
666
• For i= 1,...,d + 1 the challenger computes αi ←R Zq and ui ←gαi
i ∈G. Let β ←R Zq and set
z(0) ←e(g,...,g )  
d-way map
α1···αd+1 ∈GT, z (1) ←e(g,...,g )  
d-way map
β ∈GT.
The challenger gives (u1,...,u d+1, z(b)) ∈Gd+1 ×GT to the adversary.
• The adversary outputs some ˆb∈{0,1}.
If Wb is the event that Aoutputs 1 in Experiment b, we deﬁne A’s advantage in solving the
d-way decision Diﬃe-Hellman problem for e as
DMDHadv[A,e,d ] :=
⏐⏐⏐Pr[W0] −Pr[W1]
⏐⏐⏐. 2
Deﬁnition 15.17 (Decision MDH assumption). We say that the d-way multilinear Diﬃe-
Hellman (Decision MDH) assumption holds for e if for all eﬃcient adversaries Athe quantity
DMDHadv[A,e,d ] is negligible.
For d= 2 this deﬁnition reduces to decision BDH for a symmetric bilinear map e: G×G →GT.
Currently, we have no candidate d-linear map for d> 2, where discrete log is hard in the group G.
One can hope that the appropriate mathematical structure needed to construct such maps will
eventually be discovered. Nevertheless, we brieﬂy discuss two applications, in the hope that one
day it will be possible to instantiate them with a secure d-linear candidate.
Group key exchange. In Chapter 10 we saw how to use the Diﬃe-Hellman protocol for anony-
mous key exchange among honest parties (also called toy key exchange). The protocol can be
described using a shared public bulletin board between nparties. Let G be a cyclic group of prime
order q with generator g∈G. At setup time, every party i= 1,...,n chooses a secret αi ←R Zq and
writes ui ←gαi ∈G to the bulletin board. Once all the parties ﬁnish uploading their ui values, all
parties download u1,...,u n. At any later time, if parties iand j need a shared secret ki,j they can
compute it as ki,j := gαi·αj ∈G. Party icomputes ki,j = (uj)αi and party j computes ki,j = (ui)αj.
This process is called non-interactive key-exchange because after the initial upload and down-
load, there is no more interaction between the parties. Note that only O(n) data is written to the
bulletin board.
A natural question is whether this can be generalized beyond pair-wise keys. Suppose (d+1) ≤n
parties want to setup a group key so that they can converse as a group. The group key should be
known to all group participants, but to no one outside the group. Again, the only communication
allowed is a short upload message for setup followed by a download. Once this setup is ﬁnished,
every group of (d+1) parties should be able to establish a group key with no further communication.
Here is a direct generalization of Diﬃe-Hellman to do just that. The protocol uses a d-way
multilinear map e: Gn →GT.
• At setup time, party i, for i = 1,...,n , chooses αi ←R Zq and uploads ui ←gαi ∈G to the
bulletin board. All parties then download u1,...,u n ∈G.
• A group secret among parties S := {1,...,d + 1}is deﬁned as kS := gα1·α2···αd+1
T ∈GT (recall
that gT := e(g,...,g ) is a generator of GT). Party i ∈S computes this secret using the
d-linear map as:
kS = e(u1,...,u i−1,ui+1,...,u d)αi ∈GT.
667
Of course, there is nothing special about the set {1,...,d }. Every subset of d users in
{1,...,n }can compute a group secret this way with no interaction among the group members.
Assuming the setup step was carried out by honest parties, it is not diﬃcult to argue that if the
d-way Decision MDH holds for e, then even if all the users outside of S get together, they cannot
distinguish kS ∈GT from a random element in GT.
We know how to solve the non-interactive group key exchange problem for d = 2 using basic
Diﬃe-Hellman. We know how to solve this problem for d = 3 (i.e. groups of three users) using
the protocol above and a bilinear map. For d >3 there is currently no practical solution to this
problem, although several polynomial time, but non-practical, methods are known [35].
Software obfuscation. We brieﬂy mention that multilinear maps are used in several construc-
tions for a secure program obfuscator. An obfuscatorOfor a circuitCproduces a circuit C′←R O(C)
that is functionally equivalent to C, namely C(x) = C′(x) for all inputs x. However, C′ “hides”
internal secrets embedded in the implementation of C. We refer to [139] for the deﬁnition of
obfuscation and its many applications.
15.9 A fun application: fair exchange of signatures
Alice and Bob want to sign a contract m ∈M. They both need to sign the contract: let σa be
Alice’s signature on m, and let σb be Bob’s signature on m. They want a fair exchange protocol
for the signatures: at the end of the protocol either they both hold σa and σb, or neither holds
both signatures. The protocol should prevent a situation where one of them holds both signatures
and the other does not. This is also called an atomic swap protocol.
One solution to this problem uses a technique called gradual release. Here Alice and Bob take
turns revealing one bit of their signature at a time: Alice sends the ﬁrst bit of σa to Bob, and Bob
responds with the ﬁrst bit of σb. Then Alice sends the second bit of σa to Bob, and Bob responds
with the second bit of σb. This continues until they exchange all the bits. Of course every revealed
bit must be accompanied by a proof that convinces the peer that the transmitted bit is correct.
Now, suppose Alice terminates the protocol when ℓbits are still hidden. Then Alice can obtain σb
with at most 2ℓ work by trying all 2ℓ suﬃxes until she obtains Bob’s valid signature onm. Bob is at
most one bit behind Alice, and can therefore obtain Alice’s signature with at most 2 ℓ+1 work. We
see that if one side aborts early, then both parties can recover the other’s signature with roughly
equal work.
While gradual release may seem fair, this approach is somewhat problematic. First, the protocol
requires many rounds of communication between the parties, which can be slow. Second, there
is often an asymmetry in the computing resources of the two parties. Alice might be a large
corporation, where as Bob is an individual who only has a laptop. Alice may have a thousand
times more computing resources than Bob. She can then terminate the protocol at a point where
she can recover Bob’s signature in one day, while Bob will require a thousand days to recover Alice’s
signature. For these reasons, gradual release is not typically used in practice.
Instead, Alice and Bob decide to use a diﬀerent technique called optimistic fair exchange .
Their idea is to use an oﬄine trusted party Tracy. Tracy generates a key pair (pkt,skt) for a public
key encryption scheme ( G,E,D ). She publishes pkt and keeps her skt secret. Tracy is otherwise
uninvolved in the protocol. She is a trusted party and is assumed to behave honestly.
668
Protocol 15.1. As a ﬁrst attempt, Alice and Bob consider using the following (broken) protocol.
• Alice: encrypts her signature by computing ca ←R E(pkt,σa), and sends ca to Bob;
• Bob: once ca is received, sends σb to Alice;
• Alice: checks that σb is Bob’s valid signature on m, and if so, sends σa to Bob.
If both parties follow the protocol honestly then they both obtain both signatures, and there is no
need to contact the trusted party Tracy. However, if Alice aborts and does not send σa to Bob in
the last round, then Bob can ask Tracy to help him obtain σa:
• Bob: sends (ca,m,σ b) to Tracy;
• Tracy: veriﬁes that (i) σ := D(skt,ca,d) is Alice’s valid signature on m, and (ii) σb is Bob’s
valid signature on m. If so, she sends σ to Bob, and sends σb to Alice.
Here we are assuming that Tracy already has Alice’s and Bob’s public keys. 2
The reason Tracy gives the signatures to both Alice and Bob is to ensure that Bob is not
misbehaving by contacting Tracy without sending his signatureσb to Alice. By giving the signature
to both parties, Tracy ensures that if Bob gets Alice signature, then Alice gets Bob’s signature.
A careful reader will notice that there are two glaring problems with Protocol 15.1. The ﬁrst
problem is that Alice could send the encryption of junk in her ﬁrst message. She is supposed to
send ca ←R E(pkt,σa) to Bob, but suppose that instead she sends ˆca ←R E(pkt,0). If the encryption
scheme (G,E,D ) is semantically secure, then Bob cannot tell the diﬀerence between ˆ ca and ca.
Once Bob sends his signature σb to Alice in the second round, Alice could walk away and never
send her signature to Bob. Now Bob has no recourse: sending ˆ ca to Tracy does not help him
obtain σa.
To prevent this, Alice must somehow convince Bob that ca is an encryption of her signature on
the message m. This can be done using the tools developed in Chapter 20, and in particular, in
Exercise 20.12. However, when using the BLS signature scheme, there is no need for sophisticated
tools. It is quite easy for Bob to check validity of ca. Let us see how.
A simple instantiation using BLS signatures. Let e: G0 ×G1 →GT be a pairing where all
three groups have prime order q, and where g0 ∈G0 and g1 ∈G1 are generators. Let H : M→ G0
be a hash function. Suppose Alice has a BLS key pair (pka,α), and Bob has a BLS key pair (pkb,β),
where α,β ∈Zq are the secret keys. Tracy publishes ( u0 := gτ
0 , u1 := gτ
1 ) and keeps τ ←R Zq secret.
She is otherwise uninvolved.
Protocol 15.2. Alice and Bob sign a contract m∈M. Let σa := H(m)α ∈G0 be Alice’s signature
on m, and σb := H(m)β ∈G0 be Bob’s signature on m. They use the following protocol.
• Alice: construct a multiplicative ElGamal encryption of σa as follows: choose ρ ←R Zq, set
v0 ←gρ
0, w0 ←uρ
0 ·σa, and send ca := (v0,w0) ∈G2
0 to Bob;
• Bob: check validity of ca = (v0,w0) by verifying that
e(w0,g1) = e(v0,u1) ·e(H(m),pka), (15.36)
and if so, send σb to Alice;
669
• Alice: check that σb is Bob’s valid signature on m, and if so, send σa to Bob.
If all goes well then both parties obtain both signatures, and there is no need to contact the trusted
party Tracy. However, if Alice does not send σa to Bob in the last round, then Bob sends to Tracy(
ca = ( v0,w0), m, σb
)
. Tracy veriﬁes that (i) σb is Bob’s valid BLS signature on m, and (ii)
σ := w0/vτ
0 is Alice’s valid BLS signature on m. If so, she sends σ to Bob, and sends σb to Alice.
2
We claim that this protocol unconditionally guarantees that if Alice obtains σb, then Bob
obtains σa, even if Alice is malicious and sends a malformed pair ( v0,w0) in her ﬁrst message. In
particular, let us argue that the BLS signature σ that Bob receives from Tracy must be Alice’s
valid signature on m. Using (15.36) we obtain that
e(σ,g1) = e(w0/vτ
0 , g1) = e(w0,g1)
e(vτ
0 , g1) = e(v0,u1) ·e(H(m),pka)
e(vτ
0 ,g1)
= e(v0,u1) ·e(H(m),pka)
e(v0,gτ
1 ) = e(v0,u1) ·e(H(m),pka)
e(v0,u1) = e(H(m),pka).
Therefore, σ= w0/vτ
0 is Alice’s BLS signature on m with respect to pka, as required.
The reader may be wondering how is it that Bob is able to peek through the veil of ElGamal
encryption and verify that the plaintext is a valid BLS signature. The reason this is possible
is because DDH is false in a pairing group, as explained in (15.8), and therefore multiplicative
ElGamal is not semantically secure. Bob exploits this to verify that ca is well formed.
Nevertheless, even though multiplicative ElGamal is not semantically secure, Bob cannot extract
Alice’s signature σa from her ciphertext ca = ( v0,w0). Indeed, one can show that if Bob could
compute σa given only (pka,u0,u1,m,v 0,w0), then Bob could also break co-CDH for e.
A cheating Bob. We mentioned earlier that there are two glaring problems with Protocol 15.1.
We addressed one problem, a cheating Alice, by using BLS signatures and the validity check in
(15.36). The second problem is that Protocol 15.1 is vulnerable to a cheating Bob.
Consider the following attack. Suppose Alice and Bob want to sign a contract m using Proto-
col 15.1. The protocol begins with Alice sending her well-formed encrypted signature ca to Bob. A
cheating Bob hands ca to his friend Carol. Carol signs mherself to obtain σc, and sends (ca,m,σ c)
to Tracy. Tracy decrypts ca and sends Alice’s signature σa to Carol, which Carol forwards to Bob.
Tracy also sends Carol’s signature σc to Alice. Now Alice has a problem: Bob is holding Alice’s
signature on m, however Alice is holding Carol’s signature on m, which is not what Alice wants.
This shows that Bob and Carol working together can cheat Alice.
The problem is that Tracy cannot tell whoca is intended for. A simple solution is to require that
the contract mexplicitly specify the names, addresses, and public keys of the two aﬀected parties.
Tracy checks that the signatures being released are valid under those public keys, and if they are
not, she aborts. If the signatures are valid then Tracy sends them to the two addresses speciﬁed
in m. Informally, this ensures that if Bob receives σa then Alice must receive σb, as required. It is
suﬃcient to secure Protocol 15.2.
For Protocol 15.1, a general solution is to use a CCA secure encryption scheme that supports
associated data, as discussed in Section 12.7. Here the encryption algorithm c←R E(pk,m,d ) takes
a third argument, the associated data. Similarly, the decryption algorithm D(sk,c,d ′) takes a third
670
argument, and decryption fails if d̸= d′. When Alice runs Protocol 15.1 with Bob, she will encrypt
her signature as ca ←R E(pkt,σa,d) where the associated data is d := ( m,ida,idb) where ida is
Alice’s name, address, and public key, and idb contains the same information for Bob. If Carol
asks Tracy to decrypt ca, Tracy will attempt to run D(skt,ca,d′) where d′ := ( m,ida,idc), and
decryption will fail.
15.10 Notes
Citations to the literature to be added.
15.11 Exercises
15.1 (A chosen ciphertext attack on elliptic-curve ElGamal). Let E/Fp be an elliptic
curve where q := |E(Fp)|is a prime number and P ∈E(Fp) is a generator. Assume that the
ICDH assumption holds for the group E(Fp) and consider the ElGamal encryption scheme EEG
from Section 12.4 implemented over this group. The decryption algorithm D
(
α,(V,c)
)
operates as
in Section 12.4: it computes W ←αV, k←H(V,W ), m←Ds(k,c), and outputs m. Here H is a
function H : F4
p →K (the domain is F4
p because V and W are in F2
p). We will treat the secret key
α as an integer in [0 ,q).
In Remark 12.1 we stressed that algorithm Dmust check that the given point V is in E(Fp), which
means verifying that V = (x0,y0) satisﬁes the curve equation E : y2 = x3 +ax+b. Let’s show that
if D skips this check, then the scheme breaks completely under a chosen ciphertext attack. Here
we assume that αV is computed using the group law as described in Section 15.2. Observe that
these group law equations are independent of the constant term b. For every V1 = (x1,y1) ∈F2
p
there exists some b1 ∈Fp such that V1 is a point on the curve E1 : y2 = x3 + ax+ b1. Then,
if the adversary issues a CCA query for the ciphertext ( V1,c), algorithm D will ﬁrst compute
W1 ←αV1 ∈E1(Fp).
(a) Suppose that |E1(Fp)|is divisible by t. Show that the adversary can learn αmod t, with
probability close to 1, after at most t CCA queries.
(b) Use part (a) to show an eﬃcient CCA adversary that learns the secret key αwith probability
close to 1. You may assume that if b1 is uniform in Fp then |E1(Fp)|is approximately uniform
in the interval [ p+ 1 −2√p, p+ 1 + 2√p]. Recall that there is an eﬃcient algorithm to
compute |E1(Fp)|(see Remark 15.1).
To simplify the analysis of your adversary’s success probability, you may model H : F4
p →K as a
random oracle and assume that the symmetric cipher provides one-time ciphertext integrity.
Discussion: This attack, called an invalid curve attack , illustrates the importance of Re-
mark 12.1 for security of the ElGamal system EEG. More generally, it shows that when receiving
an elliptic curve point as a pair ( x,y) ∈F2
p, one should verify that the point is on the expected
curve E(Fp).
15.2 (Multiplication without the y-coordinate). In this exercise we show that they-coordinate
of a point is not needed for many cryptographic systems. Let E/Fp be an elliptic curve y2 =
x3 + ax+ band let P ̸= Obe a point in E(Fp). We write x(P) for the x-coordinate of the point P.
671
(a) For an integer α >0, let xα := x(αP). We leave xα undeﬁned if αP = O. Use the addition
law to show that the following formula computes x2α and x2α+1 from xα,xα+1,x1:
if (2α)P ̸= O: x2α = (x2
α −a)2 −8bxα
4(x3α + axα + b) (15.37)
if (2α+ 1)P ̸= Oand x1 ̸= 0: x2α+1 = (a−xαxα+1)2 −4b(xα + xα+1)
x1(xα −xα+1)2 (15.38)
Note that (2 α)P ̸= Oimplies that the y-coordinate of αP is non-zero and therefore the
denominator of (15.37) is non-zero. Similarly, (2 α+ 1)P ̸= Oimplies that ±αP ̸= (α+ 1)P
and therefore xα ̸= xα+1, so that the denominator of (15.38) is non-zero.
(b) Use part (a) to give an algorithm, similar to repeated squaring, for computing xα from x1,
when x1 ̸= 0. Your algorithm should take ⌈log2 α⌉steps where at every step it constructs the
pair xγ,xγ+1 for an appropriate choice of γ ∈Z.
Discussion: The algorithm in part (b) is called the Montgomery ladder. Its running
time depends on the number of bits in α, but not on the value of α. This can help defend
against timing attacks.
15.3 (Group law for Montgomery curves). Recall that an elliptic curve E/Fp in Montgomery
form is given as By2 = x3 + Ax2 + xfor some A,B ∈Fp. Work out a formula for the group law for
this curve using the chord and tangent method, as on page 618.
15.4 (Montgomery ladder on Montgomery curves). A Montgomery curve E : By2 = x3 +
Ax2 + x, where A,B ∈Fp, is well suited for x-coordinate point multiplication as in Exercise 15.2.
For a point O ̸= P ∈E(Fp) we write x(P) for the x-coordinate of P. Consider the sequence
X1/Z1, X2/Z2,... where X1 := x(P), Z1 := 1, and
X2α := (X2
α −Z2
α)2 X2α+1 := 4Z1(XαXα+1 −ZαZα+1)2
Z2α := 4XαZα(X2
α + AXαZα + Z2
α) Z2α+1 := 4X1(XαZα+1 −ZαXα+1)2
Use Exercise 15.3 to show that x(αP) = Xα/Zα whenever αP ̸= O.
Discussion: As in part (b) of Exercise 15.2, we can use these equations to compute x(αP) in
log2 αsteps. By combining like terms in these equations, each step requires only 11 multiplications
in Fp [17]. Note that choosing A to be small further speeds up the group operation.
15.5 (Repeated CDH game). Let e : G0 ×G1 →GT be a pairing. Consider the following
t-repeated CDH game, analogous to Attack Game 13.4:
• The challenger computes
α←R Zq, u 1 ←gα
1 ∈G1, y1,...,y t ←R G0
and sends (u1,y1,...,y t) to A.
• Amakes a sequence of reveal queries. Each reveal query consists of an index j ∈{1,...,t }.
Given j, the challenger sends xj := yα
j ∈G0 to A.
• Eventually the adversary outputs (ν,x), where ν ∈{1,...,t }and x∈G0.
672
We say that Awins the game if index ν is not among A’s reveal queries, and x = yα
ν. We deﬁne
A’s advantage, denoted rCDHadv[A,e,t ], as the probability that Awins the game.
Show that for every t-repeated CDH adversary Athat makes at most Qreveal queries, there exists
a co-CDH adversary B, where Bis an elementary wrapper around A, such that
rCDHadv[A,e,t ] ≤2.72 ·(Q+ 1) ·coCDHadv[B,e]. (15.39)
Hint: The proof is essentially the same as the proof of Lemma 13.6.
15.6 (An insecure hash function for BLS). Let e: G0 ×G1 →GT be a pairing where all three
groups have prime order q. Let g0,h0 be two random generators of G0 chosen at setup and treated
as system parameters. Let ˆH : M→ Z2
q be a collision resistant hash function. Deﬁne H : M→ G0
as H(m) := gm0
0 hm1
0 , where (m0,m1) ← ˆH(m) ∈Z2
q. Observe that H is collision resistant assuming
discrete log in G0 is hard. Show that the BLS signature scheme from Section 15.5.1 is insecure
when used with the hash function H.
15.7 (BLS blind signatures). In Exercise 13.15 we deﬁned the concept of a blind signature
scheme, and presented a construction based on the RSA signature scheme.
(a) Construct a blind signature scheme based on the BLS signature scheme. Your construction
should mirror the construction in Exercise 13.15.
(b) In Exercise 13.15 we deﬁned what it means for a blind signature scheme to be secure. Prove
security of the BLS blind signature scheme from part (a) based on the 1MDH assumption
from Section 11.6.3, assuming H is modeled as a random oracle.
15.8 (Insecure proofs of possession). In Section 15.5.3.2 we described an aggregate signature
scheme where every public key pk = (gα
1 , π) includes a proof of possession π := H′(
gα
1
)α ∈G0.
Here H′: G1 →G0 is a hash function.
(a) Suppose the hash function H′maps all inputs to a ﬁxed output w0 ∈G0. That is, H′(u) = w0
for all u∈G1. Show that the resulting aggregate signature scheme is insecure due to a rogue
public key attack. Show how to compute a proof of possession for your rogue public key.
(b) Recall that H : M→ G0 is the hash function used for signing messages in M. Suppose that
G1 ⊆M and we set H′(u) := H(u) for all u∈G1. That is, we use H for computing proofs
of possession. Show that the resulting aggregate signature scheme is vulnerable to a rogue
public key attack.
Hint: in your attack, the aggregate forger issues one query to its signing oracle to produce
the proof of possession that it needs for the rogue public key.
15.9 (Aggregating proofs of possession). In Section 15.5.3.2 we saw an aggregate signature
scheme where every public key pk = (gα
1 , π) includes a proof of possession π := H′(
gα
1
)α ∈G0.
Here H′: G1 →G0 is a hash function. In Remark 15.8 we mentioned the idea of reducing public key
storage by aggregating proofs of possession across many public keys. Speciﬁcally, for a collection
of n public keys {(ui,πi)}n
i=1, deﬁne the aggregate proof of possession πag as πag := π1 ···πn ∈G0.
The aggregate veriﬁcation algorithm VA(u,πag,u′,m′,σag) takes as input the complete list of all
public keys u= (u1,...,u n) ∈Gn
1 and their aggregate proof of possession πag ∈G0. It also takes a
subset u′= (u′
1,...,u ′
ℓ) ∈Gℓ
1 of the public keys in u, and a vector of messages m′= (m′
1,...,m ′
ℓ).
673
It accepts σag if (1) πag is valid, namely e(πag,g1) = ∏n
i=1 e
(
H′(ui),ui
)
, and (2) σag is valid, namely
e(σag,g1) = ∏ℓ
i=1 e
(
H(m′
i),u′
i
)
, and (3) all the public keys in u′ are also in u. As usual, checking
validity of πag need only be done once; it need not be repeated for every aggregate veriﬁcation.
(a) Modify the aggregate signature security game (Attack Game 15.2) to account for an aggregate
forger that outputs a tuple ( u,πag,u′,m′,σag) as a forgery. Note that the forged aggregate
signature can be with respect to a subset u′of the public keys in u. State your attack game
speciﬁcally for the scheme in this exercise.
(b) Show that if the adversary is required to output a forgery (u,πag,u′,m′,σag) for which u= u′
(and hence ℓ= n) and m′ satisﬁes m′
1 = ... = m′
ℓ, then one can prove security with respect
to your attack game from part (a) under the same assumptions as in Theorem 15.2. The
proof requires a small modiﬁcation to the proof of Theorem 15.2 part (b).
Discussion: The scheme in this exercise can be proven secure even when u̸= u′, but the proof
relies on a much stronger assumption than in Theorem 15.2. This exercise shows that in the special
case where every aggregate signature comes with an attached aggregate proof of possession for the
same set of public keys, then we can prove security under the same assumption as in Theorem 15.2.
15.10 (Strongly binding aggregation, part I). In this exercise we show that the signature
aggregation method SA
(1)
BLS from Section 15.5.3.1 is strongly binding as in Deﬁnition 15.6. Let
e: G0 ×G1 →GT be a pairing where G0,G1,GT are cyclic groups of prime order q. Consider the
following pairing collision problem: the adversary is given as input a random pair ( u0,v0) ←R G2
0,
and needs to output ( u1,v1) ∈G2
1, where (u1,v1) ̸= (1,1), such that
e(u0,u1) = e(v0,v1).
We say that pairing collision is hard for e if no eﬃcient adversary can solve the pairing collision
problem with non-negligible probability.
(a) Show that the aggregation scheme SA
(1)
BLS is strongly binding, as in Deﬁnition 15.6, assuming
pairing collision is hard for e and the hash function H : G1 ×M→ G0 is a random oracle.
Hint: Let Abe a strong binding adversary for SA
(1)
BLS. Your pairing collision algorithm Bwill
respond to a query from Afor H(pki,mi) by setting H(pki,mi) := uρi
0 vτi
0 where ρi,τi ←R Zq.
(b) For certain asymmetric pairings (i.e., G0 ̸= G1) it is believed that DDH holds in G0. Show
that if DDH holds in G0 then pairing collision is hard for e.
15.11 (Strongly binding aggregation, part II). In this exercise we show that the proof of
possession signature aggregation method from Section 15.5.3.2 is not strongly binding. Suppose
that two malicious signers, Carol and David, generate two valid public keys pkc = ( uc,πc) and
pkd = (ud,πd) where they deliberately ensure that uc ·ud = 1.
(a) Show that σag = 1 ∈G0 is a valid aggregate signature with respect to ( pkc,pkd) for every
m∈M. That is, VA
(
(pkc,pkd), (m,m), 1
)
= accept for all m∈M.
(b) Part (a) suggests that Carol’s and David’s behavior is irrational, since now an adversary can
forge their aggregate signature on any message m. However, let’s see how this setup can
help them cause signer confusion. Let σ′
ag be a valid aggregate signature for some message m
with respect to valid public keys pk = (pk1,..., pkn). Show that σ′
ag is also a valid aggre-
gate signature for m with respect to pk′ := (pk1,..., pkn,pkc,pkd). That is, show that if
674
VA
(
pk,mn,σ′
ag
)
= accept then VA
(
pk′,mn+2,σ′
ag
)
= accept, where mn := (m,...,m ) ∈Mn
and mn+2 := (m,...,m ) ∈Mn+2.
Discussion: In eﬀect, Carol and David caused a collision between the set of signers pk′that
includes them, and a set of honest signers pkthat does not: the signature σ′
ag could have been
generated by either pkor pk′. This can potentially be exploited in a setting where the parties
who generated (m,σ′
ag) are entitled to a reward. Carol and David can inject themselves into
the set of signers and claim a portion of the reward. This can be easily mitigated by the
augmentation method in part (c).
(c) Let’s show that any secure signature aggregation scheme SA= (G,S,V,A, VA) can be made
strongly binding. To do so, the aggregator includes a collision resistant hash of ( pk,m) in
the aggregate signature. Speciﬁcally, for a collision resistant hash function H, let SA′ =
(G,S,V,A ′,VA′) be the following modiﬁcation of SA:
• A′(pk,m,σ): set h←H(pk,m) and σag ←R A(pk,σ), and output σ′
ag := (h,σag).
• VA′(
pk, m, σ′
ag = (h,σag)
)
: accept if h= H(pk,m) and VA(pk,m,σag) = accept.
Show that SA′ is a secure aggregation scheme that is also strongly binding, as in Deﬁni-
tion 15.6, assuming SAis secure and H is collision resistant. Note that the cost of this
augmentation is a small increase in the size of the aggregate signature, since the aggregate
signature must now include the hash h.
15.12 (Another secure aggregation method). Let us see a secure signature aggregation scheme
that retains many of the beneﬁt of proofs of possession from Section 15.5.3.2, but without expanding
the public key. Let e : G0 ×G1 →GT be a pairing where G0,G1,GT are cyclic groups of prime
order q. We will need a hash function Hn : Gn
1 →Cn where C⊆ Zq is super-poly. The aggregation
scheme, denoted SA(3)
BLS, is the same as SABLS in (15.10) except that aggregation and veriﬁcation
now operate as follows.
• A
(
pk, σ): Let pk= (pk1,..., pkn) ∈Gn
1 and σ= (σ1,...,σ n) ∈Gn
0 .
– compute (τ1,...,τ n) ←Hn(pk) ∈Cn and output σag ←στ1
1 ···στn
n ∈G0.
• VA
(
pk, m∈Mn, σag
)
: Let pk= (pk1,..., pkn) and m= (m1,...,m n).
– verify that pki ̸= 1 for all i= 1,...,n ; otherwise reject and stop,
– compute (τ1,...,τ n) ←Hn(pk) ∈Cn,
– accept if e(σag,g1) = e
(
H(m1),pk1
)τ1
···e
(
H(mn),pkn
)τn
.
The security of this scheme against a forgery attack is shown in [33]. Here we consider other
properties of this scheme.
(a) Show that when all the messages in mare the same, namely m:= m1 = ···mn, the aggregate
veriﬁcation algorithm can be rewritten to use only two pairings instead of n. This shows that
the optimization in (15.11) applies to this scheme.
(b) Show that if pairing collision, as deﬁned in Exercise 15.10, is hard for e, and H and Hn are
modeled as random oracles, then SA
(3)
BLS is strongly binding as in Deﬁnition 15.6.
15.13 (One-time aggregate signatures). In Exercise 14.11 we presented a simple one-time
signature scheme whose security is based on the discrete log assumption in a group G of order q.
675
(a) Show that this scheme is aggregatable. In particular, given a vector of n signatures σ =
(σ1,...,σ n) ∈ Zn
q on n messages m = ( m1,...,m n) ∈ Mn issued under n public keys
pk = ( pk1,..., pkn) ∈(G2)n, let σag := σ1 + ... + σn ∈Zq be the aggregate signature.
Explain how the aggregate veriﬁcation algorithm VA(pk,m,σag) works.
(b) Show that there is a rogue public key attack on this basic scheme.
(c) Suppose we enhance the scheme from part (a) using the message augmentation technique
from Section 15.5.3.1. Show that there is still a rogue public key attack on this scheme.
(d) Let Hn : (G2)n →Zn
q be a hash function. Suppose we enhance the scheme from part (a) by
deﬁning the aggregate signature σag as σag ←σ1τ1 + ... + σnτn ∈Zq, where ( τ1,...,τ n) ←
Hn(pk) ∈Zn
q. Explain how aggregate veriﬁcation works. Then prove that the resulting
aggregate signature scheme is one-time secure if the discrete log assumption holds in the
group G, and the hash functions H and Hn are modeled as random oracles. Speciﬁcally,
show that your enhanced scheme satisﬁes Deﬁnition 15.5 against an adversary that issues at
most one signature query to its challenger.
15.14 (Completing the proof of Theorem 15.3). In this exercise we show how to compute
the value e(g0,h1)(αQ+2) needed to complete the proof of Theorem 15.3. We have at our disposal
a polynomial P ∈Zq[X] of degree Q+ 1, the data from (15.24), a message m ∈Zq, and a pair
(r,w) ∈Zq ×G0 such that P(m) ̸= r and w= g(P(α)−r)/(α−m)
0 , where m̸= α.
(a) First, show how to derive the quantity g1/(α−m)
0 ∈G0 from (r,w) and the data from (15.24).
Hint: First show that
(
P(X) −r
)
/(X −m) = p1(X) + s/(X −m) for some polynomial
p1 ∈Zq[X] of degree Q and some 0 ̸= s∈Zq. Then use (15.22) to compute y:= gp1(α)
0 .
(b) Now that we have a pair ( m, g1/(α−m)
0 ), show how to use h1 and h(αQ+3)
1 from (15.24) to
compute the quantity t:= e(g0,h1)
(
αQ+2+p2(α)
)
, where p2 ∈Zq[X] is a known polynomial of
degree at most Q+ 1.
Hint: Use the fact that ( XQ+3 −mQ+3)/(X−m) = XQ+2 + p2(X) where deg(p2) ≤Q+ 1.
(c) Show how to calculate y := e(g0,h1)p2(α) using (15.22). Then t/y = e(g0,h1)(αQ+2) is the
required value needed to solve ( Q+ 1)-CDH and complete the proof of Theorem 15.3.
15.15 (A non-strong signature scheme). Consider again the signature scheme SG from Sec-
tion 15.5.4. Suppose that we modify the signing algorithm so that instead of generating the nonce
r ∈Zq using a PRF, the signer simply chooses a fresh random r in Zq for every signature. Show
that the resulting signature scheme is not strongly secure.
15.16 (Proof of Theorem 15.4). In this exercise we prove security of the signature scheme
SBB from Section 15.5.4.1. We construct an adversary Bthat uses a signature forger Ato solve
a given Q-CDH challenge with secret parameter α ∈Zq. Adversary Bconstructs the public key
pk := (u1,v1,hT) to give to Aas follows. First, it chooses random ρ1,...,ρ Q ←R Zq, and constructs
the polynomial
P(X) :=
Q∏
i=1
(X−ρi) =
Q∑
i=0
γi ·Xi ∈Zq[X].
676
Then Buses (15.22) to compute h0 ←gP(α)
0 and sets hT ←e(h0,g1). Next, Bchooses δ←R Zq and
b←R {0,1}and uses gα
1 from the Q-CDH challenge to set:
if b= 0, set u1 ←gα
1 , v1 ←gδ
1, if b= 1, set u1 ←gδ
1, v1 ←gα
1 .
It obtains the public keypk := (u1,v1,hT), which it sends toA. Note that Alearns nothing about b.
(a) Show that Bcan construct h1/(α−ρi)
0 for all i= 1,...,Q .
(b) Use part (a) to show that in both cases, b= 0 and b= 1, our Bcan answer all of A’s signature
queries. Signature query number i is answered using ρi ∈Zq and h1/(α−ρi)
0 ∈G0.
(c) Show that with probability at least 1 /2, the ﬁnal forgery ( m,σ) from Aenables Bto obtain
a pair (ρ, h1/(α−ρ)
0 ), where ρ∈Zq satisﬁes ρ̸∈{ρ1,...,ρ Q}.
Hint: When b= 0, Bknows a list of signatures for m, one for each ρ∈{ρ1,...,ρ Q}. When
b= 1, Bknows another list of signatures for m. Show that if the forgery σis in exactly one of
the two lists, then with probability 1 /2 our Bcan construct the required pair ( ρ, h1/(α−ρ)
0 ),
where ρ ̸∈{ρ1,...,ρ Q}. Use the fact that Aknows nothing about b. This fails if ( m,σ) is
on both lists. Show that in that case, Bcan directly compute the secret α∈Zq. If you get
stuck, see [29].
(d) Show that Bcan construct (ρ, g1/(α−ρ)
0 ) from (ρ, h1/(α−ρ)
0 ). Use the fact that ρ̸∈{ρ1,...,ρ Q}.
Hint: Use the fact that P(X)/(X−ρ) = p(X) +s/(X−ρ) for some polynomial pof degree
at most Q−1 and 0 ̸= s∈Zq.
(e) Now, use parts (b) and (c) of Exercise 15.14 to solve the given Q-CDH challenge.
15.17 (Bounded collusion IBE). Consider a secure IBE, as in Deﬁnition 15.9, but where the
adversary is limited to a single key query. This models an environment where users do not collude,
so that every user sees only one decryption key. In this restricted setting we can construct IBE
directly from the ElGamal encryption scheme EEG = (GEG, EEG, DEG) from Section 11.5, without
any pairings. Recall that EEG operates in a cyclic group G of prime order q with generator g ∈G.
Let H : ID→ Zq be a hash function. The derived IBE scheme ( S,G,E,D ) has identity space ID
and the same message space Mas EEG. The IBE scheme works as follows:
• S() : setup runs as follows
α,β ←R Zq, u ←gα ∈G, v ←gβ ∈G,
mpk ←(u,v) ∈G2, msk ←(α,β) ∈Z2
q, output (mpk,msk).
• G(msk,id) : key generation for id ∈Zq using msk = (α,β) runs as
r←H(id) ∈Zq, σ ←r·α+ β ∈Zq, output skid := σ.
• E(mpk,id,m) : encryption using mpk = (u,v) runs as:
r←H(id) ∈Zq, pkid ←ur ·v∈G, c ←EEG(pkid ,m), output c.
• D(skid ,c): output DEG(skid ,c).
677
Notice that secret keys in this scheme correspond to signatures in the one-time signature scheme
from Exercise 14.11.
(a) Show that this IBE is scheme is secure against an adversary that issues at most a single key
query, assuming EEG is semantically secure, and H is modeled as a random oracle.
(b) This scheme is clearly not secure against an adversary that issues two key queries. Give a
construction that is secure against such adversaries, under the same assumptions as in part (a).
Use Exercise 14.11 part (b).
15.18 (CCA secure IBE). In this exercise we construct a chosen ciphertext secure IBE from a
semantically secure IBE, in the random oracle model. The construction uses the technique from
Section 12.6. Exercise to be written.
15.19 (An IBE from the signature scheme SG). In this exercise we construct an eﬃcient IBE,
called EG, where secret keys correspond to signatures in the signature schemeSG from Section 15.5.4.
The scheme is proven secure without random oracles based on the decision d-CDH assumption,
where d = Q+ 1, and Q is the number of key queries from the adversary. The scheme EG has
identity space ID:= Zq and message space M. It uses a pairing e, a symmetric cipher Es = (Es,Ds)
deﬁned over (KE,M,C), a hash function H : G1 ×G2
T →KE, and a PRF F deﬁned over (KF,Zq).
The IBE scheme EG = (S,G,E,D ) works as follows:
• S(): setup runs as follows
α,β ←R Zq, k F ←R KF, u 1 ←gα
1 , v 0 ←gβ
0 , h T ←e(v0,g1) ∈GT,
mpk ←(u1,hT), msk ←(α,β,k F), output (mpk,msk).
• G(msk,id) : key generation for id ∈Zq using msk = (α,β,k F) runs as:
r←R F(kF,id) ∈Zq, w0 ←g(β−r)/(α−id)
0 , output skid ←(r,w0) ∈Zq ×G0.
Note that w0 is undeﬁned when id = α. In that case we set w0 ←1 and r←β.
• E(mpk,id,m) : encryption using mpk = (u1,hT) and gT := e(g0,g1) runs as:
ρ←R Zq, x 1 ←(u1 ·g−id
1 )ρ, y ←gρ
T, t ←hρ
T,
kE ←H(x1,y,t ), c ←R Es(kE,m), output (x1,y,c ).
• D
(
skid , (x1,y,c )
)
: to decrypt ( x1,y,c ) using secret key skid = (r,w0) do:
t←e(w0,x1) ·yr, k E ←H(x1,y,t ), m ←Ds(kE,c), output m.
(a) Show that D(skid ,c) properly decrypts c←R E(mpk,id,m).
(b) Security of this scheme relies on a decision version of the d-CDH assumption. Formulate a
decision version of the d-CDH assumption from Deﬁnition 15.7. The goal is to distinguish
z:= e(g0,h1)(αd+1) ∈GT from a random element in GT, given the data in (15.21).
(c) Prove that EG is a secure private IBE scheme, as in Deﬁnition 15.10, assuming decisiond-CDH
holds for efor poly-bounded d, and assuming F is a secure PRF, H is a secure KDF, and Es
is semantically secure. The proof proceeds along the same lines as the proof of Theorem 15.3.
678
• The Qkey queries from the adversary are answered as signatures queries in Theorem 15.3,
using the data provided in the decision d-CDH challenge, where d= Q+ 1.
• When the adversary issues its encryption query
(
(id0,m0),(id1,m1)
)
, ﬁrst choose a
random b ←R {0,1}and compute the secret key ( r,w0) for the identity idb as in the
previous bullet. The challenge ciphertext sent back to the adversary is constructed
using the d-CDH challenge data in (15.24) as:
x1 ←h(αQ+3)
1 ·h
−idQ+3
b
1 , y ←e
(
gT(α)
0 , h1
)
·z, t ←e(w0,x1) ·yr,
kE ←H(x1,y,t ), c ←R Es(kE,mb), output (x1,y,c ).
Here z ∈GT is the ( Q+ 1)-CDH challenge, and T is the degree ( Q+ 1) polynomial
T(X) := (XQ+3 −idQ+3
b )/(X−idb) −XQ+2.
Show that when z = e(g0,h1)(αQ+1) then (x1,y,c ) is a proper encryption of mb under idb.
When z is uniform in GT then (x1,y,c ) is independent of b. Use this to complete the proof
of security.
15.20 (Identity based key exchange). An identity based non-interactive key exchange (ID-
NIKE) lets two parties, who know each other’s identity, generate a shared secret without exchanging
any messages. More precisely, an ID-NIKE is a triple of algorithms ( S,G,K ) where S is invoked
as (mpk,msk) ←R S(). As usual, mpk is given to all participants in the system, while msk is kept
secret at a trusted entity Tracy. A user with identity id can request from Tracy the secret key
skid ←R G(msk,id). Say Alice has ska ←R G(msk,ida) and Bob has skb ←R G(msk,idb). When Alice
and Bob want to generate a shared secret, Alice locally runs ka ←R K(ska,idb) and Bob locally
runs kb ←R K(skb,ida). The scheme is correct if ka = kb, so that they both obtain the same secret,
denoted ka,b.
(a) We say that an ID-NIKE scheme is secure if an eﬃcient adversary who can request secret
keys skid for identities id of his choice, cannot compute the secret kid1,id2 between id1 and
id2, assuming it did not request the secret keys for either id1 or id2. Formulate this security
property as a game between a challenger and an adversary.
(b) Let e : G ×G → GT be a symmetric pairing. Consider the following ID-NIKE scheme
(S,G,K ): algorithms S and Gare the same as in the IBE scheme EBF from Section 15.6.3.1.
Recall that these algorithms use a hash functionH : ID→ G, and a secret key for identityidx
is deﬁned asskx := H(idx)α ∈G, where α∈Zq is the master secret. For identitiesidx and idy,
algorithm K
(
skx,idy
)
is deﬁned as K
(
skx,idy
):= e
(
skx, H(idy)
)
. Show that this scheme is
correct and secure, assuming decision BDH holds for e, and H is modeled as a random oracle.
(c) Explain how to adapt the scheme from part (b) to use an asymmetric pairinge: G0×G1 →GT
where G0 ̸= G1.
15.21 (The trivial functional encryption scheme). Let F : F×M→Y be a functionality
where F = {f1,...,f n}. Suppose n = |F| is poly-bounded. Let E = ( GPKE,EPKE,DPKE) be
a public key encryption scheme deﬁned over ( Y,C). Deﬁne the following functional encryption
scheme FE= (S,G,E,D ) for the functionality F:
• S(): setup runs as follows
679
for i= 1,...,n do: ( pki,ski) ←R GPKE()
mpk ←(pk1,..., pkn), msk ←(sk1,..., skn), output (mpk,msk).
• E(mpk,m) :=
{
c←R (
EPKE
(
pk1,f1(m)
)
,...,E PKE
(
pkn,fn(m)
))
∈Cn, output c
}
.
• G(msk,fi) := ski, D (ski,c) := DPKE(ski,ci).
(a) Show that if Eis a semantically secure public key encryption scheme, thenFEis a semantically
secure functional encryption scheme. In particular, for every Athere exists a B, where Bis
an elementary wrapper around A, such that
SSadv[A,FE] ≤n·SSadv[B,E].
Use a hybrid argument across n hybrids.
(b) Show that there are functionalities F for which FE is not robust (as in Deﬁnition 15.15).
Discussion: Ciphertext size in this scheme is linear in n= |F|, which is ﬁne when nis small. The
main challenge in constructing functional encryption schemes is supporting functionalities where n
is super-poly.
15.22 (Inner-product functional encryption). In this exercise we construct a functional en-
cryption scheme for the inner-product functionality discussed in Section 15.7.1.1. Let G be a group
of prime order q generated by g∈G, and let M:= {1,...,ℓ }n ⊆Zn
q. The scheme works as follows
• S(): setup runs as follows
for i= 1,...,n do: αi ←R Zq, h i ←gαi ∈G
mpk ←(h1,...,h n) ∈Gn, msk ←(α1,...,α n) ∈M, output (mpk,msk)
• G(msk,f) : key generation for f ∈M runs as
β ←∑n
i=1 αi ·fi ∈Zq, output skf := (f, β) ∈M× Zq
• E
(
mpk,m
)
: encryption using mpk = (h1,...,h n) of m∈M runs as
ρ←R Zq, u ←gρ, v 1 ←gm1hρ
1, ..., v n ←gmnhρ
n,
output c←(u,v1,...,v n) ∈Gn+1
• D(skf, c): to decrypt c= (u,v1,...,v n) using skf = (f,β) do:
w←
(∏n
i=1 vfi
i
)
/uβ ∈G, ˆm←dlogg(w), output ˆm∈Zq.
Here dlog g(w) refers to the discrete log of w base g. Since 0 ≤ ˆm ≤ℓ2n, this step takes
time O(ℓn1/2) using the Pollard lambda algorithm, which is eﬃcient assuming n and ℓ are
poly-bounded. The algorithm outputs reject if it is not able to compute discrete log in this
time bound.
(a) Show that this scheme correctly implements an inner product functional encryption.
680
(b) Prove that the scheme is a selectively secure inner product functional encryption, assuming
DDH holds in G. Selective security means that the adversary must commit to the challenge
messages m0,m1 ∈M before seeing mpk. For completeness, we deﬁne decryption using the
null key skϵ as D(skϵ,c) := n for all c.
681
Chapter 16
Attacks on number theoretic
assumptions
In the last few chapters we presented several cryptosystems whose security depends on the diﬃculty
of number theoretic and algebraic problems. In this chapter we analyze these assumptions in more
detail. We also describe common mistakes that lead to attacks against real-world implementations.
These attacks teach us how to correctly implement these systems.
16.1 Analyzing the DL, CDH, and DDH assumptions
In Section 10.5, we introduced the discrete logarithm (DL) and related assumptions, such as the
computational Diﬃe-Hellman (CDH) and decisional Diﬃe-Hellman (DDH) assumptions. We dis-
cussed these assumptions in the context of a cyclic group G of prime order q generated by g ∈G.
In Section 10.5, we speciﬁed that G was a subgroup of Z∗
p for some large prime p. Later, in Chap-
ter 15, we introduced the notion of an elliptic curve, and discussed how G could be a prime-order
subgroup of such a curve.
In this section we present discrete log algorithms that work in every ﬁnite cyclic group G. We
will consider cyclic groups whose order is not necessarily prime. In so doing, we will gain some
insight into why we restricted our attention to prime-order groups in the ﬁrst place.
16.1.1 Square root time algorithms for discrete log
Suppose that G is a cyclic group of order q generated by g∈G. In this section, q may or may not
be prime. Given u ∈G, we wish to ﬁnd an α ∈Zq such that gα = u. Recall that α is called the
discrete log of u base g.
The simplest algorithm to solve this problem is brute-force search: try all β = 1 ,2,...,q
until we ﬁnd the discrete log of u. Here is the algorithm:
v←1 ∈G
β ←0 ∈Zq
while v̸= u do
v←v·g, β ←β+ 1
output β
This algorithm is clearly correct, and its worst-case running time requires q multiplications in G.
682
A faster algorithm. A much faster algorithm is the baby step/giant step method . The
algorithm uses only O(q1/2) group operations in the worst case.
Set m:= ⌈q1/2⌉. The algorithm makes use of the fact that we can write the (unknown) discrete
log α∈Zq as
α= γ·m+ β, for some 0 ≤β,γ <m. (16.1)
Then, to ﬁnd α, it suﬃces to ﬁnd β and γ. Exponentiating both sides of (16.1) gives:
u= gα = gγm+β = (gm)γ ·gβ. (16.2)
We can isolate β and γ to diﬀerent sides of the equality by re-writing (16.2) as
u·(g−m)γ = gβ. (16.3)
Now we can ﬁndβand γin time O(q1/2) using the meet in the middle technique from Section 4.2.3.1.
The algorithm works in two steps. First, we build a lookup table T that records all m possible
values for the right hand side of (16.3), along with the corresponding value of β. We can build T
using the following procedure (called “baby steps”):
initialize an empty associative array T : G →Zq
v←1 ∈G
for β from 0 to m−1 do
T[v] ←β, v←v·g
This algorithm takes m≈q1/2 multiplications in G to build T.
Second, after building the array T, we compute all the values of the left hand side of (16.3),
and check them one by one to see if they are in the table T. If so, then we found a pair ( β,γ) that
satisﬁes (16.3). This is done using the following procedure (called the “giant steps”):
g′←g−m, v ←u, γ ←0
(∗) while v /∈Domain(T) do
v←v·g′, γ ←γ+ 1 / / v= u·(g′)γ
β ←T[v] / / now β satisﬁes gβ = v= u·(g′)γ = u·g−mγ
α′←γm+ β / / So u= gγm+β = gα′
output α′
Observe that the loop on line (∗) will repeat until a γ is found such that v= u·(g−m)γ is contained
in the table T. Then if β = T[v], the pair ( β,γ) must satisfy (16.3), from which we recover α as
α←γm+β. Because γ is less than q1/2, this procedure will terminate after at most q1/2 iterations.
Overall, the entire baby step/giant step procedure uses about 2 q1/2 group operations in the worse
case.
While this algorithm is much faster than brute-force search, it has one drawback: storing the
table T requires a lot of memory, enough to store about q1/2 group elements. One solution is a
“time/space trade-oﬀ”. By choosing a smaller m, we get a smaller table of size O(m), but the
running time is now proportional to q/m, which gets worse the smaller m is.
Remark 16.1. The baby-step/giant-step algorithm can be easily adapted to a situation where we
know that Dloggu happens to be in an interval [ A,B] of length ℓ. The algorithm takes time and
space O(ℓ1/2). 2
683
Other square root time algorithms. Other (heuristic) algorithms have the same running time
as baby step/giant step, but require only constant space. Some examples include Pollard’s Rho
method and Pollard’s lambda method [148, Sec. 11.2.5]. These algorithms can also be modiﬁed
to take advantage of hardware parallelism or run in a distributed system [154].
All these algorithm use very diﬀerent techniques, but all of them requireO(√q) group operations
to compute discrete log with probability one half. In Section 16.3 we will show that this is not a
coincidence. Every discrete log algorithm that uses the group as a “black box” and succeeds with
probability one half, must make at least √q/3 group operations.
16.1.2 Discrete log in groups of composite order
In this section we look at algorithms for discrete log in a cyclic group G of order n, where n is
composite. As we will see, discrete log is only as hard as discrete log in the largest prime order
subgroup of G. Moreover, the decision Diﬃe-Hellman (DDH) problem is false whenever n has a
small prime factor. These two reasons, among others, explain why throughout the book, we always
focus on groups of prime order.
16.1.2.1 Groups of order qe
Let G be a cyclic group of order qe generated by g in G, where q >1 (not necessarily prime) and
e≥1. Observe that for every f = 0,...,e
gf := g(qf) ∈G generates a subgroup of G order q(e−f).
In particular, g0 = g and ge = 1. The element ge−1 generates a subgroup of order q.
The discrete log problem in G is as follows: we are given q, e, g, along with an element u∈G,
and we want to compute the discrete log of u base g. In particular, if u= gα for some 0 ≤α<q e,
then we want to ﬁnd α.
There is a simple algorithm that allows one to reduce this problem to the problem of computing
discrete logarithms in the subgroup of order q generated by ge−1. It is easiest to describe the
algorithm recursively. The base case is when e = 1, in which case the reduction does nothing
because u already lives in a subgroup of order q.
Suppose now that e >1. Let f be some integer in the range 1 ,...,e −1. If u = gα, where
0 ≤α<q e, then we can write α= qfγ+ β, where β and γ are (unknown) integers with 0 ≤β <qf
and 0 ≤γ <qe−f. Therefore,
u= gα = g(qf)γ+β = gγ
f ·gβ. (16.4)
Since gf has order q(e−f), raising both sides to the power of qe−f leads to
u(qe−f) = (ge−f)β.
Note that ge−f has order qf. Therefore, we can recursively compute the discrete logarithm of
u(qe−f) to the base ge−f, and eventually obtain β.
Having obtained β, observe that by (16.4) we know that u/gβ = gγ
f. Moreover, gf has order
qe−f, and so if we recursively compute the discrete logarithm of u/gβ to the base gf, we obtain γ.
We then ﬁnd α by computing α= qfγ+ β.
684
It turns out that to get the best running time, one should choose f ≈e/2. Let us put together the
above ideas succinctly in a recursive discrete log (RDL) procedure:
Algorithm RDL. On input q,e,g,u as above, do the following:
if e= 1 then
return loggu / / base case: use a diﬀerent algorithm
else
set f ←⌊e/2⌉
β ←RDL
(
q, f, ge−f, u(qe−f))
/ / 0 ≤β <qf
γ ←RDL
(
q, e−f, gf, u/gβ)
/ / 0 ≤γ <qe−f
return qfγ+ β
To analyze the running time of this recursive algorithm, note that the body of one invocation
(not counting the recursive calls it makes) performs O(elog(q)) group operations. To calculate the
total running time, we have to sum up the running times of all the recursive calls plus the running
times of all the base cases.
The total number of base case invocations is exactly e, and all the base cases compute discrete
logarithms to the base ge−1. If each base case takes Tbase group operations, then the total time
spent on all base cases is O(e·Tbase).
To analyze the running time of the recursion, let’s roundeup to the closest power of two. Then,
the running time of the recursion is governed by the recurrence relation:
T(e) = 2T(e/2) + O(elog(q)).
If T(1) = 1 then this evaluates to T(e) = O(elog elog q). In our case, T(1) = Tbase, and hence the
total running time of Algorithm RDL is
O
(
e·Tbase + elog elog q
)
.
group operations. Usually, Tbase is the dominant term in this expression, and the other quantities are
small (poly-bounded). We conclude that computing discrete log in G is only as hard as computing
discrete log in the subgroup of prime order q.
16.1.2.2 Groups of composite order: the Pohlig-Hellman algorithm
Now suppose we have a group G of order n generated by g∈G. Suppose that
n= qe1
1 ···qer
r
is the factorization of n into distinct primes. We assume that this factorization is known. We are
given u∈G and we want to compute the discrete log of u base g.
Let u= gα, where 0 ≤α<n . For i= 1,...,r , let us deﬁne
q∗
i := n/qei
i ∈Z.
Then for each i= 1,...,r , we have
uq∗
i =
(
gq∗
i
)α.
685
Note that gq∗
i has order qei
i in G. Hence, if αi is the discrete logarithm of uq∗
i to the base gq∗
i, then
we have 0 ≤αi <qei
i and α≡αi (mod qei
i ). Thus, if we compute the values α1,...,α r ∈Z, using
Algorithm RDL in Section 16.1.2.1, we can obtain αusing the Chinese Remainder Theorem (CRT),
discussed in Appendix A. We summarize this in the following algorithm, called the Pohlig-Hellman
algorithm:
Algorithm PH. On input n,(q1,e1),..., (qr,er),g,u as above, do the following:
for i= 1 to r
αi ←RDL
(
qi, ei, gq∗
i, uq∗
i
)
/ / compute DL in a group of order qei
i
α′←CRT
(
(α1,qe1
1 ),..., (αr,qer
r )
)
output α′
To analyze the running of this algorithm, let T(w) be the number of group operations needed to
compute discrete log in a subgroup of G of prime order w. We assume that T(w) is monotonically
increasing in w. If we deﬁne qmax := max{q1,...,q r}, then the running time of Algorithm PH is
bounded by
r∑
i=1
O
(
eiT(qi) + eilog eilog qi
)
= O
(
T(qmax) log(n) + lognlog logn
)
group operations. We conclude that
the diﬃculty of computing discrete logarithms in a cyclic group of order nis determined
by the size of qmax, the largest prime dividing n.
Let us look at some important consequences of this fact. First, suppose G is a group of order
n= 2ℓ. Since the largest prime factor of nis 2, the Pohlig-Hellman algorithm will compute discrete
log in G using O(log n·log logn) group operations. Consequently, the DL assumption is false in all
such groups. In particular, when p is a prime of the form p= 2ℓ + 1 for some integer ℓ, the group
Z∗
p has order p−1 = 2ℓ. The discrete log problem in Z∗
p for all such p is easy to compute. More
generally, discrete log is easy in every group G whose order is a product of small primes. Again,
we conclude that
For DL to hold in G, the order of G must have at least one large prime factor.
Another way to interpret the Pohlig-Hellman algorithm is as follows. Let G be a cyclic group
of order n. Let q be the largest prime factor of n, and let Gq be the subgroup of G of order q. The
Pohlig-Hellman algorithm suggests that running the Diﬃe-Hellman protocol in the entire group G
is wasteful. We might as well run the protocol in the subgroup Gq where it is more eﬃcient: in
Gq the parties need only choose random exponents in {0,...,q −1}as opposed to {0,...,n −1}
in G. The Pohlig-Hellman algorithm suggests that restricting Diﬃe-Hellman to the subgroup Gq
does not aﬀect security; discrete log in G is only marginally harder than discrete log in Gq. This
explains why in practice, cryptosystems based on discrete log use a prime order group.
16.1.3 Information leakage in composite order groups
In some cases the Pohlig-Hellman algorithm in Section 16.1.2.2 can be used to compute partial
information about the discrete log, and this can lead to attacks. Suppose G is a cyclic group of
686
order n where n= n1n2 and n1 has only small prime factors. We assume that both n1 and n2 are
known. Let g be a generator of G.
We are given ( G,g,n 1,n2) as input, along with an element u ∈G where u = gα. The Pohlig-
Hellman algorithm can be used to learn partial information about α. In particular, an adversary
can compute α1 := (αmod n1). To learn α1, run Pohlig-Hellman to compute the discrete log of
un2 to the base gn2, and observe that the result is α1. We say that discrete log in such groups is
not very discreet.
As a concrete example, suppose G is a group of order n where n is even, so that n = 2n2 for
some n2. Then given g and u:= gα, one can easily compute αmod 2. The entire Pohlig-Hellman
algorithm in this case degenerates to the following simple observation:
Lemma 16.1. With G, g, and u= gα as above,
α is even ⇐⇒ un/2 = 1
Proof. If α is even then un/2 = gαn/2 = (gα/2)n = 1. Conversely, if un/2 = 1 then gαn/2 = 1, but
then n must divide αn/2 and hence α must be even. 2
This lemma gives a simple test that tells the adversary whether Dlogg(u) is even or not. This
test is very damaging when applied to the DDH problem. Suppose the adversary is given an
instance of DDH, namely a tuple ( gα,gβ,w) ∈G3, and it needs to decide if w = gαβ. First, we
observe that the adversary can learn the parity of αβ using the following lemma.
Lemma 16.2. With G, g, and u= gα, v= gβ ∈G as above
αβ ∈Zn is even ⇐⇒ (un/2 = 1 or vn/2 = 1)
Proof. By Lemma 16.1 we know that if ( un/2 = 1 or vn/2 = 1) then either α or β is even. But in
this case α·β ∈Zn is even. The converse follows in the same way. 2
The lemma shows that the adversary can learn one bit of information about the Diﬃe-Hellman
secret gαβ. Now, to decide if ( gα,gβ,w = gγ) is a DDH tuple, the adversary does:
step 1: use Lemma 16.1 to compute b0 ←parity(γ) ∈{even,odd}
step 2: use Lemma 16.2 to compute b1 ←parity(αβ) ∈{even,odd}
step 3: output accept if b0 = b1, and output reject otherwise
Observe that the adversary always accepts when w = gαβ, but only accepts with probability 1 /2
when w is uniform in G. Consequently, this algorithm has advantage 1 /2 in breaking DDH in the
group G.
This attack on DDH in groups of even order, and more generally, in groups whose order has a
small prime factor (Exercise 16.2), is another reason why we typically use groups of prime order.
Note that the group Z∗
p for some odd prime phas order p−1, which is an even number. One should
keep in mind that the leakage described above always applies in these groups, and as a result, DDH
does not hold in Z∗
p. Instead, we can use a prime order subgroup of Z∗
p, or a prime order elliptic
curve group, where DDH is believed to hold.
687
16.1.4 An attack on static Diﬃe-Hellman
Static Diﬃe-Hellman is a situation that comes up in some systems, and can weaken the security
of the discrete log problem. To describe the static Diﬃe-Hellman setup, let G be a cyclic group of
prime order q, and let α be some random secret element in Zq. The adversary is given access to
an oracle that takes an arbitrary v∈G as input and returns vα. The adversary’s goal is to recover
the secret α by querying the oracle. More precisely, the static Diﬃe-Hellman problem, or SDH, is
captured in the following game.
Attack Game 16.1 (Static Diﬃe-Hellman). Let G be a cyclic group of prime order q. For a
given adversary A, the game runs as follows.
• The challenger selects α←R Zq
• The adversary makes a sequence of SDH queries to the challenger. Each query is of the
form v∈G, to which the challenger responds with w:= vα ∈G.
• Finally, the adversary outputs some ˆα∈Zq.
We say that Awins the game if ˆα= α. 2
As usual, we say that static Diﬃe-Hellman, or SDH, holds in G if no eﬃcient adversary A
can win the static Diﬃe-Hellman game in G with non-negligible probability.
We had previously encountered a closely related problem in Section 11.6.3 where we constructed
an oblivious PRF using the one-more Diﬃe-Hellman assumption. An adversary Athat can win the
static Diﬃe-Hellman game in G can trivially also break the one-more Diﬃe-Hellman assumption in
G. This adversary Acan also be used to break the oblivious PRF we constructed in Section 11.6.3.
Hence, for the oblivious PRF to be secure, at the very least we need SDH to hold in G.
Static Diﬃe-Hellman also comes up when analyzing the CCA security ofmultiplicative ElGamal,
introduced in Exercise 11.5. This public key encryption scheme has a multiplicative homomorphism,
as explained in part (c) of the exercise, and it is therefore not CCA secure: a CCA adversary can
break semantic security. However, if SDH were easy in G, then a CCA adversary can go a step
further and recover the complete secret key. To see how, let α∈Zq be the secret key in an instance
of multiplicative ElGamal. We construct a key recovery CCA adversary B. Our Bplays the role of
challenger to an SDH adversary A. Whenever Aissues an SDH query for v ∈G, our adversary B
does: (i) it issues a decryption query to its challenger for the ciphertext c:= (1/v,1), (ii) it receives
back the decryption w:= vα, and (iii) it sends w as a response to A’s query. Eventually the SDH
adversary Aoutputs α, which is the desired secret key.
An attack on static Diﬃe-Hellman. The oblivious PRF discussed above is an example of a
setting where we need SDH to hold in G, otherwise the oblivious PRF construction is insecure. In
particular, winning Attack Game 16.1 should be as hard as breaking discrete log in G. However,
in some groups, SDH can be a bit easier than computing discrete log. To see why, let’s construct
an SDH adversary A. The SDH adversary operates as follows:
• step 1: let g be a generator of G, and set g0 := g.
• step 2: for i= 1,...,d : issue an SDH query for gi−1 ∈G and get back gi := gα
i−1.
688
• step 3: when step 2 completes the adversary has
vα :=
(
g, gα, g(α2), g(α3), ..., g(αd)
)
∈Gd+1.
Now, use the algorithm in Exercise 16.3 to ﬁnd α.
Exercise 16.3 shows that
• if q−1 is divisible by d, then α can be found using only ˜O
(√
q/d+
√
d
)
group operations;
• if q+ 1 is divisible by d, then α can be found using only ˜O
(√
q/d+ d
)
group operations.
For moderate size d, namely d < q1/3, these algorithms ﬁnd α in time ˜O(
√
q/d). This is about a
factor of
√
dfaster than the discrete log algorithm from Section 16.1.1, which runs in time O(√q).
Note that during the attack, the SDH adversary issues d SDH queries. One can show that in a
generic group (as in Section 16.3), an SDH adversary that makes d SDH queries and has success
probability one half, must take time at least Ω(
√
q/d). Hence, a
√
d speed-up is the best possible
in a generic group.
So what does this mean? For systems that rely on static Diﬃe-Hellman for security, one could
choose a group G so that q−1 and q+ 1 do not have moderate size factors. For example, consider
the Curve25519 from Section 15.3.3, and let q be the order of its large prime order subgroup. Then
q−1 = 132 ×p107 ×p137 and q+ 1 = 7210 ×p59 ×p180,
where pn is an n-bit prime. Hence, SDH in Curve25519 is safe from the attack above if the adversary
can issue at most 258 SDH queries. In particular, the oblivious PRF in Curve25519 is safe from the
attack above as long as the server chooses a fresh PRF key after every 258 PRF evaluation requests.
Curve P256 from Section 15.3.1, also known as secp256r1, is more vulnerable to this attack.
Let q′be the order of this curve, then
q′−1 = 48 ×71 ×131 ×373 ×3407 ×17449 ×38189 ×p27 ×p29 ×p40 ×p91 and
q′+ 1 = 10 ×1879 ×p27 ×p214.
The fact that q′−1 has so many small prime factors means that in the oblivious PRF scheme, after
232 PRF evaluation requests the diﬃculty of computing the secret key drops by about a factor of√
232 which is 2 16. Curve secp256k1 from Section 15.3.1 is similarly vulnerable. The order of the
curve minus one has a number of small factors that multiply to a 24-bit factor. The order of the
curve plus one has many small factors.
Remark 16.2. Consider a variation of the SDH game above, called hash SDH, where the adver-
sary can issue queries for an arbitrary v ∈G, but receives back H(v,vα) instead of vα. Here H is
some hash function H : G2 →K. This small variation prevents the attack described above. In fact,
one can show that when H is modeled as a random oracle, this hash SDH problem in a group G is
as diﬃcult as solving the interactive Diﬃe-Hellman (ICDH) problem in G (see Exercise 12.31). 2
689
16.1.5 The relation between DL, CDH, and DDH
The Diﬃe-Hellman protocol relies on the diﬃculty of solving the CDH problem, which is quite
diﬀerent from discrete log. How hard is CDH?
Let G be a ﬁnite cyclic group with generator g∈G. The best known general purpose algorithm
for solving CDH in G is the following:
input: u:= gα, v := gβ ∈G; output: gαβ ∈G
step 1: compute the discrete log of u to the base g to obtain α,
step 2: output vα = gαβ
This suggests that if discrete log in G is diﬃcult then so is CDH. However, perhaps there is a
shortcut, namely a faster algorithm for CDH that computes the answergαβ without ﬁrst computing
the discrete log of u or v. Can we rule out such a shortcut?
As a ﬁrst step in exploring the relation between CDH and discrete log we could look for a ﬁnite
cyclic group G of prime order q where CDH is easy, but discrete log is diﬃcult. Clearly the Diﬃe-
Hellman protocol will be insecure in such a group. However, as it turns out, such a group would be
quite useful. It would mean that Alice could choose a random α ←R Zq and hand u := gα over to
Bob. Bob would not learn α, however for any polynomial f ∈Zq[X], Bob could compute v:= gf(α)
just given u. Computing v would be done by a repeated application of the algorithm to compute
the CDH problem. In eﬀect, Bob is evaluating f(α) without explicit knowledge of α. This property
is related to the concept of fully homomorphic encryption , and has quite a few applications.
Currently, no one knows how to construct a ﬁnite cyclic group where CDH is easy, but discrete
log is hard. We encourage the reader to try to construct such a group. However, it is quite likely
that no such group exists. To prove that, we need to show that in every ﬁnite cyclic group G where
CDH is easy, discrete log is also easy. That is, we need to construct a discrete log algorithm base g
from a subroutine to compute CDH base g.
Although this is still an open problem, there is a partial answer. A result of Maurer and
Wolf [110] shows that there is a short advice string S, that only depends on q, with the following
property: once S is known, there is an eﬃcient algorithm to compute discrete log base g for
any u ∈G, using a CDH subroutine base g. Although the string S is just two elements in Zq,
there is unfortunately no known eﬃcient algorithm to ﬁnd it. Nevertheless, for several standard
cryptographic groups such as P256 and Curve25519 discussed in Section 15.3, this string S was
computed explicitly. Hence, for these groups we now know that an eﬃcient algorithm to compute
CDH gives an eﬃcient algorithm to compute discrete log. In other words, in these groups, there is
no algorithmic shortcut to compute CDH without also computing discrete log.
Discrete log and DDH. The situation is quite diﬀerent with respect to the DDH problem. In
Section 15.4 we constructed a group where DDH is easy, but discrete log is believed to be hard.
Recall that the algebraic tool that made this possible is called a pairing. The algorithm to decide
DDH is described in Section 15.4. This suggests that DDH can be an easier problem than discrete
log. Nevertheless, we have many candidate groups where DDH is believed to be hard.
16.2 Discrete log in Z∗
p: the general number ﬁeld sieve
The special structure of the group Z∗
p gives rise to several discrete log algorithms that are much
faster than the generic techniques discussed in Section 16.1.1. The fastest discrete log algorithm in
690
Z∗
p, called the general number ﬁeld sieve or GNFS, runs in conjectured time:
exp
((
α+ o(1)
)
(ln p)1/3 (ln lnp)2/3
)
(16.5)
where α := (64 /9)1/3 ≈1.92. The dominant term in this expression is exp
(
(ln p)1/3)
. We will
describe the algorithm in a bit more detail in Section 16.2.2 below.
So what does this mean? To understand (16.5) let us simplify things by only focusing on the
dominant term. The GNFS running time is dominated by the expression exp( 3√ln p). If p is an
n-bit prime this function behaves as exp( 3√n), ignoring constants. Now, suppose that for a ﬁxed n
GNFS computes discrete log in Z∗
p in time T. If we double the size of our prime and use a 2 n-bit
prime, the GNFS running time will grow to about
exp(
3√
2n) ≈exp(1.26 3√n) = T1.26
This is very diﬀerent from what we know for block ciphers — doubling the size of a block cipher
key squares the amount of work to do an exhaustive key search attack. In contrast, doubling the
security parameter for discrete log in Z∗
p increases security by far less. As a result, the GNFS forces
us to use relatively large primes when using Z∗
p.
So how large should the primes be? Concretely, one wants security of the public-key system to
be comparable to the security of the symmetric-key system being used. Hence, what size prime to
use depends on what size symmetric key is being used. The national institute of standards (NIST)
[127] recommends certain size primes. A more conservative study by Lenstra [103] recommends
slightly larger primes. These recommendations are summarized in the following table:
Size of Size of
Symmetric key prime (bits) prime (bits)
length (bits) [NIST] [Lenstra]
80 1024 1329
128 3072 4440
256 15360 26268
Note that for a 256-bit AES key one has to use an extremely large prime, over 15000 bits. Computing
with such large primes can be very slow — about a hundred times slower than computing with a
1024-bit prime. As a result, it is likely that the group Z∗
p will never be used when a 256-bit AES
key is needed. This is a direct result of the GNFS.
Another group where discrete log is believed to be hard is the group of points of an elliptic
curve over a prime ﬁnite ﬁeld, discussed in Chapter 15. Discrete log in this group appears to be
much harder than in Z∗
p. In particular, the best known algorithm takes time O(√q) where q is the
order of the group. Consequently, these groups scale much better with the security parameter. For
example, when using a 256-bit AES key one need only use a 512-bit elliptic curve group.
16.2.1 Discrete log records in Z∗
p
Let us look at some concrete records for computing discrete log in Z∗
p. To set up a discrete log
challenge, one has to choose the prime p, the base g, and the challenge y. The goal is to ﬁnd an
integer α such that y = gα in Zp. Typically, one sets g = 2 and chooses p and y in some random
fashion.
691
The following table shows the progress in computing discrete log over the years. The entries
refer to a discrete log computation in Z∗
p for a generic prime p. All these records were obtained
using variants of the GNFS discrete log algorithm.
Year Bits, ⌈log2 p⌉ Team members
2014 596 Bouvier, Gaudry, Imbert, Jeljeli, Thom´ e
2016 768 Kleinjung, Diem, Lenstra, Priplata, and Stahlke
2019 795 Boudot, Gaudry, Guillevic, Heninger, Thom´ e, Zimmermann
16.2.2 A preprocessing attack on discrete log in Z∗
p
The GNFS discrete log algorithm in Z∗
p has an important feature that is often overlooked: the
bulk of the work to compute the discrete log of an element u∈Z∗
p depends on the prime p, but is
independent of the speciﬁc element u. This means that one can pre-process the prime pby carrying
out the bulk of the discrete log work ahead of time, and then quickly compute the discrete log of
challenge values in Z∗
p.
For example, an experiment carried out in 2017 showed that for a 512-bit prime p, the pre-
processing phase runs in about two days on suﬃciently many machines. However, once the one-
time pre-processing phase is complete, computing the discrete log of a given u∈Z∗
p can be done in
about a minute [2]. For a larger prime, say for a 1024-bits prime p, the estimate is that once the
one-time pre-processing phase is complete, computing discrete log for a given u∈Z∗
p can be done
in about 30 days on a single core. These times can be greatly reduced by using multiple cores. Of
course, for a 1024-bits prime, the one-time pre-processing phase requires considerable eﬀort.
This aspect of the GNFS was used in the logjam attack [2] to exploit the fact that some
cryptographic standards that use Diﬃe-Hellman key exchange ﬁx the prime p used in the stan-
dard. A prominent example is the set of Oakley groups, which is a list of “safe” primes of length
768 (Oakley Group 1), 1024 (Oakley Group 2), and 1536 (Oakley Group 5). These groups were
published in 1998 and have been used for many standards based on Diﬃe-Hellman, including IPsec-
IKE, SSH, Tor, and Oﬀ-the-Record Messaging (OTR). A nation state could carry out the expensive
pre-computation for the Oakley primes, and then quickly break any key exchange it wants.
To avoid these pre-processing attacks, deployed systems that use Z∗
p should use a much larger
prime p, at least 3072 bits long, as suggested by the table in the previous section. Alternatively,
one could move to an elliptic curve group, where there is no known practical pre-processing attack.
16.3 A lower bound on discrete log in generic prime order groups
Because so much of cryptography relies on the diﬃculty of discrete log, we would like to prove
a lower bound on the time to compute discrete log in some speciﬁc family of groups. Presently,
proving such a lower bound appears to be far beyond our reach. However, if we focus our attention
only on “generic” discrete log algorithms, namely algorithms that treat group elements as opaque
strings and use the group operation as a black box, then it is possible to prove a lower bound. In
this section we prove such a lower bound. The technique is quite general and can be used to prove
a lower bound for many computational tasks in ﬁnite cyclic groups. We will see some examples in
Exercise 16.5. Note that these lower bounds only apply to classical (i.e., non quantum) algorithms
(see Section 16.5).
To explain what is a generic algorithm we ﬁrst need to deﬁne a generic group.
692
Generic groups. A generic group is an idealized model where group elements are represented
as opaque binary strings. In formulating an attack game in the generic group model, both the
challenger and the adversary access the group through an “oracle”. To formally model a generic
cyclic group of order q, we ﬁx a set Sof bit strings, where |S|≥ q. The oracle chooses a random
injective function L: Zq →S, which we call a labeling function. The oracle responds to queries of
the following type:
Labeling queries: The oracle receives β ∈Zq and responds with L(β).
Group operations: The oracle receives (ℓ1,ℓ2,κ1,κ2) ∈S×S× Zq×Zq. If there exist β1,β2 ∈Zq
such that L(β1) = ℓ1 and L(β2) = ℓ2, the oracle responds with L(κ1β1 + κ2β2); otherwise,
the oracle responds with reject.
We think of L(β) as corresponding to gβ, where g is a ﬁxed generator for the group. Therefore,
if ℓ1,ℓ2 correspond to gβ1,gβ2, the group operation query computes
gκ1β1+κ2β2 = (gβ1)κ1 ·(gβ2)κ2.
The baby-step/giant-step algorithm from Section 16.1.1 is an example of a discrete log algorithm
in a generic group. The algorithm takes as input g ←L(1) and u ←L(α), for some α ∈Zq. It
makes a sequence of calls to the group operation oracle, and outputs α.
The following theorem gives a lower bound on the time to compute discrete log in a generic
group.
Theorem 16.3. Let Abe an adversary that attacks the discrete logarithm problem for a generic
group of prime order q. The challenger chooses α∈Zq at random, obtains L(1) and L(α) from the
group oracle via a labeling query, and gives L(1) and L(α) to A. Suppose Athen makes a series
of at most T queries to the group oracle and then outputs ˆα∈Zq. Then
Pr[α= ˆα] ≤(3T + 2)2/2 + 1
q . (16.6)
Proof. We begin by describing the discrete log attack game, which we call Game 0, in more detail.
We describe the challenger’s logic as follows:
Initialization:
choose a random injective function L: Zq →S in Funs[Zq,S]
α←R Zq
invoke (label,1) to obtain g∈S / / g is the label of 1 ∈Zq
invoke (label,α) to obtain u∈S / / u the label of α∈Zq
return (g,u) / / send (g,u) to the adversary
To process an oracle query ( label,β): / / return the label of β ∈Zq
return L(β)
To process an oracle query ( group-op,ℓ1,ℓ2,κ1,κ2):
if ℓ1 = L(β1) and ℓ2 = L(β2) for some β1,β2
then invoke (label,κ1β1 + κ2β2) and return the result
else return reject
693
At the beginning of the game, a random injective function L : Zq →S is chosen, and then the
initialization step is executed, where result (g,u) is given to the adversary. After this, the adversary
makes a series of label and group-op queries, and then outputs ˆα∈Zq. We let W0 be the event
that the adversary wins the game, that is, α= ˆα.
Game 1. We modify Game 0 so that the challenger performs a “lazy” simulation of the random
function L. To this end, the challenger maintains an associative array Map : Zq →S , initially
empty, which partially represents L. One messy detail is that we have to deal with group oper-
ation queries ( group-op,ℓ1,ℓ2,κ1,κ2) where either ℓ1 or ℓ2 are not Range( Map), and so we have
to probabilistically decide if they belong to Range( L). If we decide that ℓj does not belong to
Range(L), we have to remember that decision. To keep track of these decisions, the challenger also
maintains a set S′⊆S, initially empty, which contains the elements of Sthat are known to not be
in Range(L).
We describe the challenger’s logic as follows:
Initialization:
α←R Zq
invoke (label,1) to obtain g∈S
(1) invoke ( label,α) to obtain u∈S
return (g,u)
To process a group oracle query ( label,β):
if β /∈Domain(Map) then
ℓ←R S\ (Range(Map) ∪S′)
Map[β] ←ℓ
return Map[β]
To process a group oracle query ( group-op,ℓ1,ℓ2,κ1,κ2):
for j = 1,2 do
if ℓj /∈Range(Map) ∪S′then
/ / probabilistically decide if ℓj is in Range (L)
ﬂip a biased coin that is “heads” with probability
(2)
(
q−|Range(Map)|
)
/
(
|S|−| Range(Map)|−|S ′|
)
∈[0,1]
if the coin is “heads” / / make ℓj be in Range(L)
then βj ←R Zq
while βj ∈Domain(Map) do βj ←R Zq
Map[βj] ←ℓj
else add ℓj to S′ / / mark ℓj as not in Range (L)
if ℓ1 = Map[β1] and ℓ2 = Map[β2] for some β1,β2
then invoke (label,κ1β1 + κ2β2) and return the result
else return reject
To understand the ratio in line (2), consider a randomly chosen injective function L: Zq →S.
At any point in time, we have the following partial information about L:
(i) L(β) = Map[β] for all β ∈Domain(Map), and
(ii) Range( L) ∩S′= ∅.
694
The numerator in line (2) is the size of Range( L) \Range(Map). The denominator in line (2) is
the size of S\ (Range(Map) ∪S′), which contains Range( L) \Range(Map). If follows that for
ℓj ∈ S\(Range(Map) ∪S′), the conditional probability that ℓj ∈Range(L) given the partial
information (i) and (ii) is precisely the ratio in line (2).
From this, it is easy to verify that the challenger in Game 1 perfectly simulates the challenger
in Game 0, and in particular,
Pr[W0] = Pr[W1]. (16.7)
Game 2. We modify Game 1 so that the challenger now performs a “symbolic” simulation of
the function L. Speciﬁcally, we introduce an indeterminate X, and the associative array is now
of the form Map : Zq[X] →S. The domain of Map will consist of linear polynomials of the form
P(X) = γX + δ for some γ,δ ∈Zq. The only diﬀerence between Games 1 and 2 is that we replace
line (1) above by
(1′) invoke ( label,X) to obtain u∈S
Note that while the challenger invokes label with a non-constant polynomial in the initialization
step and indirectly in the implementation of group-op, the adversary is only allowed to make direct
label queries with constant polynomials (as in Game 1). We let W2 be the event that α = ˆα in
Game 2.
Let us deﬁne the event Z in Game 2 as the event that for two distinct polynomials
P1(X),P2(X) ∈Domain(Map), we have P1(α) = P2(α). It is straightforward to see that Games 1
and 2 proceed identically unless the event Z occurs. In Game 2, the value of α is independent of
the adversary’s view and of all of the polynomials in Domain( Map). It follows that for any two
distinct polynomials P1(X),P2(X) ∈Domain(Map), we have P1(α) = P2(α) with probability at
most 1/q. Moreover, the size of Domain( Map) is at most 3T+ 2 (each group-op query can add up
to 3 elements to Domain(Map)). It follows that Pr[Z] ≤(3T+2)2/2q, and hence (by the Diﬀerence
Lemma) we have ⏐⏐Pr[W2] −Pr[W1]
⏐⏐≤(3T + 2)2/2q. (16.8)
Finally, we observe that
Pr[W2] = 1/q, (16.9)
since α is independent of the adversary’s view in Game 2. Combining (16.7)–(16.9), we obtain
Pr[W0] ≤(3T + 2)2/2q+ 1/q,
which proves the theorem. 2
The theorem shows that an adversary that makes at most T group oracle queries (even an
otherwise computationally unbounded adversary) computes the discrete log with probability at
most O(T2/q). If we want a generic discrete log algorithm that succeeds with probability one half,
then we must have T ≥Ω(√q) — concretely, T must be greater than about √q/3. This suggests
that the O(√q)-time algorithms discussed in the beginning of the chapter are the best possible, if
one does not use any special properties of the underlying group.
One should be careful not to read too much into this lower bound. The bound says nothing
about the diﬃculty of discrete log in any speciﬁc group, where an algorithm might take advantage
of speciﬁc properties of the group. A good example is a prime order subgroup of Z∗
p, where one
can take advantage of the structure of the group to give a much faster discrete log algorithm, as
discussed in Section 16.2.
695
Remark 16.3. One exception to the generic Ω( √q) lower bound is a discrete log attack that uses
a one-time pre-processing phase to analyze the generic group G and produce an “advice” string
of size O(q1/3). Using this advice string, there is an algorithm that computes discrete log in time
O(q1/3) for every u ∈G. This algorithm is presented in Exercise 18.19. Computing the advice
string takes time O(q2/3), but this is a one time cost that then enables us to compute discrete
log in time O(q1/3) in the worst case, for all inputs. For q ≈2256, a pre-processing computation
that takes time about q2/3 is beyond the present capabilities of humanity. There is also a lower
bound that shows that in a generic group, a discrete log algorithm with advice of size O(q1/3) that
succeeds with probability one half, must take time at least Ω( q1/3). Hence, in a generic group, the
algorithm with advice in Exercise 18.19 is the best possible. 2
16.4 Analyzing the factoring and RSA assumptions
We now turn to the security of the RSA problem and the associated problem of factoring inte-
gers. We will also present several elegant real-world attacks that apply when RSA is implemented
incorrectly. As we will see, RSA is a bit fragile. Seemingly benign deviations from the speciﬁed
algorithms can lead to devastating attacks.
Let pk := ( n,e) be an RSA public key, and sk := ( n,d) be the corresponding private key.
Here n = p·q is the product of two random ℓ-bit primes p and q, and e is relatively prime to
(p−1)(q−1). Recall that the RSA trapdoor function is deﬁned as F(pk,x) := xe ∈Zn and its
inverse is I(sk,y) := y1/e = yd ∈Zn. We say that an algorithm Asolves the RSA problem if it
can compute y1/e ∈Zn given only ( n,e) and y ∈Zn as input. More precisely, Asolves the RSA
problem if RSA adv[A,ℓ,e ] is non-negligible, as in Deﬁnition 10.5. The RSA assumption, which is
the basis for RSA encryption and signatures, says that there is no eﬃcient algorithm for the RSA
problem.
How quickly can we solve the RSA problem? The best known algorithm that takes ( n,e,y ) as
input and computes y1/e ∈Zn works as follows:
step 1: factor n to obtain p and q,
step 2: compute d←e−1 mod (p−1)(q−1),
step 3: output yd ∈Zn
Steps 2 and 3 are easy. The only diﬃcult step is step 1, factoring n. Therefore, for the remainder
of this section we will study the diﬃculty of factoring integers.
A natural question is whether there is a shortcut that lets us solve the RSA problem without
factoring n. While there is likely no shortcut, and the three step algorithm above is the best
possible, proving that the RSA problem is as hard as factoring remains an open problem. This is
discussed further in Exercise 16.9.
16.4.1 Factoring algorithms
In this section we look at the diﬃculty for factoring the large integers used in the RSA trapdoor
function. Recall that an RSA integer (also called an RSA modulus) is a product of two random
equal size primes.
The current fastest algorithm for factoring RSA integers is called the general number ﬁeld
sieve, or GNFS. The asymptotic running time for factoring an integernusing GNFS is conjectured
696
to be
exp
((
α+ o(1)
)
(ln n)1/3 (ln lnn)2/3
)
(16.10)
where α:= (64/9)1/3 ≈1.92. The dominant term in this expression is exp
(
(ln n)1/3)
.
The algorithm works by constructing two “random” integers xand y such that x2 ≡y2 mod n.
Then (x−y)(x+ y) = x2 −y2 is a multiple of n, and with a reasonable probability over the choice
of x,y, either gcd(x−y,n) or gcd(x+ y,n) is a non-trivial factor of n.
To test our real-world ability to factor RSA integers, there is a need for a list of challenge
numbers to factor. Such a list, called the RSA challenge numbers , was produced by RSA
Laboratories in 1991. Every number on the list is a product of two equal size primes. The list
contains 54 numbers of varying sizes; the smallest is 330 bits and the largest is 2048 bits. Every
number has a designation that identiﬁes its size. For example, RSA250 refers to the challenge
number whose size is 250 decimal digits, while RSA768 refers to the challenge number whose size
is 768 bits.
The following table shows the progress in factoring the RSA challenge numbers over the years.
All these records were obtained using variants of the GNFS factoring algorithm.
Year Designation Bits, ⌈log2 n⌉ Team members
2005 RSA200 663 Bahr, Boehm, Franke, Kleinjung
2010 RSA768 768 Kleinjung, Aoki, et al.
2019 RSA240 795 Boudot, Gaudry, Guillevic, Heninger, Thom´ e, Zimmermann
2020 RSA250 829 Boudot, Gaudry, Guillevic, Heninger, Thom´ e, Zimmermann
Every one of these factoring projects took several months to complete, running on thousands of
machines working in parallel.
The ECM factoring algorithm. Another important factoring algorithm is called the elliptic
curve method or ECM. The interesting fact about ECM is that its running time depends pri-
marily on the size of the smallest prime factor of the given integer. The ECM is often used when
one needs to factor a large integer that has a relatively small prime factor. Its asymptotic running
time for factoring an integer n whose smallest prime factor is p is conjectured to be
exp
(
c
√
ln p·ln lnp
)
·M(n)
for some ﬁxed constant c> 0, where M(n) is the time to multiply two elements in Zn. The ECM
has been used to ﬁnd a 274-bit prime factor of a number that is almost 1000 bits long.
The dominant term in the running time of ECM is exp(√ln p). Suppose one tries to use ECM to
factor an RSA integern, where the size of the smallest prime factor is about half the size ofn. Then,
ignoring constants, the running time of ECM would be dominated by the expression exp(
√
ln n),
which is much worse than the running time for GNFS, which is dominated by exp(
3√
ln n). This
explains why all the factoring records were obtained using GNFS. Nevertheless, the ECM can be
useful in factoring an asymmetric RSA integer n = p·q, where p is chosen to be much smaller
than q.
697
16.4.2 Attacks on RSA resulting from poor key generation
Recall that the RSA key generation procedure from Section 10.3 generates two randomindependent
ℓ-bit primes pand q and then sets n←p·q. While this may seem relatively straightforward, there
are many real-world diﬃculties in implementing this correctly. We give three examples.
Let us drill a little deeper into how a cryptographic library might implement this procedure. In
particular, where do the random bits needed to generatepand qcome from? Typically, the software
uses a random number generator (RNG) of the type discussed in Section 3.10. The generator is
initialized with a random seed, and then the system periodically adds more randomness to the
generator.
Now, consider the following natural RSA modulus generation procedure:
rng.initialize(seed) / / initialize RNG with a random seed
p←generate random prime(rng) / / generate p using bits from the RNG
rng.add randomness(bits) / / add more randomness to the RNG
q←generate random prime(rng) / / generate q using bits from the RNG
n←p·q
The random values seed and bits are generated by random events on the system, such as the time
at which interrupts occur. Some systems include a true random number generator that contributes
additional entropy to these values.
The question is what happens on a cheap electronic device, like a home WiFi router. Suppose
the appliance needs to generate a cryptographic key shortly after it is powered up for the ﬁrst time
at the customer’s home. The problem is that on ﬁrst power up the device has no entropy. Then
− seed is completely determined by the initial conﬁguration of the device, but
− bits is sampled after the ﬁrst prime p is generated, and by then enough entropy may be
generated by random events on the device so that bits is properly random.
We end up with an RSA modulus n = p·q where p is not random, but q is properly random. Is
this a secure RSA key that the device can use?
Suppose that a batch of bdevices all have the same initial conﬁguration. Each device generates
one RSA key pair. We end up with b RSA public keys with moduli n1,...,n b that all have the
same prime factor p, but diﬀerent q. An attacker who observes these public moduli can factor all
of them: simply compute gcd( ni,nj) = p for some 1 ≤i̸= j ≤b. Once these moduli are factored,
the attacker can compute the private keys of all b devices.
This experiment was carried out in 2012 [86, 104]. Researchers downloaded several million
RSA public keys used on the Web. They computed the gcd of all pairs, and to their surprise, this
factored about 0.2% of the moduli, or about 10,000 keys.
The lesson is that cryptographic key generation must always be done with proper randomness.
In the case of RSA, “half randomness” as in the example above, leads to RSA keys that are easily
broken.
16.4.2.1 Attacks caused by dependent primes
Recall that the prime factors pand qof an RSA modulus are generated as random independentℓ-bit
primes. Let’s see what goes wrong when they are not independent. Consider the following broken
RSA modulus generation procedure that tries to minimize the number of random bits needed:
698
RSAModGen(ℓ) :=
choose a random ℓ-bit integer r
p←NextPrime(r) / / ﬁnd the smallest prime greater than r
choose a random integer 0 <s ≤2(ℓ/2)−1
q←NextPrime(p+ s) / / ﬁnd the smallest prime greater than p+ s
output n←p·q
This procedure generates a prime p that is close to being uniformly sampled from the set of ℓ-bit
primes. The same holds for q. However the primes p and q are not independent; they are close to
one another. We can assume that p+ s≤q <p+ 2s and therefore
⏐⏐p−q
⏐⏐≤2s≤2ℓ/2 <2n1/4. (16.11)
An integer n= p·q that satisﬁes (16.11) can be factored quite easily. To see how, we ﬁrst need a
simple lemma.
Lemma 16.4. Let n= p·q and A= (p+ q)/2. If
⏐⏐p−q
⏐⏐<2n1/4 then
⏐⏐A−√n
⏐⏐<1/2.
The proof is a simple calculation and we leave it as an exercise. Now, to factor an integer
n= p·q that satisﬁes (16.11) do:
• step 1: Since p and q are odd primes, we know that p+ q is an even integer, and therefore
A = (p+ q)/2 is an integer. Moreover, by the arithmetic mean-geometric mean (AMGM)
inequality we know that A≥√n. Then, Lemma 16.4 shows that
⏐⏐p−q
⏐⏐<2n1/4 implies that A= ⌈√n⌉.
This lets us compute A eﬃciently from n.
• step 2: Once A is computed, we obtain a system of two equations in two variables, p and q,
over the reals: { p+ q = 2 A
p·q = n
This system is easily solved over the reals and the solution gives the factors p and q.
This factoring method can be generalized to factor integersn= p·qeven when pand qare not close;
all we need is a known dependence between p and q. For example, in Exercise 16.14 we generalize
the algorithm above to the case |3p−2q|< n1/4. Here p and q can be quite far apart, and yet
n= p·q can be eﬃciently factored. In fact, this can be further generalized to other dependencies
of the form
⏐⏐ap+ bq
⏐⏐<n1/4, where a,b ∈R are known values.
The lesson from these attacks is that the primes that make up the RSA modulus must be
generated independently of one another. A known relation between them could lead to a factoring
algorithm. These mistakes occasionally happen in real-world cryptography libraries (for example,
see CVE-2022-26320).
699
16.4.2.2 Attacks caused by non-uniform primes
Our third example of poor RSA key generation is a result of an ill-advised optimization. One way
to generate a random ℓ-bit prime is as follows: (i) generate a random ℓ-bit integer t, (ii) run a
primality testing algorithm to check if t is a prime, and (iii) repeat steps (i) and (ii) until a prime
is found. One can show that this procedure outputs a random prime in reasonable time.
Clearly, if a random ℓ-bit candidate thappens to be even, or divisible by three, then there is no
reason to run the primality test on t since we already know that t is not a prime. More generally,
we can reduce the number of primality tests in the procedure above by ﬁrst ensuring that the
candidate t is not divisible by a small prime. This step is called sieving because it sieves out all
the candidates that are a multiple of 2, 3, 5, etc. So, to generate a random ℓ-bit prime one repeats
the following two steps until a prime is found:
• sieve: generate an ℓ-bit candidate prime tsuch that thas no factors less than some bound B
(e.g., B = 1000),
• test: run a primality test on t.
While the primality test is the time consuming step, some implementations further optimize the
sieving step.
Here is one way to speed up sieving that is quite dangerous. Let S be the product of the ﬁrst s
primes, namely S := 2 ·3 ·5 ·7 ···ps ∈Z. To generate an ℓ-bit candidate prime t do:
choose a random ( ℓ−⌈log2 S⌉)-bit integer k, and set t←k·S+ 1.
This way t is an ℓ-bit integer (or possibly ( ℓ−1) bits), and this t is guaranteed to not be divisible
by the ﬁrst sprimes. Now, repeatedly choose a random k until the resulting tpasses the primality
test. Finally, output the RSA modulus n as a product of two such primes p and q.
For example, let S be the product of the ﬁrst 80 primes. This S is a 552-bit number, and
therefore, when generating a 1024-bit prime, one would choose k as a random 472-bit integer. This
ensures that t := kS + 1 is a 1024-bit integer (or possibly a 1023 bits). Hence, there is ample
entropy in the primes being generated thanks to the 472-bit random k.
The problem is that the pand q generated this way are not random ℓ-bit primes. In particular
p≡q ≡1 mod S for some known S. As it turns out, if S >Ω(n1/4) then such n can be factored
eﬃciently. The factoring algorithm, due to Don Coppersmith [45, 90], takes as input n and S,
and outputs the factors p and q of n. To make matters worse, an attacker can easily identify
all vulnerable RSA moduli that are vulnerable to the attack: simply look for moduli that satisfy
n≡1 mod S.
We note that the factoring algorithm works equally well even if p were generated as in the
previous paragraph, but q were generated as a truly random ℓ-bit prime. Hence, the problem is
not the relation between the primes. The attack is possible because p is not a uniform prime.
A real-world example. A sieving optimization of this nature, though somewhat more compli-
cated, was found in a widely used crypto library used by the Inﬁneon hardware to generate RSA
keys. This was discovered in 2017 and the resulting attack, called the ROCA attack [124], factors
the resulting RSA moduli. Because the Inﬁneon key generation procedure was implemented on
many consumer smartcards, all of them had to be recalled due to the attack.
700
16.4.3 A fault injection attack on optimized RSA
We now turn to another category of attacks called fault injection attacks. Let ( n,e) be an RSA
public key, and ( n,d) be the corresponding private key, where n = p·q, and p > q. Recall that
the RSA trapdoor function is deﬁned as F(pk,x) := xe ∈Zn and its inverse is I(sk,y) := yd ∈Zn.
The inverse function I is used in RSA signature generation and RSA decryption. Its running time
is dominated by a d-th power exponentiation in Zn.
The inverse function I(sk,y) can be implemented more eﬃciently using the Chinese Remainder
Theorem (CRT), discussed in Appendix A. The optimized implementation runs about four times
faster than the naive implementation and is called RSA-CRT. It uses a slightly modiﬁed secret
key that contains ﬁve integers sk′:= (p,q,d p,dq,qinv), where p and q are the factors of n, and
dp := dmod (p−1), d q := dmod (q−1), q inv := (1/q) mod p.
The optimized I(sk′,y) works as follows:
• step 1: xp ←(ydp mod p),
• step 2: xq ←(ydq mod q),
• step 3: ﬁnd the unique x∈Zn such that x≡xp mod p and x≡xq mod q. This is done as:
(i) x′
p ←qinv ·(xp −xq) mod p, and (ii) x←xq + x′
p ·q.
The reader can verify that indeed x≡xp mod p and x≡xq mod q.
The ﬁnal output is x∈Zn.
By deﬁnition of dp and dq we know that xp = (yd mod p) and xq = (yd mod q). Therefore, by
the Chinese Remainder Theorem, the ﬁnal x satisﬁes x= (yd mod n) as required.
This optimized implementation is about four times faster than directly computingx←yd in Zn.
The source of the speed-up comes from the fact that p and q are about half the size of n, and the
fact that the exponents dp and dq are half the size of d. When using a quadratic-time multiplication
algorithm, arithmetic modulo p is four times faster than arithmetic modulo n. Moreover, because
dp is half the size of d, the overall time to compute xp ←(ydp mod p), is eight times faster than
computing yd in Zn. Since we compute both xp and xq, the overall running time is four times faster
than the naive implementation.
A fault injection attack on RSA-CRT. The RSA-CRT optimization discussed above is com-
monly used in RSA implementations. These implementations can be vulnerable to a dangerous
class of attacks called fault injection attacks.
Fault attacks are based on the fact that computers are not perfect. In particular, environmental
interference can cause a computing device to make a calculation error. For example, a bit stored in
a register can spontaneously ﬂip due to external electromagnetic interference. If this happens even
once to a register holding some intermediate value in a long calculation, then the ﬁnal computed
value will likely be incorrect. Under normal operating conditions, a spontaneous bit ﬂip fault is
rare, but it can and does happen from time to time. Even worse, in some settings, an attacker can
induce faults using a fault injection process.
Let’s see how this aﬀects the RSA inversion function when it is used to sign a message m.
Suppose that (n,e) is an RSA public key, andsk′is the corresponding RSA-CRT private key. Recall
701
that the FDH-RSA signature is deﬁned as σ:= I(sk′,H(m)) for some hash function H : M→ Zn.
To compute I(sk′,H(m)) using RSA-CRT, the algorithm begins by ﬁrst computing the integers
xp and xq. Suppose that a fault occurs during the computation of xp, but the computation of xq
completes correctly with no faults. Then xp and xq satisfy the following relations:
xp ̸= (H(m)d mod p) and xq = (H(m)d mod q).
Let ˆσ∈Zn be the ﬁnal computed (faulty) signature derived from xp and xq, then
ˆσ≡xp ̸≡H(m)d (mod p) and ˆ σ≡xq ≡H(m)d (mod q).
Raising both sides to the power of e gives then
ˆσe ̸≡H(m) (mod p) and ˆ σe ≡H(m) (mod q).
To complete the attack, observe that if we treat ˆσe −H(m) ∈Zn as an integer in [0 ,n), then this
integer is divisible by q but is not divisible by p. Hence,
gcd
(
ˆσe −H(m), n
)
= q.
In summary, knowledge of the public key ( n,e), the message m, and the faulty signature ˆσ ∈Zn,
reveals the factorization of n. Once the factorization is revealed, anyone can calculate sk and sign
arbitrary messages.
This fault attack shows that a single faulty signature on a known message will compromise the
secret signing key. For this reason, many RSA-CRT implementations verify that the computed
signature is valid before outputting it to the outside world. The same applies to RSA decryption;
RSA inversion needs to be veriﬁed before the plaintext is released from the crypto library.
While faults can happen organically because of hardware failure, they can also be induced by an
attacker. For example, suppose the RSA secret key is stored on a small secure device, like a smart
card or a credit card, that is used to sign messages. The device is designed to make it diﬃcult to
extract sk from the device. However, an attacker that obtains the device can subject it to a harsh
environment that is deliberately designed to cause a low rate of faults. The attacker then simply
runs the device over an over to sign some ﬁxed message muntil ﬁnally it receives a faulty signature
that exposes the secret key. In practice, the attack is executed using specialized equipment that
heats up a particular part of the processor using a laser. The localized increase in temperature
causes the required low rate faults to occur.
Fault injection attacks apply to many other algorithms, including ElGamal decryption, and
even AES. The RSA-CRT example is a crisp example that shows why calculation errors are so
damaging to cryptographic implementations.
16.4.4 An attack on low secret exponent RSA
Our last example is another natural optimization of RSA that can harm security. Let pk = (n,e)
be an RSA public key and let sk = (n,d) be the corresponding private key. Recall that in RSA
decryption and RSA signing, the time consuming step is the exponentiation y ←xd, for some
x∈Zn.
702
A natural (but not very good) idea to speed-up RSA decryption and signing is to choose d∈Z
as an integer that is much smaller than n, but suﬃciently large that it cannot be found by a brute
force search. For example, one could choose d as a 128-bit random integer, and then calculate the
corresponding public e. In particular, one could modify the RSA key generation procedure from
Section 10.3 to work as follows:
RSAGen(ℓ,s) := / / for example, invoked as RSAGen(2048,128)
generate a random ℓ-bit prime p
generate a random ℓ-bit prime q
n←p·q
choose a random s-bit integer d where gcd(d,p −1) = gcd(d,q −1) = 1
e←d−1 mod (p−1)(q−1)
output pk := (n,e) and sk := (n,d).
This is the reverse of how an RSA key is generated in Section 10.3, where e is chosen ﬁrst, and
then a matching dis calculated. The beneﬁt of the procedure above is that the resulting dis much
smaller, and therefore RSA decryption and signing will be signiﬁcantly faster. For example, for a
2048-bit modulus n, the secret key dis normally a 2048 bits, but with the procedure above it could
be as low as 128 bits. Decryption and signing are then 2048 /128 = 16 times faster because of the
smaller d (or about four times faster than RSA-CRT).
Of course, encryption and signature veriﬁcation will be slower because e is much larger than
normal. In some settings, this is a worthwhile tradeoﬀ, for example, when a low power signing
device, such as a smartcard, is connected to a high power veriﬁer.
Wiener’s attack. The key generation procedure described above is insecure in the worst possible
way: an eavesdropper who sees the public key ( n,e) can quickly calculate the private key ( n,d).
The attack works whenever d<n 1/4/3. For example, if n> 22048 then the attack works whenever
d< 2512/3. Hence, choosing das a 128 bit integer is clearly insecure. We state this in the following
theorem.
Theorem 16.5 (Wiener, 1989). Let (n,e) be two positive integers, where n= pq is the product
of two ℓ-bit primes. Moreover, suppose d:= e−1 mod (p−1)(q−1) exists and is less than n1/4/3.
Then there is an eﬃcient algorithm Athat takes (n,e) as input, and outputs d.
Proof. Let ϕ:= (p−1)(q−1). First, observe that d= e−1 mod ϕ implies that ed= 1 mod ϕ, and
therefore there is an integer k such that e·d = k·ϕ+ 1. Notice that this equality implies that
gcd(k,d) = 1. By dividing both sides by d·ϕ and rearranging terms, we obtain
⏐⏐⏐⏐
e
ϕ −k
d
⏐⏐⏐⏐= 1
dϕ < 1√n. (16.12)
The rightmost inequality holds because ϕ > √n. We know nothing about the fraction ( k/d).
However, we have a very good approximation to the fraction (e/ϕ). In particular, ϕ= n−p−q+1,
so that
⏐⏐n−ϕ
⏐⏐< p+ q <3√n. In other words, n is a very good approximation to ϕ. Therefore,
we can replace the fraction ( e/ϕ) in (16.12) by the fraction ( e/n) without aﬀecting the inequality
much. To do so, we will need the following simple fact:
⏐⏐⏐⏐
e
n −e
ϕ
⏐⏐⏐⏐=
⏐⏐⏐⏐
e(n−ϕ)
nϕ
⏐⏐⏐⏐< e
ϕ ·3√n
n < 3√n.
703
Now, writing (e/n) in (16.12) in place of ( e/ϕ) gives:
⏐⏐⏐⏐
e
n −k
d
⏐⏐⏐⏐≤
⏐⏐⏐⏐
e
n −e
ϕ
⏐⏐⏐⏐+
⏐⏐⏐⏐
e
ϕ −k
d
⏐⏐⏐⏐< 3√n + 1√n = 4√n < 1
2d2 , (16.13)
The last inequality follows from the assumption that d<n 1/4/3 which implies that 2 d2 <√n/4.
The inequality (16.13) says that the rational number (e/n), which we have, is close to the fraction
(k/d), which we want. The two fractions are so close, that their diﬀerence is less than 1 /(2d2).
Such a strong approximation of one fraction by another is rare: there are very few fractions that
satisfy (16.13) and all of them can be found eﬃciently. We state this in the following lemma.
Lemma 16.6. Let α:= e/n. Let S be the set of all rational numbers (a/b) that satisfy
⏐⏐⏐α−a
b
⏐⏐⏐< 1
2b2 . (16.14)
Then (i) the size of S is at most 2 log2 n and (ii) there is an eﬃcient algorithm that takes α as
input, and outputs the set S using O(log n) arithmetic operations.
The proof of the lemma is based on a theorem of Legendre from the theory of continued
fractions [84, Ch. 10]. The algorithm in part (ii) is an application of the extended Euclidean
algorithm. We prove the lemma in Exercise 16.11.
To complete the proof of Theorem 16.5, algorithm Atakes as input a public key ( n,e) and
works as follows. It runs the algorithm from part (ii) of Lemma 16.6 to obtain the set of all 2 log 2 n
rational numbers satisfying (16.14). By (16.13), one of these rational numbers must be equal to
k/d. Since gcd( k,d) = 1, the target d must be the denominator of one of the numbers in the set.
The algorithm tries all 2 log2 n denominators until it ﬁnds one that works. 2
The attack algorithm described in the proof of Theorem 16.5 is guaranteed to work up to the
bound d<n 1/4/3, but may not work when dexceeds this bound. One might be tempted to set dto
be slightly bigger than n1/4 and still beneﬁt from the speed-up obtained from a small d. However,
this is also insecure. More powerful techniques are able to recover d from (n,e) even when d is
about n0.3. It is conjectured that d can be eﬃciently found whenever d < n0.5, however, as of
this writing, this is still an open problem. Taking d on the order of n0.5 gives no speed-up over
RSA-CRT, so we might as well use the standard RSA key generation procedure and beneﬁt from
a small e.
A diﬀerent approach: small CRT exponents. In Section 16.4.3 we explained that RSA
decryption and signing is typically implemented using the Chinese Remainder Theorem (CRT).
Here the signer stores dp := (dmod p−1) and dq := (dmod q−1) and computes xd in Zn by ﬁrst
computing xdp in Zp and xdq in Zq. The same applies to RSA decryption.
To speed up signing and decryption, we could slightly modify RSAGen( ℓ,s) described at the
beginning of this section to choose dp and dq as random (small) s-bit integers, and then set d as
the smallest integer dsatisfying d≡dp mod (p−1) and d≡dq mod (q−1). This way, dis unlikely
to be a small integer, so that the attack of Theorem 16.5 does not apply. However we still obtain
a fast RSA-CRT signing algorithm because dp and dq are small. The resulting exponent dis called
a small CRT exponent.
Unfortunately, this approach is also insecure. In the notes we refer to a number of attacks on
this setup. For example, one can recover d from (n,e) whenever s< 0.244 ·ℓ.
704
This completes our discussion of attacks on RSA. This section should convince the reader that
it is best to not deviate from the RSA key generation procedure described in Section 10.3.
16.5 Quantum attacks on factoring and discrete log
To complete our review of attacks on number theoretic assumptions we must also describe the
looming threat of a quantum computer. The story begins with physicists who tried to simulate
quantum experiments on a digital computer. They quickly came to realize that the required com-
puting resources are enormous, even for relatively simple experiments. This lead Richard Feynman
to suggest that perhaps quantum experiments implement a new computing model that can be used
to solve certain computational problems faster than what can be done on a classical digital com-
puter. Astonishingly, this prediction turned out to be true. There are now numerous computing
tasks for which a quantum algorithm vastly outperforms the best known classical algorithm.
One of the best examples is a quantum algorithm, due to Peter Shor, that solves both problems
discussed in this chapter in polynomial time: the algorithm can eﬃciently factor integers, and
eﬃciently compute discrete logarithm in every group. The details of Shor’s algorithm require a
detailed introduction to quantum computing. We refer the reader to [125] for the full details. Here
we give a brief overview to explain the main idea.
Shor’s algorithm. The core observation in Shor’s algorithm is that a quantum computer is very
good at detecting periodicity. Speciﬁcally, let f : Z →S be some function. We say that f is
periodic if there exists a 0 ̸= π ∈Z such that f(x) = f(x+ π) for all x ∈Z. This π is called a
period of f. If π is a period, then so is k·π for every k∈Z. Let ℓ:= |π0|, where π0 is the smallest
non-zero period of f.
Suppose that we are only given black-box access to the function f. We can evaluate f at any
input x, but do not have access to its inner workings. Moreover, suppose that evaluating f at
an input x in {0,1,...,ℓ 2}takes time T(f) in the worst case. It is not diﬃcult to show that on
a classical computer, the best algorithm for ﬁnding a period needs to evaluate f at Ω(ℓ) inputs,
in the worst case. The magic of Shor’s algorithm is that, on a quantum computer, it can ﬁnd a
period of f (i.e., a multiple of ℓ) in time O(T(f) + logℓ). This is an exponential speed-up over a
classical computer: the linear dependence on ℓon a classical computer is replaced by a logarithmic
dependence on ℓ on a quantum computer.
We next show that factoring integers can be formulated as a period ﬁnding problem.
Quantum factoring. Let n be an integer, and let g be some non-zero element in Zn. Let
fg : Z →Zn be the function
fg(α) := gα.
This function is periodic. Let π be a positive integer such that gπ = 1, then π is a period of fg.
Indeed, fg(α) = gα = gα+π = fg(α+ π) for all α ∈Z. In fact, every period π of fg must be an
integer multiple of the order of g in the multiplicative group Zn.
To factor an RSA modulus n= pq, choose a random g in Zn, and use Shor’s algorithm to ﬁnd
a period π of the function fg : Z →Zn. We know that gπ = 1. Let t be the smallest positive
integer such that gπ/2t
̸= 1. In Exercise 16.10 we show that gcd( n, gπ/2t
−1) is a non-trivial factor
705
of nwith probability about one half over the choice of g. Hence, if we can ﬁnd a period of fg for a
random g∈Zn, we can factor n.
Shor’s algorithm ﬁnds some period π of fg in time O(T(fg) + logn), where T(fg) is the time to
compute f(α) = gα, for an integer αis in [0,n2]. The naive exponentiation algorithm runs in cubic
time, and therefore the total running time of this factoring algorithm is O(log3 n). Hence, Shor’s
algorithm factors n in polynomial time in log 2 n.
Quantum discrete log. A similar period ﬁnding approach can be used to compute discrete log.
Let G be a ﬁnite cyclic group of order q with generator g ∈G. We want an algorithm that takes
u ∈G as input, and outputs an α ∈Z such that u = gα. We can reformulate this as a period
ﬁnding problem, but this time for a function f : Z2 →S deﬁned over the two-dimensional domain
Z2. We say that π= (π1,π2) ∈Z2 is a period of f if f(x) = f(x+ π) for all x∈Z2.
Now, let u = gα. To compute the discrete log of u base g, consider the function f : Z2 →G
deﬁned as
f(γ,δ) := gγ ·uδ.
It is easy to see that (0 ,q) and (q,0) are periods of this function, but these periods reveal nothing
about the discrete log α. However, there is another period, namely ( α,−1), that is more useful.
To see that (α,−1) is a period observe that for all ( γ,δ) ∈Z2
f(γ+ α,δ −1) = gγ+α ·uδ−1 = (gγuδ) ·(gα/u) = f(γ,δ).
It is not diﬃcult to show that the set of periods of the function f is the set of all integer linear
combinations of the three vectors
{
(q,0), (0,q), (α,−1)
}
. This set forms a two dimensional lattice
L in Z2.
Shor’s quantum algorithm samples a random period (π1,π2) from Lwhose length is at most q2.
Then (π1,π2) = a·(q,0) + b·(0,q) + c·(α,−1) for some integers a,b,c ∈Z. Therefore,
(
(π1,π2) mod q
)
= (ˆcα,−ˆc) ∈Z2
q
for some ˆc∈Zq, and with high probability ˆc̸= 0. Therefore, computing ˆα:= −π1/π2 in Zq reveals
the discrete log of u to the base g, as required. The algorithm’s running time is dominated by
the time to compute the function f(γ,δ) for γ,δ ∈{0,...,q 2}. Using naive exponentiation, the
algorithm runs in time O(log3 q).
The remarkable thing about this algorithm is that it works in all groups of prime order q, and
only uses the group operation as a black box. On a classical computer, one can show that the best
algorithm in this setting runs in time Ω( √q) (see, e.g., [145]). Hence, again, a quantum algorithm
gives an exponential improvement over the best classical method.
What does this mean? If the postulates of quantum mechanics are correct, then building a
quantum computer large enough to carry out Shor’s algorithm is mainly an engineering problem,
although a very challenging one. Steady progress is being made, and it is anticipated that a large
enough quantum computer will be built sometime this century. Because of this, the National
Institute of Standards (NIST) is standardizing a family of key exchange protocols and signature
schemes that remain secure against an adversary that has access to a large scale quantum computer.
These systems are collectively called post-quantum cryptography.
706
Currently, the most eﬃcient post-quantum key exchange protocols result in considerably more
traﬃc than Diﬃe-Hellman key exchange. Hence, there is some cost to deploying these protocols in
practice. Nevertheless, for traﬃc that is encrypted today and needs to remain secret for more than
thirty years, it is important to use a post-quantum cryptosystem.
16.6 Notes
To be written.
16.7 Exercises
16.1 (Multi instance discrete log). Let G be a cyclic group of order q generated by g ∈G.
Let α1,...,α n ←R Zq, and set ui := gαi for i = 1 ,...,n . Suppose we are given the n group
elements u1,...,u n ∈G and wish to compute the discrete log of all of them to the base g. Naively,
this requires running a discrete log algorithm n times, which takes time O(n√q) using the baby
step/giant step algorithm from Section 16.1.1. Show how to modify the baby step/giant step
algorithm so that computing the discrete log of all u1,...,u n ∈G to the base g only takes time
O(√nq). This is much better than the naive method.
Hint: try running the baby step for √nq steps instead of √q steps.
16.2 (An attack on DDH in composite order groups). In Section 16.1.3 we presented an
algorithm that has advantage 1/2 in deciding DDH in a cyclic group G whose order is even. Let G
be a cyclic group of order q, and let ℓ be the smallest prime factor of q.
(a) Generalize the DDH algorithm from Section 16.1.3 to give an algorithm that has advantage
about 1/ℓin deciding DDH in G. Your algorithm should use only O(log q) group operations.
(b) Give a an algorithm to decide DDH in G with advantage 1 −(1/ℓ). Your algorithm can use
O(
√
ℓ+ logq) group operations.
Your algorithms show that if the smallest prime factor of q is small, then DDH is false in G.
16.3 (The Brown-Gallant-Cheon algorithm). Let G be a cyclic group of order q with gener-
ator g∈G. For α←R Zq, let
vα :=
(
gα,g(α2),...,g (αd)
)
.
Suppose that the best discrete log algorithm in G runs in time Ω( √q). We show that in some
groups G, the information provided in vα can lead to a faster algorithm to ﬁnd α.
(a) Suppose q−1 is divisible by an integer s≤d. Design an algorithm A1 that takes vα as input,
and outputs α using only ˜O
(√
q/s+ √s
)
group operations.
Hint: Observe that αs lies in a subgroup of Z∗
q of size (q−1)/s. Let γ ∈Z∗
q be a generator
of this subgroup and write αs = γu for some (unknown) integer 0 ≤u <(q−1)/s. First,
show how to ﬁnd u in time O
(√
q/s·log q
)
. using the baby-step-giant-step algorithm from
Section 16.1.1. Then show how to ﬁnd α in additional time O
(√s·log q
)
.
(b) Suppose q+1 is divisible by some integers≤d. Design an algorithm A2 such that A2(vα) = α,
and A2 uses only ˜O(
√
q/s+ s) group operations.
707
Hint: Use the fact that a quadratic extension of Fq has a subgroup of size q+ 1. Then use
the same approach as in part (a). If you get stuck, see [42].
Discussion: Algorithms A1 and A2 show that if either q+ 1 or q−1 are divisible by d, then
the data provided in vα can be used to speed up the recovery of α by about a factor of
√
d over a
generic discrete log algorithm. When dis small, this does not impact security much. Nevertheless,
if needed, one can compensate by either using a group G with a slightly larger q, or by ensuring
that q+ 1 and q−1 do not have moderate-size factors. See the discussion in Section 16.1.4, which
also points to several places in the book where this algorithm is relevant.
16.4 (An expected time discrete log algorithm). Let G be a group of prime order q with
generator g ∈G. Construct a randomized algorithm that has expected constant running time and
succeeds in computing discrete log in G with probability at least 1/√q. The probability is over the
random bits of the algorithm. Your algorithm should be generic: it should treat group elements as
opaque strings, and treat the group operation as a black box.
Hint: try an algorithm that tosses a biased coin that comes up heads with probability 1/√q. When
the coin comes up heads the algorithm runs for √q steps. Otherwise, the algorithm outputs 0 and
stops.
Discussion: Theorem 16.3 shows that a generic algorithm that has worst-case constant running
time can succeed with at most probability 1/qin computing discrete log. Your algorithm shows that
a generic algorithm that has expected constant running time can do much better: it succeeds with
probability 1/√q. How much better can an expected running time algorithm do? One can adapt
Theorem 16.3 and show that a generic discrete log algorithm that has expected running time T
succeeds in computing discrete log with probability O(T/√q). See [94].
16.5 (A generic lower bound on CDH). In Section 16.3 we proved a lower bound on the time
to compute discrete log in a generic group G of prime order q.
(a) Adapt the proof of Theorem 16.3 to prove a similar lower bound on the time to compute
CDH in G with probability one half.
(b) Generic lower bounds can also be proved for interactive games. Adapt the proof of Theo-
rem 16.3 to prove a lower bound on the time to solve the interactive computational Diﬃe-
Hellman (ICDH) problem from Deﬁnition 12.4 with probability one half.
16.6 (A multiple of ϕ(n) reveals the factorization of n). Let n be a composite integer and
let k be a multiple of ϕ(n) = ( p−1)(q−1), that is, k = c·ϕ(n) for some (unknown) integer c.
Show an eﬃcient algorithm that takes n and k as input, and outputs a non-trivial factor of n.
16.7 (An insecure variant of RSA). Consider again the RSA-based public-key encryption
scheme ERSA from Section 11.4.1. Suppose we change the scheme so that all users make use of the
same RSA modulus n, but every user is assigned its own encryption exponenteand a corresponding
decryption exponent d. For example, Alice is assigned ( e1,d1) and Bob is assigned ( e2,d2). Show
that this scheme is insecure: Alice can decrypt all messages sent to Bob.
16.8 (Discrete log in Zn is as hard as factoring). Let n= pq be an RSA modulus with an
unknown factorization. Let g ∈Z∗
n be an element of order at least n/k, for some k, and let G be
the subgroup of Z∗
n generated by g. Show that an oracle for computing discrete log to the base g in
708
G can be used to ﬁnd the factorization of nusing an expected O(k) calls to the oracle. This shows
that for small k, computing discrete log in G is at least as hard as factoring n.
16.9 (Computing some roots in Zn is as hard as factoring). Let n= pqbe an RSA modulus
with an unknown factorization.
(a) Let O2 be an oracle that takes as input u ∈Zn and returns a square root of u in Zn if one
exists, and ⊥otherwise. Show an eﬃcient algorithm that uses the oracle O2 to factor n.
(b) Suppose that a poly-bounded integer edivides p−1, but emay or may not divide q−1. Let
Oe be an oracle that takes as input u∈Zn and returns an e-th root of u in Zn if one exists,
and ⊥otherwise. Show an eﬃcient algorithm that uses the oracle Oe to factor n.
Discussion: These results do not prove that breaking RSA is as hard as factoring. To do that,
one has to show that an oracle Oe, for some e where gcd(e,ϕ(n)) = 1, can be used to eﬃciently
factor n. Neither parts (a) or (b) apply to such e, and in fact, this is still an open problem.
16.10 (Support for quantum factoring). In this exercise we prove some basic facts that are
needed for the quantum factoring algorithm from Section 16.5. Let n= pq be an RSA modulus.
(a) Use the Chinese Remainder Theorem to show that there are four square roots of unity in Zn.
That is, there are four distinct elements r1,r2,r3,r4 ∈Zn such that r2
1 = r2
2 = r2
3 = r2
4 = 1.
Moreover, gcd(n, ri−1) is a non-trivial factor of nfor exactly two of these four square roots.
(b) Let g ∈Z∗
n, and let π be an integer multiple of the order of g in Z∗
n. Then gπ = 1. Let t be
the smallest positive integer such that h:= gπ/2t
̸= 1. By deﬁnition h2 = 1. Show that if g is
chosen uniformly in Z∗
n, then his uniformly distributed among the four square roots of unity.
Combining parts (a) and (b) we conclude that gcd( n,gπ/2t
−1) is a non-trivial factor of n with
probability 1/2 over the choice of g in Z∗
n.
16.11 (Proof of Lemma 16.6). Let e,n be two integers. Let S be the set of all rational numbers
(a/b) such that
⏐⏐(e/n) −(a/b)
⏐⏐<1/(2b2).
(a) Show that if ( a/b) ∈S, where gcd( a,b) = 1 and b> 0, then b≤n. This proves that the set
S is ﬁnite.
Hint: use the fact that if ( a/b) ̸= (e/n) then
⏐⏐eb−an
⏐⏐is a non-zero integer, and therefore
must be at least 1.
(b) Let S = {(a1/b1),..., (ak/bk)}where b1 ≤b2 ≤... ≤bn. Show that bi+2 > 2bi for all
i= 1,...,k −2. Deduce from this and from part (a) that |S|≤ 2 log2 n, as stated in part (i)
of Lemma 16.6.
Hint: for every three consecutive numbers ( ai/bi), (ai+1/bi+1), (ai+2/bi+2) in the set S, at
least two are on the same side of ( e/n). Say, ( ai/bi) and ( ai+2,bi+2) are on the same side.
Then
⏐⏐(ai/bi) −(ai+2/bi+2)
⏐⏐<1/(2b2
i). Show that this implies that bi+2 >2bi.
(c) Show how to eﬃciently construct all the numbers inSusing the extended Euclidean algorithm.
If you get stuck, see [148, Ex. 4.18(d)].
16.12 (Factoring challenge #1). The following integer n is a product of two primes p and q
such that
⏐⏐p−q
⏐⏐<2n1/4. Use the factoring algorithm from Section 16.4.2.1 to ﬁnd the factors of
709
n = 17976931348623159077293051907890247336179769789423065727343008115 \
77326758055056206869853794492129829595855013875371640157101398586 \
47833778606925583497541085196591615128057575940752635007475935288 \
71082364994994077189561705436114947486504671101510156394068052754 \
0071584560878577663743040086340742855278549092581
16.13 (Factoring challenge #2). The following integer n is a product of two primes p and q
such that
⏐⏐p−q
⏐⏐<211 ·n1/4. Find the factors of
n = 6484558428080716696628242653467722787263437207069762630604390703787 \
9730861808111646271401527606141756919558732184025452065542490671989 \
2428844841839353281972988531310511738648965962582821502504990264452 \
1008852816733037111422964210278402893076574586452336833570778346897 \
15838646088239640236866252211790085787877
Hint: Let A= (p+ q)/2. Show that A−√n <220. Then try scanning for A from √n upwards,
until you succeed in factoring n.
16.14 (Factoring challenge #3). The following integer n is a product of two primes p and q
such that
⏐⏐3p−2q
⏐⏐<n1/4. Find the factors of
n = 72006226374735042527956443552558373833808445147399984182665305798191 \
63556901883377904234086641876639384851752649940178970835240791356868 \
77441155132015188279331812309091996246361896836573643119174094961348 \
52463970788523879939683923036467667022162701835329944324119217381272 \
9276147530748597302192751375739387929
Hint: Generalize Lemma 16.4 to show that
√
6n is close to 3p+2q
2 . Then adapt the factoring
algorithm from Section 16.4.2.1 to factor n.
16.15 (Speeding up RSA inversion). In Section 16.4.3 we saw that inverting the RSA function
using RSA-CRT is about four times faster than the naive implementations of RSA inversion.
(a) Show that if the RSA modulus n is a product of three equal size primes, n = pqr, then
inverting the RSA function using RSA-CRT is about nine times faster than the naive imple-
mentation.
(b) For the next part of the question we will need the following fact called Hensel lifting. Let
p be a prime, let c∈Z∗
p, and let x∈Z satisfy xe ≡c (mod p). Set
x′:= x−xe −c
exe−1 mod p2. (16.15)
Use the binomial theorem to show that x′∈Z satisﬁes (x′)e ≡c (mod p2). Equation (16.15)
lets us quickly calculate the e’th root of c modulo p2 from the e’th root of c modulo p. The
equation lifts the e’th root of c from p to p2.
(c) Suppose the RSA modulus nis chosen so that n= p2q, for two equal size primes pand q. Fur-
ther, suppose that the RSA public exponent eis small, say e= 3. Let dbe the corresponding
710
private exponent. Show that inverting the RSA function can be done about thirteen times
faster than the naive implementation. In particular, for a given y ∈Z∗
n, use the following
procedure to compute yd mod n: (i) compute xp := yd mod p and xq := xd mod q, (ii) use
(16.15) to compute x′
p := yd mod p2, and (iii) apply the CRT to x′
p and xq to obtain x∈Zn
such that x= yd mod n. Analyze the running time of the resulting algorithm, and show that
it is about thirteen times faster than naively computing yd in Zn.
711
Part III
Protocols
714
Chapter 18
Protocols for identiﬁcation and login
We now turn our attention to the identiﬁcation problem, also known as the login problem. Party A
wishes to identify itself to party B to gain access to resources available at B. She does so using an
identiﬁcation protocol, which is one of the fundamental tools provided by cryptography. We give a
few illustrative applications that will be used as motivation throughout the chapter.
Opening a door lock. Alice wants to identify herself to a digital door lock to gain access to a
building. Alice can use a simple password system: she inserts her key into the door lock and the
door lock opens if Alice’s key provides a valid password. A closely related scenario is a local login
screen on a computer or a mobile phone. Alice wants to identify herself to the computer to gain
access. Again, she can use a password to unlock the computer or mobile phone.
Unlocking a car. Alice wants to unlock her car using a wireless hardware key, called a key fob,
that interacts with the car. An adversary could eavesdrop on the radio channel and observe one
or more conversations between the wireless key fob and the car. Nevertheless, this eavesdropping
adversary should not be able to unlock the car itself.
Login at a bank’s automated teller machine (ATM). Alice wants to withdraw cash from her
account using a bank ATM. The problem is that she may be interacting with a fake ATM. A report
from a large ATM equipment manufacturer explains that fake ATM’s are a big concern for the
banking industry [129]:
The ﬁrst recorded instance of using fake ATMs dates back to 1993 when a criminal gang
installed a fake ATM at a shopping mall in Manchester. Like most fake equipment it
was not designed to steal money. Instead, the fake ATM appeared to customers as if it
did not work, all the while stealing card data from everyone who attempted to use it.
Using a fake ATM, the adversary can interact with Alice in an attempt to steal her credential, and
later use the credential to authenticate as Alice. We call this an active adversary. We aim to design
identiﬁcation protocols that ensure that even this active adversary cannot succeed.
Login to an online bank account. Our ﬁnal example is remote login, where Alice wants to access
her online bank account. Her web browser ﬁrst sets up a secure channel with the bank. Alice then
runs an identiﬁcation protocol over the secure channel to identify herself to the bank, say using a
password. As in the ATM example, an adversary can clone the bank’s web site and fool Alice into
identifying herself to the adversary’s site. This attack, called phishing, is another example where
the adversary can play an active role while interacting with Alice. The adversary tries to steal
715
her credential so that it can later sell the credential to anyone who wishes to impersonate Alice
to the real bank. Again, we aim to ensure that even a phishing adversary cannot learn a working
credential for Alice. We discuss phishing attacks in more detail in Section 21.11.1 where we also
discuss a potential cryptographic defense.
Identiﬁcation (ID) protocols. Identiﬁcation protocols are used in all the scenarios above.
Abstractly, the identiﬁcation problem involves two parties, a prover and a veriﬁer. In our ATM
example, Alice plays the role of prover while the ATM machine plays the role of veriﬁer. The
prover has a secret key sk that it uses to convince the veriﬁer of its identity. The veriﬁer has a
corresponding veriﬁcation key vk that it uses to conﬁrm the prover’s claim. We will occasionally
refer to the prover as a human user and refer to the veriﬁer as a computer or a server.
The motivating examples above suggest three attack models for ID protocols, ordered from
weakest to strongest. We will discuss these models in detail in the coming sections.
• Direct attacks: The door lock and local login examples describe interactions between a
prover and a veriﬁer that are in close physical proximity. Suppose that the adversary cannot
eavesdrop on this conversation. Then using no information other than what is publicly avail-
able, the adversary must somehow impersonate the prover to the veriﬁer. A simple password
protocol is suﬃcient to defend against such direct attacks.
• Eavesdropping attacks: In the wireless car key example the adversary can eavesdrop on
the radio channel and obtain the transcript of several interactions between the prover and
veriﬁer. In this case the simple password protocol is insecure. However, a slightly more
sophisticated protocol based on one-time passwords is secure.
• Active attacks: The last two examples, a fake ATM and online banking, illustrate an active
adversary that interacts with the prover. The adversary uses the interaction to try and learn
something that will let it later impersonate the prover to the veriﬁer. Identiﬁcation protocols
secure against such active attacks require interaction between the prover and veriﬁer. They
use a technique called challenge-response.
Active attacks also come up when Alice tries to log in to a local infected computer. The malware
infecting the computer could display a fake login screen and fool Alice into interacting with it, thus
mounting an active attack. Malware that steals user passwords this way is called a Trojan horse.
The stolen password can then be used to impersonate Alice to other machines.
Secret vs public veriﬁcation keys. In some ID protocols the veriﬁer must keep its veriﬁcation
key vk secret, while in other protocols vk can be public. We will see examples of both types of
protocols. Clearly protocols where vk can be public are preferable since no damage is caused if the
veriﬁer (e.g., the ATM) is compromised.
Stateless vs stateful protocols. Ideally, vk and sk should not change after they are chosen at
setup time. In some protocols, however, vk and sk are updated every time the protocol executes: the
prover updates sk and the veriﬁer updates vk. Protocols where vk and sk are ﬁxed forever are called
stateless. Protocols where vk and sk are updated are called stateful. Some stateful protocols
provide higher levels of security at lower cost than their stateless counterparts. However, stateful
protocols can be harder to use because the prover and veriﬁer must remain properly synchronized.
716
One-sided vs mutual identiﬁcation. In this chapter we only study the one-sided identiﬁ-
cation problem, namely Bob wishes to verify Alice’s identity. Mutual identiﬁcation, where Bob
also identiﬁes itself to Alice, is a related problem and is explored in Exercise 18.1. We will come
back to this question in Chapter 21, where we construct mutual identiﬁcation protocols that also
generate a shared secret key.
Security and limitations of identiﬁcation protocols. Identiﬁcation protocols are designed
to prevent an adversary from impersonating Alice without Alice’s assistance. When deﬁning the
security of identiﬁcation protocols, we may allow the adversary to eavesdrop and possibly interact
with Alice; however, when it comes time to impersonate Alice, the adversary must do so without
communicating with Alice. The examples above, such as opening a door lock, give a few settings
where the primary goal is to prevent impersonation when Alice is not present.
ID protocols, however, are not suﬃcient for establishing a secure session between Alice and a
remote party such as Alice’s bank. The problem is that ID protocols can be vulnerable to a man
in the middle (MiTM) attack. Suppose Alice runs an identiﬁcation protocol with her bank over an
insecure channel: the adversary controls the channel and can block or inject messages at will. The
adversary waits for Alice to run the identiﬁcation protocol with her bank and relays all protocol
messages from one side to the other. Once the identiﬁcation protocol completes successfully, the
adversary sends requests to the bank that appear to be originating from Alice’s computer. The
bank honors these requests, thinking that they came from Alice. In eﬀect, the adversary uses Alice
to authenticate to the bank and then “hijacks” the session to send his own messages to the bank.
To defeat MiTM attacks, one can combine an identiﬁcation protocol with a session key exchange
protocol, as discussed in Chapter 21. The shared session key between Alice and her bank prevents
the adversary from injecting messages on behalf of Alice.
18.1 Interactive protocols: general notions
Before getting into the speciﬁcs of identiﬁcation protocols, we make a bit more precise what we
mean by an interactive protocol in general.
An interactive protocol can be carried out among any number of parties, but in this text, we will
focus almost exclusively on two-party protocols. Regardless of the number of parties, a protocol
may be run many times. Each such protocol run is called a protocol instance.
In any one protocol instance, each party starts oﬀ in some initial conﬁguration. As the pro-
tocol instance runs, parties will send and receive messages, and update their local conﬁgurations.
While the precise details will vary from protocol to protocol, we can model the computation of
each party in a protocol instance in terms of an interactive protocol algorithm , which is an
eﬃcient probabilistic algorithm I that takes as input a pair ( conﬁgold,datain) and outputs a pair
(conﬁgnew,dataout). When a party executes a protocol instance, it starts by supplying an input
value, which deﬁnes the initial conﬁguration of the protocol instance for that party. When
the party receives a message over the network (presumably, from one of its peers), algorithm I
is invoked on input ( conﬁgold,datain), where conﬁgold is an encoding of the current conﬁguration,
and datain is an encoding of the incoming message; if the output of I is (conﬁgnew,dataout), then
conﬁgnew is an encoding of the new conﬁguration, and dataout encodes an outgoing message. The
party sends this outgoing message over the network (presumably, again, to one of its peers). The
party iterates this as many times as required by the protocol, until some terminal conﬁguration
717
veriﬁerprover
G
vk sk
accept or reject
Figure 18.1: Prover and veriﬁer in an ID protocol
is reached. This terminal conﬁguration may specify an output value, which may be used by the
party, presumably in some higher-level protocol.
In general, a given party may run many protocols, and even several instances of the same
protocol, possibly concurrently. The conﬁgurations of all of these diﬀerent protocol instances are
separately maintained.
18.1.1 Mathematical details
As usual, one can deﬁne things more precisely using the terminology deﬁned in Section 2.3. This is
quite straightforward: along with the inputs described above, an interactive protocol algorithm I
also takes as input a security parameter λand a system parameter Λ. There are, however, a couple
of details that deserve discussion.
For simplicity, we shall insist that the conﬁguration size of a running protocol instance is poly-
bounded — that is, the conﬁguration can be encoded as a bit string whose length is always bounded
by some ﬁxed polynomial inλ. This allows us to apply Deﬁnition 2.8 to algorithmI. That deﬁnition
assumes that the length of any input to an eﬃcient algorithm is poly-bounded. So the requirement
is that for every poly-bounded input to I, the output produced by I is poly-bounded.
The problem we are trying to grapple with here is the following. Suppose that after each round,
the conﬁguration size doubles. After a few rounds, this would lead to an exponential explosion in
the conﬁguration size, even though at every round, the computation runs in time polynomial in
the current conﬁguration size. By insisting that conﬁguration sizes remain poly-bounded, we avoid
this problematic situation.
For simplicity, we will also insist that the “round complexity” of a protocol is also poly-bounded.
We will mainly be interested here in protocols that run in a constant number of rounds. More
generally, we allow for protocols whose round complexity is bounded by some ﬁxed polynomial in
λ. This can be reasonably enforced by requiring that starting from any initial conﬁguration, after
a polynomial number of iterations of I, a terminal conﬁguration is reached.
18.2 ID protocols: deﬁnitions
We start by deﬁning the algorithms shown in Fig. 18.1 that comprise an ID protocol.
718
Deﬁnition 18.1. An identiﬁcation protocol is a triple I= (G,P,V ).
• G is a probabilistic, key generation algorithm, that takes no input, and outputs a pair
(vk,sk), where vk is called the veriﬁcation key and sk is called the secret key.
• P is an interactive protocol algorithm called the prover, which takes as input a secret key sk,
as output by G.
• V an interactive protocol algorithm called the veriﬁer, which takes as input a veriﬁcation
key vk, as output by G, and which outputs accept or reject.
We require that when P(sk) and V(vk) interact with one another, V(vk) always outputs accept.
That is, for all possible outputs (vk,sk) of G, if P is initialized with sk, and V is initialized with
vk, then with probability 1, at the end of the interaction between P and V, V outputs accept.
18.3 Password protocols: security against direct attacks
In the basic password protocol, the prover’s secret key is a password pw. In this protocol, the
prover sends pw to the veriﬁer, who checks that pw is the correct password. Thus, the secret key
sk is simply sk := pw. Clearly this protocol should only be used if the adversary cannot eavesdrop
on the interaction between prover and veriﬁer. To complete the description of the basic password
protocol, it remains to specify how the veriﬁer checks that the given password is correct.
The ﬁrst thing that comes to mind is to deﬁne the veriﬁer’s veriﬁcation key as vk := pw. The
veriﬁer then simply checks that the password it receives from the prover is equal to vk. This naive
password protocol is problematic and should never be used. The problem is that a compromise of
the veriﬁer (the server) will expose all passwords stored at the veriﬁer in the clear.
Fortunately, we can easily avoid this problem by giving the veriﬁer a hash of the password,
instead of the password itself. We refer to the modiﬁed protocol as version 1. We describe this
protocol in a rather idealized way, with passwords chosen uniformly at random from some ﬁnite
password space; in practice, this may not be the case.
Password protocol (version 1). The prover’s secret keysk is a password pw, chosen at random
from some ﬁnite password space P, while the veriﬁer’s key vk is H(pw) for some hash function
H : P→Y . Formally, the password ID protocol Ipwd = (G,P,V ) is deﬁned as follows:
• G: set pw ←R Pand output sk := pw and vk := H(pw).
• Algorithm P, on input sk = pw, and algorithm V, in input vk = H(pw), interact
as follows:
1. P sends pw to V;
2. V outputs accept if the received pw satisﬁes H(pw) = vk;
it outputs reject otherwise.
In a multi-user system the veriﬁer (server) stores a password ﬁle that abstractly looks like
Fig. 18.2. Consequently, an attack on the server does not directly expose any passwords.
To analyze the security of this protocol we formally deﬁne the notion of security against direct
attacks, and then explain why this protocol satisﬁes this deﬁnition.
719
id1 H(pw1)
id2 H(pw2)
id3 H(pw3)
... ...
Figure 18.2: The password ﬁle stored on the server (version 1)
Attack Game 18.1 (Secure identiﬁcation: direct attacks). For a given identiﬁcation protocol
I= (G,P,V ) and a given adversary A, the attack game runs as follows:
• Key generation phase. The challenger runs (vk,sk) ←R G(), and sends vk to A.
• Impersonation attempt. The challenger and Anow interact, with the challenger following
the veriﬁer’s algorithm V (with input vk), and with Aplaying the role of a prover, but not
necessarily following the prover’s algorithm P (indeed, Adoes not receive the secret key sk).
We say that the adversary wins the game if V outputs accept at the end of the interaction. We
deﬁne A’s advantage with respect to I, denoted ID1 adv[A,I], as the probability that Awins the
game. 2
Deﬁnition 18.2. We say that an identiﬁcation protocol Iis secure against direct attacks if
for all eﬃcient adversaries A, the quantity ID1adv[A,I] is negligible.
Note that the adversary in Attack Game 18.1 is given the veriﬁer’s key vk. As a result, a
naive password protocol where cleartext passwords are stored on the server does not satisfy Deﬁ-
nition 18.2. The following simple theorem shows that the version 1 protocol is secure.
Theorem 18.1. Suppose that hash function H : P→Y is one-way (as in Deﬁnition 8.6). Then
the ID protocol Ipwd is secure against direct attacks.
Proof sketch. To attack the protocol the adversary must come up with a password pw′ such that
H(pw′) = H(pw). Note that pw′ may be diﬀerent from pw. An adversary who can come up with
such a pw′obviously breaks the one-wayness of H. 2
We note that security against direct attacks (Attack Game 18.1) is a very weak notion of
security. For example, although Ipwd is secure against direct attacks, it is clearly insecure if the
adversary can eavesdrop on just a single instance of the protocol.
18.3.1 Password cracking using a dictionary attack
The password protocol Ipwd is widely used in practice because it is so easy to use. Anyone can
memorize a password pw and participate in the protocol, playing the role of prover, without any
additional hardware. The problem is that humans are terrible at generating and memorizing random
passwords. In practice, passwords are typically very short. Even worse, passwords are usually not
generated at random, but rather, selected by humans in rather predictable ways.
Figure 18.3 summarizes the results of a study [50] conducted in 2016 that examined ﬁve million
leaked passwords that were mostly held by users in North America and Western Europe. The
720
123456, password, 12345, 12345678, football,
qwerty, 1234567890, 1234567, princess, 1234,
login, welcome, solo, abc123, admin
Figure 18.3: The 15 most common passwords in 2016 listed in order
data shows that the passwords are not at all close to uniformly distributed over some large set,
and in particular, a good percentage of passwords belong to a relative small dictionary of common
passwords. About 4% of people use the password “123456” and about 10% use one of the passwords
in the list of top 25 most common passwords. The list of passwords in Figure 18.3 is remarkably
stable over time. It changes very little from year to year.
From now on, we will use the term strong password to mean a password that is chosen
uniformly at random from a large password space P. Theorem 18.1 applies only if passwords are
strong. A weak password is one that is chosen (with some arbitrary distribution) from some
small dictionary of common passwords, which we will denote by D, where D⊆P .
18.3.1.1 Online dictionary attacks
Suppose an adversary suspects that a certain user’s password is weak, and belongs to some small
dictionary Dof common passwords. Then the adversary can mount an online dictionary attack
by simply trying to log in with all words in Done after the other, until a valid password is found.
To speed things up, the attacker can sort Dby popularity and try the most popular passwords
ﬁrst.
A common defense against online dictionary attacks is to double the server’s response time after
every 2 failed login attempts for a speciﬁc user ID or from a speciﬁc IP address. Thus, after 10
failed login attempts the time to reject the next attempt is 32 times the normal response time.
This approach does not lock out an honest user who has a vague recollection of his own password.
However, trying many password guesses for a single user becomes diﬃcult.
Attackers adapt to this strategy by trying a single common password, such as 123456, across
many diﬀerent usernames. These repeated attempts leverage client machines, called bots, located
at diﬀerent IP addresses to defeat defenses that limit the number of login attempts from a single IP
address. Eventually they ﬁnd a username for which the password is valid. Because every targeted
username is subjected to a single login attempt, these attempts may not trigger the delay defense.
Compromising random accounts this way is often good enough for an attacker. The compromised
credentials can be sold on underground forums that trade in such information.
Non-cryptographic defenses are fairly eﬀective at blocking these online attacks. However, a
more devastating attack is much harder to block. We discuss this attack next.
18.3.1.2 Oﬄine dictionary attacks
An attacker that compromises a login server can steal the password database stored at the server.
This gives the attacker a large list of hashed passwords, one password for each user registered with
that system. There are many other ways to obtain password ﬁles besides a direct compromise of a
server. One study, for example, showed that used hard drives purchased on eBay can contain a lot
of interesting, unerased data, including password ﬁles [68].
721
So, suppose an adversary manages to obtain a veriﬁcation key vk = H(pw) for some user. If
the password pw is weak, and belongs to a small dictionary Dof common passwords, then the
adversary can mount an oﬄine dictionary attack, by performing the following computation:
for each w∈D:
if H(w) = vk:
output w and halt
(18.1)
If pw belongs to D, then using this procedure the adversary will obtain pw, or possibly some pw′
with H(pw) = H(pw′).
The running time of this oﬄine dictionary attack is O(|D|), assuming the time to evaluate H
at one input counts as one time unit. This computation can be carried out entirely oﬄine, with
no interaction with the prover or the veriﬁer.
Password statistics. In 2016, a password cracking service called CrackStation released a dic-
tionary Dof about 1.5 billion passwords. Empirical evidence suggests that a signiﬁcant fraction of
human generated passwords, close to 50%, are on this list. This means that after about 1.5 billion
oﬄine hashes, one in two passwords can be cracked. If the hash function H is SHA256 then this
takes less than a minute on a modern GPU. There is only one conclusion to draw from this: simply
hashing passwords using SHA256 is the wrong way to protect a password database.
As another way to illustrate the problem, observe that the total number of 8-character passwords
containing only printable characters is about 958 ≈252 (using the 95 characters on a US keyboard).
Running SHA256 on all words in this set using a modern array of GPUs can be done in a few days.
This puts all passwords of 8 characters or less at risk in case of a server compromise.
Quantum oﬄine password attacks. To make matters worse, the exhaustive search attack in
the previous paragraph will be much faster once a large-scale quantum computer becomes available.
We explained in Section 4.3.4 that a quantum computer can search a space of size n in time √n.
Thus, a quantum search through the space of 8 character passwords will only take
√
252 = 2 26
evaluations of the hash function. This takes a few seconds on a modern (classical) computer.
Put diﬀerently, because 8 character passwords are insecure due to classical exhaustive search, 16
character passwords will be insecure once we have a quantum computer that is comparable in speed
and size to a current classical computer. We discuss some defenses in Section 18.4.3.
18.3.1.3 Oﬄine dictionary attacks with preprocessing
The oﬄine dictionary attack discussed above can be made even better for the adversary by prepro-
cessing the dictionary Dbefore the attack begins. Then once a hashed password vk is obtained,
the attacker will be able to quickly ﬁnd the cleartext password pw. Speciﬁcally, we partition the
dictionary attack into two phases: a preprocessing phase that is carried out before any hashed
passwords are known, and an attack phase that cracks a given hashed password vk. Our goal is
to minimize the time needed for the attack phase to crack a speciﬁc vk.
722
A simple dictionary attack with preprocessing works as follows:
Preprocessing phase:
build a list L of pairs
(
pw,H(pw)
)
, one pair for each pw ∈D
Attack phase on an input vk:
if there is an entry ( pw,vk) in L, output pw
otherwise, output fail
(18.2)
Let’s assume that hashing a password using H counts as one time unit. Then the preprocessing
phase takes O(|D|) time. If the list L is stored in a hash table that supports a constant time look
up (such as Cuckoo hashing), then the attack phase is super fast, taking only constant time.
Batch oﬄine dictionary attacks. Once the preprocessing phase is done, the attacker can
use it to quickly crack many hashed passwords. Speciﬁcally, suppose an attacker obtains a large
database F of hashed passwords from a compromised login server. Then cracking the hashed
passwords in F using the dictionary Dnow takes only
preprocessing time: O(|D|) ; attack time: O(|F|) (18.3)
where |F|is the number of hashed passwords in F. The total work of this batch dictionary attack
is O(|D|+ |F|). This is much faster than running a separate oﬄine dictionary attack as in (18.1)
against every element of F separately, which would take time O(|D|×|F|).
Recall that the password statistics cited in Section 18.3.1.2 suggest that an adversary can ﬁnd
the passwords of about half the users in F using the CrackStation dictionary. This only takes time
O(|F|) once preprocessing is done. Eﬀectively, this attack can expose millions of cracked passwords
with very little work.
A time-space tradeoﬀ. The simple preprocessing method presented in (18.2) requires the at-
tacker to build and store a list L of all hashed dictionary words. When Dis the set of all 2 52
passwords of eight characters, the table L can be quite large and storing it can be diﬃcult. In
Section 18.7, we present a method called rainbow tables that quickly cracks passwords using a
much smaller table L constructed during the preprocessing phase. For example, with n:= |D|the
method achieves the following parameters:
table size: O(n2/3) ; preprocessing time: O(n)
attack time: O(n2/3).
The table size is reduced from O(n) to O(n2/3), as promised. However, the time to attack one
hashed password is increased from O(1) to O(n2/3). In other words, we traded a smaller table Lfor
increased attack time. For this reason this approach is called a time-space tradeoﬀ. We usually
ignore the preprocessing time: it is a one-time cost invested before the attack even begins.
This time-space tradeoﬀ further demonstrates why simply storing hashed passwords is the wrong
thing to do. We discuss defenses against this in the next section.
723
id1 salt1 H
(
pw1,salt1
)
id2 salt2 H
(
pw2,salt2
)
id3 salt3 H
(
pw3,salt3
)
... ... ...
Figure 18.4: Password ﬁle (version 2)
18.4 Making dictionary attacks harder
Oﬄine dictionary attacks, especially with preprocessing, are a real threat when storing hashes of
weak passwords on a server. In this section we discuss a number of techniques that can make these
attacks much harder for the adversary.
18.4.1 Public salts
In the previous section we showed how an attacker can preprocess the dictionary Dto build a data
structure L that then lets the attacker quickly crack one or more hashed passwords. A simple
defense called salting can prevent these preprocessing attacks. Salting ensures that cracking a
ﬁle F of hashed passwords takes time
Ω
(
|D|×|F|
)
even if the attacker is allowed inﬁnite time to preprocess D. In other words, salting ensures that
the exhaustive search approach in (18.1) is the best possible attack.
Salting works by generating a random string, called a salt, when registering a new password.
Every user in the system is assigned a fresh salt chosen at random from a set S. As we will see,
taking |S|= 2128 is suﬃcient in practice. This salt is hashed along with the password to derive the
veriﬁcation key vk. This salt must be stored in the password ﬁle in the clear, as shown in Fig. 18.4.
Only the server needs to know the salt; the user is not aware that salts are being used.
Now, the modiﬁed password protocol, called password protocol version 2 , runs as follows:
• G: set pw ←R P, salt ←R S, y←H(pw,salt),
output sk := pw and vk := (salt,y).
• Algorithm P, on input sk = pw, and algorithm V, on input vk = (salt,y), interact
as follows:
1. P sends pw to V;
2. V outputs accept if the received pw satisﬁes H(pw,salt) = y;
it outputs reject otherwise.
As in the description of version 1, the description of version 2 is rather idealized, in that passwords
are chosen uniformly at random from a password space P; in practice, this may not be the case.
With salts in place, the adversary has two strategies for attacking hashed passwords in a pass-
word ﬁle F:
• The ﬁrst strategy is to adapt the batch oﬄine dictionary attack. The problem is that the
preprocessing phase must now be applied to a large list of possible inputs to H: any element
724
in the set D×S is a possible input. Using the preprocessing algorithm in (18.2) this would
require generating a data structure Lof size |D|×|S| which is too large to generate, let alone
store. Hence, the preprocessing approach of (18.2) is no longer feasible.
• The second strategy is to run an exhaustive password search as in (18.1) for every password
in F. We already explained that this take time
O(|D|×|F|).
The salt space Sneeds to be suﬃciently large so that the second strategy is always better than
the ﬁrst. This should hold even if the adversary uses a time-space tradeoﬀ to preprocess D×S. To
derive the required bound on the size of S, we ﬁrst deﬁne more precisely what it means to invert
a salted function in the preprocessing model.
Salted one-way functions with preprocessing. To deﬁne this properly we need to split the
usual inversion adversaryAinto two separate adversariesA0 and A1. Adversary A0 has unbounded
running time and implements the preprocessing phase. Adversary A1 is eﬃcient and does the
inversion attack. The only communication allowed between them is an exchange of anℓ-bit string L
that is the result of the preprocessing phase. This is captured in the following deﬁnition, which
models H as a random oracle.
Deﬁnition 18.3. Let H be a hash function deﬁned over (D×S , Y). We deﬁne the advantage
OWsproadv[A,H] of an adversary A= (A0,A1) in defeating the one-wayness of H in the prepro-
cessing model as the probability of winning the following game:
• A0 issues queries to H and outputs an advice string L;
• the challenger chooses (pw,s) ←RD×S , sets y:= H(pw,s), and sends (L,y,s ) to A1;
• A1 issues queries to H and outputs pw ′∈D; it wins the game if H(pw′,s) = y.
Note that the adversary A1 is given both L and the salt s. It needs to ﬁnd a pre-image of y
with salt s. The following theorem gives a bound on the time to invert a salted function H in the
preprocessing model, when H is modeled as a random oracle.
Theorem 18.2 ([54]). Let H be a hash function deﬁned over (D×S , Y) where H is modeled as
random oracle and where |D|≤|Y| . Let A= (A0,A1) be an adversary as in Deﬁnition 18.3, where
A0 outputs an ℓ-bit advice string L, and A1 makes at most Qro queries to H. Then
OWsproadv[A,H] ≤O
( ℓ·Qro
|S|·|D| + Qro
|D|
)
. (18.4)
The theorem shows that if Ahas constant success probability, say 1/2, in inverting vk := y∈Y,
then the attack phase must take at least Qro ≥Ω(|D|·|S|/ℓ) time. Therefore, to prevent any speed-
up from preprocessing we should set |S|≥ Ω(ℓ). This will ensure that exhaustive search is the best
attack. For example, if we assume maximum storage space of 2 80 for the advice string L then the
salt space Sshould be at least {0,1}80. In practice one typically sets S:= {0,1}128.
Technically, Theorem 18.2 bounds the time to crack a single password. It does not bound the
time for a batch oﬄine dictionary attack where the attacker tries to crack t passwords at once, for
some t >1. One expects, however, that the theorem can be generalized to the batch settings so
that the bound |S|≥ Ω(ℓ) is suﬃcient to prevent any beneﬁt from preprocessing even for cracking
a batch of passwords.
725
id1 salt1 H
(
password1,salt1,pepper1
)
id2 salt2 H
(
password2,salt2,pepper2
)
id3 salt3 H
(
password3,salt3,pepper3
)
... ... ...
Figure 18.5: Password ﬁle (version 3)
Limits of salting. While salts defend against preprocessing attacks, they do not prevent other
attacks. For example, a user who chooses a weak password will still be vulnerable to the basic
oﬄine dictionary attack (18.1), even if a salt is used. In the next two sections we show how to
provide further protection against oﬄine dictionary attacks.
18.4.2 Secret salts
We can make the adversary’s task harder by adding artiﬁcial entropy to human passwords. The
idea is to pick a random short string, called a secret salt, or pepper, in a small space Sp and
include it in the hash computation, but not include it in the password ﬁle. The resulting password
ﬁle is shown in Fig. 18.5.
To verify a password, the server simply tries all possible values of the secret salt until it ﬁnds
one that hashes to the stored hash value. The modiﬁed password protocol, password protocol
version 3, is as follows:
• G: set pw ←R P, salt ←R S, pepper ←R Sp, y←H
(
pw,salt,pepper
)
,
output sk := pw and vk := (salt,y).
• Algorithm P, on input sk = pw, and algorithm V, on input vk = (salt,y), interact
as follows:
1. P sends pw to V;
2. V outputs accept if the receivedpw satisﬁes H(pw,salt,p) = yfor some p∈Sp;
it outputs reject otherwise.
A typical choice for the secret salt space is Sp := {0,1}12 which slows down password veriﬁcation
on the server by a factor of 4096 compared with protocol version 2. This still takes less than a
hundredth of a second and is unnoticeable by the user. More importantly, the adversary has to do
4096 times more work to ﬁnd weak passwords in the password ﬁle.
The secret salt makes an oﬄine dictionary attack harder because now the adversary has to
search through the space D×Sp instead of just D. Yet this technique has minimal impact on the
user experience. The secret salt increases the entropy of the user’s password, without forcing the
user to remember a more complicated password.
18.4.3 Slow hash functions
A diﬀerent approach to protecting weak passwords is to use a slow hash function. Recall that
hashing a password with a hash function such as SHA256 is fast. The speed of SHA256 is what
726
makes an oﬄine dictionary attack possible; the attacker can quickly evaluate the hash function on
many dictionary words.
Suppose that the server hashes passwords using a slow hash function. Say, the function takes a
hundredth of a second to evaluate on a single input, 10,000 times slower than SHA256. The user
experience is hardly aﬀected since users generally do not notice a hundredth of a second delay.
However, the adversary’s work to hash all words in the dictionary is increased by a factor of 10,000.
How do we build a slow hash function? The ﬁrst idea that comes to mind is to iterate a fast
hash function suﬃciently many times until it becomes suﬃciently slow. Speciﬁcally, for a hash
function H deﬁned over (X,X), deﬁne
H(d)(x) := H(H(H(···(x) ···))) (18.5)
where H is iterated d times (see also Section 14.3). If d = 10,000 then evaluating H(d)(x) takes
10,000 times longer than evaluating H(x). This approach, however, is problematic and should
not be used. One reason is that the function H(d) is about d times easier to invert than H (see
Exercise 14.18). We will see a better function below. First, let’s deﬁne what we mean by a slow
hash function.
Deﬁnition 18.4. A password-based key derivation function , or PBKDF, is a function H
that takes as input a password pw ∈P, a salt in S, and a diﬃculty d ∈Z>0. It outputs a value
y ∈Y. We require that H is computable by an algorithm that runs in time proportional to d. As
usual, we say that the PBKDF is deﬁned over (P,S,Y).
We discuss the security requirements for a PBKDF in Exercise 18.3. Our ﬁrst example PBKDF,
called PBKDF1, is based on (18.5) and deﬁned as:
PBKDF1H(pw,salt,d) := H(d)(pw,salt).
For a hash functionH deﬁned over (X,X), this PBKDF is deﬁned over (P,S,X), where X= P×S.
It is not used in practice because of the attack discussed in Exercise 14.18.
18.4.3.1 The function PBKDF2
A widely used method to construct a slow hash function is called PBKDF2, which stands for
password based key derivation function version 2. Let F be a PRF deﬁned over ( P,X,X) where
X := {0,1}n. The derived PBKDF, denoted PBKDF2 F, is deﬁned over ( P,X,X) and works as
follows:
PBKDF2F(pw,salt,d) :=



x0 ←F(pw, salt)
for i= 1,...,d −1:
xi ←F(pw,xi−1)
output y←x0 ⊕x1 ⊕···⊕ xd−1 ∈X



(18.6)
While (18.6) describes the basic PBKDF2, a simple extension outputs more bits if more are needed.
In particular, PKBDF2 can output an element in Xb for some 1 <b< 232 by computing:
PBKDF2(b)
F (pw,salt,d) :=
(
PBKDF2F(pw,salt1,d),..., PBKDF2F(pw,saltb,d)
)
∈Xb (18.7)
where all b salts are derived from the provided salt by setting salti ←salt ∥bin(i). Here bin( i) is
the binary representation of i∈{1,...,b }as a 32-bit string.
727
input: x0 ∈X, diﬃculty d∈Z>0
1. for i= 1,...,d : xi ←h(xi−1) / / Then xi = h(i)(x0)
2. y0 ←xd
3. for i= 1,...,d :
4. j ←int(yi−1) mod (d+ 1) / / int(yi−1) converts yi−1 ∈X to an integer
5. yi ←h(yi−1 ⊕xj) / / read random location in the array (x0,...,x d)
output yd ∈X
Figure 18.6: The function Scrypth(x0,d)
In practice, PBKDF2 is often implemented using HMAC-SHA256 as the underlying PRF. The
diﬃcultly d is set depending on the project needs and hardware speeds. For example, backup
keybags in iOS 10 are protected using PBKDF2 with dset to ten million. In Windows 10, the data
protection API (DPAPI) uses d= 8000 by default, but using HMAC-SHA512 as the PRF.
We discuss the security of PBKDF2 in more detail in Exercises 18.2 and 18.3.
18.4.4 Slow memory-hard hash functions
A signiﬁcant problem with PBKDF2 is that it is vulnerable to parallel hardware attacks. To explain
the problem, recall that the bulk of a modern processor is devoted to cache memory. The computing
unit is a tiny fraction of the overall processor area. Consequently, a commercial processor cannot
evaluate PBKDF2 on many inputs in parallel and is not well suited for an oﬄine dictionary attacks.
A sophisticated attacker will usually run an oﬄine dictionary attack on dedicated hardware
that supports a high degree of parallelism, such as GPUs, FPGAs, or even custom chips. A single
custom chip can pack over a million SHA256 engines. If each engine can do a million SHA256
evaluations per second, then the adversary can try 10 12 passwords per second per chip. Even if
the PBKDF2 diﬃculty is set to d = 10,000, a bank of about 500 such chips will run through all
252 eight character passwords in less than a day. This attack is possible because the hardware
implementation of SHA256 is relatively compact, making it possible to pack a large number of
SHA256 engines into a single chip.
This suggests that instead of SHA256 we should use a hash function H whose hardware imple-
mentation requires a large amount of on-chip area. Then only a few copies of H can be packed into
a single chip, greatly reducing the performance beneﬁts of custom hardware.
How do we build a hash function H that has a large hardware footprint? One way is to ensure
that evaluating H requires a lot of memory at every step of the computation. This forces the
attacker to allocate most of the area on the chip to the memory needed for a single hash evaluation,
which ensures that every chip can only contain a small number of hash engines.
Hash functions that require a lot of memory are called memory-hard functions. Several such
functions have been proposed and shown to be provably memory-hard in the random oracle model.
Before we discuss security let us ﬁrst see a popular construction called Scrypt. Scrypt is built from
a (memory-easy) hash function h : X →Xwhere X := {0,1}n. The resulting function, denoted
Scrypth, is shown in Fig. 18.6.
In our security analysis, we will treat the underlying hash function h as a random oracle. In
728
practice, the function his derived from the Salsa 20/8 permutation (Section 3.6). The diﬃculty dis
set based on the performance needs of the system. For example, one could set dso that evaluating
Scrypt ﬁlls the entire on-chip cache. This will ensure that evaluating Scrypt is not too slow, but
needs a lot of memory.
Fig. 18.6 is a description of Scrypt as a hash function. The Scrypt PBKDF, deﬁned over
(P,X,X), is built from the Scrypt hash and works as follows:
ScryptPBKDFh(pw,salt,d) :=



x0 ←PBKDF2F(pw, salt, 1)
y←Scrypth(x0,d)
output PBKDF2F(pw, y, 1)



(18.8)
where F is a PRF deﬁned over ( P,X,X). In practice one uses HMAC-SHA256 for F. If needed,
Scrypt can be iterated several times to make it slower without increasing the required memory.
Similarly, it can output an element in Xb for b> 1 by adjusting the application of PBKDF2 on the
last line as in (18.7).
Is Scrypt memory-hard? The Scrypt function can be evaluated in time O(d) by storing (d+1)
elements of X. Step (1) in Fig. 18.6 creates an array ( x0,...,x d) of size ( d+ 1). Then Step (5)
repeatedly reads data from random locations in this array. Because of Step (5) it seems plausible
that an algorithm that evaluates the function in time O(d) must keep the entire array ( x0,...,x d)
in memory. Clearly this intuition needs a proof.
The danger is that a time-space tradeoﬀ might enable one to evaluate Scrypt in a bit more time,
but with far less memory. That would be devastating because the reduced memory would allow an
attacker to pack many Scrypt engines into a single chip without paying much in running time per
engine. This is exactly what we want to avoid.
In Exercise 18.6 we develop a simple time-space tradeoﬀ on Scrypt. For any 1<α<d/ 2 it shows
that Scrypt can be evaluated in time O(αd) by storing only ⌈d/α⌉elements of X. In particular,
Scrypt can be evaluated in time O(d2) using constant space. However, this type of time-space
tradeoﬀ does not help the adversary. It lets the adversary pack α times as many Scrypt engines
into a single chip, but each engine must work αtimes harder. Therefore, the overall throughput of
a single chip is unchanged compared to an implementation of Scrypt as in Fig. 18.6. Nevertheless,
we need to prove that there is no better time-space tradeoﬀ against Scrypt.
Pipelining is another threat to memory hardness. Suppose it were possible to evaluate Scrypt
using an algorithm that uses O(d) memory, but only in a few steps in the algorithm. If in the
remaining time the algorithm used only constant space then it would be possible to share a single
array of size O(d) among multiple Scrypt engines arranged in a pipeline. Each engine would use the
array in the few steps where it needs O(d) memory, and then release the array for another engine
to use. This again would enable the adversary to pack many Scrypt engines into a single chip, all
sharing a single array of size O(d). To prevent this form of pipelining we need to prove that every
implementation of Scrypt that runs in time O(d) must use O(d) memory in many steps throughout
the computation.
Scrypt is memory-hard. To prove that Scrypt is memory-hard we ﬁrst deﬁne a security model
that captures the hurdles discussed above and then state the security theorem for Scrypt. We begin
by deﬁning an abstract parallel random oracle model, where an algorithm Acan query a random
oracle h: Y→Z at multiple inputs in parallel.
729
A parallel random oracle algorithm Atakes as input an x∈X and runs through a sequence
of states. At each state the algorithm issues a set of queries to the random oracle h. The algorithm
is given the responses to all its queries and it then moves to the next state. This process is repeated
until the algorithm terminates, at which point the ﬁnal state contains the output. We record all
the intermediate states to keep track of their size.
Formally, the algorithm Aimplements a deterministic mapping:
A: X×S×Z ≤p →S×Y ≤p
for some positive integer p, and operates as follows:
• Ais ﬁrst invoked as A(x,ε,ε ) and outputs a pair
(
s1,¯y1) in S×Y ≤p. Here s1 is A’s current
state and ¯y= (y1,...,y r) is its ﬁrst set of parallel queries to the random oracle h: Y→Z .
• For i= 1,...,t , when Aoutputs (si,¯yi) with ¯yi = (y1,...,y r) ∈Y≤p, we do the following:
– evaluate the oracle h in parallel by setting ¯zi ←
(
h(y1),...,h (yr)
)
, and
– re-invoke Aas (si+1,¯yi+1) ←A(x,si,¯zi).
• Eventually Aoutputs (s,ε) indicating that it is done and that the output is s.
The running time of Aon input x ∈ Xis the number of times that Ais invoked until it
terminates. Measuring running time this way captures the fact that a hardware implementation
can evaluate the hash function h at many points in parallel.
We record the data given to Ain step i as sti := (si,¯zi). We call sti the input state at time i.
For s ∈S we let |s|denote the length of s in bits, and similarly we let |z|denote the length of
z ∈Z. For ¯z = (z1,...,z r) ∈Z≤p, we let |¯z|:= ∑r
j=1|zi|. When Z= {0,1}n we have |¯z|= rn.
Finally, the bit length of an input state st = (s,¯z) is deﬁned as |st|:= |s|+ |¯z|.
Deﬁnition 18.5. Let Abe a parallel random oracle algorithm taking inputs in X. The cumulative
memory complexity of Awith respect to h: Y→Z and x∈X, denoted mem[A,h,x ], is deﬁned as
mem[A,h,x ] :=
t∑
i=1
|sti|.
The algorithm in Fig. 18.6 for computing Scrypt h(x,d) with respect to an oracle h : X →X,
where X= {0,1}n, has cumulative memory complexity of O(nd2). The following theorem shows
that this is the best possible.
Theorem 18.3 ([6]). Let X:= {0,1}n be such that |X| is super-poly and let d be chosen so that
2−d is negligible. The for all parallel random oracle algorithms Aand all x∈X,
Pr
[
A(x,d) = Scrypth(x,d)
]
≤Pr
[
mem[A,h, (x,d)] ≥Ω(d2n)
]
+ δ
for some negligible δ. Both probabilities are over the choice of random oracle h: X→X .
The theorem shows that if A(x,d) outputs Scrypt h(x,d) with probability close to 1 then the
cumulative memory complexity of Amust be Ω( d2n) for almost all choices of h. This shows that
there cannot be a time-space tradeoﬀ against Scrypt that is signiﬁcantly better than Exercise 18.6.
730
If an algorithm evaluates Scrypt with maximum space dn/α, for some α> 1, then its running time
must be Ω(dα). Otherwise its cumulative memory complexity would violate the lower bound.
Similarly, there cannot be a pipelining attack on Scrypt. Any viable algorithm for computing
Scrypt that runs in time O(d) must use Ω( dn) memory throughout the algorithm. Otherwise,
again, its cumulative memory complexity would violate the lower bound.
Technically, Theorem 18.3 bounds the time and space needed to evaluate Scrypt at a single
input. It does not bound the time for a batch oﬄine dictionary attack where the attacker tries to
evaluate Scrypt at p passwords at once, for some p >1. One expects, however, that the theorem
can be generalized to the batch settings: if an algorithm Aevaluates Scrypt correctly at p inputs
with probability close to 1, then the cumulative memory complexity of Amust be Ω( d2np). This
would show that there is no time-space tradeoﬀ or pipelining attack against Scrypt when evaluating
Scrypt at p points.
18.4.4.1 Password oblivious memory-hard functions
While Scrypt is a sound memory-hard password hashing function, it is vulnerable to a side-channel
attack of the type discussed in Section 4.3.2.
Consider a login server where a running process P validates user passwords by hashing them
with Scrypt. Suppose the adversary gains low-privilege access to this server; the adversary can
run user-level programs on the server, but cannot compromise process P and cannot observe user
passwords in the clear. However, using its foothold it can mount a clever attack, called a cache
timing attack, that lets it learn the order in which P accesses pages in memory. It learns nothing
about the contents of these pages, just the order in which they are read by P.
Now, suppose the adversary captures a hash value y which is the result of applying the Scrypt
PBKDF in (18.8) to some password pw with a public salt. Normally the adversary would need to
mount a dictionary attack where each attempt takes a large amount of timeand memory. However,
if the adversary also has the memory access pattern of process P as it was computing the Scrypt
hash of pw, then the adversary can mount a dictionary attack on pw with very little memory.
To see how, look back at the implementation of Scrypt in Fig. 18.6. The very ﬁrst time
the algorithm executes Step (5) it reads cell number j from the array ( x0,...,x d), where j =
int(y0) mod (d+ 1). By observing P’s accesses to memory, the adversary can see what memory
page was read when Step (5) was ﬁrst executed. This gives the adversary an approximate value ja
for j. The adversary does not learn the exact value of j because a single memory page may contain
multiple array cells. Nevertheless, this ja is suﬃcient to test a candidate password pw′ with little
memory. Here is how:
1. compute x′
0 ←PBKDF2F(pw′, salt, 1) as in (18.8),
2. compute y′
0 as in Step (1) of Fig. 18.6, but without storing any intermediate values, and
3. test if j′←int(y′
0) mod (d+ 1) is close to ja.
If the test fails then pw′is not the correct password. This procedure lets the adversary discard most
candidate passwords in the dictionary with very little memory. Consequently, the user’s password
is again vulnerable to a hardware password attack.
731
A solution. This attack works because Scrypt’s memory access pattern depends on the user’s
password. It would be better if we had a provably secure memory-hard hash function whose
memory access pattern is independent of the user’s password. It can still depend on the user’s
salt because the salt is not secret. Such functions are called data-oblivious memory-hard
functions. Argon2i-B is an example of such a function. It is closely related to Scrypt, but the
memory access pattern in its ﬁrst part is independent of the password. This defeats the side-channel
attack described above.
Slow hashing vs secret salts. To conclude this section we observe that both the secret salt
method and the slow hashing method increase the adversary’s work load. One should use one
method or the other, but not both. The main beneﬁt of the slow memory-hard hashing method
is that it makes it diﬃcult to mount a custom hardware attack. A secret salt used with a fast
hash function does not prevent a parallel hardware attack. Consequently, slow memory-hard hash
functions are preferable to secret salts.
18.4.5 More password management issues
The common password problem. Users frequently have accounts on multiple machines and
at multiple web sites. Ideally, all of these servers take proper precaution to prevent an adversary
from obtaining a password ﬁle, and also properly salt and hash passwords, to limit the damage
should the adversary obtain this ﬁle. Unfortunately, the designers of low-security servers (e.g.,
a conference registration web site) may not take the same security precautions as are taken for
high-security servers (e.g., a bank’s web site). Such a low-security server may be easier to break
in to. Moreover, such a low-security server may store hashes of passwords without salt, enabling
a batch dictionary attack, which will retrieve all the weak passwords; even worse, such a server
may store passwords in the clear, and the adversary retrieves all the passwords, even strong ones.
Consequently, an adversary can break in to a low-security server and retrieve some, or even all,
user ID/passwords at the server, and it is very likely that some of these passwords will also work
at a high-security server. Thus, despite all the precautions taken at the high-security server, the
security of that server can be compromised by the poor security of some completely unrelated,
low-security server. This issue is known as the common password problem.
A standard solution to the common password problem is to install client-side software that
converts a common password into unique site passwords — essentially “client-side salt.” Let H be
a hash function. When a user, whose login ID is id, types in a password pw that is to be sent to
a server, whose identity is idserver, the user’s machine (e.g. the user’s web browser) automatically
converts this password to ˆpw := H(pw,id,idserver), and sends ˆpw to the server. Thus, from the
server’s point of view, the password is ˆpw, although from the user’s point of view, the password is
still just pw. This technique will protect a user from servers that do not properly salt and hash
passwords, even if that user uses the same password on many servers.
Biometrics. The biggest diﬃcultly with password-based authentication is that users tend to
forget their passwords. A large fraction of all support calls have to do with password related
problems. As a result, several deployed systems attempt to replace passwords by human biometrics,
such as ﬁngerprints, retina scans, facial recognition, and many others. One can even use keystroke
dynamics, namely the length of time between keystrokes and the length of time a key is pressed,
as a biometric [115]. The idea is to use (features of) the biometric as the user’s password.
732
V(vk)
challenger adversary A
...
(vk,sk) ←
R
G() vk
transcript1
transcriptQ
impersonation attempt
accept or reject
Figure 18.7: Attack Game 18.2
While biometrics oﬀer clear beneﬁts over passwords (e.g., the user cannot forget his ﬁngerprint)
they have two signiﬁcant disadvantages:
• biometrics are not generally secret — people leave their ﬁngerprints on almost anything they
touch, and
• unlike passwords, biometrics are irrevocable — once a biometric is stolen the user has no
recourse.
Consequently, biometrics should not be used as the only means of identifying users. Biometrics
can be used as additional identiﬁcation (sometimes referred to as second-factor authentication) for
increased security.
18.5 One time passwords: security against eavesdropping
The password protocols in the previous section are easily compromised if an adversary can eavesdrop
on a single interaction between the prover and veriﬁer. Our goal for this section is to develop ID
protocols secure against eavesdropping. We start by deﬁning security for ID protocols in the
presence of an eavesdropper. We enhance Attack Game 18.1 by introducing a new, “eavesdropping
phase” in which the adversary is allowed to request a number of transcripts of the interaction
between the real prover and the real veriﬁer. The updated game is shown in Fig. 18.7.
Attack Game 18.2 (Secure identiﬁcation: eavesdropping attack). For a given identiﬁcation
protocol I= (G,P,V ) and a given adversary A, the attack game runs as follows:
• Key generation phase. The challenger runs (vk,sk) ←R G(), and sends vk to A.
• Eavesdropping phase. The adversary requests some number, say Q, of transcripts of conver-
sations between P and V. The challenger complies by running the interaction between P and
733
V a total of Q times, each time with P initialized with input sk and V initialized with vk.
The challenger sends these transcripts T1,...,T Q to the adversary.
• Impersonation attempt. As in Attack Game 18.1: the challenger and Ainteract, with the
challenger following the veriﬁer’s algorithm V (with input vk), and with Aplaying the role
of a prover, but not necessarily following the prover’s algorithm P.
We say that the adversary wins the game if V outputs accept at the end of the interaction. We
deﬁne A’s advantage with respect to I, denoted ID2 adv[A,I], as the probability that Awins the
game. 2
Deﬁnition 18.6. We say that an identiﬁcation protocol I is secure against eavesdropping
attacks if for all eﬃcient adversaries A, the quantity ID2adv[A,I] is negligible.
Keeping vk secret. The adversary in Attack Game 18.2 is given the veriﬁcation keyvk, meaning
that vk can be treated as public information. However, the ﬁrst eavesdropping-secure ID protocol
we present requires the veriﬁer to keep vk secret. This motivates a weaker version of Attack
Game 18.2 where the challenger does not send vk to the adversary. A small complication when vk
is kept secret is that we must now allow the adversary to make multiple impersonation attempts.
One may insist that these impersonation attempts proceed sequentially, or allow them to proceed
concurrently. In this chapter, we shall insist that they proceed sequentially. The adversary wins
the game if at least one of its impersonation attempts is accepted by the veriﬁer.
The reason we need to allow multiple impersonation attempts is that now, when vk is secret,
interactions with the veriﬁer could potentially leak some information about vk. This stronger
deﬁnition of security rules out some trivially insecure protocols, as discussed in Exercise 18.10. We
note that multiple attempts were not necessary in Attack Game 18.2 where vk is public, since the
adversary could emulate the veriﬁer itself.
Other than these two changes, the remainder of Attack Game 18.2 is unchanged. We
let wID2 adv[A,I] denote the adversary’s advantage in winning this weaker version of Attack
Game 18.2. ID protocols secure in these settings are said to be weakly secure.
Deﬁnition 18.7. We say that an identiﬁcation protocol Iis weakly secure against eavesdrop-
ping attacks if for all eﬃcient adversaries A, the quantity wID2adv[A,I] is negligible.
Stateful protocols. The password protocols in the previous section were all stateless — the
veriﬁer and prover did not maintain state between diﬀerent invocations of the protocol. In this
section, however, both protocols we present are stateful.
In a stateful protocol, after each invocation of the protocol the pair (vk,sk) changes: the prover
P updates sk and the veriﬁer V updates vk. However, we shall assume that V only updates vk if
it outputs accept.
We now consider how to modify Attack Game 18.2 to deal with stateful protocols. As before,
we allow the adversary to eavesdrop on several conversations between P and V. Also, we allow
the adversary to make several impersonation attempts (although, if vk is not kept secret, then it
suﬃces to just consider a single impersonation attempt). But there is another wrinkle. In the
stateless case, we could assume without loss of generality that the adversary obtained all of the
transcripts before making any impersonation attempts. However, with stateful protocols, this is no
longer the case, and we have to allow the adversary to interleave eavesdropping and impersonation
734
attempts. That is, the attack game proceeds in rounds. In each round the adversary can choose to
either
• eavesdrop: obtain a transcript between P and V, after which P updates sk and V updates
vk, or
• impersonate: make an impersonation attempt, interacting with V.
Furthermore, we also assume that the attack game ends as soon as one of the impersonation
attempts succeeds (in which case the adversary wins the game). Recall that we are assuming that
V does not update vk on a failed impersonation attempt, which ensures that in the eavesdropping
rounds, P and V remain properly synchronized.
18.5.1 PRF-based one-time passwords: HOTP and TOTP
The simplest ID protocols secure against eavesdropping attacks are called one-time password
protocols. These are similar to the basic password protocol of Section 18.3, except that the password
changes after every invocation of the protocol.
We begin by describing a weakly secure protocol called HOTP, which stands for hash-based
one-time password. Let F be a PRF deﬁned over (K,ZN,Y) for some large integerN, say N = 2128.
This F is used to update the password after every successful invocation. The HOTP protocol
HOTP = (G,P,V ) works as follows:
• G: choose a random k←R Kand output sk := (k,0) and vk := (k,0).
• Algorithm P given sk, and algorithm V given vk, interact as follows:
1. P
(
sk = (k,i)
)
: send r:= F(k,i) to V and set sk ←
(
k, i+ 1),
2. V
(
vk = (k,i)
)
: if the received r from P satisﬁes r= F(k,i) output accept and
set vk ←(k, i+ 1). Otherwise, output reject.
Here both vk and sk must be kept secret, and therefore HOTP is only weakly secure against
eavesdropping. Note that the integer N is chosen to be so large that, in practice, the counter iwill
never wrap around. Implementations of HOTP typically use HMAC-SHA256 as the underlying
PRF, where the output is truncated to the desired size, typically six decimal digits, as shown in
Fig. 18.8.
Theorem 18.4. Let F be a secure PRF deﬁned over (K,ZN,Y), where N and |Y|are both super-
poly. Then the ID protocol HOTP is weakly secure against eavesdropping.
Proof sketch. Since F is a secure PRF, the adversary cannot distinguish between a challenger who
uses the PRF F in Attack Game 18.2 and a challenger who uses a random function f : ZN →Y.
Moreover, when the challenger uses a random function f, an impersonation attempt succeeds with
probability at most 1 /|Y|, which is negligible, since |Y|is super-poly. Moreover, since N is large,
the counter values will not “wrap around” in any feasible attack. 2
HOTP can be used in a car key fob system to wirelessly unlock a car, as discussed at the
beginning of the chapter. The secret PRF key k is stored on the key fob and at the car. Every
time the user presses a button on the key fob, the key fob increments the internal counter iby one,
and sends the derived one-time password to the car, along with the counter i. The car maintains
735
(a) RSA SecurID token
 (b) Google authenticator
Figure 18.8: One-time password implementations
its own counter and veriﬁes the received one-time password and counter value. Note that the car
must ensure that the recieved counter value is greater than the car’s current counter value.
HOTP can also be used to authenticate a human user to a remote web server. The user is
given a security token that looks something like the token in Fig. 18.8a and displays a 6-digit one-
time password. The user authenticates to the remote server by typing this password into her web
browser. The one-time password is then sent to the remote server to be validated. The next time
the user wants to authenticate to the server she ﬁrst presses a button on the token to increment the
counter i by one. This advances the token to the next one-time password and updates the 6-digit
value displayed on the screen.
HOTP systems are problematic for a number of reasons. First, in the remote web server settings
we want to minimize the number of characters that the user needs to enter. In particular, we do not
want to require the user to type in the current counter value in addition to the 6-digit password.
Yet, the counter value is needed to synchronize the token and the remote server in case they go out
of sync. It would be better if we could use an implicit counter that is known to both sides. The
current time could serve as an implicit counter, as discussed below.
Second, there is a security problem. In HOTP the one-time password is only updated when
the user initiates the protocol. If the user authenticates infrequently, say once a month, then every
one-time password will be valid for an entire month. An attacker who somehow obtains the user’s
current one-time password, can sell it to anyone who wants to impersonate the user. The buyer
can use the purchased password at anytime, as long as it is done before the next time the user
authenticates to the server.
18.5.1.1 Time-based one-time passwords
A better one-time password scheme is called time-based one-time passwords , or TOTP. In
TOTP the counter iis incremented by one every 30 seconds, whether the user authenticates or not.
This means that every one-time password is only valid for a short time. When using a hardware
token as in Fig. 18.8a, the display changes every 30 seconds to present the latest one-time password
to the user. There is no button on the token.
Whenever the user authenticates to the remote server, the server uses the current time to
determine the value of the counter i. It then veriﬁes that the correct r:= F(k,i) was supplied by
the user. To account for clock skew between the server and the token, the server will accept any
of {F(k,(i−c)),...,F (k,(i+ c))}as valid passwords, for a small value of c such as c= 5. Within
the 2c+ 1 clock-skew window, the server prevents replay attacks by rejecting passwords that have
736
been used before.
Fig. 18.8a is a hardware token implementation of TOTP. The token is loaded with a secret PRF
key at token setup time and uses that key to derive the 6-digit one-time passwords. The server
has the same PRF key. The hardware token has an internal battery that can power the device for
several years. When the battery runs out the token is dead.
Fig. 18.8b is a TOTP implemented as an app on a modern phone. The user loads the secret
PRF key into the app by typing it in or by scanning a QR code. The app manages the user’s
one-time password with multiple systems, as shown in the ﬁgure, where the app manages one-time
passwords for Google and Facebook.
18.5.2 The S/key system
TOTP requires that the veriﬁcation key vk stored on the server remains secret. If an adversary
steals vk without being detected then all security is lost. This actually happened in a number of
well publicized cases.
The next system, called S/key, removes this limitation. The system, however, can only be used
a bounded number of times before the pair ( vk,sk) must be regenerated. We let n be a preset
poly-bounded number, say n = 106, that indicates the maximum number of times that a ( vk,sk)
pair can be used.
In Section 14.3 we deﬁned the concept of a hash chain, which will be used here too. To review,
let H : X→X be a function. For j ∈Z>0 we use H(j)(x) to denote the jth iterate of H, namely
H(j)(x) := H(H(H(···(x) ···))) where H is repeated j times. We let H(0)(x) := x.
The S/key protocol. The protocol Skeyn = ( G,P,V ), designed for n invocations, works as
follows:
• G: choose a random k←R X. Output sk := (k,n) and vk := H(n+1)(k),
• Algorithm P given sk, and algorithm V given vk, interact as follows:
1. P
(
sk = (k,i)
)
: send t:= H(i)(k) to V and set sk ←(k, i−1),
2. V(vk): if the received t from P satisﬁes vk = H(t) output accept
and set vk ←t. Otherwise, output reject.
The protocol is illustrated in Fig. 18.9. In the ﬁrst invocation the prover sends to the veriﬁer the
password H(n)(k). In the second invocation the prover sends the password H(n−1)(k), and so on.
Each password is only used once. Clearly after n invocations, the prover runs out of one time
passwords, at which point the prover can no longer authenticate to the veriﬁer, and a new ( vk,sk)
pair must be generated.
Security. We show that S/key remains secure even if vk is made public. Hence, S/key is fully
secure against eavesdropping, while HOTP is only weakly secure.
The analysis of S/key requires that H : X →X be a one-way function on n iterates as in
Deﬁnition 14.5. To review, this means that for all j = 1 ,...,n , given y ←H(j)(k) as input,
where k ←R X, it is hard to ﬁnd an element in H−1(y). Recall that Exercise 14.16 shows that a
one-way function H need not be one-way on n iterates, even when n= 2. Nevertheless, standard
cryptographic functions such as SHA256 are believed to be one-way on n-iterates for reasonable
values of n, say n≤106.
737
k H(k) H(n−2)(k) H(n−1)(k) H(n)(k) H(n+1)(k)
vkpassword
#1
password
#2
password
#3
Figure 18.9: The S/key protocol
Theorem 18.5. Let H : X→X be a one-way function on n iterates. Then the ID protocol Skeyn
is secure against eavesdropping.
Proof sketch. Since vk is public, we can assume that the adversary eavesdrops on, say, Qconversa-
tions, and then makes a single impersonation attempt. We do not know in advance what Qwill be,
but we can guess. We request y←H(n−Q+1)(k) from the iterated one-way challenger and use y to
generate Q valid conversations with respect to the initial veriﬁcation key vk = H(n+1)(k). If our
guess for Qis correct, and the adversary succeeds in its impersonation attempt, the adversary will
ﬁnd for us a pre-image of y. Thus, if the adversary impersonates with probability ϵ, we win Attack
Game 14.1 with probability ϵ/n. 2
Remark 18.1. To defend against preprocessing attacks onH, of the type discussed in Section 18.7,
algorithm G could choose a public salt at setup time and prepend this salt to the input on every
application of H. Moreover, to avoid the attack of Exercise 14.18 it is recommended to use a
diﬀerent hash function at every step in the chain. This has been analyzed in [100]. 2
The trouble with S/key. In every authentication attempt, the prover P must send to V an
element t∈X. For H to be one-way, the set Xmust be large and therefore t cannot be a 6-digit
number as in the TOTP system. In practice, t needs to be at least 128 bits to ensure that H
is one-way. This makes it inconvenient to use S/key as a one-time password scheme where the
user needs to type in a password. Encoding a 128-bit t as printable characters requires at least 22
characters.
18.6 Challenge-response: security against active attacks
We now consider a more powerful attack in which the adversary actively impersonates a legitimate
veriﬁer. For example, the adversary may clone a banking site and wait for a user (i.e., prover)
to visit the site and run the ID protocol with the adversary. As a result, the adversary gets to
repeatedly interact with the prover and send the prover arbitrary messages of its choice. The
adversary’s goal is to gain information about the prover’s key sk. After several such interactions,
the adversary turns around and attempts to authenticate as the prover to a legitimate veriﬁer.
We say that the ID protocol is secure against active attacks if the adversary still cannot fool the
veriﬁer.
The one-time password protocols HOTP and Skey in Section 18.5 are clearly insecure against
active attacks. By impersonating a veriﬁer, the adversary will learn a fresh one-time password
738
challenger adversary A
(vk,sk) ←
R
G()
vk
P(sk) probe # 1
...
P(sk) probe # Q
V(vk) impersonation attempt
accept or reject
Figure 18.10: An example active attack as in Attack Game 18.3
from the prover that the adversary can then use to authenticate to the veriﬁer. In fact, a moments
reﬂection shows that no single ﬂow protocol is secure against active attacks.
We ﬁrst deﬁne active attacks and then construct a simple two ﬂow protocol that is secure against
active attacks. For simplicity, in this section we only consider protocols where both the prover and
veriﬁer are stateless.
Attack Game 18.3 (Secure identiﬁcation: active attacks). For a given identiﬁcation proto-
col I= (G,P,V ) and a given adversary A, the attack game, shown in Fig. 18.10, runs as follows:
• Key generation phase. The challenger runs (vk,sk) ←R G(), and sends vk to A.
• Active probing phase. The adversary requests to interact with the prover. The challenger
complies by interacting with the adversary in an ID protocol with the challenger playing
the role of the prover by running algorithm P initialized with sk. The adversary plays
the role of veriﬁer, but not necessarily following the veriﬁer’s algorithm V. The adversary
may interact concurrently with many instances of the prover — these interactions may be
arbitrarily interleaved with one another.
• Impersonation attempt. As in Attack Game 18.1: the challenger and Ainteract, with the
challenger following the veriﬁer’s algorithm V (with input vk), and with Aplaying the role
of a prover, but not necessarily following the prover’s algorithm P.
We say that the adversary wins the game if the veriﬁcation protocol V outputs accept at the end of
the interaction. We deﬁne A’s advantage with respect toI, denoted ID3adv[A,I], as the probability
that Awins the game. 2
Deﬁnition 18.8. We say that an identiﬁcation protocol Iis secure against active attacks if for all
eﬃcient adversaries A, the quantity ID3adv[A,I] is negligible.
739
veriﬁerprover
k←
R
K
k k
Vmac(k,c,t )
c←
R
M
t←
R
Smac(k,c)
Figure 18.11: MAC based Challenge-Response identiﬁcation
Concurrent vs sequential attacks. Note that in the active probing phase of the attack game,
we allow the adversary to interact concurrently with many instances of the prover. One could
consider a weaker attack model in which these interactions must be run sequentially, as shown in
Fig. 18.10. However, all of the protocols we consider achieve security in this stronger, concurrent
attack model.
Keeping vk secret. Some protocols that satisfy Deﬁnition 18.8 do not require the veriﬁer to
keep any secrets. However, one of the protocols we present in this section does require vk to be
secret. This motivates a weaker version of Attack Game 18.3 where the challenger does not send
vk to the adversary. Just as in Section 18.5, if vk is kept secret, then we must now allow the
adversary to interact with the veriﬁer, since such interactions could potentially leak information
about vk. Therefore, in the active probing phase, we allow the adversary to interact concurrently
with multiple instances of both the prover and the veriﬁer. When interacting with an instance of
the veriﬁer, the adversary learns if the veriﬁer outputs accept or reject. In addition, during the
impersonation attempt, we let the adversary interact concurrently with several veriﬁers, and the
adversary wins the game if at least one of these veriﬁers accepts.
We let wID3adv[A,I] denote the adversary’s advantage in winning this weaker version of Attack
Game 18.3. ID protocols secure in these settings are said to be weakly secure.
Deﬁnition 18.9. We say that an identiﬁcation protocol I is weakly secure against active
attacks if for all eﬃcient adversaries A, the quantity wID3adv[A,I] is negligible.
18.6.1 Challenge-response protocols
We present two (stateless) ID protocols, called challenge-response, that are secure against active
attacks. The ﬁrst protocol is only weakly secure, meaning that the veriﬁer must keep the key vk
secret. The second protocol is secure even if vk is public.
Let I = ( Smac,Vmac) be a MAC deﬁned over ( K,M,T). The challenge-response protocol
ChalRespmac = (G,P,V ), shown in Fig. 18.11, works as follows:
• G: choose a random k←R K, and output sk := k and vk := k.
740
Figure 18.12: CRYPTOCard RB-1 token
• Algorithm P with input sk = k, and algorithm V with input vk = k, interact as
follows:
1. V chooses a random c←R M, and sends c to P;
2. P computes t←R Smac(k,c), and sends t to V;
3. V outputs Vmac(k,c,t ).
The random c is called the challenge while t is called the response. Clearly vk must be kept
secret for the protocol to be secure.
Theorem 18.6. Suppose Iis a secure MAC system, and that the size of the message space, |M|,
is super-poly. Then ID protocol ChalRespmac is weakly secure against active attacks.
Proof sketch. The assumption that |M|is super-poly implies that in each impersonation attempt,
the probability that the adversary receives a challenge message that it has seen before (in a previous
interaction with the prover) is negligible. So either that unlikely event happens, or the adversary
breaks the MAC system (in the sense of Attack Game 6.2). 2
Case study: CRYPTOCard. Fig. 18.12 gives an example of a Challenge-Response token.
When a user logs in to a server using his computer terminal, the server sends to the user an eight
character challenge, which appears on his computer terminal screen. The user enters this challenge
into the token using the keypad on the token. The token computes the response and displays this
on its screen. The user then types this response into his computer terminal keyboard, and this
is sent to the server to complete the protocol. The MAC is implemented as a PRF derived from
either 3DES or AES.
Challenge-response using passwords. In describing protocol ChalRespmac, the key k was
chosen at random from the key space Kof the underlying MAC system. In some settings it may
be convenient to deploy this protocol where the key k is derived from a user generated password
pw as k←H(pw) where H is a key derivation function as in Section 8.10.
This can be quite dangerous. If pw is a weak password, belonging to some relatively small
dictionary Dof common passwords, then this protocol is vulnerable to a simple oﬄine dictionary
attack. After eavesdropping on a single conversation (c,t) between prover and veriﬁer, the adversary
does the following:
741
for each w∈D do
if Vmac(H(w),c,t ) = accept then
output w and halt
In all likelihood, the output will be the password pw.
18.6.1.1 Challenge response with a public vk
The protocol in Fig. 18.11 is easily converted into a protocol where vk can be public. We need only
replace the MAC with a signature scheme ( G,Ssig,Vsig) deﬁned over (M,T). The main change to
Fig. 18.11 is that the prover responds to the challenge using algorithm Ssig and the secret signing
key. The veriﬁer checks the response using algorithm Vsig and the public veriﬁcation key. We refer
to the resulting protocol as ChalRespsig.
Theorem 18.7. Assume Sis a secure signature scheme, and that the size of the message space,
|M|, is super-poly. Then ChalRespsig is secure against active attacks.
Proof sketch. The idea is essentially the same as for that of Theorem 18.6, except that now, the
adversary must forge a signature, rather than a MAC. 2
The signature-based Challenge-Response protocol has an obvious security advantage over the
MAC-based protocol, since vk need not be kept secret. However, the MAC-based protocol has
the advantage that the response message can be short, which is crucial for CRYPTOCard-like
applications where a person must type both the challenge and the response on a keyboard. Recall
that in CRYPTOCard the response is only 48 bits long. A digital signature scheme cannot have
such short signatures and still be secure. Exercise 18.13 explores a diﬀerent challenge-response
protocol where the response message can be short.
18.7 A fun application: rainbow tables
Let h : P → Ybe a random function and set N := |P|. We look at the general problem of
inverting h. We will assume that |Y| ≥N since that is the typical situation in practice. For
example, Pmight be the set of all eight character passwords while Y= {0,1}256.
Let pw ←R Pand let y←h(pw). Clearly an exhaustive search over all of Pwill ﬁnd a preimage
of y after at most N queries to h. In this section we develop a much faster algorithm to invert h
using a method called rainbow tables. The inversion algorithm A= (A0,A1) proceeds in two
phases:
• Preprocessing phase: algorithm A0 interrogates h and outputs a table L containing ℓ pairs
in P2, for some ℓ. This preprocessing phase takes time O(N), but it is done oﬄine before the
challenge yis known. The resulting table L, called a rainbow table, must be stored somewhere
to be used in the second phase.
• Attack phase: once a challenge y ∈Y is provided, algorithm A1 is invoked as A1(L,y) and
uses L to quickly ﬁnd an inverse of y. It successfully outputs a preimage pw′in h−1(y) with
probability close to 1.
742
Let t be the running time of the attack phase A1. We will show how to invert h in time t where
t×ℓ2 ≈N2. (18.9)
For example, if we can store a table Lof size ℓ= N2/3 then we can invert hin time t≈N2/3 with
probability close to 1. This is much faster than exhaustive search over P.
Equation (18.9) is called a time-space tradeoﬀ. The more space we have for the table L, the
faster we can invert h. Of course, once we have the table L, we can use it to quickly ﬁnd the inverse
of many elements in Y.
Rainbow tables are commonly used to crack unsalted passwords, as discussed in Section 18.3.1.3.
They can also be used to recover the secret key k in a block cipher (E,D) from a known plaintext-
ciphertext pair
(
m, c= E(k,m)
)
. This is because the key k is the inverse of the function h(k) :=
E(k,m) at the point c. If m is suﬃciently long, or if multiple plaintext-ciphertexts pairs are
provided, then the inverse k is unique. Applying this to AES-128 we see that a table L of size
128 ×(2128)(2/3) ≈128 ×285 bits (about a billion exabytes) can be used to break AES in time 2 85.
This may be too much today, but could become feasible in a few decades. We discussed this threat
in Section 4.2.2.1. It is partially the reason for the shift towards AES-256. Note, however, that
building the table L requires signiﬁcant (one-time) work; about 2 128 evaluation of AES-128.
A careful reader will notice that the bound (18.9) is quite poor at the boundary ℓ= 1, where
it gives t ≈N2. This is much worse than simple exhaustive search that only takes time N. It
shows that the rainbow table algorithm is not tight for some values of ℓ. Improving the time-space
tradeoﬀ (18.9) is a long-standing open problem (see Exercise 18.7).
Hellman’s basic time-space tradeoﬀ. The ﬁrst time-space tradeoﬀ for inverting a random
function was invented by Hellman as a criticism of the short DES key size (56-bits). Hellman’s
method uses an eﬃciently computable auxiliary function g: Y→P called a reduction function.
It “reduces” an output of h in Yto an element of P. For simplicity, we will assume that g is also
a random function. Then the function f(pw) := g(h(pw)) maps Pto itself.
The preprocessing algorithm A0 uses the function f : P →P. It is parameterized by two
positive constants τ and ℓ. Recall that for τ >0 the function f(τ) is the τ-th iterate of f as deﬁned
in (18.5). Algorithm A0 works as follows, and is shown visually in Fig. 18.13a:
Algorithm A0: (preprocess h)
for i= 1,...,ℓ :
pwi ←R P
zi ←f(τ)(pwi) ∈P / / run through τ evaluations of f
output L:=
{
(pw1,z1),..., (pwℓ,zℓ)
}
⊆P2 / / output ℓ pairs in P2
Algorithm A0 builds ℓ horizontal chains as shown in Fig. 18.13a. For each chain it records the
starting and ending points in the table L. Its running time is proportional to τ ×ℓ.
Next, to invert an element y ∈Y using L we repeatedly apply f to g(y) until we hit the right
edge of Fig. 18.13a. We then use Lto jump to the starting point of the relevant chain and traverse
it until we ﬁnd a preimage of y. More precisely, to invert y do:
743
pw1
f f ··· f f z1
pw2
f f ··· f f z2
pw3
f f ··· f f z3
... ...
pwℓ
f f ··· f f zℓ
g(y)
τ
ℓ
(a) Hellman’s basic time-space tradeoﬀ
pw1
f1 f2 ··· fτ−1 fτ z1
pw2
f1 f2 ··· fτ−1 fτ z2
pw3
f1 f2 ··· fτ−1 fτ z3
... ...
pwℓ
f1 f2 ··· fτ−1 fτ zℓ
g(y)
τ
(b) rainbow tables
Figure 18.13: Time-space tradeoﬀ tables, the boxed items make up the table L.
Algorithm A1(L,y):
1. z←g(y) ∈P
2. for i= 1,...,τ :
3. if there is a ˜pw such that ( ˜pw,z) ∈L: / / if z is a chain endpoint
4. pw ←f(τ−i)( ˜pw) / / traverse chain from the beginning
5. if h(pw) = y: / / if found inverse, output it
output pw and terminate
6. z←f(z) ∈P / / move down the chain
7. output fail / / g(y) is not on any chain
If the picture looked liked Fig. 18.13a, then g(y) would be somewhere along one of the chains,
as shown in the ﬁgure. Once we ﬁnd the end of that chain, the table L would give its starting
point ˜pw. The traversal on line (4) would then give an inverse of y. The total running time to
invert y would be τ evaluations of f and at most τ lookups in L.
The situation, however, is a bit more complicated. Fig. 18.13a ignores the possibility of collisions
between chains, as shown in Fig. 18.14. The ﬁrst and second chains in the ﬁgure collide because
f(4)(pw1) = f(6)(pw2). The second and third chains collide because f(5)(pw2) = f(7)(pw3). The
input g(y) happens to lie on the top chain. As we move along the top chain, starting from g(y), we
ﬁrst ﬁnd the end of the third chain z3, then the end of the second chain z2, and only then do we
ﬁnd the end of the ﬁrst chain z1, which lets us invert y. This is why on line (5) we must check that
we found an inverse of y before outputting it, to avoid a false alarm that causes us to traverse the
wrong chain. In Fig. 18.14 both z3 and z2 will cause false alarms. A false alarm may also happen
because g(h(pw)) = g(y) but h(pw) ̸= y, which is another reason for the test on line (5).
The chain merge problem. While the basic Hellman method is quite clever, it does not work
as described, and will fail to invert almost all y= h(pw). Let’s see why. For A1 to succeed we need
to ensure that almost all pw ∈P are on at least one chain. The maximum number of passwords
744
pw1
pw2
pw3
z2 = f(10)(pw2)
z1 = f(10)(pw1)
g(y)
z3=
f(10)(pw3)
Figure 18.14: Example chain collisions, all three chains are length 10
processed by A0 is τ×ℓ. Therefore, at the very least, we need τ×ℓ≥N. For the best performance
we would like to set τ ×ℓ= N and hope that most pw in Pare on some chain.
As it turns out, this does not work. Once two chains collide, they will merge and cover the
same elements, as shown in Fig. 18.14. When building a table with a large number of long chains,
chain mergers are inevitable and happen frequently. To illustrate the magnitude of the problem,
take τ = N1/3 and ℓ = N2/3 so that τ ×ℓ = N. Let A be the set of elements in Pencountered
during preprocessing. If we model f : P→P as a random function, then one can show that the
set Ais unlikely to contain more than o(N) elements in P. This means that |A|/N tends to 0 as N
goes to inﬁnity, and algorithm A1(L,y) will fail for almost all y = h(pw). In fact, to capture a
constant fraction of Pwe would need ℓ= Ω(N) chains of length τ. This would make the table L
of size Ω(N) which makes this a non interesting time-space tradeoﬀ: with a table that big we can
trivially invert h in constant time.
Hellman’s solution to this problem is to build many small independent tables, where each table
uses a diﬀerent reduction function g. Each table contains a small number of chains of length τ
ensuring that no collisions occur within a single table. Algorithm A1 searches every table separately
and is therefore m times slower if there are m tables. This works well and achieves the bounds of
(18.9). However, a diﬀerent solution, called rainbow tables, is simpler and more eﬃcient.
Rainbow tables. An elegant solution to the chain merge problem is to use an independent
reduction function gi : Y → Pfor every column i = 1 ,...,τ of Fig. 18.13a. As before, let
fi(pw) = gi(h(pw)). The preprocessing algorithm A0 now executes the procedure illustrated in
Fig. 18.13b. It outputs the same table L as before containing the starting and ending points of
every chain. If each chain were a diﬀerent color, and slightly curved upwards, the picture would
look like a rainbow, which explains the name.
The point of using a diﬀerent function fi in every column is that a chain collision does not
necessarily cause the chains to merge. For two chains to merge they must collide at exactly the
same index. This makes chain merges far less likely (see Exercise 18.18). Moreover, if a chain rooted
at pw happens to merge with a chain rooted at pw′, the end points z and z′of both chains will be
equal. The preprocessing algorithm A0 can easily detect this duplicate end point and discard one
of the chains. The end result is that we can set τ = N1/3 and ℓ = N2/3 and capture a constant
fraction of Pduring preprocessing.
Now, to invert an element y ∈Y using the table L, observe that if gτ−1(y) is contained in
the second to last column of Fig. 18.13b then fτ(gτ−1(y)) is a chain endpoint in L. If gτ−2(y) is
contained in the third to last column of the ﬁgure then fτ
(
fτ−1(gτ−2(y))
)
is a chain endpoint in L,
and so on. This suggests the following algorithm for inverting y using L:
745
Algorithm A1(L,y):
1. for i= τ downto 1:
2. z←fτ
(
fτ−1(···fi+1(gi(y)) ···)
)
∈P / / when i= τ then z= gτ(y),
/ / when i= τ −1, z = fτ(gτ−1(y))
3. if there is a ˜pw such that ( ˜pw,z) ∈L: / / if z is a chain endpoint
4. pw ←fi−1
(
···f2(f1( ˜pw)) ···
)
/ / traverse chain from the beginning
5. if h(pw) = y: / / if found inverse, output it
output pw and terminate
6. output fail / / y is not on any chain
The bulk of the work in this algorithm is done on line (2). In the ﬁrst iteration this line evaluates f
once, in the second iteration twice, and so on. Overall, the worst case work due to line (2) is
1 + 2 + ... + τ = τ(τ + 1)/2 ≈τ2/2. Hence, the maximum running time of A1 is t := τ2/2. To
capture most of Pwe need ℓ×τ ≥N, and since τ = (2t)1/2 we obtain
ℓ×(2t)1/2 ≥N.
Squaring both sides gives ℓ2 ×t≥N2/2, which is the time-space tradeoﬀ promised in (18.9). Note
also that algorithm A1 makes at most τ lookups into the table L.
Rainbow tables in practice. Rainbow tables for many popular hash functions are readily
available. They are designed to be used with a program called RainbowCrack. For example, a
ready-made table for SHA1 of size 460 GB is designed to ﬁnd preimages in the set of all 8 character
passwords over an alphabet called ascii-32-95. This alphabet contains all 95 characters on a
standard US keyboard. The table has success rate close to 97% and is free for anyone to download.
On a GPU, cracking a SHA1 hashed password of eight characters using this table takes about an
hour.
Extensions. While rainbow tables are designed to invert arandom function, a diﬀerent algorithm
due to Fiat and Naor [61] gives a time-space tradeoﬀ for inverting an arbitrary function h: P→Y .
Their time-space tradeoﬀ satisﬁes ℓ2t ≥λN3, which means that to invert the function h with
probability 1/2 in time t, their preprocessing algorithm must generate a table of size approximately
(λN3/t)1/2. Here λis the collision probability ofhdeﬁned as λ:= Pr
[
h(x) = h(y)
]
where x,y ←R P.
When his a random function and |Y|≫|P| we have λ= 1/N, which recovers the bound in (18.9).
18.8 Another fun application: hardening password storage
To be written.
18.9 Notes
Citations to the literature to be added.
746
18.10 Exercises
18.1 (Mutual identiﬁcation). Throughout the chapter we were primarily interested in one-
sided identiﬁcation, where one party identiﬁes itself to another. We can similarly develop protocols
for the mutual identiﬁcation that provide diﬀerent levels of security. As before, the identiﬁcation
protocol is a triple (G,P,V ), but now at setup time, algorithm Goutputs (vk1,sk1) and (vk2,sk2),
one pair for each side. Each participant is given the peer’s veriﬁcation key. The participants then
run the identiﬁcation protocol and each side decides whether to accept or reject the result.
(a) Security against direct attacks is deﬁned using an attack game where the adversary is given
both veriﬁcation keys vk1,vk2, and the secret key of one side. It should be unable to suc-
cessfully complete the protocol by playing the role of the other side. Give a precise security
deﬁnition that extends Attack Game 18.1.
(b) Describe a password-like protocol that satisﬁes the security deﬁnition from part (1).
(c) Deﬁne an attack game that captures active attacks, similar to Attack Game 18.3, but applies
to mutual authentication. Describe a protocol that achieves this level of security.
18.2 (An attack on PBKDF2). Let pw ∈P be a password. Suppose the adversary obtains a
salt ∈S and three values
y0 := PBKDF2F(pw,salt,d), y1 := PBKDF2F(pw,salt,d + 1), y2 := PBKDF2F(pw,salt,d + 2)
for some d. Show that the adversary can recover pw in time O(|P|), independent of the diﬃculty d.
You may assume that the underlying PRF F is deﬁned over ( P,X,X) where |X| is much larger
than |P|, and that F : P×X→X behaves like a random function.
18.3 (Security of PBKDF2). Let Hh be a PBKDF deﬁned over ( P,S,Y), and suppose that
Hh is deﬁned with respect to some underlying function h: X→Z that we will model as a random
oracle. We say that the PBKDF is secure if no adversary that makes at most d−1 queries to h
can distinguish Hh from a random function. In particular, deﬁne security of Hh using the following
two experiments, Experiment 0 and Experiment 1. For b= 0,1 deﬁne:
Experiment b:
• The adversary Asends to the challenger a positive diﬃculty d∈Z. The challenger chooses a
random function h: X→Z .
• The adversary then issues a sequence of queries, where for i= 1,2,... query i is one of:
– an Hh query: the adversary sends pwi ∈P. In response, the challenger chooses salti ←R S
and ˜yi ←R Y. If b= 0 it sets yi ←Hh(pwi,salti,d). If b= 1 it sets yi ←˜yi. The challenger
sends (yi,salti) to the adversary.
– an h query: the adversary sends xi ∈X and gets back h(xi).
• Finally, the adversary Aoutputs a bit ˆb∈{0,1}.
For b= 0,1, let Wb be the event that Aoutputs 1 in Experiment b, and deﬁne A’s advantage with
respect to Hh as
⏐⏐Pr[W0] −Pr[W1]
⏐⏐. We say that Hh is a secure PBKDF if no adversary that
makes at most (d−1) queries to hhas a non-negligible advantage in winning the game. Show that
747
PBKDF2F is secure when the underlying PRF F is modeled as a random oracle F : P×X→X ,
and Xis super-poly.
Discussion: A security deﬁnition for a PBKDF H should require that a fast algorithm cannot
distinguish the output of H from a random value. To see why, suppose there is an algorithm
B(pw,salt,d) that quickly computes one bit of Hh(pw,salt,d). When trying to crack a hashed
password y, this Blets the adversary quickly discard about half the password candidates in the
dictionary. Any candidate password that does not match y on the bit output by Bcan be quickly
discarded. For this reason we require that no fast algorithm can distinguish the output of a secure
PBKDF from random.
More discussion: A more complete deﬁnition would allow the adversary Ato preprocess the
function h, before it engages in the game above. Speciﬁcally, we let A= (A0,A1) where A0 runs in
a preprocessing phase for unbounded time, interacts with h, and outputs an ℓ-bit advice string L.
Then A1 runs as in the game deﬁned above, taking L as input. When ℓ <|S|, the preprocessing
phase should not improve the adversary’s advantage by more than a negligible amount.
The deﬁnition can be further strengthened to require that distinguishing the output of H from
random at ppoints is ptimes harder than doing so at a single point. This stronger security notion
was studied in [13] using a deﬁnition based on indiﬀerentiability. They show that both PBKDF1
and PBKDF2 satisfy this stronger property.
18.4 (A stronger model for slow hash functions). Suppose we modify the security deﬁnition
in Exercise 18.3 so that the adversary can specify an arbitrary diﬃculty dfor every Hh query. That
is, Hh query number iis a pair (pwi,di) and both pwi and di are used to compute the response. The
rest of the security deﬁnition is unchanged. Exercise 18.2 shows that PBKDF2 is insecure under this
stronger security deﬁnition. Show that the PBKDF Hh deﬁned as Hh(pw,salt,d) = h(d)(pw,salt,d)
satisﬁes this stronger deﬁnition. Here his a function h: X→X where X= P×S× Zn and where
n is the maximum supported diﬃculty.
18.5 (Broken Scrypt). Suppose line (4) of the Scrypt hash in Fig. 18.6 were changed to the
following:
4. j ←int(h(i)) mod (d+ 1)
where iis encoded as an element of X= {0,1}n. Show how to evaluate the resulting function using
only d/3 memory cells without much impact to the running time. Use the fact that the order of
reads from the array ( x1,...,x d) is known in advance.
18.6 (A time-space tradeoﬀ attack on Scrypt). This exercise shows how to evaluate Scrypt
with little memory. Recall that for diﬃculty dScrypt can be evaluated in time O(d) using memory
for d elements of X.
(a) Show that Scrypt (Fig. 18.6) can be evaluated in constant space, by storing only two elements
of X. The running time, however, degrades to O(d2) evaluations of H instead of O(d). Your
attack shows that Scrypt is vulnerable to a time-space tradeoﬀ, but one that greatly harms
the running time.
(b) For 1 <t<d , generalize part (a) to show an algorithm that evaluates Scrypt by only storing
t elements of Xand runs in time O(d2/t).
748
18.7 (A time-space tradeoﬀ for one-way permutations). In Section 18.7 we saw a time-
space tradeoﬀ for one-way functions. In this exercise we develop a time-space tradeoﬀ for one-way
permutations, which is simpler and much better. Let π : X →Xbe a random permutation and
let N := |X|. For a given ℓ, construct an adversary A= (A0,A1) where A0 preprocesses π and
outputs an advice string Lcontaining ℓelements of X. Then for y:= π(x), where x←R X, adversary
A1(L,y) outputs x after issuing at most t:= ⌈N/ℓ⌉queries to π.
Hint: Try using the cycle structure of the permutation π.
Discussion: Your solution gives a time-space tradeoﬀ satisfying ℓ×t≥N for inverting a random
permutation. This is known to be the best possible [156, 75]. For a random function we had
ℓ2 ×t ≥N2, which is a much worse tradeoﬀ. To see why, try setting ℓ = N2/3 and see what is
the resulting time bound t in each case. It is still an open problem if there is a better time-space
tradeoﬀ for random functions.
18.8 (A time-space tradeoﬀ for iterated permutations). Let π : X → Xbe a random
permutation and let π(d) be its d-th iterate, for some d> 0. Let N := |X|. Give an algorithm that
succeeds with probability close to 1 in inverting π(d) in time t using an advice string L of length
ℓ, where t×ℓ≥N. Notice that the bound on t and ℓ is independent of d, and is the same as the
time-space tradeoﬀ bound for inverting π. This means that inverting π(d) with preprocessing is no
harder than inverting π.
18.9 (A batch-vulnerable one-way function). In Section 18.3.1.3 we discussed batch inversion
attacks on one-way functions. Let H be a one-way function deﬁned over ( X,Y). We say that H
is batch-vulnerable if inverting H at one random point can be done at about the same time
as inverting H at t random points, for some t >1. Show that the function H(x) = x2 deﬁned
over ( Zn,Zn) is a one-way function assuming factoring is hard, but is batch-vulnerable. Here
n←R RSAGen(ℓ,e) is an RSA modulus treated as a system parameter.
18.10 (Why multiple impersonation attempts for eavesdropping security). This exercise
explains why when vk is kept secret, it is necessary to allow the adversary in Attack Game 18.2
to make multiple impersonation attempts. Describe a 3-round challenge-response protocol that is
secure against eavesdropping (and even secure against active attacks) if the adversary can only
make one impersonation attempt. But is completely insecure, even against direct attacks, if the
adversary can make two impersonation attempts.
Solution:
• G: pick a random k←R Kand output sk := k and vk := k.
• Algorithm P given sk, and algorithm V given vk, interact as follows:
(a) V sends the c←R Mto P;
(b) P computes t←R S(sk,c), and sends t to V;
(c) If t= 0 then V sends to P the secret key k;
(d) V outputs V(vk,c,t ).
It should be clear that if the adversary can make two impersonation attacks then the protocol
is not secure even under a direct attack. However, if only one impersonation attempt is allowed
then no amount of eavesdropping will break the protocol since t is unlikely to be 0 in any of the
eavesdropping transcripts.
749
18.11 (Why interact with the veriﬁer for active security). In this exercise we show that
when vk is kept secret, it is necessary to allow an active adversary in Attack Game 18.3 to interact
with the veriﬁer during the probing phase. We describe a protocol that is secure if the adversary
cannot interact with the veriﬁer during the probing phase, but is trivially insecure otherwise. The
protocol is standard Challenge-Response except that the veriﬁer always uses the same challenge.
• G: choose a random k←R Kand c←R M. Output sk := k and vk := (k,c).
• Algorithm P given sk, and algorithm V given vk, interact as follows:
(a) V sends the c speciﬁed in vk to P;
(b) P computes t←R S(sk,c), and sends t to V;
(c) V outputs V(vk,c,t ).
(a) Show that this ID protocol is (weakly) secure against an active adversary playing Attack
Game 18.9 where the adversary cannot interact the veriﬁer during the probing phase.
(b) Show that the protocol is insecure against an active adversary playing Attack Game 18.9
where the adversary can interact the veriﬁer.
18.12 (Improving S/key performance). In this question we reduce the number of hash function
evaluations for the prover.
(a) Suppose the prover only stores the base of the hash chain (namely, the ﬁrst element in the
chain). After nlogins, how many times did the prover have to evaluate the hash function H?
How many times did the server evaluate the hash function H?
(b) Suppose that in addition to the base of the hash chain h0, the prover also stores the midpoint,
namely hn/2 = H(n/2)(h0) where H(n/2)(h0) refers to n/2 repeated applications of H. Explain
why this reduces the prover’s total number of hash evaluations afternlogins by about a factor
of 2.
(c) Show that by storing the base point plus one more point (i.e. the total storage is as in part
(b)) the prover can, in fact, reduce the total number of hashes after n logins to O(n3/2).
Hence, the prover does O(√n) hashes on average per login by storing only two values.
(d) Generalize part (c) — show that by storing log2 npoints along the chain the prover can reduce
the total number of hashes after n logins to O(n). Hence, the prover only does a constant
number of hashes on average per login.
18.13 (Challenge-response by decryption). Let (G′,E,D ) be a public-key encryption scheme
with message space R. Consider the following challenge-response ID protocol ( G,P,V ):
• G: run G′to obtain a public key vk and a secret key sk.
• Algorithm P given sk, and algorithm V given vk, interact as follows:
(a) V chooses a random nonce r←R R, and sends c←R E(vk,r) to P;
(b) P computes ˆr←D(sk,c), and sends ˆr to V;
(c) V outputs accept only if r= ˆr.
750
Show that this protocol is secure against active attacks, assuming that the nonce space Ris super-
poly, and the encryption scheme is non-adaptive CCA secure, as deﬁned in Exercise 12.28.
Discussion: This scheme is an attractive option for login to a remote web site (the veriﬁer) from
a laptop using a mobile phone (the prover) as a second factor. To login, the web site displays c as
a QR code on the laptop screen and the user scans the code using the phone’s camera. The phone
decrypts c and displays the six least signiﬁcant digits of r on the screen. The user then manually
types the six digits into her web browser, and this value is sent to the remote web site to be veriﬁed.
18.14 (Insecure challenge-response by decryption). Continuing with Exercise 18.13, let’s
see why non-adaptive CCA is necessary for security. Give an example public-key system (G′,E,D )
that is semantically secure, but when used in the protocol of Exercise 18.13 leads to a protocol that
is not secure against active attacks.
18.15 (Challenge-response using Diﬃe-Hellman). Let G be a cyclic group of prime order q
with generator g ∈G. Let H : G2 →T be a hash function. Consider the following three-round
identiﬁcation protocol (G,P,V ) where vk is public:
• G: choose a random α←R Zq and set u←gα ∈G. Output sk := α and vk := u.
• Algorithm P given sk, and algorithm V given vk, interact as follows:
(a) V chooses a random β ∈Zq, computes v←gβ ∈G, and sends v to P;
(b) P computes w←vα ∈G, and sends t←H(v,w) to V;
(c) V outputs accept only if t= H(v,uβ).
Show that this protocol is secure against active attacks when|T|is super-poly, the ICDH assumption
(Deﬁnition 12.4) holds for G, and H is modeled as a random oracle.
Discussion: Instantiating Exercise 18.13 with CCA-secure ElGamal encryption gives a similar
protocol to the one here, but the protocol in this exercise is much simpler.
18.16 (Identiﬁcation using a weak PRF). Let F be a PRF deﬁned over ( K,X,Y) where
Y:= {0,1}n. Consider the following three-round identiﬁcation protocol where vk is kept secret:
• G: choose random k0,k1 ←R Kand output sk := (k0,k1) and vk := (k0,k1).
• Algorithm P given sk, and algorithm V given vk, interact as follows:
(a) P chooses a random x0 ∈X and sends it to V;
(b) V chooses a random x1 ∈X and send it to P;
(c) P computes y←F(k0,x0) ⊕F(k1,x1) and sends it to V;
(d) V outputs accept only if y= F(k0,x0) ⊕F(k1,x1).
Show that this protocol provides weak security against active attacks (Deﬁnition 18.9), assuming
F is a weak PRF (as in Deﬁnition 4.3), and |X|and |Y|are super-poly. In Chapter 17 we saw an
eﬃcient weak PRF that makes this protocol computationally very cheap for the veriﬁer and the
prover.
Hint: The proof makes use of rewinding, as explained in Lemma 19.2. If you get stuck, see Section
5.2 of [55].
751
18.17 (Timing attacks). Consider a password system where the veriﬁer has a stored hashed
password h←H(pw). We treat the hashed password has a string of bytes. Given a password pw′
the veriﬁer does:
h′←H(pw′)
for i= 0,..., |h|do:
if h[i] ̸= h′[i] output reject and exit
output accept
(a) Show that this implementation is vulnerable to a timing attack. An attacker who can submit
arbitrary queries to the veriﬁer can recover a victim user’s hashed password h with at most
256 ·|h|queries to the password checker. The attacker can mount an oﬄine dictionary attack
on h.
(b) How would you implement the veriﬁer to prevent the timing attack from part (a)?
18.18 (The likelihood of a chain merge in rainbow tables). Consider the preprocessing
phase described in Fig. 18.13b. Suppose the parameters ℓ and τ are chosen so that ℓτ = N. Show
that with probability at least 1 /e≈0.37, a chain rooted at a random starting point pw ←R P, will
not merge with any of the other ℓ−1 chains. You may assume that every chain is a sequence of
random independent elements in P, unless the chain merges with another chain, in which case both
chains share all subsequent elements.
Discussion: Because A0 can easily detect chain merges, it will only need to generate every chain
three times, in expectation, to build a set of non-merging chains. A set of ℓ non-merging chains
covers about (1 −1/e) ≈0.63 of Pin expectation.
18.19 (A preprocessing attack on discrete-log). In Section 18.7 we saw how rainbow tables
are used to invert a one-way function in the preprocessing model. This exercise and the next look
at preprocessing attacks on other cryptographic primitives. We start with discrete-log.
Let G be a ﬁnite cyclic group of order q with generator g. In Chapter 16 we presented Pollard’s
algorithm for computing discrete logarithm in G in time O(q1/2). Suppose the adversary can
preprocess the group G to build an advice string L containing q1/3 group elements. We construct
an algorithm that takes L as input and computes discrete logarithm in G in time O(q1/3). This is
much faster than O(q1/2).
(a) Let H : G →{1,...,q −1}be a random function. Choose r←R Zq and let u= gr. Deﬁne the
following chain of length τ starting at u:
u1 := u, u 2 := u1 ·gH(u1), u 3 := u2 ·gH(u2), ..., u τ := uτ−1 ·gH(uτ−1). (18.10)
Observe that uτ = gr+H(u1)+···+H(uτ−1) and therefore it is easy to calculate the discrete
logarithm of uτ base gjust given uand r. The preprocessing algorithm constructs ℓchains as
in (18.10) by choosing ℓrandom starting points u(1),...,u (ℓ) ∈G that have a known discrete
logarithm. Each chain has length τ. We can represent the set of chains as an ℓ×τ matrix.
The algorithm outputs the advice string L as a table containing the last point of each chain
along with its discrete logarithm base g. Thus L contains exactly ℓ pairs in G ×Zq.
We say that preprocessing succeeds if the ℓ×τ matrix of chains contains at least ( ℓ·τ)/2
distinct elements in G. Suppose ℓ = τ = q1/3. Show that preprocessing succeeds with
probability at least 1/2.
752
(b) The algorithm to compute discrete logarithm base gtakes as input the table Land an element
h∈G. It outputs the discrete logarithm of h. The algorithm builds a chain starting at h(as
in (18.10), but of length possibly greater than τ). Show that, with constant probability, after
at most q/(τ·ℓ) steps, this chain will collide with some element in the ℓ×τ matrix of chains
constructed during preprocessing. Assume that preprocessing succeeded.
(c) After the collision from part (b) happens, if we continue along the chain rooted at h for at
most τ more steps, the chain will collide with some element in the table L. The algorithm
can detect when this happens. Show that once this collision with L happens, the algorithm
can eﬃciently compute the discrete logarithm of h.
Hence, the overall time to compute the discrete logarithm of h, once the table Lis computed,
is t= O
(
q/(τ ·ℓ) + τ
)
. Setting τ = ℓ= q1/3 gives a running time of O(q1/3) using a table L
of size ℓ= q1/3, as promised. Note that the one-time preprocessing phase takes time q2/3.
18.20 (A preprocessing attack on the Even-Mansour cipher). The same idea as in Ex-
ercise 18.19 can be used to attack the Even-Mansour cipher in the preprocessing model. For
X:= {0,1}n, recall that the Even-Mansour block cipher ( E,D), built from a random permutation
π: X→X , is deﬁned as:
E
(
(k0,k1), x
):= π(x⊕k0) ⊕k1, and D
(
(k0,k1), y
):= π−1(y⊕k1) ⊕k0.
In Exercise 4.21 we showed how to distinguish this block cipher from a random permutation after
about 2n/2 queries to π and 2n/2 queries to E. Suppose the adversary can preprocess the permuta-
tion π to build an ℓ-bit advice string L, where ℓ= 2n/3. Let’s show that using this Lwe can break
the cipher (E,D) in time 2 n/3, which is much less than 2 n/2.
(a) Fix some non-zero ∆ ∈X and deﬁne the functions
f(x) := x⊕π(x) ⊕π(x⊕∆), g (x) := x⊕E(x) ⊕E(x⊕∆),
where E(x) := E
(
(k0,k1),x
)
. Show that if x= y⊕k0 then
f(x) = g(y) ⊕k0 and x⊕f(x) = y⊕g(y). (18.11)
(b) To preprocess πthe preprocessing algorithm chooses ℓrandom starting points x1,...,x ℓ ←R X.
For each xi it builds a chain of length τ starting at xi:
xi, f(xi), f(f(xi)), ..., f τ−1(xi).
This gives an ℓ×τ matrix of elements in X. For every end point zi := fτ−1(xi) it stores
zi ⊕f(zi) in the table L. Thus, L contains exactly ℓ elements in X.
We say that preprocessing succeeds if the ℓ×τ matrix of chains contains ( ℓ·τ)/2 distinct
elements in X. Show that preprocessing succeeds with probability at least 1 /2 when ℓ= τ =
2n/3 and π is a random permutation in Perms[ X].
(c) To distinguish the Even-Mansour cipher (E,D) from a random permutation choose a random
y←R Xand build the chain
y1 := y, y 2 := g(y), y 3 := g(g(y)), ..., y ν := gν(y) (18.12)
753
for ν := 2n/(τ ·ℓ) + τ. At every step along the chain check if yj ⊕g(yj) is in the table L. If
so output 0 (indicating that E is an Even-Mansour cipher) and stop. If this never happens,
output 1 (indicating that E is random). Show that this distinguisher has advantage close
to 1. Setting ℓ= τ = 2n/3 gives ν = 2n/3 so that the distinguisher runs in time 2 n/3 using a
table L of size 2n/3, as promised.
Hint: Show that, with constant probability, after at most 2 n/(τ ·ℓ) steps, the chain (18.12)
will hit a point yi such that yi ⊕k0 in contained in the ℓ×τ matrix of chains constructed
during preprocessing (regardless of whether E implements an Evan-Mansour cipher or is a
random permutation). Once this happens, use the left hand side of (18.11) to argue that this
condition will continue to hold as the algorithm continues down the chain, assuming that E
implements an Evan-Mansour cipher. Therefore, after at most τ more steps, use the right
hand side of (18.11) to argue that the chain (18.12) will hit a point yj such that yj ⊕g(yj) is
in L. Show that this is unlikely to happen when E is a random permutation.
754
Chapter 19
Identiﬁcation and signatures from
Sigma protocols
In the previous chapter, we studied identiﬁcation protocols. In particular, in Section 18.6.1.1, we
showed how one could use a secure signature scheme to build a challenge-response identiﬁcation
scheme that provided the highest level of security, namely, security against active attacks (Deﬁni-
tion 18.8). In this chapter, we proceed in the opposite direction.
First, using a completely diﬀerent technique, we develop a new identiﬁcation protocol that
achieves security against eavesdropping attacks (Deﬁnition 18.6). This protocol is of interest in its
own right, because it is quite elegant, and can be proved secure under the DL assumption.
Second, we show how to transform this protocol into a very eﬃcient signature scheme called
the Schnorr signature scheme. The scheme is secure, under the DL assumption, in the random
oracle model.
Third, we generalize these techniques, introducing the notion of aSigma protocol. Using these
more general techniques, we develop several new identiﬁcation protocols and signature schemes. We
also explore other concepts and applications, including the notion of a “proof of knowledge”.
In the next chapter, we put these techniques to more advanced use, designing protocols that
allow one party to prove to another that certain facts are true (without revealing unnecessary
information). For example, we show how to prove that encrypted value m lies in a certain range
without revealing any other information about m.
19.1 Schnorr’s identiﬁcation protocol
We begin by describing an identiﬁcation protocol, called Schnorr identiﬁcation, named after its
inventor, C. Schnorr. This protocol can be proved secure against eavesdropping attacks, assuming
the discrete logarithm problem is hard.
Let G be a cyclic group of prime order q with generator g∈G. Suppose prover P has a secret
key α∈Zq, and the corresponding public veriﬁcation key is u= gα ∈G. To prove his identity to
a veriﬁer V, P wants to convince V that he knows α. The simplest way to do this would be for
P to simply send α to V. This protocol is essentially just the basic password protocol (version 1)
discussed in Section 18.3, with the functionH(α) := gα playing the role of the one-way function. As
such, while this protocol provides security against direct attacks, it is completely insecure against
eavesdropping attacks. Instead, Schnorr’s protocol is a cleverly designed interactive protocol that
755
P(α) V(u)
αt ←R Zq, ut ←gαt
ut
−−−−−−−−−−−−−−−−→
c←R C
c←−−−−−−−−−−−−−−−−
αz ←αt + αc
αz
−−−−−−−−−−−−−−−−→
gαz ?= ut ·uc
Figure 19.1: Schnorr’s identiﬁcation protocol
allows P to convince V that he knows the discrete logarithm of u to the base g, without actually
sending this value to V.
Here is how it works. Let C be a subset of Zq. Then Schnorr’s identiﬁcation protocol is
Isch = (G,P,V ), where:
• The key generation algorithm G runs as follows:
α←R Zq, u ←gα.
The veriﬁcation key is vk := u, and the secret key is sk := α.
• The protocol between P and V runs as follows, where the prover P is initialized with sk = α,
and the veriﬁer V is initialized with vk = u:
1. P computes αt ←R Zq, ut ←gαt, and sends ut to V;
2. V computes c←R C, and sends c to P;
3. P computes αz ←αt + αc∈Zq, and sends αz to V;
4. V checks if gαz = ut ·uc; if so V outputs accept; otherwise, V outputs reject.
Fig. 19.1 illustrates the protocol.
An interaction between P(α) and V(u) generates a conversation (ut,c,α z) ∈G ×C× Zq. We
call such a conversation an accepting conversation for uif V’s check passes, i.e., if gαz = ut ·uc.
It is easy to see that an interaction between P and V always generates an accepting conversation,
since if ut = gαt and αz = αt + αc, then
gαz = gαt+αc = gαt ·(gα)c = ut ·uc.
Therefore, Schnorr’s protocol satisﬁes the basic correctness requirement that any identiﬁcation
protocol must satisfy.
The set C is called the challenge space . To prove security, we require that |C| is super-
poly. Indeed, we could simply take Cto be Zq, but it is technically convenient to allow somewhat
smaller challenge spaces as well. Although we will eventually prove that Schnorr’s protocol is secure
against eavesdropping attacks (under the DL assumption), we begin with a simpler theorem, which
proves security only against direct attacks (Attack Game 18.1). In proving this, we will show
756
that any eﬃcient adversary that can succeed in a direct impersonation attack with non-negligible
probability can be turned into an algorithm that eﬃciently recovers the secret key α from the
veriﬁcation key u. For this reason, Schnorr’s protocol is sometimes called a “proof of knowledge”
of a discrete logarithm.
Theorem 19.1. Under the DL assumption for G, and assuming N := |C|is super-poly, Schnorr’s
identiﬁcation protocol is secure against direct attacks.
In particular, suppose Ais an eﬃcient impersonation adversary attacking Isch via a direct attack
as in Attack Game 18.1, with advantage ϵ:= ID1adv[A,Isch]. Then there exists an eﬃcient DL
adversary B(whose running time is about twice that of A), with advantage ϵ′:= DLadv[B,G],
such that
ϵ′≥ϵ2 −ϵ/N, (19.1)
which implies
ϵ≤ 1
N +
√
ϵ′. (19.2)
Proof idea. Suppose Ahas advantage ϵin attacking Isch as in Attack Game 18.1. In this game, the
challenger generates the veriﬁcation key u = gα. In his impersonation attempt, the adversary A
generates the ﬁrst ﬂowut of the protocol using some arbitrary adversarial strategy. Now, to succeed,
Amust reply to a random challenge c ∈C with a valid response αz that satisﬁes gαz = ut ·uc.
Intuitively, if Acan generate a valid response to one such random challenge with probability ϵ, it
should be able to generate a valid response to two random challenges with probability ϵ2. Making
this intuition rigorous requires a somewhat technical argument that will be presented in a lemma
below.
So here is how we can use Ato compute the discrete logarithm of a random u∈G. We use u
as the veriﬁcation key in Isch, and let Agenerate the ﬁrst ﬂow ut of the protocol. We then supply
a random challenge c to Aand hope that Agenerates a valid response αz. If this happens, we
“rewind” A’s internal state back to the point just after which it generated ut, and then supply A
with another random challenge c′, and hope that Agenerates another valid response α′
z.
If all of this happens, then we obtain two accepting conversations (ut,c,α z) and (ut,c′,α′
z) for a
given veriﬁcation key uand with matching ﬁrst ﬂows ut. Moreover, with overwhelming probability,
we have c′̸= c(this is where the assumption that Cis super-poly comes in). Given this information,
we can easily compute Dloggu. Indeed, since both conversations are accepting, we have the two
equations:
gαz = ut ·uc and gα′
z = ut ·uc′
.
Dividing the ﬁrst equation by the second, the ut’s cancel, and we have
g∆α = u∆c, where ∆α:= αz −α′
z, ∆ c:= c−c′. (19.3)
Since ∆c ̸= 0, and the group order q is prime, the inverse 1 /∆c exists in Zq. We can now raise
both sides of (19.3) to the power 1 /∆c, obtaining
g∆α/∆c = u.
Therefore, we can eﬃciently compute Dloggu as ∆α/∆c.
The reader should observe that the technique presented here for computing the discrete log
from two accepting conversations is essentially the same idea as was used in Fact 10.3. Indeed,
757
using the terminology introduced in Section 10.6.1, we see that ( αz,−c) and ( α′
z,−c′) are distinct
representations (relative to gand u) of ut, and Fact 10.3 tells us how to compute Dloggufrom these
two representations. 2
This theorem is qualitatively diﬀerent than all of the other security theorems we have presented
so far in this text. Indeed, in the proof of this theorem, while we show that every adversary A
that breaks Isch can be converted into an adversary Bthat breaks the discrete logarithm problem,
the adversary Bthat we construct is not an elementary wrapper around A. Adversary Bhas to
basically run Atwice. In addition, this theorem is quantitatively diﬀerent as well, in that the
security reduction is very loose: if Asucceeds with probability ϵ, then Bis only guaranteed to
succeed with probability ≈ϵ2.
To make the above proof idea rigorous, we need the following technical lemma:
Lemma 19.2 (Rewinding Lemma). Let S and T be ﬁnite, non-empty sets, and let f : S×T →
{0,1}be a function. Let X, Y, and Y′ be mutually independent random variables, where X takes
values in the set S, and Y and Y′ are each uniformly distributed over T. Let ϵ := Pr[f(X,Y) = 1]
and N := |T|. Then
Pr[f(X,Y) = 1 ∧f(X,Y′) = 1 ∧Y ̸= Y′] ≥ϵ2 −ϵ/N.
Proof. For each s∈S, let g(s) := Pr[f(s,Y) = 1]. First, observe that E[g(X)] = ϵ; indeed, we have
E[g(X)] =
∑
s∈S
g(s) Pr[X = s] =
∑
s∈S
Pr[f(s,Y) = 1] Pr[X = s]
=
∑
s∈S
Pr[f(s,Y) = 1 ∧X = s] (by independence)
=
∑
s∈S
Pr[f(X,Y) = 1 ∧X = s]
= Pr[f(X,Y) = 1] (by total probability)
= ϵ.
Second, consider a ﬁxed s∈S, and let Gs be the event that f(s,Y) = 1 ∧f(s,Y′) = 1 ∧Y ̸= Y′.
We claim that
Pr[Gs] = g(s)2 −g(s)/N.
To see this, let Ns be the number of t∈T satisfying f(s,t) = 1. Then there are Ns ways to choose
Y satisfying f(s,Y) = 1, and for each choice of Y, there are at least Ns −1 ways to choose Y′
satisfying f(s,Y′) = 1 ∧Y ̸= Y′(there are exactly Ns−1 ways unless Ns = 0). Since g(s) = Ns/N,
we therefore have
Pr[Gs] ≥Ns(Ns −1)/N2 = N2
s/N2 −Ns/N2 = g(s)2 −g(s)/N.
758
Finally, let Gbe the event that f(X,Y) = 1 ∧f(X,Y′) = 1 ∧Y ̸= Y′. We have
Pr[G] =
∑
s∈S
Pr[G∧X = s] (by total probability)
=
∑
s∈S
Pr[f(s,Y) = 1 ∧f(s,Y′) = 1 ∧Y ̸= Y′∧X = s]
=
∑
s∈S
Pr[f(s,Y) = 1 ∧f(s,Y′) = 1 ∧Y ̸= Y′] Pr[X = s] (by independence)
=
∑
s∈S
Pr[Gs] Pr[X = s] ≥
∑
s∈S
(g(s)2 −g(s)/N) Pr[X = s] = E[g(X)2] −E[g(X)]/N
≥E[g(X)]2 −E[g(X)]/N = ϵ2 −ϵ/N.
Here, we have used the general fact that E[Z2] ≥E[Z]2 for any random variable Z (in particular,
for Z := g(X)). This is a special case of Jensen’s inequality. 2
Proof of Theorem 19.1. Using the impersonation adversary A, which has advantage ϵ, we build a
DL adversary B, with advantage ϵ′, as follows. Adversary Bis given an instance u = gα of the
DL problem from its challenger, and our goal is to make Bcompute α, with help from A. The
computation of Bconsists of two stages.
In the ﬁrst stage of its computation, Bplays the role of challenger to A, giving Athe value u
as the veriﬁcation key. The goal of Bin this step is to compute two accepting conversations for u
with diﬀerent challenges, that is,
(ut,c,α z) and ( ut,c′,α′
z),
where
gαz = ut ·uc, gα′
z = ut ·uc′
, and c̸= c′.
Here is how Bdoes this:
1. A(playing the role of prover) sends ut to B(playing the role of veriﬁer);
2. Bsends a random c∈C to A;
3. Asends αz to B;
4. B“rewinds” A, so that A’s internal state is exactly the same as it was at the end of step 1;
then Bsends a random c′∈C to A;
5. Asends α′
z to B.
Now we apply the Rewinding Lemma. In that lemma, the random variable Y corresponds to the
challenge c, Y′ corresponds to the challenge c′, and X corresponds to all the other random choices
made by A, B, and B’s challenger (including the group G, and group elements g,u,u t ∈G). The
function f in the lemma is deﬁned to be 1 if the resulting conversation is an accepting conversation
for u, and 0 otherwise. So f(X,Y) = 1 if (ut,c,α z) is an accepting conversation foru, and f(X,Y′) = 1
if (ut,c′,α′
z) is an accepting conversation for u. Applying the lemma, we ﬁnd that the probability
that Bgets two accepting conversations with diﬀerent challenges is at least ϵ2 −ϵ/N.
759
So now assume that B has successfully computed two such conversations ( ut,c,α z) and
(ut,c′,α′
z). In the second stage of its computation, Buses these two conversations to compute
α. Indeed, as already discussed in the “proof idea” above, we can compute α = ∆α/∆c, where
∆α:= αz −α′
z, ∆c:= c−c′.
This shows (19.1). We now argue that (19.2) follows from (19.1). To do so, we may assume
that ϵ≥1/N, as otherwise, (19.2) clearly holds. So we have
(ϵ−1/N)2 = ϵ2 −2ϵ/N+ 1/N2 ≤ϵ2 −2ϵ/N+ ϵ/N (since ϵ≥1/N)
= ϵ2 −ϵ/N ≤ϵ′ (by (19.1)),
from which (19.2) is clear. 2
To recap, we proved security against direct attacks by showing how to eﬃciently extract the
secret key α from a malicious prover A. This enabled us to use the malicious prover to solve
the discrete-log problem in G. Our “extractor” works by rewinding the prover to obtain two
conversations (ut,c,α z) and (ut,c′,α′
z) where c̸= c′. Rewinding the prover Ais possible inside the
proof of security, because we have full control of A’s execution environment. In the real world,
since one cannot rewind an honest prover P, an attacker cannot use this strategy to extract the
secret key from P.
19.1.1 Honest veriﬁer zero knowledge and security against eavesdropping
We have shown that Schnorr’s identiﬁcation protocol is secure against direct attacks, under the DL
assumption. In fact, under the same assumption, we can show that Schnorr’s identiﬁcation protocol
is secure against eavesdropping attacks as well. Now, in an eavesdropping attack, the adversary
obtains vk and a list of transcripts — conversations between P (on input sk) and V (on input
vk). The idea is to show that these conversations do not help the adversary, because the adversary
could have eﬃciently generated these conversations by himself, given vk (but not sk). If we can
show this, then we are done. Indeed, suppose Ais an adversary whose advantage in carrying out
a successful impersonation via an eavesdropping attack is non-negligible. Then we replace Aby
another adversary B, that works the same as A, except that Bgenerates the transcripts by himself,
instead of obtaining them from his challenger. Thus, Bcarries out a direct attack, but has the
same advantage as Ain carrying out a successful impersonation.
We shall develop this idea in a more general way, introducing the notion of honest veriﬁer
zero knowledge.
Deﬁnition 19.1. Let I= (G,P,V ) be an identiﬁcation protocol. We say that Iis honest veriﬁer
zero knowledge, or HVZK for short, if there exists an eﬃcient probabilistic algorithm Sim (called
a simulator) such that for all possible outputs (vk,sk) of G, the output distribution of Sim on input
vk is identical to the distribution of a transcript of a conversation between P (on input sk) and V
(on input vk).
Some comments on the terminology are in order. The term “zero knowledge” is meant to suggest
that an adversary learns nothing from P, because an adversary can simulate conversations on his
own (using the algorithm Sim), without knowing sk. The term “honest veriﬁer” conveys the fact
this simulation only works for conversations between P and the actual, “honest” veriﬁer V, and
not some arbitrary, “dishonest” veriﬁer, such as may arise in an active attack on the identiﬁcation
760
Direct Challenger
Direct adversary B
Eavesdropping
Adversary A
(sk,vk) ←
R
G vk
Sim(vk)
Sim(vk)
impersonation attemptV(vk)
accept or reject
Figure 19.2: Adversary Bin the proof of Theorem 19.3.
protocol. The notion of zero knowledge (including honest veriﬁer zero knowledge, and many other
variants) arises in many other types of protocols besides identiﬁcation protocols.
Theorem 19.3. If an identiﬁcation protocol Iis secure against direct attacks, and is HVZK, then
it is secure against eavesdropping attacks.
In particular, if Iis HVZK with simulator Sim, then for every impersonation adversary Athat
attacks Ivia an eavesdropping attack, as in Attack Game 18.2, obtaining up to Q transcripts,
there is an adversary Bthat attacks Ivia a direct attack, as in Attack Game 18.1, where Bis
an elementary wrapper around A(and where Bruns Sim at most Q times), such that
ID2adv[A,I] = ID1adv[B,I].
Proof. Bworks the same as A, except that instead of obtaining transcripts from its challenger, it
generates the transcripts itself using Sim. Adversary Bis shown in Fig. 19.2. 2
Let us now return to Schnorr’s identiﬁcation protocol.
Theorem 19.4. Schnorr’s identiﬁcation protocol is HVZK.
Proof. The idea is that in generating a simulated conversation (ut,c,α z), we do not need to generate
the messages of the conversation in the given order, as in a real conversation between P and V.
Indeed, our simulator Sim generates the messages in reverse order. On input vk = u, the simulator
Sim computes
αz ←R Zq, c ←R C, ut ←gαz/uc,
and outputs the conversation ( ut,c,α z).
Now we argue that the output of Sim on input vk = u has the right distribution. The key
observation is that in a real interaction, c and αz are independent, with c uniformly distributed
over Cand αz uniformly distributed over Zq; moreover, given c and αz, the value ut is uniquely
761
determined by the equation gαz = ut ·uc. It should be clear that this is the same as the output
distribution of the simulator. 2
As a corollary, we immediately obtain:
Theorem 19.5. If Schnorr’s identiﬁcation protocol is secure against direct attacks, then it is also
secure against eavesdropping attacks.
In particular, for every impersonation adversary Athat attacks Isch via an eavesdropping attack,
as in Attack Game 18.2, there is an adversary Bthat attacks Isch via a direct attack, as in Attack
Game 18.1, where Bis an elementary wrapper around A, such that
ID2adv[A,Isch] = ID1adv[B,Isch].
At ﬁrst blush, our results about Schnorr’s protocol may seem counter-intuitive, or perhaps even
contradictory. Namely, how can it be hard to carry out an impersonation attack, knowing only
vk, and yet be easy to generate a conversation, also knowing only vk? The answer is that in
carrying out an impersonation attack, the veriﬁer V is actively involved in the conversation, and
the timing and ordering of the messages is critical: the adversary (playing the role of a prover)
must generate the ﬁrst message ut before it sees the challenge c generated by V. However, the
simulator is free to generate the messages in any convenient order: our simulator in the proof
of Theorem 19.4 generates c and αz, and then computes ut. Indeed, what these results do show
is that Schnorr’s identiﬁcation protocol would be completely insecure if the challenge space were
small: in its impersonation attempt, an adversary could use the simulator to prepare an accepting
conversation (ut,c,α z), send ut to V, and then hope that the challenge chosen by V is equal to its
prepared challenge c, and if so, the adversary could then respond with αz, and so make V accept.
Thus, it is trivial to break Schnorr’s identiﬁcation protocol with advantage 1 /|C|; therefore, the
challenge space |C|must be super-poly in order to ensure security.
It is an open question as to whether Schnorr’s identiﬁcation protocol is secure against active
attacks as in Attack Game 18.3: there are no known eﬀective, active attacks, but there is also no
proof that rules out such an attack under the DL assumption. Later in this chapter, we shall present
a slight variation on Schnorr’s identiﬁcation that can be proven secure against active attacks under
the DL assumption.
19.2 From identiﬁcation protocols to signatures
In this section, we show how to convert Schnorr’s identiﬁcation protocol into a signature scheme.
The signature scheme is secure in the random oracle model under the DL assumption. Later in
this chapter, we will see that this construction is actually a speciﬁc instance of a more general
construction.
We start with Schnorr’s identiﬁcation protocol Isch, which is deﬁned in terms of a cyclic group
G of prime order q with generator g ∈G, along with a challenge space C⊆ Zq. We also need a
hash function H : M× G →C, which will be modeled as a random oracle in the security proof.
Here, Mwill be the message space of the signature scheme.
The basic idea of the construction is that a signature on a messagem∈M will be a pair (ut,αz),
where (ut,c,α z) is an accepting conversation for the veriﬁcation key u in Schnorr’s identiﬁcation
protocol, and the challenge c is computed as c ←H(m,ut). Intuitively, the hash function H is
playing the role of veriﬁer in Schnorr’s identiﬁcation protocol.
762
In detail, the Schnorr signature scheme is Ssch = (G,S,V ), where:
• The key generation algorithm G runs as follows:
α←R Zq, u ←gα.
The public key is pk := u, and the secret key is sk := α.
• To sign a message m∈M using a secret key sk = α, the signing algorithm runs as follows:
S( sk, m) := αt ←R Zq, u t ←gαt, c ←H(m,ut), α z ←αt + αc
output σ:= (ut,αz).
• To verify a signature σ = (ut,αz) on a message m ∈M, using the public key pk = u, the
signature veriﬁcation algorithm V computes c←H(m,ut), and outputs accept if gαz = ut ·uc,
and outputs reject, otherwise.
Although we described the signing algorithm as a randomized algorithm, this is not essential.
Exercise 13.6 shows how to derandomize the signing algorithm. This derandomization is important
in practice, to avoid bad randomness attacks, as in Exercise 19.1.
We will show that if we model H as a random oracle, then Schnorr’s signature scheme is
secure if Schnorr’s identiﬁcation protocol is secure against eavesdropping attacks, which was already
established in Theorem 19.5. It is advantageous, however, to ﬁrst consider a slightly enhanced
version of the eavesdropping attack game.
19.2.1 A useful abstraction: repeated impersonation attacks
We shall consider a slightly enhanced type of impersonation attack against an identiﬁcation scheme,
in which we allow the adversary to make many impersonation attempts (against several instances of
the veriﬁer, running concurrently, and using the same veriﬁcation key). One could deﬁne this notion
for either direct, eavesdropping, or active attacks, but we shall just consider eavesdropping attacks
here, as that is all we need for our application. Also, we only consider identiﬁcation protocols that
are stateless and have a public veriﬁcation key.
Here is the attack game in more detail.
Attack Game 19.1 ( r-impersonation eavesdropping attack). For a given identiﬁcation pro-
tocol I= (G,P,V ), positive integer r, and adversary A, the attack game runs as follows. The key
generation and eavesdropping phase is exactly the same as in Attack Game 18.2.
The only diﬀerence is that in the impersonation phase, the adversary Ais allowed to interact
concurrently with up to r veriﬁers. The challenger, of course, plays the role of these veriﬁers, all of
which use the same veriﬁcation key as generated during the key generation phase. The adversary
wins the game if it makes any of these veriﬁers output accept.
We deﬁne A’s advantage with respect to Iand r, denoted rID2 adv[A,I,r], as the probability
that Awins the game. 2
The following lemma shows that the r-impersonation eavesdropping attack is equivalent to the
ordinary eavesdropping attack. That is, winning Attack Game 19.1 is not much easier than winning
Attack Game 18.2.
763
Lemma 19.6. Let Ibe an identiﬁcation protocol. For every r-impersonation eavesdropping ad-
versary A, there exists a standard eavesdropping adversary B, where Bis an elementary wrapper
around A, such that
rID2adv[A,I,r] ≤r·ID2adv[B,I]. (19.4)
Proof sketch. This is a simple “guessing argument”. Adversary Bsimply chooses ω∈{1,...,r }at
random, and then plays the role of challenger toA. It starts out by obtaining from its own challenger
the veriﬁcation key as well as the transcripts of several conversations, and passes these along to
A. During the impersonation phase, for the jth instance of the veriﬁer, if j ̸= ω, our adversary B
plays the role of veriﬁer itself; otherwise, for j = ω, it acts as a simple conduit between Aand its
own challenger in Attack Game 18.2. It should be clear that Amakes one of the veriﬁers accept
when playing against Bwith the same probability that it does in Attack Game 19.1. Moreover, B
wins its attack game if it guesses the index of one of these accepting veriﬁers, which happens with
probability at least 1/r. 2
19.2.2 Security analysis of Schnorr signatures
We now show that Schnorr’s signature scheme is secure in the random oracle model, provided
Schnorr’s identiﬁcation scheme is secure against eavesdropping attacks.
Theorem 19.7. If H is modeled as a random oracle and Schnorr’s identiﬁcation scheme is secure
against eavesdropping attacks, then Schnorr’s signature scheme is also secure.
In particular, let Abe an adversary attacking Ssch as in the random oracle version of Attack
Game 13.1. Moreover, assume that Aissues at most Qs signing queries and Qro random oracle
queries. Then there exists a (Qro + 1)-impersonation adversary B that attacks Isch via an
eavesdropping attack as in Attack Game 19.1, where Bis an elementary wrapper around A,
such that
SIGroadv[A,Ssch] ≤Qs(Qs + Qro + 1)/q+ rID2adv[B,Isch,Qro + 1]. (19.5)
Proof idea. The goal is to convert an adversary Athat forges a signature into an adversary Bthat
breaks the security of Schnorr’s identiﬁcation scheme in an r-impersonation eavesdropping attack,
where r:= Qro + 1.
The ﬁrst idea is that we have to somehow answer A’s signing queries without using the secret
key. This is done by using the transcripts from eavesdropped conversations to build the required
signatures, “ﬁxing up” the random oracle representing H to be consistent with these signatures.
This “ﬁxing up” will fail only if the random oracle needs to be queried at a point at which it has
already been queried. But since the input to the random oracle includes a random group element,
this is unlikely to happen. This is where the term Qs(Qs + Qro + 1)/q in (19.5) arises.
Once we have gotten rid of the signing queries, we argue that if the adversary successfully forges
a signature, he can be eﬀectively used in an r-impersonation attack on Isch. Again, we exploit the
fact that H is modeled as a random oracle. Since a signature forgery must be on a message not
submitted as a signing query, the corresponding random oracle query must be at a point distinct
from all those made by any of the signing queries, and so the value of the random oracle at that
point essentially acts as a random challenge in a run of the identiﬁcation protocol. We do not know
in advance which random oracle query will correspond to the forgery, which is why we have to use
the r-impersonation attack game. 2
764
Proof. To simplify the analysis, we shall assume that when Aoutputs a forgery pair ( m,σ), where
σ= (ut,αz), then Amust have already explicitly queried the random oracle at the point (m,ut). If
necessary, we modify Ato ensure that this is the case, so that the total number of random oracle
queries made by the modiﬁed version of Ais at most Qro + 1.
We deﬁne two attack games. Game 0 is essentially the original signature attack game, with H
modeled as a random oracle. Game 1 is a slight modiﬁcation. For j = 0,1, Wj is the event that A
wins in Game j.
Game 0. The challenger works as in the random oracle version of Attack Game 13.1. As usual, we
implement the random oracle using an associative array Map : M×G →C. We also maintain an
associative array Explicit : M×G →Z that keeps track of those points at which the random oracle
was ﬁrst queried explicitly by the adversary, rather than (implicitly) by the signing algorithm. The
logic of the challenger is shown in Fig. 19.3.
To process a signing query mi, the challenger runs the signing algorithm as usual: ﬁrst it
generates a random αti ∈Zq and computes uti ←gαti; it then generates a random “default” value
ci ∈C for the value of Map[mi,uti]; if the test in the line marked (1) detects that Map[mi,uti] was
already deﬁned, then that previously deﬁned value is used, instead of the default value.
To process a random oracle query (ˆmj,ˆuj), if the valueMap[ ˆmj,ˆuj] has not already been deﬁned,
by either a previous signing or random oracle query, then it is deﬁned here, and in addition, we set
Explicit[ ˆmj,ˆuj] ←j.
Suppose that the adversary submits ( m,ut,αz) as its forgery attempt, and that m is diﬀerent
from all the mi’s submitted as signing queries. By our simplifying assumption, the adversary must
have previously submitted (m,ut) as a random oracle query, and it must be the case that ( m,ut) is
in Domain(Explicit) at that point. It follows that if ( ut,αz) is a valid signature, then the challenger
will output “win” and therefore
SIGroadv[A,Ssch] ≤Pr[W0].
Game 1. This is the same as Game 0, except that the line marked (1) in Fig. 19.3 is deleted. By
a straightforward application of the Diﬀerence Lemma, we obtain
|Pr[W1] −Pr[W0]|≤ Qs(Qs + Qro + 1)/q.
Indeed, for the ith signing query, uti is uniformly distributed over G, the union bound implies that
the probability that the random oracle was previously queried at the point ( m,uti) (either directly
by the adversary, or indirectly via a previous signing query) is at most ( Qs + Qro + 1)/q. Another
application of the union bound gives the overall bound Qs(Qs + Qro + 1)/q on the probability that
this occurs for any signing query.
The point of making this change is that now in Game 1, a fresh random challenge is used to
process each signing query, just as an honest veriﬁer in Schnorr’s identiﬁcation protocol.
At this point, it is easy to construct an adversary Bthat plays the r-impersonation eavesdrop-
ping attack game with r= Qro + 1 against a challenger, and itself plays the role of challenger to A
in Game 1, so that
Pr[W1] = rID2adv[B,Isch,r].
The detailed logic of Bis shown in Fig. 19.4. Here, for j = 1,...,r , we denote by Vj the jth veriﬁer
in the r-impersonation attack game. The theorem now follows immediately. 2
765
initialization:
α←R Zq, u←gα
initialize empty associative arrays Map : M×G →C and
Explicit : M×G →Z
send the public key u to A;
upon receiving the ith signing query mi ∈M:
αti ←R Zq, uti ←gαti, ci ←R C
(1) if ( mi,uti) ∈Domain(Map) then ci ←Map[mi,uti]
Map[mi,uti] ←ci
αzi ←αti + αci
send (uti,αzi) to A;
upon receiving the jth random oracle query ( ˆmj,ˆuj) ∈M× G:
if ( ˆmj,ˆuj) /∈Domain(Map) then
Map[ ˆmj,ˆuj] ←R C, Explicit[ ˆmj,ˆuj] ←j
send Map[ ˆmj,ˆuj] to A;
upon receiving a forgery attempt ( m,ut,αz):
/ / By assumption, (m,ut) ∈Domain(Explicit)
if gαz = ut ·uc where c= Map[m,ut]
then output “win”
else output “lose”
Figure 19.3: Game 0 challenger
766
initialization:
obtain the veriﬁcation key u from challenger
obtain eavesdropped conversations (uti,ci,αzi) for i= 1,...,Q s from challenger
initialize empty associative arrays Map : M×G →C and
Explicit : M×G →Z
send u to A;
upon receiving the ith signing query mi ∈M from A:
Map[mi,uti] ←ci
send (uti,αzi) to A;
upon receiving the jth random oracle query ( ˆmj,ˆuj) ∈M× G:
if ( ˆmj,ˆuj) /∈Domain(Map) then
initiate an impersonation attempt with veriﬁer Vj:
send ˆuj to Vj, who responds with a challenge ˆcj
Map[ ˆmj,ˆuj] ←ˆcj, Explicit[ ˆmj,ˆuj] ←j
send Map[ ˆmj,ˆuj] to A;
upon receiving a forgery attempt ( m,ut,αz):
/ / By assumption, (m,ut) ∈Domain(Explicit)
send the ﬁnal message αz to Vj, where j = Explicit[m,ut]
Figure 19.4: Adversary B
767
Putting it all together. If we string together the results of Theorem 19.7, Lemma 19.6, and
Theorems 19.5 and 19.1, we get the following reduction from attacking the Schnorr signature scheme
to computing discrete-log:
Let A be an eﬃcient adversary attacking Ssch as in the random oracle version of Attack
Game 13.1. Moreover, assume that Aissues at most Qs signing queries and Qro random oracle
queries. Then there exists an eﬃcient DL adversary B(whose running time is about twice that
of A), such that
SIGroadv[A,Ssch] ≤Qs(Qs + Qro + 1)
q + Qro + 1
N + (Qro + 1)
√
DLadv[B,G], (19.6)
where N is the size of the challenge space.
This reduction is very loose. The scalar ( Qro + 1) multiplying the term
√
DLadv[B,G] is the
most problematic. It turns out that we can get a somewhat tighter reduction, essentially replacing
(Qro + 1) by
√
(Qro + 1), which is much better. The trick is to combine the “guessing step” made
in Lemma 19.6 and the “rewinding step” made in Theorem 19.1 into a single, direct reduction.
Lemma 19.8. Consider Schnorr’s identiﬁcation protocol Isch, deﬁned with respect to a group G of
prime order q generated by g ∈G, and with a challenge space Cof size N. For every eﬃcient r-
impersonation eavesdropping adversary Aattacking Isch, with advantage ϵ:= rID2adv[A,I,r], there
exists an eﬃcient DL adversary B(whose running time is about twice that of A), with advantage
ϵ′:= DLadv[B,G], such that
ϵ′≥ϵ2/r−ϵ/N, (19.7)
which implies
ϵ≤ r
N +
√
rϵ′. (19.8)
Proof. Let us begin by reviewing how A’s attack game works. First, the challenger in Attack
Game 19.1 gives to Aa veriﬁcation key u ∈G for Schnorr’s identiﬁcation protocol. Second, the
challenger gives to Aseveral transcripts of conversations. Third, Aenters the impersonation phase,
where it attempts to make at least one of r veriﬁers accept. In more detail, this works as follows.
For j running from 1 to at most r, Asends utj to the challenger, who responds with a random
challenge cj ∈C. After receiving all of these challenges, Aeither outputs fail or a pair ( i,αz) such
that (uti,ci,αz) is an accepting conversation for the veriﬁcation key u. In the latter case, we say A
succeeds at veriﬁer i. Observe that A’s advantage is ϵ= ∑r
j=1 ϵj, where ϵj is the probability that
Asucceeds at veriﬁer j.
Note that we have assumed a somewhat simpliﬁed behavior for the adversary in the imperson-
ation phase. However, since the adversary can see for himself whether a conversation is accepting
or not, this is not really a restriction: any adversary can be put in the form described without
changing its advantage at all, and without increasing its running time signiﬁcantly. (Also, the
r-impersonation adversary constructed in the proof of Theorem 19.7 is already essentially of this
form.)
We now describe our DL adversary B, which is given u∈G, and is tasked to compute Dloggu.
As usual, Bplays the role of challenger to A. First, Bgives uto Aas the veriﬁcation key. Second,
Bgenerates transcripts of conversations, using the simulator from Theorem 19.4, and gives these
to A. Third, Blets Arun through the impersonation phase to completion, supplying random
challenges c1,...,c r. If Aoutputs a pair ( i,αz) such that ( uti,ci,αz) is an accepting conversation
768
for the veriﬁcation key u, then Brewinds Aback to the point where it submitted uti to the ith
veriﬁer. Instead of the challenge ci, our adversary Bresponds with a fresh, random challenge c′∈C.
It then lets Arun through the remainder of the impersonation phase, using the same challenge
cj for j = i+ 1,...,r . If Aoutputs a pair ( i′,α′
z) such that i′ = i, ( uti,c′,α′
z) is an accepting
conversation, and c′̸= ci, then Buses these two accepting conversations to compute Dloggu, just
as we did in the proof of Theorem 19.1. In this case, we say Bsucceeds at veriﬁer i. Observe that
B’s advantage is ϵ′= ∑r
j=1 ϵ′
j, where ϵ′
j is the probability that Bsucceeds at veriﬁer j.
It remains to prove (19.7) — note that (19.8) follows from (19.7) using a calculation almost
identical to that used in the proof of Theorem 19.1.
We claim that for j = 1,...,r , we have
ϵ′
j ≥ϵ2
j −ϵj/N. (19.9)
Indeed, for a ﬁxed index j, this inequality follows from an application of the rewinding lemma
(Lemma 19.2), where Y corresponds to the challenge cj, Y′ corresponds to the challenge c′, and X
corresponds to all the other random choices made by A, B, and B’s challenger. The function f in
the lemma is deﬁned to be 1 if Asucceeds at veriﬁer j. So f(X,Y) = 1 if i= j and (utj,cj,αz) is an
accepting conversation; similarly, f(X,Y′) = 1 if i′= j and (utj,c′,α′
z) is an accepting conversation.
From (19.9), we obtain
ϵ′=
r∑
j=1
ϵ′
j ≥
r∑
j=1
ϵ2
j −
r∑
j=1
ϵj/N ≥ϵ2/r−ϵ/N,
where for the last inequality, we used the fact that for any function g: {1,...,r }→ R, we have
r∑
j=1
g(j)2 ≥
( r∑
j=1
g(j)
)2
/r.
This follows, for example, from the fact that E[Z2] ≥E[Z]2 for any random variable Z, and in
particular, for Z := g(R), where R is uniformly distributed over {1,...,r }. 2
With this result, we can replace the bound (19.6) by:
SIGroadv[A,Ssch] ≤Qs(Qs + Qro + 1)
q + Qro + 1
N +
√
(Qro + 1)DLadv[B,G]. (19.10)
19.2.3 A concrete implementation and an optimization
We might take G to be the elliptic curve group P256 deﬁned over a ﬁnite ﬁeld Fp where p is a
256-bit prime (Section 15.3). It will be suﬃcient to work with 128-bit challenges. In this case each
component of the Schnorr signature ( ut,αz) is 256 bits. Overall, a Schnorr signature is about 512
bits.
Because the length of a challenge is much shorter than the encoding length of a group element,
the following “optimized” variant of Schnorr’s signature scheme can be used to obtain much shorter
signatures. Instead of deﬁning a signature on m to be a pair ( ut,αz) satisfying
gαz = ut ·uc,
769
where c:= H(m,ut), we can deﬁne it to be a pair ( c,αz) satisfying
c= H(m,ut),
where ut := gαz/uc. The transformation (ut,αz) ↦→(H(m,ut),αz) maps a regular Schnorr signature
on m to an optimized Schnorr signature, while the transformation ( c,αz) ↦→(gαz/uc,αz) maps an
optimized Schnorr signature to a regular Schnorr signature. It follows that forging an optimized
Schnorr signature is equivalent to forging a regular Schnorr signature. As a further optimization,
one can store u−1 in the public key instead of u, which will speed up veriﬁcation.
With the above choices of parameters, we reduce the length of a signature from 512 bits to
about 128 + 256 = 384 bits — a 25% reduction in size.
19.3 Case study: ECDSA signatures
In 1991, when it came time to adopt a federal standard for digital signatures, the National Institute
of Standards (NIST) considered a number of viable candidates. Because the Schnorr system was
protected by a patent, NIST opted for a more ad-hoc signature scheme based on a prime-order
subgroup of Z∗
p that eventually became known as the Digital Signature Algorithm or DSA.
The standard was later updated to support elliptic curve groups deﬁned over a ﬁnite ﬁeld. The
resulting signature scheme, called ECDSA, is used in many real-world systems. We brieﬂy describe
how ECDSA works and discuss some security issues that aﬀect it.
The ECDSA signature scheme ( G,S,V ) uses the group of points G of an elliptic curve over a
ﬁnite ﬁeld Fp. Let g be a generator of G and let q be the order of the group G, which we assume
is prime. We will use multiplicative notation for the group operation. We will also need a hash
function H deﬁned over (M,Z∗
q).
The scheme works as follows:
• G(): Choose α←R Z∗
q and set u←gα ∈G. Output sk := α and pk := u.
• S(sk,m): To sign a message m∈M with secret key sk = α do:
repeat:
αt ←R Z∗
q, u t ←gαt
let ut = (x,y) ∈G where x,y ∈Fp
treat x as an integer in [0 ,p) and set r←[x]q ∈Zq / / reduce x modulo q
s←
(
H(m) + rα
)
/αt ∈Zq
until r̸= 0 and s̸= 0
output (r,s) ∈Z2
q
• V(pk,m,σ ): To verify a signature σ= (r,s) ∈Z2
q on m∈M with pk = u∈G do:
if r= 0 or s= 0 then output reject and stop
a←H(m)/s∈Zq, b ←r/s∈Zq
ˆut ←gaub ∈G
if ˆut is the point at inﬁnity in G then output reject and stop
let ˆut = (ˆx,ˆy) ∈G where ˆx,ˆy∈Fp
treat ˆx as an integer in [0 ,p) and set ˆr←[ˆx]q ∈Zq / / reduce ˆx modulo q
if r= ˆr output accept; else output reject
770
When using the elliptic curve P256, bothpand qare 256-bit primes. An ECDSA signature σ= (r,s)
is then 512 bits long.
A straightforward calculation shows that the scheme is correct: for every key pair ( pk,sk)
output by G, and every message m ∈M, if σ ←R S(sk,m) then V(pk,m,σ ) outputs accept. The
reason is that ˆut computed by V is the same as ut computed by S.
The security of this scheme has only been established somewhat heuristically, speciﬁcally, when
we model G as a “generic group” (see Section 16.3).
For security, it is important that the random valueαt generated during signing be a fresh uniform
value in Z∗
q. Otherwise the scheme can become insecure in a strong sense: an attacker can learn
the secret signing key α. This was used in a successful attack on the Sony PlayStation 3 because
αt was the same for all issued signatures. It has also lead to attacks on some Bitcoin wallets [48].
Because generating randomness on some hardware platforms can be diﬃcult, a common solution
is to modify the signing algorithm so that α is generated deterministically using a secure PRF, as
described in Exercise 13.6. This variant is called deterministic ECDSA. The Schnorr signature
scheme suﬀers from the same issue and this modiﬁcation applies equally well to it.
ECDSA is not strongly secure. While the Schnorr signature scheme is strongly secure (see
Exercise 19.17), the ECDSA scheme is not. Given an ECDSA signature σ= (r,s) on a message m,
anyone can generate more signatures on m. For example, σ′ := (r,−s) ∈(Z∗
q)2 is another valid
signature on m. This σ′ is valid because the x-coordinate of the elliptic curve point ut ∈G is the
same as the x-coordinate of the point 1 /ut ∈G.
19.4 Sigma protocols: basic deﬁnitions
Schnorr’s identiﬁcation protocol is a special case of an incredibly useful class of protocols called
Sigma protocols . In this section, we will introduce the basic concepts associated with Sigma
protocols. Later, we will consider many examples of Sigma protocols and their applications:
• We will see how we can use Sigma protocols to build new secure identiﬁcation schemes and
signature schemes.
• We will see how to build identiﬁcation schemes that we can prove (without the random oracle
heuristic) are secure against active attacks. Recall that for Schnorr’s identiﬁcation protocol
we could only prove security against eavesdropping attacks.
• In the next chapter, we will also see how to use Sigma protocols for other applications that
have nothing to do with identiﬁcation and signatures. For example, we will see how one
can encrypt a message m and then “prove” to a skeptical veriﬁer that m satisﬁes certain
properties, without revealing to the veriﬁer anything else about m. We will illustrate this
idea with an electronic voting protocol.
Consider again Schnorr’s identiﬁcation protocol. Intuitively, that protocol allows a prover P to
convince a skeptical veriﬁer V that he knows a secret that satisﬁes some relation, without revealing
any useful information to V about the secret. For Schnorr’s protocol, the prover’s secret was α∈Zq
satisfying the relation gα = u.
We can generalize this to more general and interesting types of relations.
771
P(x,y) V(y)
generate commitment t
t−−−−−−−−−−−−−−−−→
generate challenge: c←R C
c←−−−−−−−−−−−−−−−−
generate response z
z−−−−−−−−−−−−−−−−→
output accept or reject
Figure 19.5: Execution of a Sigma protocol
Deﬁnition 19.2 (Eﬀective relation). An eﬀective relation is a binary relation R⊆X×Y ,
where X, Yand Rare eﬃciently recognizable ﬁnite sets. Elements of Yare called statements. If
(x,y) ∈R, then x is called a witness for y.
We now deﬁne the syntax of a Sigma protocol.
Deﬁnition 19.3 (Sigma protocol). Let R⊆X×Y be an eﬀective relation. A Sigma protocol
for Ris a pair (P,V ).
• P is an interactive protocol algorithm called the prover, which takes as input a witness-
statement pair (x,y) ∈R.
• V is an interactive protocol algorithm called the veriﬁer, which takes as input a statement
y∈Y, and which outputs accept or reject.
• P and V are structured so that an interaction between them always works as follows:
– To start the protocol, P computes a message t, called the commitment, and sends t to
V;
– Upon receiving P’s commitment t, V chooses a challenge c at random from a ﬁnite
challenge space C, and sends c to P;
– Upon receiving V’s challenge c, P computes a response z, and sends z to V;
– Upon receivingP’s response z, V outputs either accept or reject, which must be computed
strictly as a function of the statement y and the conversation (t,c,z ). In particular, V
does not make any random choices other than the selection of the challenge — all other
computations are completely deterministic.
We require that for all (x,y) ∈R, when P(x,y) and V(y) interact with each other, V(y) always
outputs accept.
See Fig. 19.5, which illustrates the execution of a Sigma protocol. The name Sigma protocol
comes from the fact that the “shape” of the message ﬂows in such a protocol is vaguely reminiscent
of the shape of the Greek letter Σ.
As stated in the deﬁnition, we require that the veriﬁer computes its output as a function of
the statement y and its conversation ( t,c,z ) with the prover. If the output is accept we call the
772
conversation (t,c,z ) an accepting conversation fory. Of course, interactions between the veriﬁer
and an honest prover only produce accepting conversations; non-accepting conversation can arise,
for example, if the veriﬁer interacts with a “dishonest” prover that is not following the protocol.
In most applications of Sigma protocols, we will require that the size of the challenge space is
super-poly. To state this requirement more succinctly, we will simply say that the protocol has a
large challenge space.
Example 19.1. It should be clear that for Schnorr’s identiﬁcation protocol ( G,P,V ), the pair
(P,V ) is an example of a Sigma protocol for the relation R⊆X×Y , where
X= Zq, Y= G, and R= {(α,u) ∈Zq ×G : gα = u }.
The challenge space Cis a subset of Zq. We call ( P,V ) Schnorr’s Sigma protocol.
The reader should observe that unlike an identiﬁcation protocol, a Sigma protocol itself does
not specify an algorithm for generating elements of R.
Note also that the relation Rin this case is parameterized by a description of the group G
(which includes its order q and the generator g ∈G). In general, we allow eﬀective relations that
are deﬁned in terms of such “system parameters,” which are assumed to be generated at system
setup time, and publicly known to all parties.
A statement for Schnorr’s Sigma protocol is a group element u ∈G, and a witness for u is
α∈Zq such that gα = u. Thus, every statement has a unique witness. An accepting conversation
for u is a triple of the form ( ut,c,α z), with ut ∈G, c∈C, and αz ∈Zq, that satisﬁes the equation
gαz = ut ·uc.
The reader may have noticed that, as we have deﬁned it, the prover P from Schnorr’s identiﬁ-
cation protocol takes as input just the witness α, rather than the witness/statement pair ( α,u), as
formally required in our deﬁnition of a Sigma protocol. In fact, in this and many other examples
of Sigma protocols, the prover does not actually use the statement explicitly in its computation. 2
19.4.1 Special soundness
We next deﬁne a critical security property for Sigma protocols, which is called special soundness.
Deﬁnition 19.4 (Special soundness). Let (P,V ) be a Sigma protocol for R⊆X×Y . We say
that (P,V ) provides special soundness if there is an eﬃcient deterministic algorithm Ext, called
a witness extractor , with the following property: whenever Ext is given as input a statement
y ∈Y, and two accepting conversations (t,c,z ) and (t,c′,z′), with c ̸= c′, algorithm Ext always
outputs x∈X such that (x,y) ∈R (i.e., x is a witness for y).
Example 19.2. Continuing with Example 19.1, we can easily verify that Schnorr’s Sigma protocol
provides special soundness. The witness extractor takes as input the statement u∈G, along with
two accepting conversations ( ut,c,α z) and ( ut,c′,α′
z) for u, with c ̸= c′. Just as we did in the
proof of Theorem 19.1, we can compute the corresponding witness α = Dloggu from these two
conversations as ∆α/∆c∈Zq, where ∆α:= αz −α′
z and ∆c:= c−c′. 2
Suppose (P,V ) is a Sigma protocol for R⊆X×Y . Moreover, suppose ( P,V ) provides special
soundness and has a large challenge space. Then in a certain sense, ( P,V ) acts as a “proof of
knowledge.” Indeed, consider an arbitrary prover P∗ (even a potentially “cheating” one) that
773
makes V accept a statement y with non-negligible probability. Then P∗ must “know” a witness
for y, in the following sense: just as in the proof of Theorem 19.1, we can rewind P∗ to get two
accepting conversations (t,c,z ) and (t,c′,z′) for y, with c̸= c′, and then use the witness extractor
to compute the witness x.
More generally, when a cryptographer says that P∗ must “know” a witness for a statement y,
what she means is that the witness can be extracted from P∗ using rewinding. Although we will
not formally deﬁne the notion of a “proof of knowledge,” we will apply special soundness in several
applications.
19.4.2 Special honest veriﬁer zero knowledge
We introduced the notion of honest veriﬁer zero knowledge (HVZK) in Section 19.1.1 for identiﬁ-
cation protocols. We can easily adapt this notion to the context of Sigma protocols.
Let (P,V ) be a Sigma protocol for R⊆X×Y . Intuitively, what we want to say is that for
(x,y) ∈R, a conversation between P(x,y) and V(y) should not reveal anything about the witness
x. Just as in Section 19.1.1, we will formalize this intuition by saying that we can eﬃciently simulate
conversations between P(x,y) and V(y) without knowing the witness x. However, we will add a
few extra requirements, which will streamline some constructions and applications.
Deﬁnition 19.5 (Special HVZK). Let (P,V ) be a Sigma protocol for R⊆X×Y with challenge
space C. We say that (P,V ) is special honest veriﬁer zero knowledge , or special HVZK,
if there exists an eﬃcient probabilistic algorithm Sim (called a simulator) that takes as input
(y,c) ∈Y×C , and satisﬁes the following properties:
(i) for all inputs (y,c) ∈Y×C , algorithm Sim always outputs a pair (t,z) such that (t,c,z ) is
an accepting conversation for y;
(ii) for all (x,y) ∈R, if we compute
c←RC, (t,z) ←RSim(y,c),
then (t,c,z ) has the same distribution as that of a transcript of a conversation between P(x,y)
and V(y).
The reader should take note of a couple of features of this deﬁnition. First, the simulator
takes the challenge c as an additional input. Second, it is required that the simulator produce an
accepting conversation even when the statement y does not have a witness. These two properties
are the reason for the word “special” in “special HVZK.”
Example 19.3. Continuing with Example 19.2, we can easily verify that Schnorr’s Sigma protocol
is special HVZK. Indeed, the simulator in the proof of Theorem 19.4 is easily adapted to the present
setting. On input u∈G and c∈C, the simulator computes
αz ←R Zq, ut ←gαz/uc,
and outputs the pair (ut,αz). We leave it to the reader to verify that this simulator satisﬁes all the
requirements of Deﬁnition 19.5. 2
774
19.5 Sigma protocols: examples
So far, the only Sigma protocol we have seen is that of Schnorr, which allows a prover to convince a
skeptical veriﬁer that it “knows” the discrete logarithm of a given group element, without revealing
anything about the discrete logarithm to the veriﬁer. In this section, we present several additional
examples of Sigma protocols. These examples not only serve to ﬂesh out the general theory of
Sigma protocols, they also have many practical applications, some of which we will discuss below.
19.5.1 Okamoto’s protocol for representations
Let G be a cyclic group of prime order q generated by g ∈G. Let h∈G be some arbitrary group
element. We will think of hfor now as a system parameter — generated once and for all at system
setup time, and publicly available to all parties. Recall (see Section 10.6.1) that for u ∈G, a
representation of u (relative to g and h) is a pair ( α,β) ∈Z2
q such that gαhβ = u.
Okamoto’s protocol allows a prover to convince a skeptical veriﬁer that he “knows” a represen-
tation of a given u∈G, without revealing anything about that representation to the veriﬁer. More
precisely, it is a Sigma protocol for the relation
R=
{(
(α,β), u
)
∈Z2
q ×G : gαhβ = u
}
. (19.11)
A witness for the statement u ∈G is (α,β) ∈Z2
q such that gαhβ = u, i.e., a representation of u.
Thus, in this example, every statement has many witnesses (precisely q, in fact).
The challenge space Cfor Okamoto’s protocol is assumed to be a subset of Zq. The protocol
(P,V ) runs as follows, where the prover P is initialized with (( α,β),u) ∈R and the veriﬁer V is
initialized with u∈G:
1. P computes
αt ←R Zq, βt ←R Zq, ut ←gαthβt,
and sends the commitment ut to V;
2. V computes c←R C, and sends the challenge c to P;
3. P computes
αz ←αt + αc∈Zq, βz ←βt + βc ∈Zq,
and sends the response ( αz,βz) to V;
4. V checks if gαzhβz = ut ·uc; if so V outputs accept; otherwise, V outputs reject.
See Fig. 19.6.
Theorem 19.9. Okamoto’s protocol is a Sigma protocol for the relation Rdeﬁned in (19.11).
Moreover, it provides special soundness and is special HVZK.
Proof. Clearly, Okamoto’s protocol has the required syntactic structure of a Sigma protocol. An
accepting conversation for u∈G is of the form
(ut,c, (αz,βz)) such that gαzhβz = ut ·uc.
775
P((α,β),u) V(u)
αt ←R Zq, βt ←R Zq, ut ←gαthβt
ut
−−−−−−−−−−−−−−−−→
c←R C
c←−−−−−−−−−−−−−−−−
αz ←αt + αc
βz ←βt + βc
αz, βz
−−−−−−−−−−−−−−−−→
gαzhβz ?= ut ·uc
Figure 19.6: Okamoto’s protocol
Correctness. We have to verify that the protocol satisﬁes the basic correctness requirement that
an interaction between an honest prover and an honest veriﬁer always produces an accepting con-
versation. This is easy to verify, since if
ut = gαthβt, α z = αt + αc, and βz = βt + βc,
then we have
gαzhβz = gαt+αchβt+βc = gαthβt ·(gαhβ)c = ut ·uc.
Special soundness. Next, we show that Okamoto’s protocol provides special soundness. Suppose
we have two accepting conversations
(ut,c, (αz,βz)) and ( ut,c′,(α′
z,β′
z))
for the statement u, where c̸= c′. We have to show how to eﬃciently extract a representation of u
from these two conversations. The computation here is very similar to that in Schnorr’s protocol.
Observe that
gαzhβz = ut ·uc and gα′
zhβ′
z = ut ·uc′
,
and dividing the ﬁrst equation by the second, the ut’s cancel, and we have
g∆αh∆β = u∆c, where ∆α:= αz −α′
z, ∆ β := βz −β′
z, ∆ c:= c−c′.
and so the witness extractor can eﬃciently compute a representation ( α,β) ∈Z2
q of u as follows:
α←∆α/∆c, β ←∆β/∆c.
Note that because c̸= c′, the value ∆c is invertible in Zq. Here we use the fact that q is a prime.
Special HVZK. Finally, we show that Okamoto’s protocol is special HVZK by exhibiting a simulator.
Again, this is very similar to what we did for Schnorr’s protocol. On input u∈G and c∈C, the
simulator computes
αz ←R Zq, βz ←R Zq, ut ←gαzhβz/uc,
776
P(β,(u,v,w )) V(u,v,w )
βt ←R Zq, vt ←gβt, wt ←uβt
vt,wt
−−−−−−−−−−−−−−−−→
c←R C
c←−−−−−−−−−−−−−−−−
βz ←βt + βc
βz
−−−−−−−−−−−−−−−−→
gβz ?= vt ·vc and uβz ?= wt ·wc
Figure 19.7: The Chaum-Pedersen protocol
and outputs ( ut,(αz,βz)). Observe that the output always yields an accepting conversation, as
required.
Now we argue that whenc∈C is chosen at random, the output of the simulator on inputu,c has
the right distribution. The key observation is that in a real conversation, c, αz, and βz are mutually
independent, with c uniformly distributed over C, and αz and βz both uniformly distributed over
Zq; moreover, given c, αz, and βz, the value ut is uniquely determined by the equation
gαzhβz = ut ·uc.
It should be clear that this is the same as the output distribution of the simulator. 2
19.5.2 The Chaum-Pedersen protocol for DH-triples
The Chaum-Pedersen protocol allows a prover to convince a skeptical veriﬁer that a given triple is
a DH-triple, without revealing anything else to the veriﬁer.
Let G be a cyclic group of prime order q generated by g∈G, as usual. Recall (see Section 10.5)
that for α,β,γ ∈Zq, we say that ( gα,gβ,gγ) is a DH-triple if γ = αβ. Equivalently, ( u,v,w ) is a
DH-triple if and only if there exists β ∈Zq such that v= gβ and w= uβ.
The Chaum-Pedersen protocol is a Sigma protocol for the relation
R:=
{(
β, (u,v,w )
)
∈Zq ×G3 : v= gβ and w= uβ
}
. (19.12)
A witness for the statement ( u,v,w ) ∈G3 is β ∈Zq such that v = gβ and w = uβ. Thus, a
statement has a witness if and only if it is a DH-triple. Unlike the other examples we have seen so
far, not all statements have a witness.
The Chaum-Pedersen protocol (P,V ) is given in Fig. 19.7. The challenge space Cis a subset of
Zq.
Theorem 19.10. The Chaum-Pedersen protocol is a Sigma protocol for the relation Rdeﬁned in
(19.12). Moreover, it provides special soundness and is special HVZK.
Proof. The protocol has the required syntactic structure of a Sigma protocol. An accepting con-
versation for (u,v,w ) ∈G3 is of the form
((vt,wt),c,β z) such that gβz = vt ·vc and uβz = wt ·wc.
777
We leave it to the reader to verify that an interaction between an honest prover and an honest
veriﬁer always produces an accepting conversation.
Special soundness. Suppose we have two accepting conversations
((vt,wt),c,β z) and (( vt,wt),c′,β′
z)
for the statement (u,v,w ), where c̸= c′. The reader may verify that
β := ∆β/∆c, where ∆β := βz −β′
z, ∆c:= c−c′,
is the corresponding witness.
Special HVZK. On input (u,v,w ) ∈G3 and c∈C, the simulator computes
βz ←R Zq, vt ←gβz/vc, wt ←uβz/wc.
and outputs (( vt,wt),βz). Observe that the output always yields an accepting conversation, as
required.
Now we argue that when c ∈C is chosen at random, the output of the simulator on input
((u,v,w ),c) has the right distribution. The key observation is that in a real conversation, c and
βz are independent, with c uniformly distributed over C and βz uniformly distributed over Zq;
moreover, given c and βz, the values vt and wt are uniquely determined by the equations
gβz = vt ·vc and uβz = wt ·wc.
It should be clear that this is the same as the output distribution of the simulator. 2
19.5.3 A Sigma protocol for arbitrary linear relations
The reader may have noticed a certain similarity among the Schnorr, Okamoto, and Chaum-
Pedersen protocols. In fact, they are all special cases of a generic Sigma protocol for proving linear
relations among group elements.
As usual, let G be a cyclic group of prime order q generated by g ∈G. We shall consider
boolean formulas φ of the following type:
φ(x1,...,x n) :=



n∏
j=1
gxj
1j = u1 ∧ ··· ∧
n∏
j=1
gxj
mj = um


. (19.13)
In such a formula φ, the gij’s and ui’s are elements of the group G. Some of these group elements
could be system parameters or even constants, while others are speciﬁc to the formula. The xi’s
are the formal variables of the formula. When we assign values in Zq to the variables x1,...,x n,
the formula evaluates to true if all the equalities in (19.13) hold.
For a speciﬁc class Fof such formulas, we can deﬁne the relation
R:=
{(
(α1,...,α n), φ
)
∈Zn
q ×F : φ(α1,...,α n) = true
}
. (19.14)
778
P((α1,...,α n),φ) V(φ)
αtj ←R Zq (j = 1,...,n )
uti ←∏n
j=1 gαtj
ij (i= 1,...,m )
ut1,...,u tm ∈G−−−−−−−−−−−−−−−−→
c←R C
c←−−−−−−−−−−−−−−−−
αzj ←αtj + αjc (j = 1,...,n )
αz1,...,α zn ∈Zq
−−−−−−−−−−−−−−−−→∏n
j=1 gαzj
ij
?= uti ·uc
i (i= 1,...,m )
Figure 19.8: The generic linear protocol
So a statement is a formula φ∈F, and a witness for φ is an assignment ( α1,...,α n) ∈Zn
q to the
variables x1,...,x n that makes the formula true. The reason we call this a set of “linear” relations
is because if we take discrete logarithms, (19.13) can be written as the system of linear equations
Dlogg(ui) =
n∑
j=1
xj ·Dlogg(gij) for i= 1,...,m .
A witness is a solution to this system of equations.
The generic linear protocol (P,V ) for such a relation Ris given in Fig. 19.8. The prover
has φ and a witness ( α1,...,α n) ∈Zn
q. As usual, the challenge space Cis a subset of Zq. All the
Sigma protocols presented so far are special cases of the generic linear protocol:
• Schnorr’s protocol is a special case with φ1(x) :=
{
u= gx}
.
• Okamoto’s protocol is a special case with φ2(x,y) :=
{
u= gxhy}
.
• The Chaum-Pedersen protocol is a special case with φ3(x) :=
{
v= gx ∧ w= ux}
.
One can prove the following theorem by mimicking the proofs of the corresponding theorems
for Schnorr, Okamoto, and Chaum-Pedersen. We leave it as an exercise for the reader.
Theorem 19.11. The generic linear protocol in Fig. 19.8 is a Sigma protocol for the relation R
deﬁned in (19.14). Moreover, it provides special soundness and is special HVZK.
We can generalize the generic linear protocol even further, where we allow the various equations
in (19.13) to be over diﬀerent groups. The only requirement is that all groups have the same
prime order q. The protocol is exactly the same. A typical situation that arises in applications
is where there are two types of equations: the ﬁrst type are equations over a cryptographically
interesting group G of order q, and the second type are equations over Zq, which are of the form
κi = ∑n
j=1 λijxj, where the κi’s and λij’s are elements of Zq.
779
P
(
α∈H1, ψ) V(u∈H2, ψ)
αt ←R H1, ut ←ψ(αt)
ut ∈H2−−−−−−−−−−−−−−−−→
c←R C
c∈Z←−−−−−−−−−−−−−−−−
αz ←αt + α·c∈H1
αz ∈H1−−−−−−−−−−−−−−−−→
ψ(αz) ?= ut ·uc
Figure 19.9: A Sigma protocol for the preimage of a homomorphism
19.5.4 A Sigma protocol for the pre-image of a homomorphism
All the Sigma protocols presented so far, including the general linear protocol, can be described
more clearly and succinctly using the language of group homomorphisms. Let H1 and H2 be two
ﬁnite abelian groups of known order and let ψ : H1 →H2 be a group homomorphism. We will
write the group operation in H1 additively and the group operation in H2 multiplicatively.
Let u∈H2. Fig. 19.9 gives a Sigma protocol that allows a prover to convince a veriﬁer that it
“knows” a preimage of u under ψ. Speciﬁcally, the protocol is a Sigma protocol for the relation
R:=
{(
α,(u,ψ)
)
∈H1 ×(H2 ×F) : ψ(α) = u
}
. (19.15)
Here α∈H1 is the preimage under ψ for u∈H2. The prover in Fig. 19.9 has the witness α∈H1,
the veriﬁer has the image u ∈H2, and both parties have ( H1,H2,ψ). The challenge space Cis
{0,1,...,N −1}⊆ Z for some integer N.
Let us see how this protocol encompases all the example Sigma protocols so far. Let G be a
group of prime order q with generators g,h,u ∈G.
• Okamoto’s protocol is a special case with H1 := Z2
q, H2 := G, and ψ1(x,y) := gxhy.
• The Chaum-Pedersen protocol is a special case with
H1 := Zq, H2 := G2, and ψ2(x) := (gx,ux).
We can even set H2 := G1 ×G2 with g ∈G1, u ∈G2, and |G1|= |G2|. Then for a given
(v,w) ∈G1 ×G2, proving knowledge of a ψ2 preimage of (v,w) proves equality of discrete-logs
Dlogg(v) = Dlogu(w) in distinct groups G1 and G2.
• The general linear protocol in Fig. 19.8 is a special case with
H1 := (Zq)n, H2 := Gm, and ψ3(x1,...,x n) :=


n∏
j=1
gxj
1j, ...,
n∏
j=1
gxj
mj

.
where gij ∈G for all i= 1,...,m and j = 1,...,n .
780
One can easily verify that the maps ψ1,ψ2,ψ3 are group homomorphims. By using these homomor-
phims in the protocol of Fig. 19.9 we obtain all the example protocols in this section as a special
case.
Theorem 19.12. The protocol in Fig. 19.9 is a Sigma protocol for the relationRdeﬁned in (19.15).
Moreover, it is special HVZK, and provides special soundness whenever the smallest prime factor
of |H1|×|H2|is at least |C|.
The proof exactly mimicks the proof of the corresponding theorem for the Schnorr protocol.
We require the lower bound on the smallest prime factor of |H1|×|H2|to ensure that the witness
extractor can obtain a ψ preimage from two accepting conversations ( ut,c,α z) and ( ut,c′,α′
z).
As in the witness extractor for the Schnorr protocol, we obtain a relation ψ(∆α) = u∆c where
∆α := ( αz −α′
z) ∈H1 and ∆ c := ( c−c′) ∈Z. The lower bound on the prime factors of |H1|
and |H2|ensures that (1) we can divide ∆ α by ∆c in H1, and (2) we can take a ∆ c root of the
right hand side by raising it to the power of ((∆ c)−1 mod |H2|) ∈Z. We then obtain the relation
ψ(∆α/∆c) = u so that ∆α/∆c∈H1 is a preimage of u∈H2 under ψ, as required.
19.5.5 A Sigma protocol for RSA
Lest the reader think that Sigma protocols are only for problems related to discrete logarithms, we
present one related to RSA.
Let (n,e) be an RSA public key, where e is a prime number. We will view ( n,e) as a system
parameter. The Guillou-Quisquater (GQ) protocol allows a prover to convince a skeptical veriﬁer
that he “knows” an eth root of y ∈Z∗
n, without revealing anything else. More precisely, it is a
Sigma protocol for the relation
R=
{
(x,y) ∈Z∗
n ×Z∗
n : xe = y
}
. (19.16)
A witness for a statement y ∈Z∗
n is x ∈Z∗
n such that xe = y. Since ( n,e) is an RSA public key,
the map that sends x ∈Z∗
n to y = xe ∈Z∗
n is bijective. Therefore, every statement has a unique
witness.
The GQ protocol (P,V ) is given in Fig. 19.10. The challenge space Cis a subset of {0,...,e −1}.
Notice that when e is small, the challenge space is small. If needed, it can be enlarged using the
method of Exercise 19.6. However, when using this protocol we will typically ensure that the
challenge space is large by taking e to be a large prime.
The GQ protocol in Fig. 19.10 is a special case of the protocol in Fig. 19.9 for proving knowledge
of the preimage of a homomorphism. Here the homomorphism is ψ: Z∗
n →Z∗
n deﬁned by ψ(x) = xe.
However, special soundness does not follow from Theorem 19.12 because the groupZ∗
n has unknown
order. Instead, we have to give a seperate proof of these properties. We do so in the following
theorem.
Theorem 19.13. The GQ protocol is a Sigma protocol for the relation Rdeﬁned in (19.16).
Moreover, it provides special soundness and is special HVZK.
Proof. An accepting conversation for y is of the form ( xt,c,x z), where xe
z = yt ·yc. The reader
may easily verify the basic correctness requirement: an interaction between an honest prover and
an honest veriﬁer always produces an accepting conversation.
781
P(x,y) V(y)
xt ←R Z∗
n, yt ←xe
t
yt
−−−−−−−−−−−−−−−−→
c←R C
c←−−−−−−−−−−−−−−−−
xz ←xt ·xc
xz
−−−−−−−−−−−−−−−−→
xe
z
?= yt ·yc
Figure 19.10: The GQ protocol
Special soundness. Next, we show that the GQ protocol provides special soundness. Suppose we
have two accepting conversations ( xt,c,x z) and ( xt,c′,x′
z) for the statement y, where c ̸= c′. We
have to show how to eﬃciently compute an eth root of y. Observe that
xe
z = yt ·yc and ( x′
z)e = yt ·yc′
.
Dividing the ﬁrst equation by the second, we obtain
(∆x)e = y∆c, where ∆x:= xz/x′
z, ∆c:= c−c′.
Observe that because c ̸= c′ and both c and c′ belong to the interval {0,...,e −1}, we have
0 < |∆c|< e, and so e ∤ ∆c; moreover, since e is prime, it follows that gcd( e,∆c) = 1. Thus, we
may apply Theorem 10.6 (with the given e, f := ∆c, and w:= ∆x), to obtain an eth root of y.
The reader should observe that the technique presented here for computing an RSA inverse from
two accepting conversations is essentially the same idea that was used in the proof of Theorem 10.7.
Indeed, the two accepting conversations yield a collision (( xz,−cmod e),(x′
z,−c′mod e)) on the
hash function Hrsa(a,b) := aeyb.
Special HVZK. Finally, we show that the GQ protocol is special HVZK by exhibiting a simulator.
On input y∈Z∗
n and c∈C, the simulator computes
xz ←R Z∗
n, yt ←xe
z /yc
and outputs (yt,xz). The key observation is that in a real conversation, c and xz are independent,
with c uniformly distributed over Cand xz uniformly distributed over Z∗
n; moreover, given c and
xz, the value yt is uniquely determined by the equation xe
z = yt ·yc. It should be clear that this is
the same as the output distribution of the simulator. 2
19.6 Identiﬁcation and signatures from Sigma protocols
By mimicking the Schnorr constructions, we can easily convert any Sigma protocol into a corre-
sponding identiﬁcation scheme and signature scheme.
782
Suppose we have a Sigma protocol ( P,V ) for a relation R⊆X×Y . In addition to P and V,
we need a key generation algorithm for R. This is a probabilistic algorithm G that generates
a public-key/secret-key pair (pk,sk), where pk = y and sk = (x,y) for some ( x,y) ∈R.
To get secure identiﬁcation and signature schemes we need the following “one-wayness” prop-
erty: given a public key pk = y ∈Y output by G, it should be hard to compute ˆx∈X such that
(ˆx,y) ∈R. This notion is made precise by the following attack game.
Attack Game 19.2 (One-way key generation). Let G be a key generation algorithm for
R⊆X×Y . For a given adversary A, the attack game runs as follows:
• The challenger runs (pk,sk) ←R G(), and sends pk = y to A;
• Aoutputs ˆx∈X.
We say that the adversary wins the game if (ˆx,y) ∈R. We deﬁne A’s advantage with respect to
G, denoted OWadv[A,G], as the probability that Awins the game. 2
Deﬁnition 19.6. We say that a key generation algorithm G is one way if for all eﬃcient adver-
saries A, the quantity OWadv[A,G] is negligible.
Example 19.4. For the Schnorr Sigma protocol (Example 19.1), the most natural key generation
algorithm computes α ←R Zq and u ←gα ∈G, and outputs pk := u and sk := (α,u). It is clear
that this key generation algorithm is one-way under the DL assumption. 2
Example 19.5. Consider the GQ protocol in Section 19.5.5. Recall that the RSA public key
(n,e) is viewed here as a system parameter. The most natural key generation algorithm computes
x←R Z∗
n and y ←xe ∈Z∗
n. It outputs pk := y and sk := (x,y). It is clear that this key generation
algorithm is one-way under the RSA assumption (see Theorem 10.5). 2
A Sigma protocol ( P,V ) with a key generation algorithm G gives an identiﬁcation scheme
(G,P,V ). The next two theorems prove that it is secure against eavesdropping attacks.
Theorem 19.14. Let (P,V ) be a Sigma protocol for an eﬀective relation Rwith a large challenge
space. Let G be a key generation algorithm for R. If (P,V ) provides special soundness and G is
one-way, then the identiﬁcation scheme I:= (G,P,V ) is secure against direct attacks.
In particular, suppose Ais an eﬃcient impersonation adversary attacking Ivia a direct attack
as in Attack Game 18.1, with advantage ϵ := ID1 adv[A,I]. Then there exists an eﬃcient
adversary Battacking G as in Attack Game 19.2 (whose running time is about twice that of
A), with advantage ϵ′:= OWadv[B,G], such that
ϵ′≥ϵ2 −ϵ/N, (19.17)
where N is the size of the challenge space, which implies
ϵ≤ 1
N +
√
ϵ′. (19.18)
Proof. We can just mimic the proof of Theorem 19.1. Using the impersonation adversary A, we
build an adversary Bthat breaks the one-wayness of G, as follows. Adversary Bis given a public
key pk = y from its challenger, and our goal is to make Bcompute ˆx such that (ˆx,y) ∈R, with
help from A. The computation of Bconsists of two stages.
783
In the ﬁrst stage of its computation, Bplays the role of challenger to A, giving Athe value
pk = yas the veriﬁcation key. Using the same rewinding argument as in the proof of Theorem 19.1,
with probability at least ϵ2 −ϵ/N, adversary Bobtains two accepting conversations ( t,c,z ) and
(t,c′,z′) for y with c̸= c′. In more detail, Bawaits A’s commitment t, gives Aa random challenge
c, and awaits A’s response z. After this happens, Brewinds A’s internal state back to the point
just after which it generated t, gives Aanother random challenge c′, and awaits A’s response z′.
By the Rewinding Lemma (Lemma 19.2), this procedure will yield the two required accepting
conversations with probability at least ϵ2 −ϵ/N.
In the second stage of the computation, Bfeeds these two conversations into a witness extractor
(which is guaranteed by the special soundness property) to extract a witness ˆx for y.
That proves (19.17), and (19.18) follows by the same calculation as in Theorem 19.1. 2
Theorem 19.3 obviously applies to identiﬁcation protocols derived from special HVZK Sigma
protocols:
Theorem 19.15. Let (P,V ) be a Sigma protocol for an eﬀective relation R. Let G be a key
generation algorithm for R. If the identiﬁcation protocol I = ( G,P,V ) is secure against direct
attacks, and (P,V ) is special HVZK, then Iis also secure against eavesdropping attacks.
In particular, for every impersonation adversary Athat attacks Ivia an eavesdropping attack,
as in Attack Game 18.2, there is an adversary Bthat attacks Ivia a direct attack on, as in
Attack Game 18.1, where Bis an elementary wrapper around A, such that
ID2adv[A,I] = ID1adv[B,I].
Example 19.6. If we augment the GQ protocol ( P,V ) with the key generation algorithm G
in Example 19.5, then we get an identiﬁcation scheme IGQ = ( G,P,V ) that is secure against
eavesdropping attacks under the RSA assumption (provided the challenge space is large). 2
19.6.1 The Fiat-Shamir heuristic for signatures
We can convert Sigma protocols to signature schemes, using the same technique developed in
Section 19.2. The general technique is originally due to Fiat and Shamir. The building blocks are
as follows:
• a Sigma protocol ( P,V ) for a relation R⊆X×Y ; we assume that conversations are of the
form (t,c,z ), where t∈T , c∈C, and z∈Z;
• a key generation algorithm G for R;
• a hash function H : M×T →C, which will be modeled as a random oracle; the set Mwill
be the message space of the signature scheme.
The Fiat-Shamir signature scheme derived from G and (P,V ) works as follows:
• The key generation algorithm is G, so a public key is of the form pk = y, where y ∈Y, and
a secret key is of the form sk = (x,y) ∈R.
• To sign a messagem∈M using a secret key sk = (x,y), the signing algorithm runs as follows:
– it starts the prover P(x,y), obtaining a commitment t∈T ;
784
– it computes a challenge c←H(m,t);
– ﬁnally, it feeds c to the prover, obtaining a response z, and outputs the signature σ :=
(t,z) ∈T ×Z.
• To verify a signature σ = (t,z) ∈T ×Z on a message m ∈M using a public key pk = y,
the veriﬁcation algorithm computes c ←H(m,t), and checks that ( t,c,z ) is an accepting
conversation for y.
Just as we did for Schnorr, we will show that the Fiat-Shamir signature scheme is secure in
the random oracle model if the corresponding identiﬁcation scheme ( G,P,V ) is secure against
eavesdropping attacks. However, we will need one more technical assumption, which essentially all
Sigma protocols of interest satisfy.
Deﬁnition 19.7 (Unpredictable commitments). Let (P,V ) be a Sigma protocol for R⊆X×Y ,
and suppose that all conversations (t,c,z ) lie in T×C×Z . We say that (P,V ) has δ-unpredictable
commitments if for every (x,y) ∈R and ˆt∈T , with probability at most δ, an interaction between
P(x,y) and V(y) produces a conversation(t,c,z ) with t= ˆt. We say that (P,V ) has unpredictable
commitments if it is has δ-unpredictable commitments for negligible δ.
Theorem 19.16. If H is modeled as a random oracle, the identiﬁcation scheme I= ( G,P,V )
is secure against eavesdropping attacks, and (P,V ) has unpredictable commitments, then the Fiat-
Shamir signature scheme Sderived from G and (P,V ) is secure.
In particular, let Abe an adversary attacking S as in the random oracle version of Attack
Game 13.1. Moreover, assume that Aissues at most Qs signing queries and Qro random or-
acle queries, and that (P,V ) has δ-unpredictable commitments. Then there exist a (Qro + 1)-
impersonation adversary Bthat attacks Ivia an eavesdropping attack as in Attack Game 19.1,
where Bis an elementary wrapper around A, such that
SIGroadv[A,S] ≤Qs(Qs + Qro + 1)δ+ rID2adv[B,I,Qro + 1].
The proof of this theorem is almost identical to that of Theorem 19.7. We leave the details to
the reader.
Putting everything together, suppose that we start with a Sigma protocol ( P,V ) that is special
HVZK and provides special soundness. Further, suppose ( P,V ) has unpredictable commitments
and a large challenge space. Then, if we combine ( P,V ) with a one-way key generation algorithm
G, the Fiat-Shamir signature construction gives us a secure signature scheme (that is, if we model
H as a random oracle). The Schnorr signature scheme is a special case of this construction.
Just as we did for Schnorr signatures, we could use Lemma 19.6 to reduce from r-impersonation
to 1-impersonation; however, a tighter reduction is possible. Indeed, the proof of Lemma 19.8 goes
through, essentially unchanged:
Lemma 19.17. Let (P,V ) be a special HVZK Sigma protocol for a relation R ⊆ X×Ythat
provides special soundness, let G be a key generation algorithm for R, and consider the resulting
identiﬁcation protocol I = ( G,P,V ). Suppose Ais an eﬃcient r-impersonation eavesdropping
adversary attacking I, as in Attack Game 19.1, with advantage ϵ:= rID2adv[A,I,r]. Then there
exists an eﬃcient adversary Battacking G as in Attack Game 19.2 (whose running time is about
twice that of A), with advantage ϵ′:= OWadv[B,G], such that
ϵ′≥ϵ2/r−ϵ/N, (19.19)
785
where N is the size of the challenge space, which implies
ϵ≤ r
N +
√
rϵ′. (19.20)
Using this, we get the following concrete security bound for Theorem 19.16, assuming ( P,V ) is
special HVZK and provides special soundness:
Let Abe an eﬃcient adversary attacking Sas in the random oracle version of Attack Game 13.1.
Moreover, assume that Aissues at most Qs signing queries and Qro random oracle queries. Then
there exists an eﬃcient adversary Battacking G as in Attack Game 19.2 (whose running time
is about twice that of A), such that
SIGroadv[A,S] ≤Qs(Qs + Qro + 1)δ+ (Qro + 1)/N+
√
(Qro + 1)OWadv[B,G]), (19.21)
where N is the size of the challenge space.
19.6.1.1 The GQ signature scheme
The Fiat-Shamir signature construction above applied to the GQ Sigma protocol (Section 19.5.5)
gives us a new signature scheme based on RSA. The scheme makes use of an RSA public key ( n,e)
as a system parameter, where the encryption exponent e is a large prime. If desired, this system
parameter can be shared by many users. We need a hash function H : M×T →C, where T is a
set into which all elements of Z∗
n can be encoded, Mis the message space of the signature scheme,
and Cis a subset of {0,...,e −1}. The GQ signature scheme is SGQ = (G,S,V ), where:
• The key generation algorithm G runs as follows:
x←R Z∗
n, y ←xe.
The public key is pk := y, and the secret key is sk := x.
• To sign a message m∈M using a secret key sk = x, the signing algorithm runs as follows:
S( sk, m) := xt ←R Z∗
n, y t ←xe
t , c ←H(m,yt), x z ←xt ·xc
output σ:= (yt,xz).
• To verify a signature σ = (yt,xz) on a message m ∈M, using the public key pk = y, the
signature veriﬁcation algorithm V computes c:= H(m,yt). It outputs accept if xe
z = yt ·yc,
and outputs reject, otherwise.
As we saw in Example 19.6, the GQ identiﬁcation scheme is secure against eavesdropping attacks
under the RSA assumption (provided the challenge space is large). Also, we observe that the GQ
Sigma protocol has 1 /φ(n)-unpredictable commitments. It follows from Theorem 19.16 that the
corresponding signature scheme is secure in the random oracle model, under the RSA assumption.
The advantage of GQ signatures over RSA signatures, such as SRSA-FDH, is that the signing
algorithm is much faster. Signing with SRSA-FDH requires a large exponantiation. Signing with GQ
requires two exponentiations with exponents e and c, but both can be only 128 bits. Fast signing
is important when the signer is a weak device, as in the case of a chip enabled creditcard that signs
every creditcard transaction.
786
An optimization. The GQ signature scheme can be optimized in the same way as the Schnorr
signature scheme. Instead of deﬁning a signature on m to be a pair ( yt,xz) satisfying
xe
z = yt ·yc,
where c:= H(m,yt), we can deﬁne it to be a pair ( c,xz) satisfying
c= H(m,yt),
where yt := xe
z /yc. As a further optimization, one can store y−1 in the public key instead of y,
which will speed up veriﬁcation.
It turns out that this same optimization can be applied to most instances of the Fiat-Shamir
signature construction. See Exercise 19.19.
19.7 Combining Sigma protocols: AND and OR proofs
In this section we show how Sigma protocols can be combined to make new Sigma protocols. In
the AND-proof construction, a prover can convince a veriﬁer that he “knows” witnesses for a pair
of statements. In the OR-proof construction, a prover can convince a veriﬁer that he “knows”
witnesses for one of two statements.
19.7.1 The AND-proof construction
Suppose we have a Sigma protocol ( P0,V0) for R0 ⊆X0 ×Y0, and a Sigma protocol ( P1,V1) for
R1 ⊆X1 ×Y1. Further, let us assume that both Sigma protocols use the same challenge space C.
We can combine them to form a Sigma protocol for the relation
RAND =
{(
(x0,x1), (y0,y1)
)
∈(X0×X1)×(Y0×Y1) : ( x0,y0) ∈R0 and (x1,y1) ∈R1
}
. (19.22)
In other words, for a given pair of statements y0 ∈Y0 and y1 ∈Y1, this AND protocol allows a
prover to convince a skeptical veriﬁer that he “knows” a witness for y0 and a witness for y1. The
protocol (P,V ) runs as follows, where the prover P is initialized with (( x0,x1),(y0,y1)) ∈RAND,
the veriﬁer V is initialized with ( y0,y1) ∈Y0 ×Y1:
1. P runs P0(x0,y0) to get a commitment t0 and runs P1(x1,y1) to get a commitment t1, and
sends the commitment pair ( t0,t1) to V;
2. V computes c←R C, and sends the challenge c to P;
3. P feeds the challenge c to both P0(x0,y0) and P1(x1,y1), obtaining responses z0 and z1, and
sends the response pair ( z0,z1) to V;
4. V checks that (t0,c,z 0) is an accepting conversation for y0 and that (t1,c,z 1) is an accepting
conversation for y1.
Theorem 19.18. The AND protocol (P,V ) is a Sigma protocol for the relation RAND deﬁned in
(19.22). If (P0,V0) and (P1,V1) provide special soundness, then so does (P,V ). If (P0,V0) and
(P1,V1) are special HVZK, then so is (P,V ).
787
Proof sketch. Correctness is clear.
For special soundness, if ( P0,V0) has extractor Ext0 and (P1,V1) has extractor Ext1, then the
extractor for (P,V ) is
Ext
(
(y0,y1),
(
(t0,t1),c, (z0,z1)
)
,
(
(t0,t1),c′,(z′
0,z′
1)
))
:=
(
Ext0
(
y0,(t0,c,z 0),(t0,c′,z′
0)
)
, Ext1
(
y1,(t1,c,z 1),(t1,c′,z′
1)
))
.
For special HVZK, if ( P0,V0) has simulator Sim0 and ( P1,V1) has simulator Sim1, then the
simulator for (P,V ) is
Sim((y0,y1),c) := ((t0,t1),(z0,z1)),
where
(t0,z0) ←R Sim0(y0,c) and ( t1,z1) ←R Sim1(y1,c).
We leave it to the reader to ﬁll in the details. However, we point out that in our construction
of Sim, we have exploited the fact that in our deﬁnition of special HVZK, the challenge is an input
to the simulator, which we can feed to both Sim0 and Sim1. This is one of the main reasons for
this aspect of the deﬁnition. 2
19.7.2 The OR-proof construction
Suppose we have a Sigma protocol ( P0,V0) for R0 ⊆X0 ×Y0, and a Sigma protocol ( P1,V1) for
R1 ⊆X1 ×Y1. We need to make some additional assumptions:
• Both Sigma protocols use the same challenge space C, which is of the form C= {0,1}n. (Note
that in the examples we have seen where challenges are numbers, we can always encode bit
strings as numbers in binary notation.)
• Both protocols are special HVZK, with simulators Sim0 and Sim1, respectively.
We can combine them to form a Sigma protocol for the relation
ROR =
{(
(b,x), (y0,y1)
)
∈
(
{0,1}×(X0 ∪X1)
)
×(Y0 ×Y1) : ( x,yb) ∈Rb
}
. (19.23)
In other words, for a given pair of statements y0 ∈Y0 and y1 ∈Y1, this OR protocol allows a
prover to convince a skeptical veriﬁer that he “knows” a witness for y0 or a witness for y1. Nothing
else should be revealed. In particular the protocol should not reveal if the prover has a witness for
y0 or for y1.
The protocol ( P,V ) runs as follows, where the prover P is initialized with (( b,x),(y0,y1)) ∈
ROR, the veriﬁer V is initialized with ( y0,y1) ∈Y0 ×Y1, and d:= 1 −b:
1. P computes cd ←R C, (td,zd) ←R Simd(yd,cd).
P also runs Pb(x,yb) to get a commitment tb, and sends the commitment pair ( t0,t1) to V;
2. V computes c←R C, and sends the challenge c to P;
3. P computes cb ←c⊕cd
P feeds the challenge cb to Pb(x,yb), obtaining a response zb, and sends ( c0,z0,z1) to V;
788
4. V computes c1 ←c⊕c0, and checks that ( t0,c0,z0) is an accepting conversation for y0, and
that (t1,c1,z1) is an accepting conversation for y1.
Theorem 19.19. The OR protocol (P,V ) is a special HVZK Sigma protocol for the relation ROR
deﬁned in (19.23). If (P0,V0) and (P1,V1) provide special soundness, then so does (P,V ).
Proof sketch. Correctness is clear.
For special soundness, if ( P0,V0) has extractor Ext0 and (P1,V1) has extractor Ext1, then the
extractor Ext for (P,V ) takes as input ( y0,y1) and a pair of accepting conversations
(
(t0,t1), c, (c0,z0,z1)
)
and
(
(t0,t1), c′, (c′
0,z′
0,z′
1)
)
.
Let c1 := c⊕c0 and c′
1 := c′⊕c′
0. The key observation is that if c̸= c′, then we must have either
c0 ̸= c′
0 or c1 ̸= c′
1. So Ext works as follows:
if c0 ̸= c′
0
then output
(
0, Ext0(y0,(t0,c0,z0),(t0,c′
0,z′
0))
)
else output
(
1, Ext1(y1,(t1,c1,z1),(t1,c′
1,z′
1))
)
For special HVZK, the simulator for ( P,V ) is
Sim((y0,y1),c) := ((t0,t1),(c0,z0,z1)),
where
c0 ←R C, c1 ←c⊕c0, (t0,z0) ←R Sim0(y0,c0), (t1,z1) ←R Sim1(y1,c1).
We leave it to the reader to ﬁll in the details. However, we point out that to guarantee cor-
rectness, we have exploited the fact that in our deﬁnition of special HVZK, the simulator always
outputs an accepting conversation. This is one of the main reasons for this aspect of the deﬁnition.
2
19.8 Witness independence and applications
We next study a useful property of Sigma protocols called witness independence.
For a given statement there may be several witnesses. Roughly speaking, witness independence
means the following: if a “cheating” veriﬁer V∗ (one that need not follow the protocol) interacts
with an honest prover P, then V∗ cannot tell which witness P is using. In particular, even if V∗
is very powerful and/or very clever and is able to compute a witness after interacting with P, this
witness will be unrelated to P’s witness. Of course, this property is only interesting if a given
statement has more than one witness.
First, we deﬁne this property more precisely. Second, we show that special HVZK implies wit-
ness independence. This is perhaps a bit surprising, as HVZK is a property about honest veriﬁers,
while witness independence applies to all veriﬁers (even computationally unbounded cheating veri-
ﬁers). Finally, as an application, we show how to use witness independence to design identiﬁcation
protocols that are secure against active attacks, rather than just eavesdropping attacks. These
identiﬁcation protocols are simple and eﬃcient, and their security can be based on either the DL
or RSA assumptions (and without relying on the random oracle heuristic).
789
19.8.1 Deﬁnition of witness independence
We deﬁne witness independence using an attack game.
Attack Game 19.3 (Witness independence). Let Π = (P,V ) be a Sigma protocol for R⊆X×
Y. For a given adversary A, we deﬁne an experiment ( x,y) for each (x,y) ∈R. Experiment ( x,y)
runs as follows.
• Initially, the adversary is given the value y.
• The adversary then interacts with several instances of the prover P(x,y) — in each of these
interactions, the challenger carries out the provers’ computations, while the adversary plays
the role of a cheating veriﬁer (i.e., one that need not follow V’s protocol). These interac-
tions may be concurrent (in particular, the adversary may issue challenges that depend on
commitments and responses output so far by all prover instances).
• At the end of the game, the adversary outputs some value s, which belongs to a ﬁnite output
space S(which may depend on A).
For each (x,y) ∈R and s ∈S, we deﬁne θA,Π(x,y,s ) to be the probability that Aoutputs s in
Experiment (x,y). 2
Deﬁnition 19.8. Let Π = ( P,V ) be a Sigma protocol for R ⊆X×Y . We say that (P,V ) is
witness independent if for every adversary A, for every y ∈Y, for every x,x′ ∈X such that
(x,y) ∈R and (x′,y) ∈R, and for every s in the output space of A, we have
θA,Π(x,y,s ) = θA,Π(x′,y,s ).
The deﬁnition states that for every y ∈Y and s∈S, the quantity θA,Π(x,y,s ) is the same for
all x∈X for which (x,y) ∈R. Note that in this deﬁnition, Aneed not be eﬃcient. We also note
that in this deﬁnition, if the Sigma protocol makes use of a system parameter, which itself may be
randomly generated, we insist that the deﬁning property should hold for every possible choice of
system parameter.
This deﬁnition captures in a very strong sense the idea that the adversary’s behavior depends
only on the statement, but not on the particular witness that the prover is using.
In the analysis of identiﬁcation schemes, it is sometimes convenient to apply the deﬁnition of
witness independence as follows. Suppose ( P,V ) is a Sigma protocol for R⊆X×Y , and that Gis
a key generation algorithm for R. Suppose we run the key generation algorithm to obtain pk = y
and sk = (x,y), and then run Experiment ( x,y) in Attack Game 19.3 with an adversary A. Let us
deﬁne random variables X, Y, S, as follows:
• X represents the witness x generated by G;
• Y represents the statement y generated by G;
• S represents the adversary’s output s∈S.
Fact 19.20. If (P,V ) is witness independent, then we have
Pr[X = x ∧S = s|Y = y] = Pr[X = x|Y = y] ·Pr[S = s|Y = y] (19.24)
for all (x,y) ∈R and s∈S.
790
We leave the proof of Fact 19.20 as a straightforward exercise for the reader. Equation (19.24)
says that conditioned on Y = yfor any particular y, the random variables X and S are independent.
One can rewrite (19.24) in a number of diﬀerent ways. For example, it is equivalent to saying
Pr[X = x|S = s ∧Y = y] = Pr[X = x|Y = y]. (19.25)
Example 19.7. Theorem 19.21 below will show that the OR-protocol (Section 19.7.2) and
Okamoto’s protocol (Section 19.6) are both witness independent protocols. 2
19.8.2 Special HVZK implies witness independence
As promised, we now prove that special HVZK implies witness independence.
Theorem 19.21 (Special HVZK =⇒ WI). If a Sigma protocol is special HVZK, then it is
witness independent.
Proof. Let (P,V ) be a Sigma protocol for R⊆X×Y . Suppose that all conversations ( t,c,z ) lie in
T ×C×Z.
Let Coins be a random variable representing the possible random choices coins made by P. For
example, in Schnorr’s protocol, coins is the valueαt ∈Zq, and Coins is uniformly distributed overZq.
The prover P’s logic can be completely characterized by some function γ that maps (x,y,c, coins)
to (t,z), where (x,y) ∈R and (t,c,z ) ∈T ×C×Z .
Consider the probability that a real conversation betweenP(x,y) and V(y) produces a particular
conversation (t,c,z ). This is precisely
Pr[γ(x,y,c, Coins) = (t,z)] / |C|. (19.26)
Now consider a simulator Sim that is guaranteed by the special HVZK property. For all
(x,y) ∈R, c ∈C, and ( t,z) ∈T ×Z, we deﬁne p(y,t,c,z ) to be the probability that Sim(y,c)
outputs ( t,z). The probability that the conversation produced by running the simulator on a
random challenge is equal to a particular conversation ( t,c,z ) is precisely
p(y,t,c,z ) / |C|. (19.27)
As the probabilities (19.26) and (19.27) must be equal, we conclude that for all ( x,y) ∈R and
(t,c,z ) ∈T ×C×Z , we have
Pr[γ(x,y,c, Coins) = (t,z)] = p(y,t,c,z ),
which does not depend on x. This fact is really the crux of the proof, even if the details get a bit
involved.
Now consider Experiment (x,y) of Attack Game 19.3, and assume that the adversaryAinteracts
with Qcopies of the prover P. The logic of the entire collection of provers can be characterized by a
function γ∗that maps (x,y,c ∗,coins∗) to (t∗,z∗), where now t∗,c∗,z∗, and coins∗are corresponding
vectors of length Q. Moreover, if Coins∗is a vector of Qindependent copies of the random variable
Coins, then for all ( x,y) ∈R and (t∗,c∗,z∗) ∈T Q ×CQ ×ZQ, we have
Pr[γ∗(x,y,c ∗,Coins∗) = (t∗,z∗)] =
∏
i
p(y,t∗[i],c∗[i],z∗[i]),
791
which again, does not depend on x.
Let Coins′ be a random variable representing the possible random choices coins′ made by the
adversary. The adversary’s logic can be characterized by a function γ′ that maps (y,t∗,z∗,coins′)
to (c∗,s). Here, ( t∗,c∗,z∗) ∈T Q ×CQ ×ZQ, s ∈S is the adversary’s output, and coins′ denotes
the particular random choices made by the adversary.
Let Sx,y be a random variable that represents the output of Ain Experiment (x,y) of the attack
game. Let Tx,y be the random variable representing the possible transcripts t = (t∗,c∗,z∗). For
s∈S and t= (t∗,c∗,z∗), deﬁne events Γ∗(x,y; t) and Γ′(y,s; t) as follows:
Γ∗(x,y; t) : γ∗(x,y,c ∗,Coins∗) = (t∗,z∗), Γ′(y,s; t) : γ′(y,t∗,z∗,Coins′) = (c∗,s).
Note that Γ∗(x,y; t) and Γ′(y,s; t) are independent events. Also, as we observed above, the prob-
ability Pr[Γ∗(x,y; t)] does not depend on x.
For s ∈S , we calculate Pr[ Sx,y = s] by summing over all possible transcripts t, using total
probability:
Pr[Sx,y = s] =
∑
t
Pr[Sx,y = s ∧Tx,y = t]
=
∑
t
Pr[Γ∗(x,y; t) ∧Γ′(y,s; t)]
=
∑
t
Pr[Γ∗(x,y; t)] ·Pr[Γ′(y,s; t)] (by independence) .
In this last expression, we see that neither Pr[ Γ∗(x,y; t)] nor Pr[ Γ′(y,s; t)] depends on x, which
proves the theorem. 2
19.8.3 Actively secure identiﬁcation protocols
As promised, we now show how to use witness independence to design actively secure identiﬁcation
protocols. The construction is quite general. The basic ingredients are a Sigma protocol, along
with a one-way key generation algorithm. We also make use of the OR-proof construction in
Section 19.7.2.
Let (P,V ) be a Sigma protocol for R⊆X×Y . We will assume that ( P,V ) is special HVZK
and that its challenge space is of the form C= {0,1}n. These assumptions will allow us to apply
the OR-proof construction presented in Section 19.7.2. In the security analysis, we will also need
to assume that ( P,V ) provides special soundness.
As we saw in Section 19.6, to build an identiﬁcation protocol from ( P,V ), we also need a one-
way key generation algorithm G for the relation R. The identiﬁcation scheme I:= (G,P,V ) is
secure against eavesdropping. However, without too much more eﬀort, and without making any
additional assumptions, we can build an identiﬁcation scheme that is secure against active attacks
(as deﬁned in Section 18.6).
First, we build a new Sigma protocol ( P′,V ′) by applying the OR-proof construction to
(P0,V0) := (P,V ) and (P1,V1) := (P,V ). Let R′:= ROR be the corresponding relation: a statement
for R′is of the form Y = (y0,y1) ∈Y2, and a witness for Y is of the form X = (b,x) ∈{0,1}×X ,
where (x,yb) ∈R. For a witness X = (b,x), let us call the bit b its type.
Second, we build a new key generation algorithm G′for the relation R′. Algorithm G′runs as
follows:
792
G′: ( y0,(x0,y0)) ←R G(), (y1,(x1,y1)) ←R G()
b←R {0,1}
Y ←(y0,y1)
X ←(b,xb)
output (Y,(X,Y ))
A key property of G′is that, as random variables, Y and b are independent. That is, after we see
the statement Y, we cannot infer if X is (0,x0) or (1,x1).
We now prove that the identiﬁcation protocol I′:= (G′,P′,V ′) is secure against active attacks.
Theorem 19.22. Let (P,V ) be a Sigma protocol for an eﬀective relation Rwith a large challenge
space of the form {0,1}n. Assume that (P,V ) is special HVZK and provides special soundness.
Further, assume that the key generation algorithm G for Ris one-way. Then the identiﬁcation
scheme I′:= (G′,P′,V ′) deﬁned above is secure against active attacks.
In particular, suppose Ais an impersonation adversary attacking I′ via an active attack as in
Attack Game 18.3, with advantage ϵ:= ID3adv[A,I′]. Then there exists an eﬃcient adversary
B(whose running time is about twice that of A), such that
OWadv[B,G] ≥1
2(ϵ2 −ϵ/N),
where N := 2n.
Proof. Let us begin by reviewing how an active impersonation attack against (P′,V ′) works. There
are three phases.
Key generation phase. The challenger runs the key generation algorithm G′, obtaining a public
key pk′= Y and a secret key sk′= (X,Y ), and sends pk′to the adversary A.
Active probing phase. The adversary interacts with the prover P′(sk′). Here, the challenger
plays the role of the prover, while the adversary plays the role of a possibly “cheating” veriﬁer.
The adversary may interact concurrently with many instances of the prover.
Impersonation attempt. As in a direct attack, the adversary now interacts with the veriﬁer
V′(pk′), attempting to make it accept. Here, the challenger plays the role of the veriﬁer, while the
adversary plays the role of a possibly “cheating” prover. In this phase, the adversary (acting as
prover) supplies a commitment, to which the challenger replies (acting as veriﬁer) with a random
challenge. The adversary wins the game if its response to the random challenge yields an accepting
conversation.
So let ϵ be the probability that Awins this game.
We now describe our adversary Bfor breaking the one-wayness assumption for G. To start
with, B’s challenger computes ( y∗,(x∗,y∗)) ←R G() and gives y∗to B. The goal of Bis to compute
a witness for y∗.
Our adversary Bbegins by playing the role of challenger to A, running Aonce through all three
phases. In the key generation phase, Bcomputes (pk′,sk′) = (Y,(X,Y )) as follows:
b←R {0,1}
(y,(x,y)) ←R G()
if b= 0
then Y ←(y,y∗)
else Y ←(y∗,y)
X ←(b,x)
793
Observe that the distribution of ( pk′,sk′) is precisely the same as the output distribution of G′.
After running all three phases, Brewinds Aback to the point in the third phase where the
challenger (as veriﬁer) gave Aits random challenge, and gives to Aa fresh random challenge. If
this results in two accepting conversations with distinct challenges, then by special soundness, B
can extract a witness ˆX = (ˆb,ˆx) for Y. Moreover, if ˆb̸= b, then ˆx is a witness for y∗, as required.
So it remains to analyze B’s success probability. Now, Bsucceeds if it extracts a witness ˆX
from A, and ˆX and X have unequal types. By the Rewinding Lemma (Lemma 19.2), we know that
Bwill extract some witness ˆX from Awith probability at least ϵ2 −ϵ/N. Moreover, we know that
Y by itself reveals nothing about the type of X to A, and witness independence essentially says
that the active probing phase reveals nothing more about the type of X to A. Therefore, for any
particular witness that Bextracts, its type will match that of X with probability 1/2. This means
that B’s overall success probability is at least ( ϵ2 −ϵ/N) ×1
2 , as required.
We can make the above argument about B’s success probability a bit more rigorous, if we like,
using the deﬁnition of witness independence directly (in the form of (19.25)). To this end, we use
the letters X,ˆX,Y to denote random variables, and the letters X, ˆX,Y to denote particular values
that these random variables might take. If Bfails to extract a witness, we deﬁne ˆX := ⊥. If σ is
B’s success probability, then we have
σ= Pr[(ˆX,Y) ∈R′∧type(X) ̸= type(ˆX)].
Using total probability, we sum over all ( ˆX,Y ) ∈R′:
σ=
∑
( ˆX,Y)∈R′
Pr[type(X) ̸= type(ˆX) ∧ˆX = ˆX ∧Y = Y]
=
∑
( ˆX,Y)∈R′
Pr[type(X) ̸= type( ˆX) |ˆX = ˆX ∧Y = Y] ·Pr[ˆX = ˆX ∧Y = Y]
=
∑
( ˆX,Y)∈R′
Pr[type(X) ̸= type( ˆX) |Y = Y] ·Pr[ˆX = ˆX ∧Y = Y] (witness independence)
= 1
2
∑
( ˆX,Y)∈R′
Pr[ˆX = ˆX ∧Y = Y] (independence of Y and type(X))
= 1
2 Pr[(ˆX,Y) ∈R′] ≥1
2(ϵ2 −ϵ/N). 2
Concrete instantiations. The above construction immediately gives us two concrete identiﬁca-
tion protocols that are secure against active attacks. One, derived from Schnorr, whose security is
based on the DL assumption, and the other, derived from GQ, whose security is based from the
RSA assumption. These two actively secure protocols are roughly twice as expensive (in terms of
computation and bandwidth) as their eavesdropping secure counterparts.
19.8.4 Okamoto’s identiﬁcation protocol
We just saw how to build an identiﬁcation protocol whose security against active attacks is based
on the DL assumption. We now look at a more eﬃcient approach, based on Okamoto’s protocol.
Recall Okamoto’s protocol (P,V ) in Section 19.5.1. In addition to the cyclic group G of order q
generated by g∈G, this protocol also makes use of a second group element h∈G, which we view
794
as a system parameter. The most natural key generation algorithm G for this protocol computes
α,β ←R Zq, and outputs pk = u and sk = (( α,β),u) where u := gαhβ ∈G. This gives us the
identiﬁcation protocol IO = (G,P,V ), which we call Okamoto’s identiﬁcation protocol. Using
the concept of witness independence, it is not hard to show that IO is secure against active attacks.
Theorem 19.23. Let IO = (G,P,V ) be Okamoto’s identiﬁcation protocol, and assume that the
challenge space is large. Also, assume that the system parameter h is generated uniformly over G.
Then IO is secure against active attacks, assuming the DL assumption holds for G.
In particular, suppose Ais an impersonation adversary attacking IO via an active attack as in
Attack Game 18.3, with advantage ϵ:= ID3adv[A,IO]. Then there exists an eﬃcient adversary
B(whose running time is about twice that of A), such that
DLadv[B,G] ≥(1 −1/q)(ϵ2 −ϵ/N),
where N is the size of the challenge space.
Proof. The proof has the same basic structure as that of Theorem 19.22.
Suppose Ahas advantage ϵin attacking IO in Attack Game 18.3. Our DL adversary Breceives
a random group element h ∈G from its challenger. The goal of Bis to compute Dloggh, making
use of A.
Our adversary Bbegins by playing the role of challenger to A, running Aonce through all three
phases of Attack Game 18.3. Our adversary Buses the group element has the system parameter for
Okamoto’s protocol, but otherwise follows the logic of the challenger in Attack Game 18.3 without
modiﬁcation:
Key generation phase. Bcomputes α,β ←R Zq, u←gαhβ, and sends the public key pk := u to A,
keeping the secret key sk := ((α,β),u) to itself.
Active probing phase. A interacts (possibly concurrently) with several instances of the prover
P(sk). The role of these provers is played by B.
Impersonation attempt. Atries to make the veriﬁer V(pk) accept. The role of the veriﬁer is played
by B.
After running all three phases, Brewinds Aback to the point in the third phase where the
veriﬁer gave Aits random challenge, and gives to Aa new, random challenge. If this results in two
accepting conversations with distinct challenges, then by special soundness, Bcan extract a witness
(ˆα,ˆβ) for u. Moreover, if ( α,β) ̸= (ˆα,ˆβ), then we have two distinct representations (relative to g
and h) of u, and therefore, Bcan compute Dloggh as in Fact 10.3.
Our adversary Bsucceeds if it extracts a witness from Athat is diﬀerent from ( α,β). By
the Rewinding Lemma (Lemma 19.2), we know that Bwill extract some witness from Awith
probability at least ϵ2 −ϵ/N. Moreover, u by itself reveals nothing about which of the q possible
witnesses for uthat Bis using, and witness independence says that the active probing phase reveals
nothing more about this witness to A. Therefore, for any particular witness that Bextracts from
A, the probability that it is equal to ( α,β) is 1/q. This means that B’s overall success probability
is at least ( ϵ2 −ϵ/N) ×(1 −1/q), as required. 2
795
19.9 Multi-extractability: another notion of “proof of knowledge”
In Section 19.4.1 we noted that Sigma protocols such as Schnorr’s acts as a “proof of knowledge”,
meaning that a witness can be extracted from a prover using a particular rewinding technique.
We used this rewinding technique to prove the security of Schnorr’s identiﬁcation and signature
schemes (and similar schemes based on more general Sigma protocols).
The notion of a “proof of knowledge” is often used as a step towards proving security of some
candidate scheme (such as security of a signature scheme). We have not attempted to give a general
deﬁnition for a “proof of knowledge” because the exact deﬁnition can depend to a certain degree
on the application at hand, and on the exact statement one is trying to prove.
In this section we explore an interesting “proof of knowledge” property that Sigma protocols
satsify called multi-extractability. This property is also based on a rewinding technique, but
the technique is diﬀerent than the rewinding technique used to prove the security of Schnorr’s
identiﬁcation and signature schemes. We shall discuss some applications and limitations of multi-
extractability.
Intuitively, multi-extractability means the following. Suppose that during an attack, an adver-
sary is allowed to interact concurrently with many veriﬁers, supplying each veriﬁer with a statement
of the adversary’s choice, and at the end of the attack, the adversary makes some of these veriﬁers
accept. Then we can “shake the adversary” (i.e., “rewind” or “re-run” the adversary) over and
over again until it ﬁnally “spits out what it knows” (i.e., we extract witnesses for all statements
supplied to the accepting veriﬁers).
19.9.1 Multi-extractable Sigma protocols
Let Π be a Sigma protcol for an eﬀective relation R⊆X×Y . We assume that conversations
are of the form ( t,c,z ), where t ∈ Tis a “commitment”, c ∈ Cis a “challenge”, and z ∈ Z
is a “response”. Suppose Ais an adversary that makes a series of queries to a veriﬁer oracle ,
where each query consists of a statement and a commitment, to which the veriﬁer oracle responds
with a random challenge. That is, for i = 1 ,2,..., the ith veriﬁer oracle query is of the form
(yi,ti) ∈Y×T , to which the veriﬁer oracle responds with a random challenge ci ∈C. Suppose that
the adversary makes a total of r such queries. After this, Aoutputs
(J, {zj}j∈J),
where J⊆{ 1,...,r }, and for each j ∈J, the conversation (tj,cj,zj) is an accepting conversation
for the statement yj.
Roughly speaking, multi-extractability means that there is an eﬃcient algorithm, called a
multi-extractor, that can be used to augment the execution of Aso that for each j ∈J , it
also outputs a witness xj for yj. The multi-extractor does so by “rewinding” or “re-running” A,
and is allowed to fail with some prescribed failure probability θ. Moreover, the number of times it
“rewinds” or “re-runs” the adversary should not be too large (i.e., bounded by a polynomial 1 /θ
and the number of veriﬁer oracle queries).
We now deﬁne multi-extractability more precisely. We say that Ais B-bounded if for all
executions of A, it makes at most r ≤ B veriﬁer oracle queries. We shall assume that Ais
probabilistic. However, for technical reasons, we shall treat Aas a deterministic algorithm that
explicitly requests random bits from a random bit generator. However, these random bit requests
need not be made at the beginning of A’s execution — they may be arbitrarily interleaved with
796
A’s veriﬁer oracle queries. The way a multi-extractor MEx is allowed to interact with an adversary
Aas above is described in the following experiment.
Multi-extraction experiment. This experiment involves a B-bounded adversary Aas above, a
multi-extractor MEx, and a prescribed failure probability θ.
Stage 1: We initially run A to completion, responding to veriﬁer oracle queries with random
challenges, and fulﬁlling all random bit requests with random bits.
Stage 2: We initialize MEx with B and θ, along with a transcript of A’s initial run. This tran-
script consists of a list of A’s veriﬁer oracle query/response pairs((y1,t1),c1),..., ((yr,tr),cr),
as well as A’s output (J,{zj}j∈J).
Recall that A’s output must satisfy J ⊆{1,...,r }, and for each j ∈J , the conversation
(tj,cj,zj) is an accepting conversation for yj. The transcript does not include any information
about A’s random bit requests.
The multi-extractor MEx is now allowed to re-run Amany times. In each such re-run, A
speciﬁes a rewinding index j ∈J, and then we do the following:
• we rewind A’s execution to the state in the initial run at the point where it makes its
jth veriﬁer oracle query, and then run it forward:
– responding to the jth and subsequent veriﬁer oracle queries with fresh random chal-
lenges;
– fulﬁlling subsequent random bit requests with fresh random bits;
• when Aﬁnishes, we give MEx the transcript from this re-run of A.
After performing all of these re-runs, MEx either outputs fail or {xj}j∈J, where xj is a witness
for yj for each j ∈J.
Note that if in the initial run, Adoes not output anything (i.e., J = ∅), then MEx need not
produce any witnesses (i.e., MEx may output ∅, rather than fail).
Deﬁnition 19.9 (Multi-extractability). A Sigma protocol Π is called multi-extractable if
there exists a multi-extractor MEx such that for every
(a) poly-bounded B,
(b) prescribed failure probability θ with 1/θ poly-bounded,
(c) B-bounded eﬃcient adversary A,
there exists a negligible value η such that in the above multi-extraction experiment, we have:
(i) the probability that MEx fails is at most θ+ η,
(ii) the number of times that MEx re-runs Ain the worst case is bounded by a ﬁxed polynomial
in 1/θ and B (and the security parameter), and
(iii) the running time of all other computations performed by MEx is bounded in the worst case by
a ﬁxed polynomial in 1/θ and B (and the security parameter).
797
When the multi-extraction experiment is run under the constraints of Deﬁnition 19.9, the entire
experiment will be executed in time polynomial in the implicit security parameter. This means
that in designing MEx, we can make computational assumptions, if necessary. The negligible “error
term” ηin condition (i) of the deﬁnition can be used, for example, to model a situation whereMEx’s
failure probability is at most θ, except if some failure condition associated with the computational
assumption occurs (see Exercise 19.32). It can also be used to simply model a situation where
MEx’s failure probability is unconditionally at most θ for all suﬃciently large values of the security
parameter (which we will exploit below in Theorem 19.24).
The above deﬁnition of multi-extractability has a number of very particular features. These
are included (a) because they can be achieved in a number of settings, and (b) because they are
suﬃcient to prove some interesting results. As we already said, there is no one “right” deﬁnition
for a “proof of knowledge”, but this one is nice because it is useful in some interesting applications.
The next theorem says that under appropriate conditions, Sigma protocols are multi-extractable.
Theorem 19.24 (Sigma protocols are multi-extractable). Let Π be Sigma protocol with a
large challenge space that provides special soundness. Then Π is multi-extractable.
In particular, there exists a multi-extractor, such that for every B-bounded adversary, and for
every prescribed error probability θ, multi-extractor re-runs the adversary O(B/θ) times, and
fails with probability at most θ provided θ≥2B/N, where N is the size of Π’s challenge space.
Proof. We have a Sigma protocol Π for a relation consisting of pairs (x,y), where xis a witness and
yis a statement. A conversation for Π is a triple ( t,c,z ), where ccomes from a challenge space Cof
size N, where 1/N is negligible. We are assuming that Π provides special soundness, which means
that there is an eﬃcient algorithm that given two accepting conversations ( t,c,z ) and (t,c′,z′) for
a statement y, where c̸= c′, outputs a witness x for y.
Assume a B-bounded adversary Athat makes a sequence of veriﬁer oracle queries, where the
query/response pairs are
((y1,t1),c1),..., ((yr,tr),cr),
and outputs (J,{zj}j∈J), where J⊆{ 1,...,r }, and for each j ∈J, the conversation (tj,cj,zj) is
an accepting conversation for yj.
Any run of Ais completely determined by the random bits it requests and the challenge vector
(c1,...,c B),
where ci is the response to the ith veriﬁer oracle query, for i= 1,...,B .
Our goal is to design a multi-extractor MEx that works as in the multi-extraction experiment
above. At the end, MEx should output either fail or {xj}j∈J, where xj is a witness for yj for
each j ∈J, corresponding to the oracle query/response pairs (( y1,t1),c1),..., ((yr,tr),cr) and A’s
output (J,{zj}j∈J) in the initial run of A.
Our multi-extractor MEx will be given the bound B and a prescribed failure probability θ, and
will re-run Aat most ⌈2B/θ⌉times in total. Moreover, MEx will fail with probability at most θ
assuming
θ≥2B
N . (19.28)
(Note that since 1 /N is negligible, and 1 /θ and B are poly-bounded, (19.28) will hold for all
suﬃciently large values of the security parameter.)
MEx works as follows:
798
• Let (c1,...,c B) be the challenge vector of A’s initial run.
• MEx processes each output zj for j ∈J in its initial run as follows:
– MEx then repeatedly re-runs Awith rewinding index j. In each such re-run, the corre-
sponding challenge vector is
( c1,...,c j−1, ˜cj,..., ˜cB ),
where c1,...,c j−1 are the same as in the initial run, and ˜ cj,..., ˜cB are fresh random
values, chosen uniformly and independently from the challenge space C. Note also that
in each re-run, the random bits obtained by Aup through the jth veriﬁer oracle query
are the same as in the initial run, while subsequent random bits are freshly generated.
In particular, in each such re-run, the jth veriﬁer oracle query is the same as in the
initial run, namely, (yj,tj).
– This is repeated until a challenge ˜ cj is found such that ˜ cj ̸= cj and A outputs
(K,{˜zk}k∈K) such that Kcontains j, which means that such that (tj,˜cj,˜zj) is an accept-
ing conversation for the statement yj. By special soundness, this allows MEx to extract
a witness xj for the statement yj.
• The above procedure is carried out for each j ∈J output by A. If the total number of re-runs
reaches ⌈2B/θ⌉without ﬁnding all required witnesses, MEx stops and outputs fail.
Claim 1. MEx fails with probability at most θ, assuming (19.28).
To prove this, let us ﬁrst consider a modiﬁed version of MEx, which we call MEx1. The only
diﬀerence between MEx and MEx1 is that MEx1 implements the failure condition a bit diﬀerently.
Instead of failing if the total number of re-runs reaches ⌈2B/θ⌉without ﬁnding all witnesses, it
works as follows:
for each j ∈J, it re-runs Arepeatedly with rewinding index j at most ⌈2B/θ⌉times or
until it ﬁnds a witness, whichever comes ﬁrst.
The reason for considering MEx1 is that its behavior is easier to model mathematically, as the
attempt to extract a witness for one statement has no impact on the the attempt to to extract a
witness for another statement.
Claim 2. The expected value of the total number of re-runs performed by MEx1 is at most 2 B,
assuming (19.28).
Claim 1 follows from Claim 2. Indeed, we ﬁrst observe that MEx and MEx1 proceed identically
until such point that MEx fails. Moreover, if MEx fails, then MEx1 performs at least ⌈2B/θ⌉re-runs,
and by Claim 2 and Markov’s inequality, this happens with probability at most θ.
We now prove Claim 2. For i = 1,...,B , we consider the number Ri of re-runs that the ith
veriﬁer oracle query may generate. We will show that the expected value of Ri is at most 2, from
which the claim will follow.
To show that this holds, we show that it holds conditioned on any ﬁxed values of
c1,...,c i−1,ρ
799
where c1,...,c i−1 represent the ﬁrst i−1 veriﬁer oracle responses of A’s initial run, and ρ is a bit
string that represents the random bits obtained by Ain its initial run up to its ith veriﬁer oracle
query. We may assume that the ﬁxed values c1,...,c i−1,ρ lead Ato make an ith veriﬁer oracle
query (yi,ti), which is completely determined by these ﬁxed values.
Starting from the state determined by the ﬁxed values c1,...,c i−1,ρ, the remainder of A’s
execution is determined by the random variables
ci,ci+1 ...,c B,ρ′
  
=:d
,
where ci,ci+1,...,c B represent the remaining veriﬁer oracle responses, and ρ′ is a bit string that
represents the remaining random bits obtained byA. We call the random variable (ci,d) a continua-
tion. Deﬁne the random variable G(ci,d) := 1 if the continuation (ci,d) makes Agenerate an output
of the form ( J,·) such that J contains i, and G(ci,d) := 0 otherwise. Let γ := Pr[G(ci,d) = 1].
Now let (ci,d) denote A’s continuation in its initial run. Then by total expectation, we have
E[Ri] = E[Ri |G(ci,d) = 1] Pr[G(ci,d) = 1] + E[Ri |G(ci,d) = 0] Pr[G(ci,d) = 0]
= E[Ri |G(ci,d) = 1] ·γ,
since Ri = 0 whenever G(ci,d) = 0.
There are two cases to consider.
Case 1: γ <2/N.
If G(ci,d) = 1 occurs, MEx1 will re-run Aat most ⌈2B/θ⌉times. So we have
E[Ri] = E[Ri |G(ci,d) = 1] ·γ <⌈2B/θ⌉· 2
N,
since γ <2/N. Moreover,
⌈2B/θ⌉· 2
N ≤2 ⇐⇒ ⌈2B/θ⌉≤ N ⇐⇒2B/θ ≤N,
which holds by (19.28).
Case 2: γ ≥2/N.
In any re-run of A, we run Ato completion using an independent continuation (˜ci, ˜d). More-
over, we have
Pr[G(ci,d) = 1 ∧G(˜ci, ˜d) = 1 ∧˜ci ̸= ci] ≥γ2 −γ/N.
This follows from Exercise 19.5(a). So we have
Pr[G(˜ci, ˜d) = 1 ∧˜ci ̸= ci |G(ci,d) = 1] ≥γ−1/N ≥γ/2,
since γ ≥2/N. It follows that
E[Ri |G(ci,d) = 1] ≤2/γ.
Therefore
E[Ri] = E[Ri |G(ci,d) = 1] ·γ ≤(2/γ) ·γ = 2.
That completes the proof of the theorem. 2
800
19.9.2 Applications and limitations
To illustrate some applications of multi-extractability, as well as its limitations, we consider a
“folklore” construction for a chosen ciphertext secure encryption scheme.
Recall from Chapter 12 that a public key encryption scheme is secure against chosen ciphertext
attack, or CCA secure, if an adversary cannot learn anything about an unknown message mgiven
its encryption c, even the adversary is allowed to query an “encryption oracle” that will decrypt
any ciphertexts given to it other than c itself.
The folklore construction is a technique called “encrypt then prove”. The idea is to start with
a semantically secure encryption scheme and augment each ciphertext with “proof of knowledge”.
The “proof of knowledge” is derived from an appropriate Sigma protocol using the Fiat-Shamir
heuristic (see Section 19.6.1). Assuming the Sigma protocol is honest veriﬁer zero knowledge
(HVZK), the proof should not leak any information about the plaintext. The “proof of knowledge”
property should, at least intuitively, ensure that interacting with a decryption oracle does not help
an attacker, since the attacker must already “know” what the response from the decryption oracle
will be. As we will see, in some limited attack scenarios, this is indeed the case; however, in a
general CCA attack, a proof of this seems out of reach (although we know of no attack, either).
19.9.2.1 A concrete “encrypt then prove” scheme
To make things concrete, let us start with the multiplicative ElGamal scheme EMEG. We have a
cyclic group G of prime order q generated by g∈G.
• The secret key is a random α∈Zq and the public key is u:= gα ∈G.
• An encryption of m∈G is (v,e) ∈G2, computed as β ←R Zq, v←gβ, w←uβ, e←w·m.
• A ciphertext (v,e) ∈G2 decrypts to m∈G, computed as w←vα, m←e/w.
We know that EMEG is semantically secure under the DDH (see Exercise 11.5) but is completely
insecure against a chosen ciphertext attack (see Exercise 12.1).
We now augment EMEG so that a ciphertext is of the form ( v,e,π ), where π is a “proof of
knowledge” of β ∈Zq such that v= gβ. Intuitively, if the encryptor “knows” β, then he must also
“know” the plaintext m = e/uβ. When decrypting such a ciphertext, we always check that the
proof is valid before computing the plaintext.
To design such a “proof of knowledge”, we start with a Sigma protocol Π for the relation
R= {(β,(u,v,e )) ∈Zq ×G3 : gβ = v }.
For this, we can essentially use Schnorr’s Sigma protocol (see Example 19.1). The challenge space
Cis a subset of Zq of size N, and we will assume that 1 /N is negligible. Π works as follows:
1. the prover computes βt ←R Zq, vt ←gβt, and sends vt to the veriﬁer;
2. the veriﬁer computes c←R C, and sends c to the prover;
3. the prover computes βz ←βt + αc∈Zq, and sends βz to the veriﬁer;
4. the veriﬁer checks that gβz = vt ·vc.
801
Π provides special soundness and is special HVZK (just as we observed in Section 19.4 for the
Sigma protocol corresponding to Schnorr’s identiﬁcation protocol).
In applying the Fiat-Shamir heuristic to Π, the challenge will be computed by applying a hash
function H to the statement ( u,v,e ) together with the ﬁrst ﬂow vt of the Sigma protocol. From
this, we obtain the augmented multiplicative ElGamal encryption scheme EaMEG:
• The secret key is a random α∈Zq and the public key is u:= gα ∈G.
• An encryption of m∈G is (v,e,π ) = (v,e, (vt,βz) ) ∈G ×G ×(G ×Zq), computed as
β ←R Zq, v←gβ, w←uβ, e←w·m,
and
βt ←R Zq, vt ←gβt, c←H( (u,v,e ), vt ), βz ←βt + βc.
• A ciphertext (v,e,π ) = (v,e, (vt,βz) ) ∈G ×G ×(G ×Zq) is decrypted as follows:
c←H( (u,v,e ), vt )
if gβz = vt ·vc
then w←vα, m←e/w, output m
else output reject.
Note that the only place the group element e∈G in a ciphertext (v,e,π ) is used in constructing
or verifying the proof π is as an input to the hash H. This is essential, however, as otherwise the
encryption scheme EaMEG would be trivially insecure against a chosen ciphertext attack. Indeed,
in this case, given an encryption of ( v,e,π ) of m, an attacker could ask the decryption oracle for a
decryption of ( v,e ·g,π), obtaining m·g, from which it can compute m. Including e in the hash
prevents this type of attack. But the question remains: can we actually prove that EaMEG is CCA
secure (under the DDH assumption, modeling H as a random oracle)?
19.9.2.2 Security properties of “encrypt then prove”
We now examine in more detail the question of whether the “encrypt then prove” construction
yields a CCA encryption scheme. To keep things concrete, we focus on the scheme EaMEG in
Section 19.9.2.1 presented above, although all of our analysis quite generally applies to any “encrypt
then prove” based encryption scheme. (See Exercise 20.28.)
The short answer to this question is that while there is no known chosen ciphertext attack on
EaMEG, it seems unlikely that we can prove this; however, we can prove security of EaMEG under a
very restricted form of chosen ciphertext attack.
Deﬁnition of CCA security. Before going further, let us recall more precisely what it means for
EaMEG to be CCA secure, adapting the notation and terminology of Chapter 12 to our particular
setting. We shall work with the bit-guessing version of 1CCA security deﬁned in Section 12.1,
which is equivalent to general CCA security. In the attack game deﬁning 1CCA security where H
is modeled as a random oracle, we have an adversary Athat is allowed to interact with a challenger
as follows.
Key generation step: ﬁrst, the challenger generates a random secret key α∈Zq, computes the
public key u←gα ∈G, and sends u to A.
802
Pre-challenge phase: second, Asubmits a series of decryption queries to the challenger; each
such query is a ciphertext ( v∗,e∗,π∗), to which the challenger responds with the decryption
of (v∗,e∗,π∗) under the secret key α (invoking the random oracle to validate π∗).
Challenge step: third, Asubmits a single encryption query (m0,m1) ∈G2 to the challenger; the
challenger chooses a random b∈{0,1}, encrypts mb under the public key u, computing the
“challenge ciphertext” (v,e,π ) (invoking the random oracle to compute π), and sends (v,e,π )
to A.
Post-challenge phase: fourth, the adversary again submits a series of decryption queries to the
challenger; each such decryption query is a ciphertext ( v∗,e∗,π∗) ̸= ( v,e,π ), to which the
challenger responds with the decryption of ( v∗,e∗,π∗) under the secret key α (invoking the
random oracle to validate π∗).
Output step: at the end of the game, the adversary outputs a bit ˆb∈{0,1}.
Random oracle queries: the adversary may query the random oracle directly, any number of
times, at any point in the game between the key generation and output steps.
Adversary A’s 1CCA bit-guessing advantageis deﬁned to be |Pr[ˆb= b]−1/2|. 1CCA security
for EaMEG means that A’s 1CCA bit-guessing advantage is negligible for all eﬃcient adversaries A.
Parallel 1CCA security. One can prove that EaMEG satisﬁes a weaker notion of 1CCA security
called parallel 1CCA security. This notion of security restricts the adversary’s ability to query
the decryption oracle. In this restricted attack game, the adversary is allowed to request the
decryption of many ciphertexts, but is required to submit all of these ciphertexts as a single batch .
So for EaMEG, this means the adversary is allowed to make only one query to the decryption oracle
of the form
(v∗
1,e∗
1,π∗
1), ..., (v∗
n,e∗
n,π∗
n),
and the decryption oracle will decrypt all of these. This decryption request can come either before
or after the challenge step in the attack game — if it comes after, and the target ciphertext is
(v,e,π ), then we must have (v∗
k,e∗
k,π∗
k) ̸= (v,e,π ) for each k= 1,...,n . Without loss of generality,
we may assume that each ( v∗
k,e∗
k) is distinct and that each proof π∗
k is valid.
Theorem 19.25. EaMEG is parallel 1CCA secure under the DDH assumption, if we model H as a
random oracle.
Proof sketch. Since the Sigma protocol Π provides special soundness and the challenge space is
of size N, where 1 /N is negligible, Theorem 19.24 applies, and so this Sigma protocol is multi-
extractable. So there is a multi-extractor MEx for Π that satisﬁes Deﬁnition 19.9.
We know that the non-augmented scheme EMEG is semantically secure under the DDH assump-
tion. So to prove the theorem, we shall show that if the augmented scheme EaMEG is not parallel
1CCA secure, then the non-augmented scheme EMEG is not semantically secure. To this end, as-
sume EaMEG is not parallel 1CCA secure. This means there exists there exists an eﬃcient adversary
Awhose bit-guessing advantage in the parallel 1CCA attack game is non-negligible. This implies
there exists δsuch that 1/δis poly-bounded and A’s advantage is at least δ. (Technically speaking,
A’s advantage is at leastδfor inﬁnitely many settings of the security parameter (see Example 2.11),
803
but this is suﬃcient for our purposes.) Our goal is to use Ato build another eﬃcient adversary B
that breaks the semantic security of EMEG.
Recall that in the parallel 1CCA attack game deﬁned above,Ais allowed to make one encryption
query and one batch decryption query, which may come either before or after the encryption query.
We shall assume that Amakes its batch decryption query in the after its encryption query. This
is the most interesting case to consider, and we leave the other case to the reader.
Let B be a bound on the number of random oracle queries that Amakes, so that B is also
poly-bounded. For simplicity, we assume that Aqueries the random oracle at most once on any
given input. Without loss of generality, we assume thatAveriﬁes the proofs in all of the ciphertexts
in its batch decryption query, and only submits ciphertexts with valid proofs. In particular, we
assume that Ahas already queried the random oracle at all inputs required to validate these proofs
prior to making its batch decryption query. We also assume that Amakes all of its own random
choices based on a random bit string ρA, and is otherwise deterministic.
We begin by constructing a bounded adversary A′ that we can use in the multi-extractability
experiment deﬁned above. Recall that such an adversary must make explicit requests for random
bits, and is otherwise deterministic. The essential idea is that A′ play the role of both Aand the
challenger in the 1CCA attack game, and will forward all random oracle queries made by Ato its
own veriﬁer oracle – there is one exception to this, as indicated below in Step 4. In more detail,
adversary A′runs as follows:
Step 1: A′requests random bit strings ρAand ρkg. The string ρAis the randomness for A, while
ρkg is the randomness for the key-generation step in the 1CCA attack game.
A′ then runs the key generation step of the 1CCA attack game, computing the secret key α
from ρkg, and sending the public key u:= gα to A. During this time, A′forwards all random
oracle queries to its own verifer oracle.
Step 2: A′then runs Aup to the point where A′makes its encryption query ( m0,m1) ∈G2.
Step 3: At this point (and no earlier!), A′ requests a random string ρenc used by the challenger
in the 1CCA attack game to process the encryption query. Speciﬁcally, ρenc is used to derive
the random bit b chosen by the challenger, as well as the random value β used in the non-
augmented ciphertext (v,e).
Next, A′requests a random string ρprv that is used to generate a simulated proof π= (vt,βz)
using the HVZK property of Schnorr’s Sigma protocol. Speciﬁcally, ρprv is used to generate
random c∈C and βz ∈Zq, and eﬀectively sets the value of the random oracle H at the point
((u,v,e ),vt) to c, where vt ←gβz/vc. This step my fail (but only with negligible probability)
if Apreviously queried H at this same point (in which case A′will abort).
A′then sends (v,e,π ) to A.
Step 4: Finally, A′runs Aup to the point where Amakes its batch decryption query
(v∗
1,e∗
1,π∗
1), ..., (v∗
n,e∗
n,π∗
n).
During this time, A′forwards all random oracle queries to its own verifer oracle, except that
if Asubsequently queries H at the point (( u,v,e ),vt) used to construct the proof in Step 3,
A′will return the corresponding value c to A.
Under our assumptions, and by the fact that Schnorr’s Sigma protocol has unique responses
(see Exercise 19.14), the following holds:
804
for each k= 1,...,n , if π∗
k = (v∗
tk,β∗
zk), then there is a unique veriﬁer oracle query
of the form (( u,v∗
k,e∗
k),v∗
tk) — in particular, (( u,v∗
k,e∗
k),v∗
tk) ̸= ((u,v,e ),vt).
For each such veriﬁer oracle query, A′ outputs a corresponding value β∗
zk. More precisely,
suppose that for each k = 1 ,...,n , the kth ciphertext corresponds to the j(k)th veriﬁer
oracle query. Then Aoutputs (J,{zj}j∈J), where J= {j(k) : k= 1,...,n }and zj(k) = β∗
zk
for k= 1,...,n .
Now consider the execution of the multi-extraction experiment with the B-bounded adversary
A′and multi-extractor MEx with prescribed error probability θ:= δ/2. In this experiment, we run
A′ to completion, and then we run MEx, which either outputs corresponding witnesses β∗
1,...,β ∗
n
or fails. The probability that this experiment fails is at most δ/2 plus some negligible amount. If
the experiment succeeds, then from these witnesses, we can readily decrypt all of the ciphertexts
(v∗
1,e∗
1,π∗
1), ..., (v∗
n,e∗
n,π∗
n).
Note that A′only makes veriﬁer oracle queries in Steps 2 and 4. This implies that in any re-run
of A′, we always use the original string ρkg from the initial run of A, and we either
• start from a veriﬁer oracle query made in Step 2, running Step 3 ofA′using a freshly generated
random strings ˜ρenc and ˜ρprv, in place of the original strings ρenc and ρprv from the initial run
of A′, or
• start from a veriﬁer oracle query made in Step 4, and use the original strings ρenc and ρprv
from the initial run of A′.
It follows that we can convert the multi-extraction experiment with A′into an adversary Bthat
attacks the semantic security of the non-augmented scheme EMEG as in the bit-guessing version
of Attack Game 11.1. The idea is that the random strings ρkg and ρenc in the multi-extraction
experiment are moved to the challenger in the the bit-guessing version of Attack Game 11.1, while
the remaining logic of the multi-extraction experiment is moved to B. Adversary B uses the
extracted witnesses β∗
1,...,β ∗
n to decrypt the ciphertexts submitted by Afor decryption. If this
procedure fails for any reason, Boutputs a random bit; otherwise, Bgives the decryptions of these
ciphertexts to Aand outputs whatever Aoutputs. Adversary Bre-runs Aat most O(B/δ) times
and has bit-guessing advantage at least δ/2 −ϵ for some negligible quantity ϵ. (Again, technically
speaking, the advantage of Bis at least δ/2−ϵfor inﬁnitely many settings of the security parameter,
which is suﬃcient for our purposes.) 2
Note that the security reduction in the proof of Theorem 19.25 is very loose: given an adversary
Aattacking EaMEG in the parallel 1CCA attack game with advantageδand making Brandom oracle
queries, we get an adversary that breaks the DDH with advantage roughly δ/2, but which has to
re-run Aon the order of B/δ times.
Parallel 1CCA security allows an attacker to make one encryption query and one batch de-
cryption query. Analogous to Theorem 12.1, we can use a simple hybrid argument to show that
this implies security against an attacker who can make many encryption queries and one batch
decryption query (but this makes the security reduction for EaMEG even looser).
It also turns out that parallel 1CCA security is formally equivalent to the property of non-
malleability, which we discussed informally in Section 12.2.1 (but to prove this, one ﬁrst needs a
formal deﬁnition of non-malleability, which we do not present here). In any case, we have already
805
seen in Chapter 12 very eﬃcient encryption schemes that provide full CCA security with a much
tighter reduction under similar assumptions. Nevertheless, EaMEG has some some properties that
are still useful — we will later examine a very similar encryption scheme in the context of voting
protcols in Section 20.3.1. However, there are very similar encryption schemes which achieve full
CCA security under the DDH assumption with a much tighter security reduction, and which are
only a bit less eﬃcient (see Exercise 20.20).
On the limitations of rewinding techniques. While we have shown that EaMEG is parallel
1CCA secure, it seems impossible to prove that it provides full CCA security, at least using the
rewinding techniques that we have developed. We will illustrate the obstruction to proving full
CCA security with an example. While this example is very speciﬁc, it highlights a fundamental
and very general limitation of using rewinding techniques in security proofs.
In the full CCA attack game, the adversary is allowed to make a sequence of possibly dependent
decryption queries. That is, each decryption query may depend on the result of the previous
decryption queries. At the very least, the adversary may simply refuse to submit its next decryption
query before it obtains replies to its earlier queries. So if we want to use some kind of technique to
extract the plaintext from the ciphertext, we will have to extract the plaintext immediately for each
decryption query — we cannot wait, as we did in the proof of Theorem 19.25, for the adversary to
ﬁrst submit all of its decryption queries. However, if we try to use a rewinding technique as we did
in the multi-extractor presented in Theorem 19.24, the running time of the multi-extractor blows
up exponentially.
As promised, here is the example that illustrates the issue. Consider an adversary that submits
a sequence of ciphertexts to its decryption oracle in the CCA attack game for EaMEG:
(v∗
1,e∗
1,π∗
1),..., (v∗
n,e∗
n,π∗
n).
Each ciphertext encrypts a message m∗
k, so that v∗
k = gβ∗
k and e∗
k = uβ∗
km∗
k. Each proof π∗
k is of the
form
(v∗
tk,β∗
zk) = (gβ∗
tk,β∗
tk + ckβ∗
k),
where the corresponding random oracle query is (( u,v∗
k,e∗
k),v∗
tk), which deﬁnes the challenge ck =
H((u,v∗
k,e∗
k),v∗
tk). The adversary will prepare these ciphertexts ahead of time, in reverse order, so
that the kth ciphertext depends on the challenges ck+1,...,c n. In particular, the adversary will
generate the kth ciphertext using the usual encryption algorithm, except that m∗
k, β∗
k, and β∗
tk are
computed as a cryptographic hash of ck+1,...,c n and a secret key known only to the adversary.
Fig. 19.11 shows a diagram illustrating the attack.
Now imagine how a reduction to semantic security would work based on rewinding techniques.
When the adversary makes his ﬁrst decryption query ( v∗
1,e∗
1,π∗
1), we could rewind the adversary
back to the point where he obtained the challenge c1, feed the adversary a fresh random challenge
c′
1, and run him forward and hope that he submits an appropriate ciphertext that will allow us to
extract β∗
1 (via special soundness) and hence m∗
1 = e∗
1/uβ∗
1 . In general, we would have to be rather
lucky for this to happen, but in this example we will succeed (with overwhelming probability) on
the ﬁrst try.
Now we run the adversary forward until he makes his second decryption query ( v∗
2,e∗
2,π∗
2).
As above, we rewind the adversary to the point where he obtained the challenge c2 and run him
forward with fresh random challenges c′
2,c′
1. Unfortunately, the challenge c2 was obtained before
the adversary made his ﬁrst decryption query, so when we run him forward again, he will make
806
((u,v∗n,e∗n),v∗
tn)−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→random oraclecn←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−...
((u,v∗k,e∗k),v∗
tk)−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→random oracleck←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−...
((u,v∗1,e∗1),v∗
t1)−−−−−−−−−−−−−−−−−−→random oraclec1←−−−−−−−−−−−−−−−−−−
(v∗1,e∗1,π∗1)−−−−−−−−−−−−−−−−−−→decryption oraclem∗1←−−−−−−−−−−−−−−−−−−...
(v∗k,e∗k,π∗k)−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→decryption oraclem∗k←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−...
(v∗n,e∗n,π∗n)−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→decryption oraclem∗n←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
Figure 19.11: Attack on EaMEG
another “ﬁrst” decryption query for a ciphertext ( v∗∗
1 ,e∗∗
1 ,π∗∗
1 ) before he ever gets to the second
decryption query; therefore, we must ﬁrst somehow extract the plaintext from (v∗∗
1 ,e∗∗
1 ,π∗∗
1 ). More-
over, (v∗∗
1 ,e∗∗
1 ,π∗∗
1 ) was computed by the adversary as a function of c′
2,c3,...,c n, and so will not
be related at all to ( v∗
1,e∗
1,π∗
1), which was computed as a function of c2,c3,...,c n. Given that our
only tool is rewinding, we will have to recursively rewind the adversary just to extract the plaintext
from (v∗∗
1 ,e∗∗
1 ,π∗∗
1 ), which will allow us to then extract the plaintext from ( v∗
2,e∗
2,π∗
2).
It is not too hard to see that the above rewinding strategy takes time exponential in n. Indeed,
let us just count the number of times we run the special-soundness witness extractor. Let f(k) be
the number of times we runs the extractor in order to respond to the ﬁrst k decryption queries.
Then we have
f(0) = 0 and f(k) = 2f(k−1) + 1 for k≥1,
since to respond to the kth decryption query, we ﬁrst have to respond to the ﬁrst k−1 decryp-
tion queries on the main execution path, and then we have to respond to the ﬁrst k−1 decryption
queries on a second, completely independent execution path, and then we get two accepting conver-
sations from which we extract the witness for the kth decryption query using the special-soundness
extractor. It follows that f(n) = 2n −1.
The above is by no means a proof that EaMEG is not secure against a CCA attack. Indeed, there
is no known chosen ciphertext attack against EaMEG, and in fact, one can prove that EaMEG is CCA
secure if we model H as a random oracle and model G as a “generic group” (see Section 16.3).
However, it remains an open question as to whether EaMEG is CCA secure modeling H as a random
oracle, under some concrete security assumption for G.
We saw how the rewinding technique can blow-up exponentially if we use it to try to prove that
807
the “encrypt then prove” construction is CCA secure. The same type of exponential blow-up can
occur if we try to analyze the security of a system in which we have many concurrent instances of
a protocol that uses such a “proof of knowledge”. As we try to rewind one instance of the protocol
to extract a witness, this can lead to a cascade of rewinding just as we saw above.
Another subtle issue related to rewinding techniques in particular, and “proofs of knowledge”
more generally, is that of determining who really knows something. For example, suppose we have
an adversary Awho is interacting with some honest party Z. If Ahas a “proof of knowledge” π
for a witness x for some statement y, it may be the case that Asimply got that proof from Z;
therefore, while Zmay well know x, adversary Aitself may not have any idea of what x is. We
already saw an example of this in the proof of Theorem 19.25. There, the party Zcorresponds to
the encryption oracle in the parallel 1CCA attack game. The proof of that theorem still worked
because
(a) in the parallel 1CCA attack game, we were not required to extract a witness for the statement
produced by the encryption oracle and so did not have to rewind the encryption oracle itself,
which would have invalidated our security proof of that theorem, and
(b) although we may have had to rewind the adversary back to a point before it queried the
encryption oracle, in those rewindings we eﬀectively re-ran the encryption oracle in its entirety
using fresh randomness (which is the reason that in our deﬁnition of multi-extractability,
we explicitly modeled when randomness was generated and did not assume that it was all
generated at the beginning).
It was precisely these types of subtleties that introduced some of the complexity in Deﬁnition 19.9.
19.10 A fun application: a two round witness independent proto-
col
To be written.
19.11 Notes
Citations to the literature to be added.
19.12 Exercises
19.1 (Bad randomness attack on Schnorr signatures). Let (sk,pk) be a key pair for the
Schnorr signature scheme (Section 19.2). Suppose the signing algorithm is faulty and chooses
dependent values for αt in consecutively issued signatures. In particular, when signing a message m0
the signing algorithm chooses a uniformly random αt0 in Zq, as required. However, when signing
m1 it choose αt1 as αt1 ←a·αt0 + b for some known a,b ∈Zq. Show that if the adversary obtains
the corresponding Schnorr message-signature pairs ( m0,σ0) and (m1,σ1) and knows a,b and pk, it
can learn the secret signing key sk, with high probability.
808
Discussion: This attack illustrates why it is important to derandomize signature schemes derived
from Sigma protocols using the method of Exercise 13.6. It ensures that the signer is not dependent
on the security of its entropy source.
19.2 (Batch Schnorr veriﬁcation). Consider the unoptimized Schnorr signature scheme Ssch
(Section 19.2). Let {(mi,σi)}n
i=1 be n message-signature pairs, signed relative to a public key u.
In this exercise we show that verifying these n signatures as a batch may be faster than verifying
them one by one. Recall that a signature σ = (uti,αzi) on message mi is valid if gαzi = uci ·uti,
where ci = H(mi,uti). To batch verify n signatures, the veriﬁer does:
1. Choose random β1,...,β n ←R C,
2. Compute ¯ α←∑n
i=1 βiαzi ∈Zq and ¯c←∑n
i=1 βici ∈Zq,
3. Accept all n signatures if g¯α = u¯c ·∏n
i=1 uβi
ti.
(a) Show that if one of the n signatures is invalid, then the entire batch will be rejected with
probability at least 1/|C|.
Discussion: The bulk of the work is in line (3), which can be faster than verifying the
signatures one by one, at least for a veriﬁer with bounded memory. A veriﬁer with suﬃcient
memory can speed up single signature veriﬁcation by pre-computing exponentiation tables
for g and u, as discussed in Appendix A. In this case, batching may not save much time.
(b) Generalize the algorithm and show how to batch verify n triples {(ui,mi,σi)}n
i=1 where ui is
a public key, and each (mi,σi) is a message-signature pair with respect to ui.
19.3 (Tight reduction for multi-key Schnorr signatures). In Exercise 13.2, you were asked
to show that if a signature scheme is secure, it is also secure in the multi-key setting. However, the
security bound degrades by a factor proportional to the number of keys.
Suppose that we modify Schnorr’s signature scheme (Section 19.2) slightly, so that instead of
computing the challenge as c←H(m,ut), we compute it as c←H(pk,m,u t). That is, we include
the public key in the hash. Consider the security of this modiﬁed signature scheme in the multi-
key setting, modeling H as a random oracle. Show that the bound (19.10) holds in the multi-key
setting, independent of the number of keys, but assuming that all keys are generated using the
same group G. In this setting, the term Qs in (19.10) represents the total number of signing queries
performed under all the keys.
Hint: Use the random self-reducibility property of the DL problem (see Section 10.5.1).
19.4 (Schnorr signatures with related public keys). Let u:= gα ∈G be a random Schnorr
public key, generated as in Section 19.2. Deﬁne n related Schnorr public keys pk1,..., pkn by
setting pki := u·gi ∈G for i= 1,...,n . Consider the Schnorr signature scheme from Section 19.2,
modiﬁed as in the previous exercise to include the public key as an input to the hash function H.
(a) Show that this scheme is multi-key secure with respect to the n public keys pk1,..., pkn.
That is, consider an adversary that is given u and can issue signing queries to all n public
keys. Each query is a pair ( pk(j),mj), for j = 1,...,Q , and the response is a signature on
mj with respect to pk(j) ∈{pk1,..., pkn}. Show that the adversary cannot produce, with
non-negligible probability, an existential forgery ( pk,m,σ ) where pk ∈{pk1,..., pkn}and
809
(pk,m) is not one of the signing queries. Security is based on discrete-log in G, where the
hash function used in the scheme is modeled as a random oracle.
(b) Consider again the unmodiﬁed Schnorr signature scheme from Section 19.2, where the public
key is not included as part of the input to H. Show that this scheme is insecure with respect
to the related public keys pk1,..., pkn. In particular, a signature on message mwith respect
to pkj lets the adversary construct a signature on m with respect to pki for some i̸= j.
(c) Suppose that the nrelated public keys are generated as pki := ui ∈G for i= 1,...,n . Show
that the Schnorr signature scheme from Section 19.2 is multi-key secure with respect to these
public keys pk1,..., pkn, even if we do not include the public key as part of the input to H.
Discussion: A variant of the result in part (a) is widely used in crypto-currencies such as Bitcoin,
where a public-facing server generates many public keys from u = gα, without knowledge of the
secret key α (this is called hierarchical deterministic (HD) key creation). The secret key α can be
stored in a vault and later used to compute the secret keys for the public keys generated by the
public-facing server.
19.5 (Rewinding lemma variations). This exercise looks as some variants of Lemma 19.2. Let
T and U be ﬁnite, non-empty sets, and let f : T ×U →{0,1}be a function.
(a) Let Y, Z, Y′, and Z′ be mutually independent random variables. Assume that Y and Y′ are
each uniformly distributed over T and that Z and Z′ take values in the set U and have the
same distribution. Let ϵ:= Pr[f(Y,Z) = 1] and N := |T|. Show that
Pr
[
f(Y,Z) = 1 ∧f(Y′,Z′) = 1 ∧Y ̸= Y′]
≥ϵ2 −ϵ/N.
(b) Generalize part (a) as follows. Let Y1,..., Yn and Z1,..., Zn be mutually independent random
variables, where each Yi has the same distribution as Y, and each Zi has the same distribution
as Z. For i= 1,...,n , deﬁne
ϵi = Pr
[
f(Y1,Z1) = ··· = f(Yi,Zi) = 1 ∧Y1,..., Yi are pairwise distinct
]
.
Observe that ϵ1 = ϵ. Prove that for i= 2,...,n , we have
ϵi ≥ϵi−1(ϵ−(i−1)/N).
19.6 (Enlarging the challenge space). Many applications of Sigma protocols require a large
challenge space. This exercise shows that we can always take a Sigma protocol with a small challenge
space and turn it into one with a large challenge space, essentially by parallel repetition.
Let ( P,V ) be a Sigma protocol for a relation R⊆X×Y , with challenge space C. Let k be a
positive integer. Deﬁne a new Sigma protocol ( Pk,V k) as follows. Here, the prover Pk takes as
input (x,y) ∈R, the veriﬁer Vk takes as input y∈Y, and the challenge space is Ck.
• Pk initializes k instances of P on input ( x,y), obtaining commitments t1,...,t k, and sends
these to Vk.
• Vk chooses (c1,...,c k) ∈Ck at random, and sends this to Pk.
• For i= 1,...,k , the prover Pk feeds ci into the ith instance of P, obtaining a response zi. It
then sends (z1,...,z k) to Vk.
810
• For i= 1,...,k , the veriﬁer Vk veriﬁes’ that (ti,ci,zi) is an accepting conversation for y.
(a) Show that ( Pk,V k) is Sigma protocol for R.
(b) Show that if ( P,V ) provides special soundness, then so does ( Pk,V k).
(c) Show that if ( P,V ) is special HVZK, then so is ( Pk,V k).
Discussion: For example, if we want to use the GQ protocol (see Section 19.5.5) to prove knowl-
edge of an eth root of y modulo n, where e is small (say e = 3), then we can use this technique
to increase the size of the challenge space to 3 k, which is essential to get a secure ID scheme. Of
course, this blows up the complexity of the protocol by a factor of k, which is unfortunate. See Ex-
ercise 19.10 below that shows that some simple ideas to increase the challenge space more eﬃciently
do not work. See also Exercise 19.29 for a more eﬃcient scheme in an “amortized” setting.
19.7 (A soundness bound on Sigma protocols). Let (P,V ) be a Sigma protocol for a relation
R⊆X×Y , with challenge space C. Suppose that ( P,V ) is special HVZK. Show that a dishonest
prover ˆP that is initialized with a statement y ∈Y (but is not given the corresponding witness
x ∈X ) can succeed in getting the veriﬁer to accept with probability 1 /|C|. This is why Sigma
protocols must use a challenge space Cwhere |C|is super-poly.
19.8 (The Schnorr protocol in composite order groups). In this exercise we explore the
security of the Schnorr protocol in groups whose order is not a prime. Let G be a cyclic group of
order n = ℓq where ℓ is poly-bounded and q is super-poly prime (for example take ℓ = 2). Let
g ∈G be a generator. The prover has a secret key α∈Zn and the corresponding veriﬁcation key
u:= gα ∈G.
(a) Show that if the challenge space C in Schnorr’s protocol is Cq := {0,...,q −1}then the
protocol provides special soundness and is special HVZK.
(b) Suppose we use a larger challenge space CB := {0,...,B }for some B ≥q. Show that a prover
that is only given u= gα ∈G (but is not given α) can fool the veriﬁer with probability 1 /q.
Hence, the enlarged challenge space does not reduce the probability that a dishonest prover
succeeds in convincing the veriﬁer.
Discussion: One can show that when B ≥q the Schnorr protocol with challenge space CB
does not have special soundness, assuming discrete-log in G is hard.
(c) Let’s go back to a challenge space Cq := {0,...,q −1}and let ˆg:= gℓ ∈G. This ˆg generates
a strict subgroup of G of order q. Let u = gα ∈G where α is relatively prime to ℓ. This
u is not in the subgroup generated by ˆ g and therefore not a power of ˆg. Now consider the
Schnorr identiﬁcation protocol deﬁned base ˆ g between a prover and veriﬁer: the prover is
given ˆg and αand the veriﬁer is given ˆg and u. The Schnorr protocol is designed to convince
the veriﬁer that the prover knows the discrete-log of u base ˆg, and hence the veriﬁer should
reject because the discrete-log does not exist. However, show that the prover can fool the
veriﬁer with probability about 1 /ℓ. In particular, the prover can cause the veriﬁer to accept
whenever the challenge c is divisible by ℓ.
Discussion: This example shows that if the base ˆ g ∈G is not a generator of G, then the
Schnorr protocol can lose soundness altogether if the order of G has small prime factors. To
prevent this attack the veriﬁer must explicitely check that u is in the group generated by ˆg.
811
19.9 (Idealized ECDSA signatures). In this exercise we analyze an idealized version of the
ECDSA signature scheme from Section 19.3. Let G be a group of prime order q with generator
g∈G. The secret key is sk := αwhere α←R Zq. The public key is pk := u∈G where u←gα. Let
H : M→ Zq and H′: G →Z∗
q be hash functions.
• To sign a message m∈M using a secret key sk = α, the signing algorithm does:
S(sk,m) :=



repeat until s̸= 0:
αt ←R Z∗
q, u t ←gαt, r ←H′(ut), s ←
(
H(m) + rα
)
/αt ∈Zq
output σ:= (r,s).


.
• To verify a signature σ = ( r,s) on a message m ∈M, using the public key pk = u, the
signature veriﬁcation algorithm outputs accept only if H′(
gH(m)/s ·ur/s)
= r.
Prove that this signature scheme is secure assuming discrete log inG is hard, H is collision resistant,
and H′is modeled as a random oracle.
Discussion: This scheme is identical to the actual ECDSA signature scheme from Section 19.3,
except that here, we are computing r←H′(ut), instead of computing r as the x-coordinate of ut,
reduced modulo q.
19.10 (GQ security). This exercise explains why the challenge space in the GQ protocol (see
Section 19.5.5) is restricted to a subset of {0,...,e −1}.
(a) Suppose we set the challenge space in the GQ protocol to C:= {0,...,b ·e}for some integer
b >1. Show that a prover that is only given y = xe ∈Z∗
n (but is not given x) can fool
the veriﬁer with probability 1 /e. Hence, the enlarged challenge space does not reduce the
probability that a dishonest prover succeeds in convincing the veriﬁer.
(b) Suppose we set the challenge space in the GQ protocol to C:= {0,...,e }. Show that the
protocol no longer has special soundness. To do so, show that an eﬃcient witness extractor
Ext would give an eﬃcient algorithm to compute an eth root of y in Z∗
n. This would violate
the RSA assumption.
19.11 (Okamato’s RSA-based Sigma protocol). Okamoto’s protocol (see Section 19.5.1) is
based on the discrete logarithm problem. There is a variant of Okamoto’s protocol that is based on
the RSA problem. By way of analogy, Okamoto’s DL-based protocol was a “proof of knowledge”
of a preimage of the hash function Hdl in Section 10.6.1, and Okamato’s RSA-based protocol is a
“proof of knowledge” of a preimage of the hash function Hrsa in Section 10.6.2.
The setup is as follows. Let ( n,e) be an RSA public key, where the encryption exponent e is a
prime number. Also, let y be a random number in Z∗
n. We shall view the values n, e, and y as
systems parameters. Let Ie := {0,...,e −1}.
The relation of interest is the following:
R= {( (a,b), u) ∈(Z∗
n ×Ie) ×Z∗
n : u= aeyb }.
The protocol ( P,V ) runs as follows, where the prover is initialized with (( a,b),u) ∈R and the
veriﬁer V is initialized with u∈Z∗
n, and the challenge space Cis a subset of Ie:
812
• P computes
at ←R Z∗
n, bt ←R Ie, ut ←ae
t ybt,
and sends the commitment ut to V;
• V computes c←R C, and sends the challenge c to P;
• P computes
b′←bt + cb, az ←at ·ac ·y⌊b′/e⌋, bz ←b′mod e,
and sends the response ( az,bz) to V;
• V checks if ae
z ybz = ut ·uc; if so V outputs accept; otherwise, V outputs reject.
Prove that this protocol is a Sigma protocol for the relation Rdeﬁned above, and that it provides
special soundness and is special HVZK.
19.12 (An insecure variant of Fiat-Shamir signatures). Consider the signature system
derived from a Sigma protocol (P,V ) as in Section 19.6.1. Assume (P,V ) is special HVZK. Suppose
that during signing we set the challenge as c ←H(m) instead of c ←H(m,t). Show that the
resulting signature system is insecure.
Hint: Use the HVZK simulator to forge the signature on any message of your choice.
19.13 (Computational special soundness). Let Π = (P,V ) be a Sigma protocol for a relation
R⊆X×Y , with challenge space C. Recall that our deﬁnition of special soundness (see Deﬁni-
tion 19.4) says that there is an eﬃcient witness extractor algorithm Ext that on input y∈Y, along
with any two accepting conversations ( t,c,z ) and ( t,c′,z′) with c ̸= c′, outputs a witness x for y.
We can relax this deﬁnition, by insisting only that it is computationally infeasible to ﬁnd inputs
to the witness extraction algorithm of the required form on which the algorithm fails to output a
witness.
More precisely, for a given adversaryA, we deﬁne cSSadv[A,Π,Ext] to be the probability thatAout-
puts two accepting conversations (t,c,z ) and (t,c′,z′) with c̸= c′, such that Ext(y,(t,c,z ),(t,c′,z′))
is not a witness for y. We say Π provides computational special soundness if there ex-
ists an eﬃcient witness extractor Ext for Π, such that for every eﬃcient adversary A, the value
cSSadv[A,Π,Ext] is negligible.
Consider the Fiat-Shamir signature construction in Section 19.6.1 built from a Sigma protocol
(P,V ) and a key generation algorithm G. Assume that Gis one way. Also assume that (P,V ) that
is special HVZK, provides special soundness, has unpredictable commitments and a large challenge
space. We showed that the resulting signature scheme is secure (in the random oracle model),
satisfying the bound in (19.21). Show that if ( P,V ) only provides computational special soundness
(instead of special soundness), then the resulting signature scheme is still secure (in the random
oracle model), and show how the bound (19.21) needs to be updated.
19.14 (Unique responses). Let Π be a Sigma protocol. We say that Π has unique responses
if for every statement y and for every pair of accepting conversations ( t,c,z ) and (t,c,z ′) for y, we
must have z= z′.
(a) Prove that Schnorr’s Sigma protocol has unique responses.
(b) Prove that the Chaum-Pedersen protocol (see Section 19.5.2) has unique responses.
813
(c) Consider the generic linear protocol in Section 19.5.3. A particular instance of this protocol
is deﬁned in terms of a class Fof formulas φ of the form (19.13). For such a formula φ, we
can consider its homogenized form φ′, which is obtained by replacing each ui in (19.13) by
the group identity 1. Prove that the generic linear protocol for formulas in F has unique
responses if the following holds: for every φ ∈ F, its homogenized form φ′ has a unique
solution (namely, αj = 1 for j = 1,...,n ).
(d) Prove that the GQ protocol (see Section 19.5.5) has unique responses.
19.15 (Strong special soundness). Let Π be a Sigma protocol for a relation R⊆X×Y . Recall
that our deﬁnition of special soundness (see Deﬁnition 19.4) says that there is an eﬃcient witness
extractor algorithm Ext that on input y ∈Y, along with any two accepting conversations ( t,c,z )
and (t,c′,z′) with c̸= c′, outputs a witness xfor y. We can strengthen the requirement by insisting
that Ext should output a witness for y assuming only that ( c,z) ̸= (c′,z′), rather than c̸= c′. We
say that Π provides strong special soundness if there exists an eﬃcient witness extractor with
this property.
(a) Prove that if Π provides special soundness and has unique responses (see previous exercise),
then it provides strong special soundness.
(b) Consider the OR-proof construction in Section 19.7.2, which combines two Sigma protocols
(P0,V0) and (P1,V1) into a new Sigma protocol (P,V ) for the relation ROR in (19.23). Prove
that if (P0,V0) and (P1,V1) provide strong special soundness, then so does ( P,V ).
19.16 (Computational strong special soundness). We can relax the notion of strong special
soundness, which was introduced in the previous exercise, by insisting only that it is computationally
infeasible to ﬁnd inputs to the witness extraction algorithm of the required form on which the
algorithm fails to output a witness.
More precisely, for a given adversary A, we deﬁne cSSS adv[A,Π,Ext] to be the probability that
A outputs two accepting conversations ( t,c,z ) and ( t,c′,z′) with ( c,z) ̸= ( c′,z′), such that
Ext(y,(t,c,z ),(t,c′,z′)) is not a witness for y. We say Π provides computational strong special
soundness if there exists an eﬃcient witness extractor Ext for Π, such that for every eﬃcient
adversary A, the value cSSSadv[A,Π,Ext] is negligible.
(a) Prove that computational strong special soundness implies computational special soundness
(as deﬁned in Exercise 19.13).
(b) Prove that Okamoto’s protocol (see Section 19.5.1) provides computational strong special
soundness, under the DL assumption. Here, we are assuming that the system parameter
h ∈G used by Okamoto’s protocol is uniformly distributed over G. You should show that
an adversary that can ﬁnd two accepting conversations for some statement with diﬀerent
responses, but with the same commitment and challenge, can compute Dloggh.
(c) Prove that Okamoto’s RSA-based protocol (see Exercise 19.11) provides computational strong
special soundness, under the RSA assumption. You should show that an adversary that can
ﬁnd two accepting conversations for some statement with diﬀerent responses, but with the
same commitment and challenge, can compute y1/e ∈Z∗
n.
(d) Generalize part (b) of the previous exercise, showing that if ( P0,V0) and ( P1,V1) provide
computational strong special soundness, then so does ( P,V ).
814
19.17 (Strongly secure signature schemes). Consider the Fiat-Shamir signature construction
in Section 19.6.1 built from a Sigma protocol ( P,V ) and a key generation algorithm G. Assume
that (P,V ) that is special HVZK, has unpredictable commitments, and a large challenge space.
Also assume that G is one way.
(a) Prove that if ( P,V ) provides special soundness and has unique responses (see Exercise 19.14),
then the resulting signature scheme is strongly secure (in the sense of Deﬁnition 13.3), mod-
eling H as a random oracle. You should prove the same bound as in (19.21), but for
stSIGroadv[A,S] instead of SIG roadv[A,S].
(b) Prove that if ( P,V ) provides computational strong special soundness (see previous exercise),
then the resulting signature scheme is strongly secure, again, modeling H as a random oracle.
Derive a concrete security bound as a part of your analysis.
Discussion: As a consequence of part (a), we see that Schnorr and GQ are strongly secure
signature schemes.
19.18 (Backward computable commitments). Most of the examples of Sigma protocols we
have seen in this chapter have the following special structure: if the relation is R⊆X×Y , and if
conversations (t,c,z ) lie in the setT×C×Z , then for every (y,c,z ) ∈Y×C×Z , there exists a unique
t∈T such that (t,c,z ) is an accepting conversation fory; moreover, the function f mapping (y,c,z )
to tis eﬃciently computable. Let us say that ( P,V ) has backward computable commitments
in this case. (In fact, all of the special HVZK simulators essentially work by choosing z at random
and computing t= f(y,c,z ). Note that the range proof protocol in Section 20.4.1 is an example of
a Sigma protocol that does not have backward computable commitments.)
(a) Verify that the generic linear protocol (see Section 19.5.3) and the GQ protocol (see Sec-
tion 19.5.5) have backward computable commitments.
(b) Show that if ( P0,V0) and ( P1,V1) have backward computable commitments, then so do the
AND-proof and OR-proof constructions derived from ( P0,V0) and (P1,V1) (see Section 19.7).
19.19 (Optimized Fiat-Shamir signatures). The optimization we made for Schnorr and GQ
signatures can be applied to Fiat-Shamir signatures derived from most Sigma protocols. Consider
the Fiat-Shamir signature scheme derived from a Sigma protocol ( P,V ) for a relation R⊆X×Y ,
and a key generation algorithm Gfor R. Recall that a Fiat-Shamir signature on a message mis of
the form (t,z), where (t,c,z ) ∈T ×C×Z is an accepting conversation, and c:= H(m,t).
Assume that ( P,V ) has backward computable commitments, as in the previous exercise, and let
f : Y×C×Z →T be the corresponding function that computes a commitment from a given
statement, challenge, and response. Then we can optimize the Fiat-Shamir signature scheme, so
that instead of using ( t,z) as the signature, we use ( c,z) as the signature. To verify such an
optimized signature ( c,z), we compute t ←f(c,z), and verify that c = H(m,t). Note that c is
usually much smaller than t, so these optimized signatures are usually much more compact.
(a) Show that if the Fiat-Shamir signature scheme is secure, then so is the optimized Fiat-Shamir
signature scheme.
(b) Show that if the Fiat-Shamir signature scheme is strongly secure, then so is the optimized
Fiat-Shamir signature scheme.
815
Note: For both parts, you should show that any adversary that breaks the optimized scheme can
be converted to one that is just as eﬃcient, and breaks the unoptimized scheme with the same
advantage.
19.20 (Collision resistance from Sigma protocols). Suppose (P,V ) is a Sigma protocol for
a relation R⊆X×Y . Furthermore, assume that ( P,V ) has backward computable commitments,
as in Exercise 19.18, where f : Y×C×Z →T is the corresponding function that computes a
commitment from a given statement, challenge, and response. Also assume that ( P,V ) provides
computational strong special soundness, as in Exercise 19.16. Finally, let G be a one-way key
generation algorithm for R.
From these components, we can build a hash function H : C×Z →T , as follows. The hash
function makes use of a system parameter y ∈Y, which is obtained by running ( y,(x,y)) ←R G().
For (c,z) ∈C×Z , and a given system parameter y∈Y, we deﬁne H(c,z) := f(y,c,z ) ∈T .
Prove that H is collision resistant.
Discussion: The hash function Hdl in Section 10.6.1 can be viewed as a special case of this result,
applied to Schnorr’s protocol. The hash function Hrsa in Section 10.6.2 can be viewed as a special
case of this result, applied to the GQ protocol.
19.21 (Strongly secure one-time signatures from Sigma protocols (I)). Suppose (P,V )
is a Sigma protocol for a relation R⊆X×Y and that (P,V ) has conversations in T×C×Z . Also
assume that ( P,V ) provides computational strong special soundness (as in Exercise 19.16) and is
special HVZK. Let G be a one-way key generation algorithm for R. Finally, let H : M→C be a
hash function.
We can deﬁne a signature scheme ( G∗,S∗,V ∗) with message space Mas folows.
• G∗ computes ( y,(x,y)) ←R G(), and then initializes a prover instance P(x,y), obtaining a
commitment t∈T . It outputs the public key pk∗:= (y,t). The secret key sk∗is the internal
state of the prover instance P(x,y).
• Given a secret key sk∗ as above, and a message m ∈M, the signing algorithm S∗ feeds
c←H(m) to the prover instance P(x,y), obtaining a response z∈Z. The signature is z.
• Given a public key pk∗= (y,t) ∈Y×T , a message m∈M, and a signature z∈Z, the veri-
ﬁcation algorithm computes c←H(m) and checks that ( t,c,z ) is an accepting conversation
for y.
Prove that (G∗,P∗,V ∗) is a strongly secure one-time signature scheme (see Deﬁnition 14.2) if we
model H as a random oracle.
Discussion: If we instantiate this with Schnorr’s protocol, we get the signature scheme discussed
in Exercise 14.11.
19.22 (Type hiding key generation). In Section 19.8, we introduced the notion of witness
independence, and we saw that this property (which is implied by special HVZK) could be used to
design actively secure identiﬁcation protocols. This exercise generalizes these results, establishing
more general conditions under which a Sigma-protocol based ID scheme can be proved actively
secure using WI. Let ( P,V ) be a Sigma protocol for R⊆X×Y . Let G be a key generation
algorithm for R.
816
Suppose that type : X →Tis a function from X into some ﬁnite set T, where |T| > 1. We say
that G is second-preimage resistant (relative to the function type) if it is hard for any eﬃcient
adversary Ato win the following game:
• challenger computes (y,(x,y)) ←R G() and sends ( x,y) to A;
• Awins the game if he can compute ˆx∈X such that (ˆx,y) ∈R and type(ˆx) ̸= type(x).
We also need an information-theoretic notion that says that G generates public keys that do not
leak any information about the type of the secret key. LetX and Y be random variables representing
the witness and statement output by G. We say G is type hiding if for all (ˆx,y) ∈R, we have
Pr[type(X) = type(ˆx) |Y = y] = 1
|T|.
This is equivalent to saying that Y and type(X) are independent, with type(X) uniformly distributed
over T.
(a) Suppose ( P,V ) is a Sigma protocol forRthat provides special soundness and is special HVZK,
and has a large challenge space. Further, suppose that G is a key generation algorithm for
Rthat is second-preimage resistant and type hiding for some type function. Prove that the
identiﬁcation protocol (G,P,V ) is secure against active attacks.
(b) Show that key generation algorithm G′for the OR-proof-based Sigma protocol (P′,V ′) in Sec-
tion 19.8.3 is second-preimage resistant (under the assumption that underlying key generation
algorithm G is one-way) and type hiding, using the type function type(b,x) := b∈{0,1}.
(c) Show that key generation algorithm G for Okamoto’s protocol ( P,V ) in Section 19.8.4 is
second-preimage resistant (under the DL assumption) and type hiding, using the type function
type(α,β) := β ∈Zq.
(d) Consider Okamoto’s RSA-based Sigma protocol ( P,V ) in Exercise 19.11. Deﬁne the key
generation G, which outputs the statement u and witness (a,b), where a←R Z∗
n, b←R Ie, and
u←R ayyb. Show that G is second-preimage resistant (under the RSA assumption) and type
hiding, using the type function type(a,b) := b∈Ie. Conclude that the identiﬁcation scheme
(G,P,V ) is secure against active attacks, under the RSA assumption.
19.23 (Public-key equivalence). We can use the notion of witness independence to simplify
certain schemes built from Sigma protocols.
Suppose (P,V ) is Sigma protocol for a relation R⊆X×Y . Let G0 and G1 be two key generation
algorithms for R. We say that these two algorithms are public-key equivalent if the public keys
generated by these two algorithms have the same distribution.
Consider the following attack game, which consists of two experiments. In Experiment b, where
b ∈{0,1}, the challenger computes ( pk,sk) ←R Gb(), to obtain pk = y and sk = (x,y), and then
interacts with an adversary Aas in Experiment ( x,y) of Attack Game 19.3, at the end of which
the adversary outputs a bit ˆb∈{0,1}.
Show that if ( P,V ) is witness independent and G0 and G1 are public-key equivalent, then the
probability that Aoutputs 1 is the same in both experiments.
817
19.24 (Simpliﬁed identiﬁcation protocols). We can use the result of the previous exercise
to obtain somewhat simpliﬁed, and more eﬃcient, identiﬁcation protocols that are secure against
active attacks.
(a) Suppose ( P,V ) is a witness independent Sigma protocol for a relationR, Gis a key generation
for R, and that the identiﬁcation protocol ( G,P,V ) is secure against active attacks. Further,
suppose that G0 is a key generation algorithm that is public-key equivalent to G, as in the
previous exercise. Show that the identiﬁcation protocol ( G0,P,V ) is just as secure against
active attacks, in the sense that any impersonation adversary that breaks the security of
(G0,P,V ) breaks (G,P,V ) with the same advantage.
(b) Consider the OR-proof-based identiﬁcation protocol ( G′,P′,V ′) in Section 19.8.3. Argue that
we can replace G′ by G′
0, which always sets b ←0, instead of b ←R {0,1}, and the resulting
identiﬁcation protocol (G′
0,P′,V ′) is just as secure against active attacks.
(c) Consider Okamoto’s identiﬁcation protocol ( G,P,V ) in Section 19.8.4. Argue that we can
replace Gby G0, which always sets β ←0, instead of β ←R Zq, and the resulting identiﬁcation
protocol (G0,P,V ) is just as secure against active attacks. Describe the resulting scheme in
detail.
(d) Consider Okamoto’s RSA-based identiﬁcation protocol (G,P,V ) in part (d) of Exercise 19.22.
Argue that we can replace G by G0, which always sets b ←0, instead of b ←R Ie, and the
resulting identiﬁcation protocol ( G0,P,V ) is just as secure against active attacks. Describe
the resulting scheme in detail.
19.25 (Strongly secure one-time signatures from Sigma protocols (II)).In Exercise 19.21,
we saw how to build strongly secure one-time signature schemes from Sigma protocol using a random
oracle. In this exercise, we do the same, without relying on random oracles; however, we require
Sigma protocols with stronger properties.
Suppose (P,V ) is a Sigma protocol for a relation R⊆X×Y , and that ( P,V ) has conversations
in T ×C×Z. Let G0 be a key generation algorithm for R. We can deﬁne a signature scheme
(G∗
0,S∗,V ∗), with message space C, as follows.
• G∗
0 computes (y,(x,y)) ←R G0(), and then initializes a prover instance P(x,y), obtaining a
commitment t∈T . It outputs the public key pk∗:= (y,t). The secret key sk∗is the internal
state of the prover instance P(x,y).
• Given a secret key sk∗as above, and a message c∈C, the signing algorithm S∗feeds cto the
prover instance P(x,y), obtaining a response z∈Z. The signature is z.
• Given a public key pk∗ = ( y,t) ∈Y×T , a message c ∈C , and a signature z ∈Z , the
veriﬁcation algorithm checks that (t,c,z ) is an accepting conversation for y.
(a) Assume that ( P,V ) provides computational strong special soundness (see Exercise 19.16) and
is special HVZK. Further, assume that G0 is public-key equivalent (see Exercise 19.23) to a
key generation algorithm G that is second-preimage resistant and type hiding for some type
function (see Exercise 19.22). Prove that ( G∗
0,P∗,V ∗) is a strongly secure one-time signature
scheme (see Deﬁnition 14.2).
818
(b) Describe in detail the signature schemes based on the Sigma protocols and key generation
algorithms in parts (b)–(d) of the previous exercise, and argue that they are strongly secure
one-time signature schemes.
Note: The scheme based on part (c) of the previous exercise is actually the same scheme
that was presented in Exercise 14.12.
19.26 (Generalized AND-proofs and OR-proofs). Generalize the AND-proof and OR-proof
constructions in Section 19.7 from two Sigma protocols to n protocols. You can view n as either
a constant or a system parameter. If n is not constant, then it is perhaps simplest to assume that
all the Sigma protocols are the same. State the relations for your new Sigma protocols, and argue
that they provide special soundness and are special HVZK under appropriate assumptions. The
computational and communication complexity of your protocols should scale linearly in n.
19.27 (Special HVZK with non-uniform challenges). Suppose (P,V ) is a Sigma protocol
for a relation R ⊆X×Y , with challenge space C. Further, suppose ( P,V ) is special HVZK with
simulator Sim. Now let D be an arbitrary probability distribution on C. Consider a challenger VD
that generates its challenge according to the distribution D, rather than uniformly over C. Show
the following: for all (x,y) ∈R, if we compute
c←RD, (t,z) ←RSim(y,c),
then (t,c,z ) has the same distribution as that of a transcript of a conversation between P(x,y) and
VD(y).
19.28 (Threshold proofs). The OR-proof construction in Section 19.7.2 allows a prover to
convince a veriﬁer that he knows a witness for one of two given statements. In this exercise,
we develop a generalization that allows a prover to convince a veriﬁer that he knows at least k
witnesses for n given statements.
Let (P,V ) be a Sigma protocol for a relation R⊆X×Y . Assume that ( P,V ) provides special
soundness and is special HVZK, with simulator Sim. We also assume that C= Zq for some prime q.
Let nand kbe integers, with 0 <k<n<q . We can think of nand kas being constants or system
parameters.
We shall build a Sigma protocol ( P′,V ′) for the relation
R′=
{(
(x1,...,x n), (y1,...,y n)
)
∈(X∪{⊥} )n ×Yn :
⏐⏐{i∈{1,...,n }: (xi,yi) ∈R}
⏐⏐≥k
}
.
Suppose the prover P′ is given the witness ( x1,...,x n) and the statement ( y1,...,y n), and the
veriﬁer V′is given the statement (y1,...,y n). Let I denote the set of indicesisuch that (xi,yi) ∈R.
We know that |I|≥ k. We shall assume that |I|= k, removing indices from I if necessary. Let
J := {1,...,n }\I, so |J|= n−k. The protocol runs as follows.
1. For each j ∈J, the prover chooses cj ∈Zq at random, and runs Sim on input ( yj,cj) to
obtain (tj,zj). For each i∈I, the prover initializes an instance of P with (xi,yi), obtaining
a commitment ti. The prover then sends ( t1,...,t n) to the veriﬁer.
819
2. The veriﬁer generates a challenge c∈Zq at random, and sends c to the prover.
3. The prover computes the unique polynomial f ∈Zq[w] of degree at most n−k such that
f(0) = c and f(j) = cj for all j ∈J using a polynomial interpolation algorithm. It then
computes the challenges ci := f(i) for all i ∈I. For each i ∈I, the prover then feeds the
challenge ci to the instance of P it initialized with (xi,yi), obtaining a response zi. The prover
then sends (f, z1,...,z n) to the veriﬁer.
4. First, the veriﬁer checks that f is a polynomial of degree at most n−k with constant term
c. Then, for ℓ = 1,...,n , it computes cℓ := f(ℓ). Finally, for ℓ = 1,...,n , it veriﬁes that
(tℓ,cℓ,zℓ) is an accepting conversation for yℓ.
Show that (P′,V ′) is a Sigma protocol for R′that provides special soundness and is special HVZK.
Hint: The previous exercise may be helpful in arguing special HVZK.
Discussion: For simplicity, we presented the protocol for n identical relations R. The protocol
also works essentially “as is” even if the relations are not all the same.
19.29 (Amortized complexity of Sigma protocols). This exercise illustrates a technique that
can be used to increase the challenge space size of a Sigma protocol without increasing its commu-
nication complexity, at least in an amortized sense. We illustrate the technique on the GQ protocol
for proving knowledge of an eth root modulo n, where e is a small prime. However, the technique
(or variations thereon) can be applied more generally.
Suppose that for i= 1,...,ℓ , the prover knows xi ∈Z∗
n such that xe
i = yi, and wants to convince
a skeptical veriﬁer of this. If eis small, we could use the technique of Exercise 19.6 to increase the
challenge space size toek, and then apply the generalized AND-proof construction of Exercise 19.26.
The resulting protocol would have communication complexity proportional to kℓtimes the cost of a
single run of the GQ protocol (O(kℓ) elements of Z∗
n). In this exercise, we show how to do this with
a protocol whose challenge space is of size eℓ and whose communication complexity is proportional
to just ℓ times the cost of a single run of the GQ protocol.
Suppose v = (v1,...,v ℓ) ∈(Z∗
n)1×ℓ is row vector of length ℓ with entries in the group Z∗
n. Sup-
pose M = (mij) ∈Zℓ×ℓ is an ℓ×ℓ matrix with integer entries. We deﬁne vM to be the vector
(w1,...,w ℓ) ∈(Z∗
n)1×ℓ, where
wi = vm1i
1 ···vmℓi
ℓ for i= 1,...,ℓ.
This is really just the usual rule for vector-matrix multiplication, except that the scalar “addition”
operation in the group Z∗
n is written multiplicatively. For two vectors v,w ∈(Z∗
n)1×ℓ, we write
v·w ∈(Z∗
n)1×ℓ for the component-wise product of v and w. The usual rules of vector-matrix
arithmetic carry over, for example, we have
(v·w)M = vM ·wM, vM+N = vM ·vN, and vMN = (vM)N.
For v∈(Z∗
n)1×ℓ and integer f, we write vf ∈(Z∗
n)1×ℓ for the component-wise fth power of v, that
is, the vector whose ith entry is vf
i ∈Z∗
n.
Let ebe a prime, and let Ie := {0,...,e −1}. The challenge space Cfor our Sigma protocol is I1×ℓ
e .
With each challenge c∈C, we associate an eﬃciently computable matrix Mc ∈Iℓ×ℓ
e . The essential
820
property of these associated matrices is the following: given two distinct challenges cand c′ in C,
we can eﬃciently compute a matrix N ∈Iℓ×ℓ
e , such that (Mc−Mc′)N ≡I (mod e), where I is the
identity matrix. In other words, for all distinct c,c′∈C, the matrix (Mc−Mc′) mod eis invertible
over the ﬁeld Fe.
If the statement is y∈(Z∗
n)1×ℓ, and the witness is x∈(Z∗
n)1×ℓ such that xe = y, then the protocol
works as follows:
xt ←R (Z∗
n)1×ℓ, yt ←xe
t
yt
−−−−−−−−−−−−−−−−→
c←R C
c←−−−−−−−−−−−−−−−−
xz ←xt ·xMc
xz
−−−−−−−−−−−−−−−−→
xe
z
?= yt ·yMc
(a) Assuming the associated matrices Mc have the stated properties, prove that the above pro-
tocol provides special soundness and is special HVZK.
(b) Show how to deﬁne the matrix Mc associated with challenge c∈C with the stated properties.
Hint: Use a ﬁnite ﬁeld of cardinality eℓ.
(c) A straightfoward implementation takes O(ℓ2 log(e)) multiplications in Z∗
n for both prover and
veriﬁer. Show how to reduce this to O(ℓ2 log(e)/log(ℓ)) with precomputation.
19.30 (ℓ-special soundness and multi-extractability). We can generalize Deﬁnition 19.4 for
special soundness as follows. Let ( P,V ) be a Sigma protocol for R⊆X×Y . Let ℓ ≥2 be a
constant or poly-bounded system parameter. We say that ( P,V ) provides ℓ-special soundness if
there is an eﬃcient witness extractor algorithm Ext with the following property: whenever Ext is
given as input a statement y∈Y, and ℓ accepting conversations of the form
(t,c1,z1),..., (t,cℓ,zℓ),
where the ci’s are pairwise distinct, algorithm Ext always outputs x∈X such that (x,y) ∈R (i.e.,
x is a witness for y).
Prove the following generalization of Theorem 19.24: if Π is a non-interactive proof system derived,
is a Sigma protocol Π with a large challenge space that provides ℓ-special soundness, then Φ is
multi-extractable.
To do this, you should design a multi-extractor that for every B-bounded adversary, and for every
prescribed error probability θ, re-runs the adversary O(Bℓ/θ) times, and fails with probability at
most θ provided θ≥2B(ℓ−1)/N, where N is the size of Π’s challenge space.
Hint: Exercise 19.5(b) may be helpful.
19.31 (Compressed n-wise Schnorr). This exercise asks you to analyze a compressed n-wise
Schnorr protocol. Let G be a group of prime order qwith generator g∈G. Let nbe a poly-bounded
parameter, and let Rn ⊆(Zn
q) ×(Gn) be the following relation:
Rn :=
{(
(α1,...,α n),(u1,...,u n)
)
: ui = gαi for i= 1,...,n
}
.
821
P(α1,...,α n) V(u1,...,u n)
α0 ←R Zq, u0 ←gα0 ∈G u0−−−−−−−−−−−−−−−−→
c←R C
c←−−−−−−−−−−−−−−−−
β ←
n∑
i=0
αici ∈Zq
β−−−−−−−−−−−−−−−−→
gβ ?=
n∏
i=0
u(ci)
i
Figure 19.12: A compressed n-wise Schnorr protocol
Our goal is to design a Sigma protocol that lets a prover P convince a veriﬁer V who has u :=
(u1,...,u n) that he knows α such that ( α,u) ∈Rn. One solution is to run n instances of the
Schnorr protocol from Section 19.1 in parallel (see Exercise 19.26). However, the amount of traﬃc
in the resulting protocol is linear in n. In this exercise we develop a protocol that requires only
constant communication. The complete protocol is shown in Fig. 19.12.
(a) Show that protocol is special HVZK.
(b) Show that the protocol provides ( n+ 1)-special soundness, as deﬁned in Exercise 19.30.
Discussion: Exercise 20.27 shows how to apply the same technique to the Chaum-Pedersen
protocol for DH-triples.
19.32 (Multi-extractability under weaker conditions). Show that Theorem 19.24 holds if
Π only provides computational special soundness (as in Exercise 19.13) rather than special sound-
ness. The concrete bounds stated in that theorem still hold, but the multi-extractor may fail with
probability θ+ η, where η the advantage of an adversary in the computational special soundness
attack game.
822
Chapter 20
Proving properties in zero-knowledge
In the previous chapter, we saw how to use Sigma protocols to construct identiﬁcation and signature
schemes. In these applications we used Sigma protocols as “proofs of knowledge” — using rewinding
and special soundness, we could eﬀectively extract a witness from any convincing prover.
In this chapter, we will see how to use Sigma protocols to prove that certain facts are true
(without disclosing much else). In applications that use Sigma protocols in this way, security
hinges on the truth of the alleged fact, not any notion of knowledge. For example, the Chaum-
Pedersen protocol (Section 19.5.2) allows a prover to convince a veriﬁer that a given triple of
group elements is a DH-triple. That ability in itself is a useful tool in constructing and analyzing
interesting cryptographic protocols.
In Section 20.1, we begin by deﬁning thelanguage of true statements associated with an eﬀective
relation: this is just the set of statements for which there exists a corresponding witness. Then
we deﬁne a notion of soundness for a Sigma protocol, which just means that it is infeasible for
any prover to make the veriﬁer accept a statement that is not true (i.e., does not have a witness).
This notion diﬀers from special soundness, in that we do not require any kind of witness extractor.
However, we shall see that special soundness implies soundness.
In Section 20.2, we will present a series of examples that illustrate soundness. These examples
revolve around the idea of proving properties on encrypted data.
In Section 20.3, we show how to turn Sigma protocols into non-interactive proofs, using a variant
of the Fiat-Shamir transform (see Section 19.6.1).
In later sections, we examine more advanced techniques for building proof systems.
20.1 Languages and soundness
We begin with a deﬁnition.
Deﬁnition 20.1 (The language of true statements). Let R⊆X×Y be an eﬀective relation.
We say a statement y ∈Y is a true statement if (x,y) ∈R for some x∈X; otherwise, we say
y ∈Y is a false statement . We deﬁne LR, which is called language deﬁned by R, to be the
set of all true statements; that is, LR:= {y∈Y : (x,y) ∈R for some x∈X}.
The term “language” comes from complexity theory. In this chapter, we will look at a number
of interesting relations Rand the languages LR deﬁned by them. To give an example from the
previous chapter, recall that the Chaum-Pedersen protocol is a Sigma protocol for the following
823
relation:
R:=
{(
β, (u,v,w )
)
∈Zq ×G3 : v= gβ and w= uβ
}
.
The language LRdeﬁned by Ris the set of all DH-triples ( u,v,w ) ∈G3.
We can now deﬁne the notion of soundness using the following attack game:
Attack Game 20.1 (Soundness). Let Π = ( P,V ) be a Sigma protocol for R⊆X×Y . For a
given adversary A, the attack game runs as follows:
• The adversary chooses a statement y∗∈Y and gives this to the challenger.
• The adversary now interacts with the veriﬁer V(y∗), where the challenger plays the role of
veriﬁer and the adversary plays the role of a possibly “cheating” prover.
We say that the adversary wins the game if V(y∗) outputs accept but y∗ /∈LR. We deﬁne A’s
advantage with respect to Π, denoted Snd adv[A,Π], as the probability that Awins the game. 2
Deﬁnition 20.2. We say that Π is sound if for all eﬃcient adversaries A, the quantity
Sndadv[A,Π] is negligible.
Theorem 20.1 (Special soundness implies soundness). Let Π be a Sigma protocol with a
large challenge space. If Π provides special soundness, then Π is sound.
In particular, for every adversary A, we have
Sndadv[A,Π] ≤ 1
N, (20.1)
where N is the size of the challenge space.
Proof. It will suﬃce to show that if Achooses a false statement y∗ and a commitment t∗, then
there can be at most one challenge c for which there exists a response z that yields an accepting
conversation (t∗,c,z ) for y∗. Observe that if there were two such challenges, then there would be
two accepting conversations (t∗,c,z ) and (t∗,c′,z′) for y∗, with c̸= c′, and special soundness would
imply that there exists a witness for y∗, which is not the case. 2
Remark 20.1 (Statistical vs computational soundness). Note that the above theorem holds
even for arbitrarily powerful adversaries. In this case we may say that the proof system is sta-
tistically sound. Our notion of soundness is sometimes called computational soundness, to
explicitly distinguish it from statistical soundness. 2
We put these ideas to use in the next section.
20.2 Proving properties on encrypted data
In a number of applications, the following scenario arises. Alice encrypts a message munder Bob’s
public key, obtaining a ciphertext c. In addition, Alice wants to prove to a third party, say Charlie
(who gets to see cbut not m), that the encrypted plaintext msatisﬁes a certain property, without
revealing to Charlie anything else about m.
824
A Sigma protocol that is sound and special HVZK can be used to solve this type of problem.
Such a protocol is not a complete solution, however. One problem is that the HVZK property
only ensures that no information about m is leaked assuming that Charlie honestly follows the
veriﬁcation protocol. One way to address this issue is to use the same idea that we used in
Section 19.6.1 to turn interactive identiﬁcation protocols into signatures. That is, instead of using
an actual veriﬁer to generate the random challenge, we instead generate the challenge using a hash
function. We will investigate this approach in detail in the next section. For now, let us look
at a few interesting and important examples that show how we can use Sigma protocols to prove
properties on encrypted data.
In our examples, it is convenient to use the multiplicative variant of the ElGamal encryption
scheme, discussed in Exercise 11.5. This scheme makes use of a cyclic group G of prime order q
generated by g ∈G. The secret key is α ∈Zq (which is chosen at random) and the public key is
u := gα ∈G. The encryption of m ∈G is (v,e) ∈G2, where v := gβ, e := uβ ·m, and β ∈Zq
is chosen at random. To decrypt ( v,e) using the secret key α, one computes m := e/vα. As you
were asked to show in Exercise 11.5, this scheme is semantically secure under the DDH assumption
for G.
Example 20.1 (Equal plaintexts). Suppose Alice has one ciphertext ( v0,e0) that encrypts a
message m under Bob’s public key u0, and another ( v1,e1), that encrypts the same message m
under Bill’s public key u1. She wants to convince Charlie that this is the case, without revealing
anything else. For example, some protocols may require that Alice broadcast the same message
to Bob and Bill. A protocol for this problem allows Alice to do this, while keeping her message
encrypted, but proving that she really did encrypt the same message.
So we want a Sigma protocol for the relation
R:=
{
( (β0,β1,m), (u0,v0,e0, u1,v1,e1) ) : v0 = gβ0, e0 = uβ0
0 ·m, v1 = gβ1, e1 = uβ1
1 ·m
}
.
The language LR is precisely the set of tuples ( u0,v0,e0, u1,v1,e1) such that ( v0,e0) and ( v1,e1)
encrypt the same message under the public keys u0 and u1.
To design an eﬃcient Sigma protocol for R, we observe that
(u0,v0,e0, u1,v1,e1) ∈LR ⇐⇒ v0 = gβ0, v 1 = gβ1, and e0/e1 = uβ0
0 u−β1
1
for some β0,β1 ∈Zq.
Based on this observation, we can implement a Sigma protocol for Rusing the generic linear
protocol from Section 19.5.3. Speciﬁcally, Alice proves to Charlie that there exist β0,β1 satisfying
the system of equations
v0 = gβ0, v 1 = gβ1, e 0/e1 = uβ0
0 u−β1
1 .
The result is a sound, special HVZK Sigma protocol for the relation R.
Note that while Alice does not explicitly use the message min the above protocol, she anyway
needs to know it, since she needs to know both β0 and β1, either one of which determine m. 2
Example 20.2 (Equal plaintexts, again). Consider a variation of the previous example in
which Alice has two ciphertexts, ( v0,e0) and ( v1,e1), that encrypt the same message under Bob’s
public key u. The diﬀerence now is that both ciphertexts encrypt the same message under the same
825
public key. Again, she wants to convince Charlie that this is the case, without revealing anything
else. Observe that if ( v0,e0) and (v1,e1) encrypt the same message, then
v0 = gβ0, e 0 = uβ0 ·m, v 1 = gβ1, e 1 = uβ1 ·m
for some β0,β1 ∈Zq and m ∈G. Dividing the ﬁrst equation by the third, and the second by the
fourth, we have
v0/v1 = gβ and e0/e1 = uβ, (20.2)
where β := β0 −β1. Moreover, it is not hard to see that if (20.2) holds for some β ∈Zq, then
(v0,e0) and (v1,e1) encrypt the same message.
Therefore, all Alice needs to do is to convince Charlie that there exists β satisfying (20.2). She
can do this using the generic linear protocol from Section 19.5.3, which in this case is really just
the Chaum-Pedersen protocol (see Section 19.5.2) for proving that ( u,v0/v1,e0/e1) is a DH-triple.
Note that to prove that (v0,e0) and (v1,e1) encrypt the same message, Alice only needs to know
the value β satisfying (20.2) — she does not need to know the message itself. In particular, Alice
need not have been the party that generated these ciphertexts. In fact, she could have received
the ciphertext (v0,e0) from another party, and then created a new encryption ( v1,e1) of the same
message by computing v1 := v0 ·gβ and e1 := e0 ·uβ for a value β of her choice. Some anonymity
services perform precisely this type of function, creating a fresh re-encryption of an encrypted
message. This protocol can be used to ensure that this was done correctly. 2
Example 20.3 (Encrypted bits). To encrypt a bit b∈{0,1}, it is convenient to encode bas the
group element gb ∈G, and then encrypt gb using multiplicative ElGamal. So suppose Alice has
encrypted a bit bin this way, under Bob’s public key u, producing a ciphertext (v,e) = (gβ,uβ·gb).
She wants to convince Charlie that (v,e) really does encrypt a bit under Bob’s public key (and not,
say, g17), without revealing anything else.
So we want a Sigma protocol for the relation
R:=
{
( (b,β), (u,v,e ) ) : v= gβ, e = uβ ·gb, b ∈{0,1}
}
.
The language LRcorresponding to this relation is precisely the set of tuples (u,v,e ) such that (v,e)
encrypts a bit under the public key u.
Our Sigma protocol for Ris based on the observation that
(u,v,e ) ∈LR ⇐⇒ either (u,v,e ) or (u,v,e/g ) is a DH-triple .
The Chaum-Pedersen protocol in Section 19.5.2 allows a party to prove that a given triple is a
DH-triple. We combine this with the OR-proof construction in Section 19.7.2. This gives us a
Sigma protocol for the relation
R′:=
{(
(b,β), ((u0,v0,w0),(u1,v1,w1))
)
: vb = gβ and wb = uβ
b
}
.
A statement
(
(u0,v0,w0),(u1,v1,w1)
)
is in LR′ if at least one of ( u0,v0,w0) or ( u1,v1,w1) is a
DH-triple. Then, we have
(u,v,e ) ∈LR ⇐⇒ ((u,v,e ),(u,v,e/g )) ∈LR′.
826
P
(
(b,β),(u,v,e )
)
V(u,v,e )
set w0 := e, w1 := e/g
βtb ←R Zq, vtb ←gβtb, wtb ←uβtb
d←1 −b, cd ←R C, βzd ←R Zq
vtd ←gβzd/vcd, wtd ←uβzd/wcd
d vt0,wt0,vt1,wt1−−−−−−−−−−−−−−−−→c←R C
cb ←c⊕cd, βzb ←βtb + βcb
c←−−−−−−−−−−−−−−−−
c0,βz0,βz1−−−−−−−−−−−−−−−−→compute c1 ←c⊕c0 and verify that
gβz0 = vt0 ·vc0, uβz0 = wt0 ·wc0
0
gβz1 = vt1 ·vc1, uβz1 = wt1 ·wc1
1
Figure 20.1: Sigma protocol for encrypted bits
So, for Alice to prove to Charlie that ( u,v,e ) ∈LR, they run the Sigma protocol for R′, using the
statement ((u,v,e ),(u,v,e/g )) and the witness ( b,β). For completeness, we give the entire Sigma
protocol for Rin Fig. 20.1. In the ﬁrst line of the prover’s logic, the prover is initiating the proof
for the witness it knows, and the second and third lines are running the HVZK simulator for the
witness it does not know. The resulting Sigma protocol for Ris sound and special HVZK.
This protocol generalizes to proving that a ciphertext ( v,e) encrypts a value 0 ≤b < Bfor
B >2, as discussed in Exercise 20.1. The protocol transcript grows linearly in B, so this can only
be used for relatively small B. We will see how to handle larger B in Section 20.4.1. 2
Example 20.4 (Encrypted DH-triples). Suppose Alice has a DH-triple ( gγ1,gγ2,gγ3), where
γ3 = γ1γ2. She encrypts each element under Bob’s public key u, producing three ciphertexts
(v1,e1),(v2,e2),(v3,e3), where
vi = gβi, e i = uβigγi for i= 1,2,3. (20.3)
She presents these ciphertexts to Charlie, and wants to convince him that these ciphertexts really
do encrypt a DH-triple, without revealing anything else.
So we want a Sigma protocol for the relation
R:=
{(
(β1,β2,β3,γ1,γ2,γ3), (u, v1,e1, v2,e2, v3,e3)
)
:
vi = gβi, ei = uβigγi for i= 1,2,3 and γ3 = γ1γ2
}
.
The corresponding language LRis precisely the set of tuples (u, v1,e1, v2,e2, v3,e3) such that the
ciphertexts (v1,e1),(v2,e2),(v3,e3) encrypt a DH-triple under the public key u.
While the relation Ris inherently non-linear because of the condition γ3 = γ1γ2, we can
nevertheless design a Sigma protocol for Rusing the generic linear protocol from Section 19.5.3.
827
The basic idea is that Alice proves to Charlie that there exist β1,β3,γ1,τ satisfying the system of
equations:
v1 = gβ1, e 1 = uβ1gγ1, v 3 = gβ3, v γ1
2 = gτ, e γ1
2 uβ3 = e3uτ. (20.4)
To prove that this works, we claim that ( u, v1,e1, v2,e2, v3,e3) ∈LR if and only if there
exist β1,β3,γ1,τ satisfying (20.4). Observe that the ciphertexts ( v1,e1),(v2,e2),(v3,e3) uniquely
determine βi’s and the γi’s satisfying (20.3). These values of β1, β3, and γ1 are also the unique values
satisfying the ﬁrst three equations in (20.4). The fourth equation in (20.4) is satisﬁed uniquely by
setting τ := γ1β2. So it remains to consider the last equation in (20.4). The left-hand side is
eγ1
2 uβ3 = (uβ2gγ2)γ1uβ3 = uβ3+τgγ1γ2,
while the right-hand side is
e3uτ = (uβ3gγ3)uτ = uβ3+τgγ3.
So this equation is satisﬁed if and only if γ1γ2 = γ3. That proves the claim.
So this gives us a Sigma protocol for R. To run the protocol, Alice runs the generic linear
protocol for (20.4) using the witness ( β1,β3,γ1,τ := γ1β2). Correctness, soundness, and special
HVZK all follow from the corresponding properties for the generic linear protocol. 2
Example 20.5 (Encrypted bits, again). We can use the idea from the previous example to get
another Sigma protocol for the encrypted bits problem in Example 20.3.
If Alice wants to prove to Charlie that a ciphertext (v,e) is of the form v= gβ, e= uβgb, where
b∈{0,1}, it suﬃces for her to show that b2 = b, as the only values of b∈Zq that satisfy b2 = bare
b= 0 and b= 1.
So, using the generic linear protocol, Alice proves to Charlie that there exist b,β,τ (= βb)
satisfying the system of equations:
v= gβ, e = uβgb, v b = gτ, e b = uτgb.
We leave it to the reader to verify that this yields a sound, special HVZK Sigma protocol for the
relation Rin Example 20.3. The resulting protocol oﬀers similar performance as the encrypted
bits protocol of Example 20.3.
The protocol generalizes to prove to Charlie that a ciphertext (v,e) encrypts a value bsatisfying
0 ≤b<B for some B >2. The generalization uses a Sigma protocol, presented in the next example,
to convince Charlie that b satisﬁes the polynomial relation b(b−1)(b−2) ···(b−(B−1)) = 0.
This relation implies that 0 ≤b < B. The protocol transcript grows linearly in B and therefore
can only be used for small B. 2
Example 20.6 (Polynomial relations). We can extend the idea from Example 20.4 even further.
Suppose Alice has two ciphertexts ( v,e) and (v′,e′) under Bob’s public key u. The ﬁrst ciphertext
encrypts a group element gγ and the second encrypts gγ′
. Alice wants to convince Charlie that
γ′= f(γ) for some speciﬁc polynomial f(x) = ∑d
i=0 λixi. We shall assume that the degree d and
the coeﬃcients λ0,...,λ d of f(x) are ﬁxed, public values (constants or system parameters).
So we want a Sigma protocol for the relation
R=
{
( (β,γ,β ′,γ′),(u, v,e, v′,e′) ) : v= gβ, e= uβ ·gγ, v′= gβ′
, e′= uβ′
·gγ′
, γ′= f(γ)
}
.
828
To get a Sigma protocol for R, Alice and Charlie use the generic linear protocol, where Alice
proves to Charlie that there exist
β, γ 1,...,γ d, τ 1,...,τ d−1, β ′,γ′
satisfying the system of equations:
v= gβ, e = uβgγ1, v ′= gβ′
, e ′= uβ′
gγ′
, γ ′= λ0 + λ1γ1 + ··· + λdγd,
vγi = gτi, e γi = uτigγi+1 (i= 1,...,d −1).
Note that here, we are using the generalized version of the generic linear protocol, which handles
the equations over both G and Zq (see discussion after Theorem 19.11). Alice runs the protocol
using γi := γi for i= 1,...,d and τi := βγi for i= 1,...,d −1. The reader may verify that these
are in fact the only values that satisfy this system of equations. This is easily seen by a simple
induction argument. It follows that the resulting Sigma protocol is a sound, special HVZK Sigma
protocol for the relation R. 2
The above examples all illustrate the notion of alanguage reduction. In general, such a reduction
from R⊆X×Y to R′ ⊆X ′×Y′ is a pair of eﬃciently computable maps f : X×Y→X ′ and
g: Y→Y ′, such that
(i) ( f(x,y),g(y)) ∈R′for all (x,y) ∈R, and
(ii) g(y) ∈LR′ =⇒ y∈LRfor all y∈Y.
Using such a reduction, we can use a Sigma protocol Π ′for R′to build a Sigma protocol Π for R.
The ﬁrst condition ensures that Π inherits correctness and special HVZK from Π ′, and the second
ensures that Π inherits soundness from Π ′. Knowledge soundness need not always be inherited —
that is, it is not required that a witness for ycan be recovered from a witness for g(y). In almost all
of the above examples above, the relation R′was a special case of the generic linear relation. The
only exception was Example 20.3, where the relation R′arose from the OR-proof construction.
20.2.1 A generic protocol for non-linear relations
In several of the examples above, we saw that we could use the generic linear protocol to prove
certain non-linear relations. We now show how to do this with much greater generality. As we
will see, the protocol for polynomial evaluation in Example 20.6 can be easily derived as a special
case of this construction. This same general construction could also be used to derive protocols for
the problems in Examples 20.4 and 20.5; however, the resulting protocols would not be quite as
eﬃcient as the ones presented in those two examples.
As usual, let G be a cyclic group of prime order q generated by g ∈G. Consider the generic
linear protocol in Section 19.5.3. That protocol works with formulas φ of the form described in
(19.13). Suppose that we also allow non-linear equations of the form xi = xj ·xk in φ. To make
this construction work, we will require that for each such non-linear equation, φ also contains two
auxiliary equations, which are of the form
v= gxℓ and e= uxℓgxj, (20.5)
829
where u,v, and eare group elements, and xℓ is some variable. To keep things simple, let us assume
that in the description of φ, there is a pointer of some kind from each non-linear equation to the
corresponding auxiliary equations.
We can transform such a formula φinto a formula φ′that can be handled by the generic linear
protocol, as follows. For each non-linear equation xi = xj ·xk in φ, with corresponding auxiliary
equations as in (20.5), we introduce a new temporary variable t, and replace xi = xj ·xk by the
pair of equations
vxk = gt and exk = uthxi. (20.6)
The result of this transformation is a formula φ′ that can be handled by the generic linear
protocol. The Sigma protocol for φworks as follows. Both prover and veriﬁer can transform φinto
φ′. Suppose the prover has an assignment ( α1,...,α n) to the variables (x1,...,x n) that makes the
formula φtrue. For each non-linear equation xi = xj ·xk in φ, the prover assigns to the temporary
variable tin (20.6) the value αkαℓ, and then runs the generic linear protocol for φ′with the veriﬁer,
using this extended assignment.
We leave it to the reader to verify that this transformation yields a Sigma protocol that is
special HVZK and provides special soundness for the relation (19.14), where the formulas φ are
now allowed to have the non-linear form described above.
Polynomial evaluation, again. The protocol in Example 20.6 can be derived using this trans-
formation. With notation as in that example, Alice proves to Charlie that there exist
β, γ 1,...,γ d, β ′,γ′
satisfying the system of equations:
v= gβ, e = uβgγ1, v ′= gβ′
, e ′= uβ′
gγ′
, γ ′= λ0 + λ1γ1 + ··· + λdγd,
γi+1 = γ1 ·γi (i= 1,...,d −1).
The reader should verify that the non-linear to linear transformation converts each equation γi+1 =
γ1 ·γi to the pair of equations vγi = gτi and γi+1 = γ1 ·γi.
Encrypted bits, yet again. The protocol in Example 20.5 can be derived using this transfor-
mation. Alice proves to Charlie that there exist b,β such that
v= gβ, e = uβgb, b = b·b.
We leave it to the reader to show that applying the non-linear to linear transformation to this
system of equations yields precisely the protocol in Example 20.5.
Encrypted DH triples, again. We could also attempt to use this technique to design a protocol
for the problem in Example 20.4. The most obvious approach would be for Alice to prove to Charlie
that there exist
β1,β2,β3, γ 1,γ2,γ3
such that
vi = gβi, ei = uβigγi for i= 1,2,3 and γ3 = γ1γ2.
We can just plug this system of equations in the above non-linear to linear transformation. This
works, but the resulting protocol would not be quite as eﬃcient as the one in Example 20.4.
830
Removing constraints on the non-linear equation. While our generic transformation is
quite useful, it is still somewhat constrained. Indeed, we essentially require that for each non-linear
equation xi = xj·xk, the system of equations must also include equations describing the encryption
of either xj or xk using multiplicative ElGamal. Later, in Section 20.4.3, we will see how to drop
this requirement, if we are willing to work with a weaker (but still useful) form of HVZK (or a
weaker form of special soundness — see Exercise 20.6).
20.3 Non-interactive proof systems
In this section, we show how to use the Fiat-Shamir transform (see Section 19.6.1) to convert any
Sigma protocol into a non-interactive proof system.
The basic idea is very simple: instead of relying on a veriﬁer to generate a random challenge,
we use a hash function H to derive the challenge from the statement and the commitment. If we
model H as a random oracle, then we can prove the following:
(i) if the Sigma protocol is sound, then so is the non-interactive proof system;
(ii) if the Sigma protocol is special HVZK, then running the non-interactive proof system does
not reveal any useful information about the prover’s witness.
The ﬁrst property is a fairly straightforward adaptation of the notion of soundness to the non-
interactive setting. The second property is a new type of “zero knowledge” property that is a bit
tricky to deﬁne.
20.3.1 Example: a voting protocol
Before getting into the formalities, we illustrate the utility of non-interactive proofs by showing
how they can be used in the context of voting protocols.
It takes considerable eﬀort to properly model a voting protocol — just formulating all of the
security requirements is quite challenging. We will not attempt to do this here; rather, we will just
illustrate some of the essential ideas, and hint at some of the remaining issues.
Suppose we have n voters, where each voter wants to cast a vote of 0 or 1. At the end of the
election, all the parties should learn the sum of the votes.
Of course, each voter could simply publish their vote. However, this is not such a great solution,
as we would like to allow voters to keep their votes private. To this end, some voting protocols
make use of an encryption scheme, so that each voter publishes an encryption of their vote.
A convenient scheme to use for this purpose is the multiplicative variant of the ElGamal scheme,
discussed in Section 20.2. Again, the setting is that we have a cyclic group G of prime order q
generated by g∈G. The secret key is α∈Zq and the public key is u:= gα ∈G. An encryption of
m∈G is (v,e), where v:= gβ, e:= uβ ·m.
Here is an initial attempt at a voting protocol that provides some privacy to the voters.
Suppose that we have a trusted server, called the vote tallying center (VTC) , that runs the key
generation algorithm, obtaining a public key pk = u and a secret key sk = α. It publishes pk for
all voters to see, and keeps sk to itself.
Voting stage. In the voting stage, the ith voter encrypts its vote bi ∈{0,1}by encoding the
vote bi as the group element gbi ∈G, and encrypting this group element under the VTC’s public
831
key, obtaining a ciphertext (vi,ei). Note that vi = gβi and ei = uβi ·gbi, where βi ∈Zq is chosen at
random. All of these ciphertexts are published.
Tallying stage. The VTC takes all of the published ciphertexts and aggregates them into a
single ciphertext (v∗,e∗), where
v∗:=
n∏
i=1
vi and e∗:=
n∏
i=1
ei.
If β∗:= ∑
iβi and σ:= ∑
ibi, then we see that
v∗= gβ∗ and e∗= uβ∗gσ.
Thus, (v∗,e∗) is an encryption of gσ. So, the VTC can decrypt ( v∗,e∗) and publish the result, so
all the voters can see gσ. Since σ itself is a small number, it is easy to compute σ from gσ, just by
brute-force search or table lookup.
If all the voters and the VTC correctly follow the protocol, then, at least intuitively, the semantic
security of ElGamal encryption ensures that no voter learns anyone else’s vote at the end of the
voting stage. Moreover, at the end of the tallying stage, the voters learn only the sum of the votes.
No extra information about any of the votes is revealed.
The above protocol is not very robust, in the sense that if any of the voters or the VTC are
corrupt, both the correctness of the election result and the privacy of the votes may be compromised.
For the time being, let us continue to assume that the VTC is honest (some of the exercises in this
chapter will develop ideas that can be used to prevent the VTC from cheating). Rather, let us
focus on the possibility of a cheating voter.
One way a voter can cheat is to encrypt a vote other than 0 or 1. So, for example, instead of
encrypting the group element g0 or g1, he might encrypt the group element g100. This would be
equivalent to casting 100 1-votes, which would allow the voter to unfairly inﬂuence the outcome of
the election.
To prevent this, when a voter casts its vote, we might insist that he proves that its encrypted
vote (v,e) is valid, in the sense that it is of the form ( gβ,uβ ·gb), where b ∈{0,1}. To do this,
we apply the Fiat-Shamir transform to the Sigma protocol in Example 20.3. The voter (using the
witness ( b,β)) simply runs the prover’s logic in Fig. 20.1, computing the challenge for itself by
hashing the statement and the commitment, in this case, as
c←H( (u,v,e ), (vt0,wt0,vt1,wt1) ). (20.7)
The voter then publishes the proof
π= ( (vt0,wt0,vt1,wt1), (c0,βz0,βz1) ), (20.8)
along with the ciphertext ( v,e). Anyone (in particular, the VTC) can verify the validity of the
proof π by checking that the same conditions that the veriﬁer would normally check in Fig. 20.1
are satisﬁed, where c is computed from the hash function as in (20.7).
As we shall see, if we model the hash function H as a random oracle, then the proof is sound,
in the sense that it is computationally infeasible to come up with a valid proof if the encrypted
vote is not valid. Moreover, the zero-knowledge property will ensure that the proof itself does not
leak any additional information about the vote. Indeed, if we deﬁne a new, augmented encryption
832
scheme where ciphertexts are of the form (v,e,π ), as above, then one can show that this augmented
encryption scheme is semantically secure (under the DDH assumption, withHmodeled as a random
oracle model). In fact, this augmented encryption scheme is very similar to the scheme EaMEG
introduced in Section 19.9.2.1. Moreover, the analysis in Section 19.9.2.2 carries over to this
scheme, so that one can show that this scheme is parallel CCA secure (see Exercise 20.28) and
hence non-malleable (but just as in Section 19.9.2.2, the security reduction is very loose). In the
application to voting, this is typically a a suﬃcient security property, rather than full CCA security.
However, see Exercise 20.20 for a similar encryption scheme that provides full CCA security (with
a tight security reduction) that is suitable for use in voting protocols as well as other applications.
We can optimize this proof system along the same lines that we optimized Schnorr’s signatures
in Section 19.2.3. Namely, instead of a proof π as in (20.8), we can use a proof of the form
π∗= (c0,c1,βz0,βz1).
To verify such a proof, one derives the values vt0,wt0,vt1,wt1 from the veriﬁcation equations (com-
puting vt0 ←gβz0/vc0, and so on), and then checks that c0 ⊕c1 = H((u,v,e ),(vt0,wt0,vt1,wt1)). In
practice, one would use this optimized system, as the proofs are much more compact, and provide
the same security properties (both soundness and zero knowledge) as the unoptimized system. See
Exercise 20.14 for more general conditions under which this type of optimization is possible.
20.3.2 Non-interactive proofs: basic syntax
We now get down to the business of deﬁning non-interactive proofs in general, their security prop-
erties, and the details of the Fiat-Shamir transform.
We begin by deﬁning the basic syntax of a non-interactive proof.
Deﬁnition 20.3 (Non-interactive proof system). Let R⊆X×Y be an eﬀective relation. A
non-interactive proof system for Ris a pair of algorithms (GenPrf ,VrfyPrf ), where:
• GenPrf is an eﬃcient probabilistic algorithm that is invoked as π ←R GenPrf (x,y), where
(x,y) ∈R, and π belongs to some proof space PS;
• VrfyPrf is an eﬃcient deterministic algorithm that is invoked as VrfyPrf (y,π), where y∈Y
and π ∈PS; the output of VrfyPrf is either accept or reject. If VrfyPrf (y,π) = accept, we
say π is a valid proof for y.
We require that for all (x,y) ∈R, the output of GenPrf (x,y) is always a valid proof for y.
20.3.3 The Fiat-Shamir transform
We now present in detail the Fiat-Shamir transform that converts a Sigma protocol into non-
interactive proof system.
Let Π = ( P,V ) be a Sigma protocol for a relation R⊆X×Y . Assume that conversations
(t,c,z ) for Π belong to T ×C×Z. Let H : Y×T →C be a hash function. We deﬁne the Fiat-
Shamir non-interactive proof system FS-Π = (GenPrf ,VrfyPrf ), with proof space PS= T×Z , as
follows:
• on input (x,y) in R, GenPrf ﬁrst runs P(x,y) to obtain a commitment t∈T ; it then feeds
the challenge c:= H(y,t) to P(x,y), obtaining a response z∈Z; the output is (t,z) ∈T×Z ;
833
• on input (y,(t,z)) ∈Y× (T ×Z), VrfyPrf veriﬁes that (t,c,z ) is an accepting conversation
for y, where c:= H(y,t).
20.3.4 Non-interactive soundness
We next adapt our deﬁnition of soundness to the non-interactive setting. Essentially, the deﬁnition
says that it is hard to cook up a valid proof of a false statement.
Attack Game 20.2 (Non-interactive Soundness). Let Φ = ( GenPrf ,VrfyPrf ) be a non-
interactive proof system for R⊆X×Y with proof space PS. To attack Φ, an adversary Aoutputs
a statement y∗∈Y and a proof π∗∈PS.
We say that the adversary wins the game if VrfyPrf (y∗,π∗) = accept but y∗ /∈LR. We deﬁne
A’s advantage with respect to Φ, denoted niSndadv[A,Φ], as the probability that Awins the game.
2
Deﬁnition 20.4. We say that Φ is sound if for all eﬃcient adversaries A, the quantity
niSndadv[A,Φ] is negligible.
We next show that under appropriate assumptions, the Fiat-Shamir transform yields a sound
non-interactive proof system, if we model the hash function as a random oracle.
Theorem 20.2 (Fiat-Shamir-based proofs are sound).Let Π be a Sigma protocol for a relation
R⊆X×Y , and let FS-Π be the Fiat-Shamir non-interactive proof system derived from Π with hash
function H. If Π is sound, and if we model H as a random oracle, then FS-Π is sound.
In particular, let Abe an adversary attacking the soundness of FS-Π as in the random oracle ver-
sion of Attack Game 20.2. Moreover, assume that Aissues at most Qro random oracle queries.
Then there exists an adversary Bthat attacks the soundness of Π as in Attack Game 20.1, where
Bis an elementary wrapper around A, such that
niSndroadv[A,FS-Π] ≤(Qro + 1)Sndadv[B,Π].
Proof sketch. The basic idea is similar to what we did in the proof of security of Schnorr’s signature
scheme (Theorem 19.7). Suppose that Aproduces a valid proof ( t∗,z∗) on a false statement y∗.
This means that (t∗,c,z ∗) is a valid conversation for y∗, where cis the output of the random oracle
at the point ( y∗,t∗). Without loss of generality, we can assume that Aqueries the random oracle
at this point — if not, we make it so, increasing the number of random oracle queries to Qro + 1.
Our adversary Bthen starts out by guessing (in advance) which of the A’s random oracle queries
will be the relevant one. At the point when Amakes that random oracle query, Binitiates a proof
attempt with its own challenger, supplying y∗as the statement and t∗as the commitment message;
B’s challenger responds with a random challenge c, which Bforwards to Aas if this were the value
of the random oracle at the point ( y∗,t∗). If B’s guess was correct, then the value z∗ in A’s proof
will let Bsucceed in his attack game. The factor ( Qro + 1) in the concrete security bound comes
from the fact that B’s guess will be correct with probability 1 /(Qro + 1). 2
20.3.5 Non-interactive zero knowledge
Let Φ = (GenPrf ,VrfyPrf ) be a non-interactive proof system for a relation R⊆X×Y with proof
space PS. We wish to deﬁne a useful notion of “zero knowledge”. Intuitively, we want this notion
834
to capture the idea that the output of GenPrf on input ( x,y) reveals nothing more than the fact
that y∈LR.
Deﬁning such a notion is rather tricky. The approach we take is similar to the approach we
took for deﬁning HVZK — namely, we want to say that there is a simulator that on input y∈LR
can faithfully simulate the output distribution of GenPrf (x,y). Unfortunately, it is essentially
impossible to make this idea work without giving the simulator some kind of “insider advantage”.
Indeed, if a simulator can generate a valid proof on input y ∈LR, it may very well be the case
that it outputs a valid proof on input y /∈LR, which would violate soundness; moreover, if the
simulator failed to output a valid proof on input y /∈LR, we could use the simulator itself to
distinguish between elements of LR and elements of Y\ LR, which for most languages of interest
is computationally infeasible.
We shall only attempt to formulate non-interactive zero knowledge in the random oracle model,
and the “insider advantage” that we give to our simulator is that it is allowed to simultaneously
manage both the simulated output of GenPrf and the access to the random oracle.
Suppose that Φ makes use of a hash function H : U→C , and that we wish to model H as a
random oracle. A simulator for Φ is an interactive machine Sim1 that responds to a series of
queries, where each query is one of two types:
• (sim-proof-query,y), where y∈Y, to which Sim replies with π∈PS;
• (sim-oracle-query,u), where u∈U, to which Sim replies with c∈C.
Our deﬁnition of non-interactive zero knowledge (niZK) says that an eﬃcient adversary cannot
distinguish between a “real world”, in which it asks for real proofs of true statements and a “simu-
lated world” in which it just gets simulated proofs as generated by Sim. In both worlds, the hash
function H is modeled as a random oracle, and the adversary gets to make random oracle queries,
but in the simulated world, Sim processes these queries as well.
Attack Game 20.3 (Non-interactive zero knowledge). Let Φ = ( GenPrf ,VrfyPrf ) be a
non-interactive proof system for a relation R⊆X×Y with proof space PS. Suppose that Φ makes
use of a hash function H : U→C , which is modeled as a random oracle. Let Sim be a simulator for
Φ, as above. For a given adversary A, we deﬁne two experiments, Experiment 0 and Experiment 1.
In both experiments, the adversary makes a series of queries to the challenger, each of which is of
the form:
• a proof query, which is of the form ( x,y) ∈ R, and to which the challenger replies with
π∈PS;
• a random oracle query , which is of the form u∈U, and to which the challenger replies with
c∈C.
In Experiment 0 (the “real world”), the challenger choosesO∈ Funs[U,C] at random, answering
each proof query ( x,y) ∈R by running GenPrf (x,y), using Oin place of H, and answering each
random oracle query u∈U with O(u).
In Experiment 1 (the “simulated world”), the challenger answers each proof query ( x,y) ∈R
by passing ( sim-proof-query,y) to Sim, and answers each random oracle query u ∈U by passing
(sim-oracle-query,u) to Sim.
1Formally, a simulator should be an eﬃcient interface, as in Deﬁnition 2.12.
835
initialization:
initialize an empty associative array Map : Y×T →C;
upon receiving (sim-proof-query,y):
c←R C, (t,z) ←R Sim1(y,c)
if (y,t) /∈Domain(Map)
then Map[y,t] ←c
return (t,z)
else return fail;
upon receiving (sim-oracle-query,(ˆy,ˆt)):
if (ˆy,ˆt) /∈Domain(Map) then Map[ˆy,ˆt] ←R C
return Map[ˆy,ˆt]
Figure 20.2: niZK Simulator for Fiat-Shamir
For b= 0,1, let Wb be the event that Aoutputs 1 in Experiment b. We deﬁne A’s advantage
with respect to Φ and Sim as
niZKadv[A,Φ,Sim] :=
⏐⏐Pr[W0] −Pr[W1]
⏐⏐. 2
Deﬁnition 20.5. We say Φ provides non-interactive zero knowledge (niZK) in the ran-
dom oracle model , if there exists an eﬃcient simulator Sim for Φ, such that for every eﬃcient
adversary A, the value niZKadv[A,Φ,Sim] is negligible.
We note that in the simulated world in Attack Game 20.3, for the proof queries, the adversary
must supply a witness, even though this witness is not passed along to the simulator. Thus, the
simulator only needs to generate simulated proofs for true statements. We do not require that the
simulator always returns a valid proof, but this should happen with overwhelming probability, if
Deﬁnition 20.5 is to be satisﬁed.
We next show that the Fiat-Shamir transform always yields niZK, provided the underlying
Sigma protocol is special HVZK and has unpredictable commitments (see Deﬁnition 19.7).
Theorem 20.3 (Fiat-Shamir-based proofs are zero knowledge). Let Π = (P,V ) be a special
HVZK Sigma protocol for a relation R⊆X×Y with unpredictable commitments, and let FS-Π be
the Fiat-Shamir non-interactive proof system derived from Π with hash function H. If we model H
as a random oracle, then FS-Π is niZK.
In particular, there exists a simulator Sim such that if Ais an adversary that attacks FS-Π and
Sim as in Attack Game 20.3, making at most Qp proof queries and at most Qro random oracle
queries, and if Π has δ-unpredictable commitments, then we have
niZKadv[A,FS-Π,Sim] ≤Qp(Qp + Qro) ·δ. (20.9)
Proof sketch. The basic idea is similar to one we already saw in the proof of security of Schnorr’s
signature scheme in Theorem 19.7. Our niZK simulator is given in Fig. 20.2. Here, we assume
836
that Sim1 is the simulator guaranteed by the special HVZK property for Π. We leave it to the
reader to verify the inequality (20.9) — the argument is very similar to that made in the proof of
Theorem 19.7. 2
20.3.6 An example: applying the Fiat-Shamir transform to the Chaum-
Pedersen protocol
In this section, we work out the details of applying the Fiat-Shamir transform to a particular Sigma
protocol, namely, the Chaum-Pedersen protocol from Section 19.5.2.
Recall that the Chaum-Pedersen protocol allows a prover to convince a skeptical veriﬁer that a
given triple is a DH-triple. Speciﬁcally, it is a Sigma protocol for the relation
R:=
{(
β, (u,v,w )
)
∈Zq ×G3 : v= gβ and w= uβ
}
.
Here, G is a group of prime order q generated by g ∈G. Also, a statement is ( u,v,w ) ∈G3, and
a witness for such a statement is β ∈Zq satsifying v = gβ and w = uβ. The reader should review
the details of the Chaum-Pedersen protocol in Fig. 19.7. The challenge space Cis a subset of Zq.
An accepting conversation for a statement ( u,v,w ) ∈G3 is of the form
((vt,wt),c,β z) ∈G2 ×C× Zq such that gβz = vt ·vc and uβz = wt ·wc.
To apply the Fiat-Shamir transform, we ﬁrst need a hash function
H : G3 ×G2 →C.
The resulting non-interactive proof system Φ = ( GenPrf ,VrfyPrf ) is deﬁned as follows:
• on input
(
β, (u,v,w )
)
∈R, GenPrf computes
βt ←R Zq, vt ←gβt, wt ←uβt
c←H( (u,v,w ), (vt,wt) ) ∈C
βz ←βt + βc
output ( (vt,wt), βz ) ∈G2 ×Zq;
• on input
(
(u,v,w ), ( (vt,wt), βz )
)
∈G3 ×(G2 ×Zq), VrfyPrf computes
c←H( (u,v,w ), (vt,wt) )
if gβz = vt ·vc and uβz = wt ·wc
then output accept
else output reject.
Here, the proof is ( ( vt,wt), βz ) ∈G2 ×Zq.
Soundness. Assuming N := |C|is super-poly, and if we model H as a random oracle, then Φ is
sound (as in Deﬁnition 20.4). In particular, let Abe an adversary attacking the soundness of Φ as
in the random oracle version of Attack Game 20.2. Moreover, assume that Aissues at most Qro
random oracle queries. Then
niSndroadv[A,Φ] ≤(Qro + 1)/N.
837
This means that the probability that Acan create a valid proof of a false statement is at most
(Qro +1)/N. This follows from Theorems 20.2, 20.1, and 19.10. In Attack Game 20.2, the adversary
is allowed to make one attempt at creating a valid proof of a false statement. As you are asked to
show in Exercise 20.18, if the adversary is allowed to make r≥1 attempts at creating a valid proof
of a false statement, he will succeed with probability most ( Qro + r)/N.
Zero knowledge. If we model H as a random oracle, then Φ provides non-interactive zero
knowledge (as in Deﬁnition 20.5). In particular, there exists a simulator Sim such that if Ais an
adversary that attacks Φ and Sim as in Attack Game 20.3, making at most Qp proof queries and
at most Qro random oracle queries, then we have
niZKadv[A,Φ,Sim] ≤Qp(Qp + Qro)/q.
This follows from Theorems 20.3 and 19.10, along with the fact that the Chaum-Pedersen protocol
has (1/q)-unpredictable commitments (as in Deﬁnition 19.7).
An optimized version of the proof system. We can optimize this proof system along the
same lines that we optimized Schnorr’s signatures in Section 19.2.3 to obtain a system with more
compact proofs. This optimized non-interactive proof system Φ ′= (GenPrf ′,VrfyPrf ′) is deﬁned
as follows:
• on input
(
β, (u,v,w )
)
∈R, GenPrf ′computes
βt ←R Zq, vt ←gβt, wt ←uβt
c←H( (u,v,w ), (vt,wt) ) ∈C
βz ←βt + βc
output (c,βz) ∈C× Zq;
• on input
(
(u,v,w ), (c,βz)
)
∈G3 ×(G2 ×Zq), VrfyPrf ′computes
vt ←gβz/vc
wt ←uβz/wc
if c= H( (u,v,w ), (vt,wt) )
then output accept
else output reject.
The proof now is ( c,βz) ∈C× Zq. All of the results above on soundness and zero knowledge for Φ
are also true for Φ ′— see Exercises 20.14 and 20.18(b).
20.4 Computational zero-knowledge and applications
It turns out that for some relations, we need a more relaxed notion of zero knowledge in order to
get an eﬃcient Sigma protocol. We will motivate and illustrate the idea with an example.
838
20.4.1 Example: range proofs
We again use the multiplicative ElGamal encryption scheme that we used in the examples in
Section 20.2. Bob’s public key is u= gα ∈G and his secret key is α∈Zq. As usual, G is a cyclic
group of order q with generator g∈G.
Suppose we generalize Example 20.3, so that instead of encrypting a bit b, Alice encrypts a d-bit
number x, so x∈{0,..., 2d−1}. To perform the encryption, Alice encodes xas the group element
gx, and then encrypts this group element under Bob’s public key. The resulting ciphertext will be
of the form ( v,e), where v= gβ and e= uβgx. We shall assume that 2 d <q, so that the encoding
of xis one-to-one. As usual, Alice wants to convince Charlie that ( v,e) does indeed encrypt a d-bit
number under Bob’s public key, without revealing anything else.
So we want a Sigma protocol for the relation
R=
{
( (β,γ,x ), (u,v,e ) ) : v= gβ, e= uβ ·gx, x∈{0,..., 2d −1}
}
. (20.10)
Here, we will assume that d is a ﬁxed, public value.
A straightforward approach is just to use the same OR-proof technology that we used in Ex-
ample 20.3. Namely, Alice essentially proves that x= 0, or x= 1, or . . . , x= 2d −1. While this
idea works, the communication and computational complexity of the resulting Sigma protocol will
be proportional to 2d. It turns out that we can do much better. Namely, we can construct a Sigma
protocol that scales linearly in d, rather than exponentially in d.
Here is how. Alice starts by writing x in binary, so x = ∑
i2ibi, where bi ∈{0,1}for i =
0,...,d −1. Next, next Alice encrypts each bit. To get a simpler and more eﬃcient protocol,
she uses the variation of the ElGamal encryption scheme discussed in Exercise 11.8. Speciﬁcally,
Alice generates a random public key (u0,...,u d−1) ∈Gd; she then chooses β0 ∈Zq at random, and
computes v0 ←gβ0; ﬁnally, she computes ei ←uβ0
i gbi for i= 0,...,d −1. So ( v0,e0,...,e d−1) is an
encryption of (b0,...,b d−1) under the public key (u0,...,u d−1). Alice then sends v0, (u0,...,u d−1),
and (e0,...,e d−1) to Charlie, and proves to him that (i) each encrypted value bi is a bit, and (ii)∑
i2ibi = x. To prove (i), Alice will use a technique similar to that used in Example 20.5, exploiting
the fact that bi ∈{0,1} ⇐⇒b2
i = bi.
To prove (i) and (ii), Alice and Charlie can use the generic linear protocol from Section 19.5.3.
So Alice proves to Charlie that there exist
β,x, β 0, b 0,...,b d−1, τ 0,...,τ d−1
such that
v= gβ, e = uβgx,
v0 = gβ0
ei = uβ0
i gbi, v bi
0 = gτi, e bi
i = uτi
i gbi (i= 0,...,d −1),
x= b0 + 2b1 + ··· + 2d−1bd−1.



(20.11)
The ﬁrst line of (20.11) says that (v,e) encrypts gx under u. The third line says that each encrypted
value bi is a bit, using a variant of the technique in Example 20.5, where τi = β0bi. The fourth line
says that these bits are precisely the bits in the binary representation of x.
So the overall structure of the protocol is as follows:
839
1. Alice generates v0, (u0,...,u d−1), and (e0,...,e d−1), and sends these auxiliary group elements
to Charlie.
2. Alice and Charlie engage in the generic linear Sigma protocol for the system of equations
(20.11).
The ﬁrst observation we make is that by having Alice “piggyback” the auxiliary group elements
on top of the commitment message of the generic linear Sigma protocol, the overall protocol has
the basic structure of a Sigma protocol.
We leave it to the reader to verify that the protocol provides soundness.
The question of interest to us here is: in what sense is this protocol zero knowledge? The
problem is that while the generic linear protocol is special HVZK, the overall protocol is not, in the
sense that the encryptions of the bits of x could conceivably leak information about x to Charlie.
Intuitively, under the DDH assumption, these encryptions should not leak any information. So the
protocol is still zero knowledge, but only in a computational sense. To put this on ﬁrmer ground,
we need to formulate the notion of special computational HVZK.
20.4.2 Special computational HVZK
We relax Deﬁnition 19.5, which deﬁnes the notion of special HVZK for a Sigma protocol, to obtain
the weaker notion of special computational HVZK, or special cHVZK, for short. The idea is that
instead of requiring that the distributions of the real and simulated deﬁnitions be identical, we only
require them to be computationally indistinguishable.
Let Π = (P,V ) be a Sigma protocol forR⊆X×Y , with challenge spaceC. As in Deﬁnition 19.5,
a simulator for Π is an eﬃcient probabilistic algorithm Sim that takes as input (y,c) ∈Y×C , and
always outputs a pair ( t,z) such that (t,c,z ) is an accepting conversation for y.
Attack Game 20.4 (Special cHVZK). Let Π = ( P,V ) be a Sigma protocol for R⊆X×Y ,
with challenge space C. Let Sim be a simulator for Π, as above. For a given adversary A, we
deﬁne two experiments, Experiment 0 and Experiment 1. In both experiments, Astarts out by
computing (x,y) ∈R and submitting (x,y) to the challenger.
• In Experiment 0, the challenger runs the protocol between P(x,y) and V(y), and gives the
resulting conversation (t,c,z ) to A.
• In Experiment 1, the challenger computes
c←R C, (t,z) ←R Sim(y,c),
and gives the simulated conversation ( t,c,z ) to A.
At the end of the game, Acomputes and outputs a bit ˆb∈{0,1}.
For b= 0,1, let Wb be the event that Aoutputs 1 in Experiment b. We deﬁne A’s advantage
with respect to Π and Sim as
cHVZKadv[A,Π,Sim] :=
⏐⏐Pr[W0] −Pr[W1]
⏐⏐. 2
Deﬁnition 20.6. We say Π is special computational HVZK , or special cHVZK , if
there exists a simulator Sim for Π, such that for every eﬃcient adversary A, the value
cHVZKadv[A,Π,Sim] is negligible.
840
Many results that hold for special HVZK Sigma protocols also hold for special cHVZK Sigma
protocols:
• Theorem 19.15 also holds if we use a cHVZK protocol instead of an HVZK protocol, although
the concrete security bound becomes
ID2adv[A,I] ≤ID1adv[B,I] + Q·cHVZKadv[B′,Π,Sim],
where Qis an upper bound on the number of transcripts obtained in the eavesdropping attack.
This factor of Qarises from applying a standard hybrid argument, which allows us to replace
Q real conversations by Q simulated conversations.
• Lemma 19.17 can also be adapted to work with a cHVZK protocol, instead of an HVZK
protocol. The security bound (19.20) becomes
ϵ≤ r
N +
√
rϵ′+ Q·cHVZKadv[B′,Π,Sim],
where, again, Qis an upper bound on the number of transcripts obtained in the eavesdropping
attack.
• Theorem 20.3 also holds if we use a cHVZK protocol instead of an HVZK protocol. Again, the
concrete security bound degrades with an extra additive term of Qp ·cHVZKadv[B,Π,Sim1],
where Qp is the number of proof queries.
We remark, however, that Theorem 19.21 (on witness independence) does not carry over under
cHVZK.
Range proofs. We leave it as a simple exercise to the reader to prove that our protocol in
Section 20.4.1 for proving that an encrypted value lies in the range [0 ,2d) is special cHVZK.
20.4.3 An unconstrained generic protocol for non-linear relations
The technique used in Section 20.4.1 can be generalized, allowing us to add non-linear relations of
the form xi = xj·xk to the systems of linear equations handled by the generic linear protocol, as we
did in Section 20.2.1. However, unlike in Section 20.2.1, we do not require any auxiliary equations.
The price we pay for this generality is that we achieve only special cHVZK, rather than HVZK.
Again, let G be a cyclic group of prime order q generated by g ∈G, and let φ be a formula as
in (19.13), but with non-linear equations of the form xi = xj ·xk as well. Suppose the prover and
veriﬁer are both given φ, and the prover is also given an assignment ( α1,...,α n) to the variables
(x1,...,x n) that satisﬁes φ. The prover generates a new formula φ′, as follows. The prover chooses
β ∈Zq at random, sets v ←gβ, and adds the equation v = gy to φ, where y is a new variable.
Then, for each non-linear equation xi = xj ·xk in φ, the prover chooses u ∈G at random and
computes e←uβgαj, and adds the equations
e= uygxj, v xk = gt, and exk = utgxi (20.12)
to φ, where t is another new variable. This results in a new formula φ′ that can be handled by
the generic linear protocol. The prover then sends to the veriﬁer the collection of auxiliary group
841
elements, consisting of v, along with the group elements u and e corresponding to each non-linear
equation.
Given these auxiliary group elements, the veriﬁer can reconstruct the formula φ′, and now both
prover and veriﬁer can run the generic linear protocol on φ′. The prover assigns the value β to the
variable y, and the valueτ := βαk to the variabletarising from each non-linear equationxi = xj·xk.
Also, the prover can “piggy-back” the auxiliary group elements on top of the commitment message
from the generic linear protocol, so that the resulting protocol has the right communication pattern.
We leave it to the reader to verify that this transformation yields a Sigma protocol that is
special cHVZK (under the DDH assumption, using Exercise 11.8) and provides special soundness
for the relation (19.14), where the formulas φare now allowed to have the non-linear form described
above.
There are a couple of obvious opportunities for eﬃciency improvements to the above trans-
formation. For example, the value u and the ﬁrst equation in (20.12) can be reused across all
non-linear equations in which xj appears as the ﬁrst multiplicand. Similarly, the variable tand the
second equation in (20.12) can be reused across all non-linear equations in which xk appears as the
second multiplicand.
Range proofs, again. It is easy to see that our range proof protocol can be derived using this
transformation. Alice proves to Charlie that there exist
β,x, ,b 0,...,b d−1
such that
v= gβ, e = uβgx, x =
d−1∑
i=0
2ibi, and bi = b2
i (i= 0,...,d −1).
We leave it to the reader to verify that applying the above non-linear to linear transformation yields
precisely the protocol in Section 20.4.1 (with the values v0 and β0 playing the roles of v and β in
the transformation).
20.5 Bulletproofs: compressed Sigma protocols
To be written.
20.6 Succinct non-interactive zero-knowledge proofs (SNARKs)
To be written.
20.7 A fun application: everything that can be proved, can be
proved in zero knowledge
To be written.
20.8 Notes
Citations to the literature to be added.
842
20.9 Exercises
20.1 (Generalized encrypted bits). Use the generalized OR-proof construction from Exer-
cise 19.26 to generalize the encrypted bits protocol from Example 20.3 to give a sound, special
HVZK Sigma protocol for proving that a ciphertext ( v,e) encrypts a value b(encoded as gb) satis-
fying 0 ≤b<B for some constant B >2. Write out the protocol for B = 3.
The following two exercises ask you to design sound, HVZK Sigma protocols for proving properties
on encrypted data, as in Section 20.2, using the multiplicative ElGamal encryption scheme. You
should not use the techniques introduced in Section 20.4, which yield only computational HVZK
protocols. You may, however, use the techniques in Section 20.2.1.
20.2 (A 2-input mixnet). Consider the following generalization of the scenario discussed in
Example 20.2. Here, Alice is implementing a 2-input mixnet service, which can be used to help to
foil traﬃc analysis. In this setting, Alice receives two ciphertexts (v0,e0) and (v1,e1), which encrypt
messages under Bob’s public key u. Alice does not know these messages, but she can re-randomize
the ciphertexts, choosing β0 and β1 in Zq at random, and computing ( v′
i,e′
i) := (vi ·gβi,ei ·uβ) for
i = 0,1. Further, she chooses b ∈{0,1}at random and sets ( v′′
i,e′′
i) := (v′
i⊕b,e′
i⊕b) for i = 0,1.
Finally, she outputs the pair of ciphertexts (v′′
0 ,e′′
0) and (v′′
1 ,e′′
1). Thus, Alice re-randomizes the two
ciphertexts, and with probability 1 /2 she ﬂips their order.
Design a sound, special HVZK Sigma protocol that allows Alice to prove to Charlie that she has
performed this task correctly. That is, she should prove that the output ciphertexts encrypt the
same messages as the input ciphertexts, but with the ordering of the ciphertexts possibly ﬂipped.
The statement for the Sigma protocol should include Bob’s public key, Alice’s two input ciphertexts,
and Alice’s two output ciphertexts.
20.3 (Encrypted polynomial relations). Consider again the task in Example 20.6, where Alice
encrypts gγ and gγ′
under Bob’s public key, and wants to prove to Charlie that γ′= f(γ) for some
polynomial f(x) = ∑d
i=0 λixi. However, suppose now that the coeﬃcients of f are also encrypted
under Bob’s public key. That is, each coeﬃcient λi is encrypted as ( vi,ei) = ( gβi,hβigλi), and
these d+ 1 ciphertexts are included in the statement, along with the ciphertexts ( v,e) and (v′,e′)
encrypting gγ and gγ′
.
Design a sound, special HVZK Sigma protocol for this problem. The complexity (computational
and communication) of your protocol should grow linearly in d.
20.4 (Computational special soundness implies soundness). Prove the following gener-
alization of Theorem 20.1. Suppose Π is a Signma protocol that provides computational special
soundness (as deﬁned in Exercise 19.13) with extractor Ext, and that Π has a large challenge space
of size N. Then Π is sound.
In particular, suppose Ais an adversary attacking the soundness of Π as in Attack Game 20.1,
with advantage ϵ:= Sndadv[A,Π]. Then there exists an eﬃcient adversary B(whose running time
is about twice that of A), such that cSSadv[B,Π,Ext] ≥ϵ2 −ϵ/N.
20.5 (Zero knowledge range proofs). Consider again the range proof problem introduced
in Section 20.4.1, where Alice wants to prove to Charlie that she has encrypted a d-bit integer
843
under Bob’s public key. The Sigma protocol we presented there provides statistical soundness (see
Remark 20.1), but only computational zero knowledge (special cHVZK). This exercise develops
an alternative Sigma protocol for the relation Rdeﬁned in (20.10). This new Sigma protocol is
zero knowledge (special HVZK), but provides only computational soundness under an intractibility
assumption.
Suppose that we have a system parameterh∈G. We assume that his uniformly distributed overG,
and that nobody knows Dloggh(especially Alice). The protocol is the same as that in Section 20.4.1,
except that instead of encrypting each bit bi, Alice just “commits” to it, by computing βi ←R Zq
and ui ←gβihbi. In the protocol, Alice sends u0,...,u d−1 to Charlie, and proves that she knows
β,x, β 0,...,β d−1, b 0,...,b d−1, τ 0,...,τ d−1
such that
v= gβ, e = uβgx,
ui = gβihbi, u bi
i = gτihbi (i= 0,...,d −1),
x= b0 + 2b1 + ··· + 2d−1bd−1
using the generic linear protocol. To run the protocol, Alice sets τi := βibi.
Show that this is a Sigma protocol for Rthat is special HVZK and that provides computational
special soundness (deﬁned in Exercise 19.13) under the DL assumption for G. To prove compu-
tational special soundness, you should make use of the fact that the generic linear protocol itself
provides special soundness.
Hint: If you break computational special soundness, you can compute Dloggh.
20.6 (Zero knowledge protocols for non-linear relations). Design and analyze a construc-
tion for non-linear relations as in Section 20.4.3. However, the resulting protocol should be a
Sigma protocol that is special HVZK and provides computational special soundness (deﬁned in
Exercise 19.13) under the DL assumption.
Hint: Generalize the technique in the previous exercise.
The following four exercises ask you to design sound, computational HVZK Sigma protocols for
proving properties on encrypted data, using the techniques developed in Section 20.4. These protocols
are actually statistically sound (see Remark 20.1). Alternatively, instead of computational zero
knowledge, you may apply the techniques developed in Exercise 20.6 to achieve zero knowledge, but
resulting protocols are only computationally sound.
20.7 (Generalized range proofs). Generalize the protocol in Section 20.4.1, so that instead of
proving that x∈{0,..., 2d−1}, Alice proves to Charlie that x∈[a,b] for arbitrary integers aand
b. For this exercise, you can assume that a and b are ﬁxed, public values. Your protocol should
have complexity proportional to log(b−a), and should be a sound, special cHVZK Sigma protocol
for this problem.
20.8 (Encrypted range proofs). Generalize the previous problem, so that now, the values ga
and gb are encrypted under Bob’s public key. You may assume that b−a <2d for some ﬁxed,
public value d.
844
20.9 (High-degree relations). Consider the following variation on Example 20.6. Instead of
proving to Charlie thatγ′= f(γ), Alice proves thatγ′= γk, for some speciﬁc, large, positive integer
k. Assume that k is a ﬁxed, public value. Your protocol should have complexity proportional to
log k, and should be a sound, special cHVZK Sigma protocol for this problem.
20.10 (Encrypted high-degree relations). Generalize the previous problem, so that now, the
value gk is encrypted under Bob’s public key. You may assume that k <2d for some ﬁxed, public
value d.
20.11 (Encrypting a discrete logarithm). Suppose Alice wants to encrypt a discrete logarithm
under Bob’s public key, and prove to Charlie that she has done so. Again, we are assuming that we
are using the multiplicative ElGamal encryption scheme, as in Section 20.2. So Alice knows γ ∈Zq
such that h= gγ ∈G. She is willing to make the value h public, and wants to somehow encrypt γ
under Bob’s public key u∈G, and prove to Charlie that she has done so.
One approach is the following. Alice can encrypt the bits ofγone at a time, resulting in a ciphertext
containing O(log q) group elements. She can then run a Sigma protocol to convince Charlie that
these bits form the binary representation of γ. Work out the details of this approach.
20.12 (Encrypting a signature). We can use the result of the previous exercise to allow Alice
to veriﬁably encrypt a signature. In this setting, Alice has a signature on a message m under
Bill’s public key. Assume that Bill is using Schnorr’s signature scheme with public key u0 ∈G.
So a signature on m is of the form ( ut,αz), where gαz = ut ·uc
0 and c = H(m,ut). Suppose that
Alice presents to Charlie the values m, ut, and an encryption ψ of αz under Bob’s public key, as
in the previous exercise. Suppose she also presents to Charlie a non-interactive proof π that the
ciphertext ψ indeed encrypts Dlogg(ut ·uc
0). The proof she presents is the Fiat-Shamir proof (see
Section 20.3.3) derived from the Sigma protocol of the previous exercise.
(a) Work out the details of this approach.
(b) Using the soundness property of the Fiat-Shamir non-interactive proof system, argue that
after seeing the values m,ut,ψ,π , and verifying that πis a valid proof, Charlie can be assured
that ψ decrypts to a value from which a valid signature on m can be recovered.
(c) Using the zero-knowledge property of the Fiat-Shamir non-interactive proof system, argue
that after seeing the values m,ut,ψ,π , Charlie cannot forge a signature on m under Bill’s
public key. Formulate this problem as an attack game, and prove that if Charlie can win this
game, he can break the DDH assumption.
20.13 (Broken Fiat-Shamir proofs). In Section 20.3.3, we showed how to turn a Sigma protocol
into a non-interactive proof system by computing the challenge as c := H(y,t), where y is the
statement and tis the commitment. The point of this exercise is to illustrate that the statement y
must be included in the hash to maintain soundness. To this end, suppose that we transform the
Chaum-Pedersen protocol (see Section 19.5.2) into a non-interactive proof by deriving the challenge
from the hash of the commitment only, the resulting non-interactive proof system is not sound.
20.14 (Optimized Fiat-Shamir proofs). We can optimize Fiat-Shamir non-interactive proof
systems (see Section 20.3.3) just as we did Fiat-Shamir signatures in Exercise 19.19. Consider the
Fiat-Shamir proof system scheme derived from a Sigma protocol ( P,V ) for a relation R⊆X×Y .
845
Recall that a proof π for a statement y is ( t,z), where ( t,c,z ) ∈ T ×C×Z is an accepting
conversation, and c:= H(y,t).
Assume that ( P,V ) has backward computable commitments, as in Exercise 19.18, and let f :
Y×C×Z→T be the corresponding function that computes a commitment from a given statement,
challenge, and response. Then we can optimize the Fiat-Shamir proof system, so that instead of
using (t,z) as the proof, we use ( c,z) as the proof. To verify such an optimized proof ( c,z), we
compute t←f(c,z), and verify that c= H(y,t).
(a) Show that Theorem 20.2 holds for the optimized Fiat-Shamir proof system.
(b) We can modify the niZK simulator in Fig. 20.2, so that in processing proof query yi, we return
(ci,zi), instead of (ti,zi). Show that Theorem 20.3 holds for the optimized Fiat-Shamir proof
system, using the modiﬁed simulator.
20.15 (Veriﬁable decryption). In Section 20.3.1, we described a voting protocol, which required
the Vote Tallying Center (VTC) to decrypt a ciphertext ( v∗,e∗) and publish the result. Design a
Sigma protocol that allows the VTC to prove that it performed the decryption correctly. Then
covert the Sigma protocol to a corresponding non-interactive proof system using the optimized
Fiat-Shamir transform from the previous exercise.
20.16 (A veriﬁable random function). The notion of a veriﬁable random function (VRF) was
introduced in Exercise 13.20. This exercise develops an instantiation of this notion — actually, as
we will see, it satisﬁes a slightly weaker property, which is still good enough for most applications.
Let G be a cyclic group of prime orderqgenerated by g∈G. Let Π be the Chaum-Pedersen protocol,
as discussed in Section 19.5.2, for the relation (19.12), and assume that Π has a large challenge
space C. Let Φ be the optimized Fiat-Shamir proof system derived from Π, as in Exercise 20.14,
using a hash function H′: G3 ×G2 →C.
Let F be the PRF deﬁned over ( Zq,M,G) as in Exercise 11.2, so F(k,m) := H(m)k, where
H : M→ G is a hash function. You were asked to show in that exercise that if we model H as a
random oracle, then F is a PRF under the DDH (with a very tight reduction, in fact).
Our VRF is (G′,F′,V ′), which is deﬁned over (M,G); G′chooses k∈Zq at random, kis the secret
key, and gk is the public key; F′(k,m) := (y,π), where y = F(k,m) = H(m)k and π is a proof,
generated using Φ, that (H(m),gk,H(m)k) is a DH-triple; V′(gk,m,y,π ) checks that (H(m),gk,y)
is a DH-triple by verifying the proof π using Φ.
(a) Describe the functions F′and V′in detail.
(b) Using the zero knowledge property for Φ (in particular, Theorem 20.3 and part (b) of Ex-
ercise 20.14), show that if we model both H and H′ as random oracles, then under the
DDH assumption for G, the VRF ( G′,F′,V ′) satisﬁes the VRF security property deﬁned in
Exercise 13.20. Give a concrete security bound (which should be fairly tight).
(c) This VRF does not satisfy the uniqueness property deﬁned in Exercise 13.20. Nevertheless,
it does satisfy a weaker, but still useful property. Using the soundness property for Φ (in
particular, Theorem 20.2 and part (a) of Exercise 20.14), show that it is infeasible for an
adversary to come up with a triple ( m,y,π ) such that V′(gk,m,y,π ) = accept yet y ̸=
F(k,m). Give a concrete security bound.
846
20.17 (Signatures schemes based on DDH and CDH). In the previous exercise, we saw
how to construct a “quasi-VRF” ( G′,F′,V ′) based on the DDH. We can build a signature scheme
Squite easily from this. The key generation algorithm for Sis G′, a signature on a message m
under secret key k is F′(k,m) = (y,π), and the veriﬁcation algorithm on a public key gk, message
m, and signature ( y,π) simply runs V′(gk,m,y,π ).
(a) Using the results of the previous exercise, show that Sis secure under the DDH assumption
in the random oracle model. Give a concrete security bound.
Discussion: We will see a simpler signature scheme based on the DDH below in Exer-
cise 20.23.
(b) Prove that S is secure in the random oracle model under the CDH assumption . Give a
concrete security bound. Can you use the ideas in the proof of Lemma 13.6 to get a better
security bound?
(c) Can you use the ideas in Section 13.5 to modify Sslightly so as to get a signature scheme
with a much tighter reduction to CDH in the random oracle model?
20.18 (Multi-attempt Fiat-Shamir soundness). We can generalize Attack Game 20.2,
allowing the adversary to output many attempts ( y∗
1,π∗
1),..., (y∗
r,π∗
r), winning the game if
VrfyPrf (y∗
j,π∗
j) = accept but y∗
j /∈LR for some j = 1 ,...,r . For such a r-attempt adversary
A, we deﬁne its advantage rniSnd adv[A,Φ,r] to be the probability that Awins the game.
(a) Let Φ be an non-interactive proof system. Show that for every r-attempt adversary Aat-
tacking Φ as above, there exists an adversary Battacking Φ as in Attack Game 20.2, where
Bis an elementary wrapper around A, such that rniSndadv[A,Φ,r] ≤r·niSndadv[B,Φ].
(b) Let Φ be the non-interactive proof derived using the Fiat-Shamir transform from a Sigma
protocol Π. Show that in the random oracle model of the above r-attempt attack game,
if Amakes at most Qro random oracle queries, then there exists an adversary Battack-
ing Π as in Attack Game 20.1, where B is an elementary wrapper around A, such that
rniSndroadv[A,Φ,r] ≤(r+ Qro)Sndadv[B,Π].
Discussion: This reduction is much more eﬃcient than applying the reduction in part (a)
and then the reduction in Theorem 20.2. Note that if Π provides special soundness and the
has a challenge space of size N, then this result, together with Theorem 20.1, implies that
rniSndroadv[A,Φ,r] ≤(r+ Qro)/N.
(c) Show that the result of part (b) holds as well for optimized Fiat-Shamir proofs (see Exer-
cise 20.14).
20.19 (An AD-only CCA secure scheme (II)). The AD-only CCA secure encryption scheme
discussed in Exercise 12.20 relied on an algorithm ODDH for recognizing DH-triples. In this exercise,
we shall see how we can replace such an algorithm by a non-interactive proof system. To this end,
let Φ = (GenPrf ,VrfyPrf ) be a non-interactive proof system for the relation
R= {( β, (u,v,w )) ) ∈ Zq ×G3 : v= gβ, w= uβ };
We presented the details of such a proof system in Section 20.3.6 based on the application of
Fiat-Shamir transform to the Chaum-Pedersen protocol (see Section 19.5.2). As discussed in Sec-
tion 20.3.6, this proof system (as well as the optimized version) provides both soundness and zero
knowledge.
847
The encryption scheme E†
GS = (G†,E†,D†) deﬁned as follows:
• the key generation algorithm runs as follows:
G†() := α∈Zq, u←gα
output (pk,sk) ←(u,α)
• the encryption algorithm runs as follows:
E†(pk := u,m,d ) := β ←R Zq, v←gβ, w←uβ, k←H(v,w), c←R Es(k,m),
u←HG(v,d), w←uβ, π←R GenPrf ( β, (u,v,w ) )
output (v,c,w ,π)
• the decryption algorithm runs as follows:
D(sk := α, (v,c,w ,π), d) := u:= HG(v,d)
if VrfyPrf ( (u,v,w ), π) = reject
then output reject
else w←vα, k←H(v,w), m←Ds(k,c)
output m
Just as in Exercise 12.20, your task is to prove thatE†
GS is AD-only CCA secure in the stronger sense
described there (where the adversary gets ˆw:= ˆvα rather than ˆmin processing decryption queries).
In your proof, you should assume that the CDH assumption holds for G, that Es is semantically
secure, and that Φ provides both soundness (as in Deﬁnition 20.4) and zero knowledge (as in
Deﬁnition 20.5). In particular, show that the advantage of any adversary Ain this strengthened
AD-only CCA attack game is bounded by
2QHk ·CDHadv[Bcdh,G] + 2·niZKadv[Bzk,Φ,Sim] +
2 ·rniSndadv[Bes,Φ,Qd] + Qe ·SSadv[Bs,Es],
where QHk is a bound on the number of HK-queries made by A, Qe is a bound on the number of
encryption queries made by A, Qd is a bound on the number of decryption queries made by A,
Bcdh, Bzk, Bes, and Bs are elementary wrappers around A, Sim is a zero knowledge simulator (as
in Deﬁnition 20.5), and rniSnd adv is as deﬁned in Exercise 20.18.
Hint: Your proof should follow the same outline as that provided in Exercise 12.20, except that
you will now rely on the soundness and zero knowledge properties of Φ, rather than on ODDH. Note
that in Exercise 12.20, we relied on ODDH to get a tight reduction to CDH, but here, our adversary
Bcdh will simply prepare a list of at most QHk group elements, one of which should be a solution
to the given instance of the CDH problem, and then simply guess which one is correct — this is
where the factor QHk comes from.
Discussion: We remind the reader that although the above scheme only provides AD-only CCA
security, it is easily transformed into an eﬃcient scheme that provides full CCA security, as discussed
in Exercise 14.13.
20.20 (An AD-only CCA secure scheme (III)). This exercise develops a multiplicative
version of the encryption scheme in the previous exercise. The encryption schemeE‡
GS = (G‡,E‡,D‡)
with message space G is deﬁned as follows:
848
• the key generation algorithm runs as follows:
G‡() := α∈Zq, u←gα
output (pk,sk) ←(u,α)
• the encryption algorithm runs as follows:
E‡(pk := u,m,d ) := β ←R Zq, v←gβ, w←uβ, e←w·m,
u←HG(v,d), w←uβ, π←R GenPrf ( β, (u,v,w ) )
output (v,e,w ,π)
• the decryption algorithm runs as follows:
D(sk := α, (v,e,w ,π), d) := u:= HG(v,d)
if VrfyPrf ( (u,v,w ), π) = reject
then output reject
else w←vα, m←e/w
output m
Prove that E‡
GS satisﬁes the same security property as in the previous exercise, but under the DDH
assumption rather than the CDH assumption, and speciﬁcally, with the following concrete security
bound:
2 ·
(
DDHadv[Bddh,G] + 1/q+ niZKadv[Bzk,Φ,Sim] + rniSndadv[Bes,Φ,Qd]
)
,
which is independent of the number of encryption queries.
Hint: Apply the random self-reduction for DDH from Exercise 10.11 with n= 1.
Discussion: Again, this scheme may be easily transformed into an eﬃcient scheme that provides
full CCA security, as discussed in Exercise 14.13.
Because of the multiplicative structure of the encryption, it is easy to augment such a ciphertext,
as in the voting protocol in Section 20.3.1, with a proof that the ciphertext encrypts a valid vote.
In fact, as an optimzation, one could fold this proof into the proof π that is already a part of the
ciphertext. To that end, one would replace the relation Rabove by the relation
R′= {( (β,b) (u,v,w ,u,e ) ) : v= gβ, w= uβ, e= uβgb, b∈{0,1}}.
In addition, the associated data can be used to associate an encrypted vote with a particular voter
ID. CCA security ensures these encrypted votes arenon-malleable (see Section 12.2.1), which means
(among other things) that a malicious voter cannot copy or negate the vote of any other voter.
This scheme also can be easily implemented as a threshold decryption scheme (see Section 22.3).
By using threshold decryption, the VTC in the voting protocol can also be distributed among a
number of servers, so that the voting protocol will still work correctly and securely even if some of
these servers are compromised.
Also because of the multiplicative structure of the encryption, one can apply the ideas from Ex-
ercise 20.11 to this scheme to obtain an AD-only CCA or CCA secure encryption scheme that
veriﬁably encrypts the discrete logarithm of a given group element. The construction in the follow-
ing exercise may also be used to obtain a more eﬃcient scheme.
849
20.21 (An AD-only CCA secure scheme (IV)). This exercise generalizes the scheme in the
previous exercise so that instead of encrypting a single message m ∈G, it encrypts n messages
(m1,...,m n), along same lines as in Exercise 11.8. In this multi-message scheme, the secret key
is (α1,...,α n) and the public key is ( u1,...,u n), where each αi is a random element of Zq, and
ui := gαi for i= 1,...,n . Encryption generates a ciphertext ( v,e1,...,e n,w,π), where β, v, u, w,
and π are generated just as before, and ei ←uβ
i ·mi for i= 1,...,n . Decryption checks that the
proof π is valid, and then computes mi ←ei/vαi for i = 1,...,n . Note that the same ciphertext
elements v, w, and πare used to encrypt all nmessage elements m1,...,m n. Prove that this multi-
message scheme satisﬁes the same security property as in the previous exercise with the following
concrete security bound:
2n·
(
DDHadv[Bddh,G] + 1/q+ niZKadv[Bzk,Φ,Sim] + rniSndadv[Bes,Φ,Qd]
)
.
Hint: Apply a hybrid argument.
Discussion: By exploiting simulation soundness (see Exercise 20.22 below), the term 2 n ·
niZKadv[Bzk,Φ,Sim] in the above security may be replaced by 2 ·niZKadv[Bzk,Φ,Sim]. The inter-
ested reader may wish to work out the details of this.
20.22 (Simulation soundness). This exercise develops a security notion for non-interactive
proof systems (see Section 20.3) that combines the notions of soundness (see Section 20.3.4) and
zero knowledge (see Section 20.3.5) in a way that is perhaps a bit unintuitive, but that has a number
of useful applications, some of which will be developed in subsequent exercises. Roughly speaking,
simulation soundness means that after seeing simulated proofs of both true and false statements,
it should be hard to come up with a new valid proof of a false statement.
Let Φ be a non-interactive proof system for a relation R⊆X×Y . Suppose that Φ makes use of
a hash function H : U→C , which we model as a random oracle. Consider a simulator Sim for Φ,
as deﬁned in Section 20.3.5, which is an interactive machine that responds to queries of the form
(sim-proof-query,y), where y∈Y, and (sim-oracle-query,u), where u∈U.
Consider the following attack game played between an adversaryAand a challenger. The adversary
makes a number of queries, each of which is of the following form:
• (sim-proof-query,y), which the challenger passes through directly to Sim;
• (sim-oracle-query,u), which the challenger passes through directly to Sim;
• (attempt-query,y∗,π∗), which the challenger processes by checking whether π∗is a valid proof
for y∗, and responds with accept if this is the case, and reject, otherwise; to make this check,
the challenger may need to evaluate the random oracle at various points, and it does so by
invoking Sim with queries of the form ( sim-oracle-query,·), as necessary.
Note that for queries of the form ( sim-proof-query,y), the statement y may very well be a false
statement, but nevertheless, Sim responds with a simulated proof π, and we say ( y,π) is a proof
query/response pair.
We say that Awins the game if the challenger responds with accept to any query of the form
(attempt-query,y∗,π∗) where ( y∗,π∗) is not a previously generated proof query/response pair and
y∗is a false statement. We denote by simSnd adv[A,Φ,Sim] the probability that Awins the game.
850
We say that Φ is simulation sound ZK if there exists a simulator Sim such that
niZKadv[A,Φ,Sim] is negligible for all eﬃcient adversaries Aand simSnd adv[A,Φ,Sim] is neg-
ligible for all eﬃcient adversaries A.
Let Π = (P,V ) be a special HVZK Sigma protocol, and consider the corresponding simulator Sim
in Fig. 20.2.
(a) Suppose that Π has unique responses (see Exercise 19.14). Show that for every adversary A
that makes at most Qro random oracle queries and Qa attempt queries in the above simulation
soundness attack game, there exists an adversaryBattacking Π as in Attack Game 20.1, where
Bis an elementary wrapper around A, such that
simSndadv[A,FS-Π,Sim] ≤(Qro + Qa)Sndadv[B,Π].
Discussion: In particular, if Π is sound and special HVZK, and has unique responses and
unpredictable commitments, then FS-Π is simulation sound ZK.
(b) More generally, show that for every adversary Athat makes at most Qro random oracle
queries and Qa attempt queries in the above simulation soundness attack game, there exists
an adversary Battacking Π as in Attack Game 20.1 and an adversary B′ attacking Π with
extractor Ext as in Exercise 19.16, where B,B′are elementary wrappers around A, such that
simSndadv[A,FS-Π,Sim] ≤(Qro + Qa)Sndadv[B,Π] + cSSSadv[B′,Π,Ext].
Discussion: In particular, if Π provides computational strong special soundness, is special
HVZK, and has unpredictable commitments, then FS-Π is simulation sound ZK.
(c) Show that if Π has backward computable commitments, then the results of part (a) and (b)
also hold for the optimized Fiat-Shamir proof system discussed in Exercise 20.14, using the
modiﬁed simulator in part (b) of that exercise.
20.23 (A DDH-based signature scheme from simulation soundness). This exercise de-
velops a simple, strongly secure signature scheme with a very tight security reduction to DDH. Let
G be a cyclic group of prime order q generated by g∈G. Let Mbe the desired message space for
the signature scheme S. A public key for Sis a random DH-triple ( u,v,w ) ∈G3, and the secret
key is β ∈Zq such that v= gβ and w= uβ.
The signature scheme is based on a Sigma protocol Π for the relation
R=
{(
β, (u,v,w,m )
)
∈Zq ×(G3 ×M) : v= gβ and w= uβ
}
,
which generalizes the relation used in the Chaum-Pedersen protocol (see Section 19.5.2). In fact,
the protocol Π for Ris really just the Chaum-Pedersen protocol — both the prover and the veriﬁer
can just ignore m. We assume that Π has a large challenge space Cof size N.
Now consider the optimized version Φ of the non-interactive proof system obtained by applying
the Fiat-Shamir transform to Π (see Exercise 20.14). The proof system Φ uses a hash function
H : (G3 ×M)×G2 →C, which we will model as a random oracle in the security analysis. Although
mis ignored by Π, it is not ignored by Φ, as it is included in the hash used to derive the challenge.
A valid signature on a message m under public key ( u,v,w ) is simply a valid proof π for the
statement (u,v,w,m ) under Φ.
851
(a) Describe the signature scheme Sin detail.
(b) Prove that the signature scheme S is strongly secure in the random oracle model under
the DDH assumption. In particular, use the zero knowledge simulator and the result from
Theorem 20.3, along with the result of Exercise 20.22, to prove the following: for every
adversary Aattacking Sas in the random oracle version of Attack Game 13.2, and making
at at most Qs signing queries and Qro random oracle queries, there exists a DDH adversary
B, which is an elementary wrapper around A, such that
stSIGadv[A,S] ≤Qs(Qs + Qro + 1)1
q + DDHadv[B,G] + 1
q + (Qro + 1)/N.
Hint: Game 1: replace the signer by a simulator; Game 2: replace ( u,v,w ) by a random
non-DH-triple (see Exercise 10.7), and then use simulation soundness to bound the advantage.
(c) Analyze the security of the scheme Sin the multi-key setting (as in Exercise 13.2). Show that
if at most Qk signature keys are used in the multi-key attack, then the bound in part (a), but
with an extra additive term of Qk/q, also holds in the multi-key setting.
Hint: Use Exercise 10.11.
20.24 (CCA secure encryption from simulation soundness). Let (G,E,D ) be a seman-
tically secure public-key encryption scheme with message space M. Let us assume that E is a
deterministic algorithm takes as input a public key pk, a message m∈M, and a randomizer s∈S,
so to encrypt a message m, one computes s←R Sand c←E(pk,m; s).
Let us also assume that Φ = ( GenPrf ,VrfyPrf ) is a simulation sound ZK non-interactive proof
system for the relation
R=
{
( (m,s0,s1), (pk0,c0,pk1,c1) ) : c0 = E(pk0,m; s0) and E(pk1,m; s1)
}
.
Thus, (pk0,c0,pk1,c1) ∈LRiﬀ c0 encrypts some message under the public key pk0 and c1 encrypts
the same message under the public key pk1.
We build a new encryption scheme ( G′,E′,D′) as follows. The key generation algorithm G′ runs
Gtwice, obtaining (pk0,sk0) and (pk1,sk1). The public key is pk′:= (pk0,pk1) and the secret key
is (sk0, pk0,pk1). Given a message m, the encryption algorithm E′computes
s0,s1 ←R S, c0 ←E(pk0,m; s0), c1 ←E(pk1,m; s1), π←GenPrf ((m,s0,s1),(pk0,c0,pk1,c1)),
and outputs the ciphertext c′:= (c0,c1,π). To decrypt such a ciphertext c′, the decryption algo-
rithm D′ checks that the proof π is valid and, if so, outputs D(sk0,c0), and otherwise, outputs
reject.
Prove that (G′,E′,D′) is CCA secure in the random oracle model.
Hint: Start with Attack Game 12.1, but deﬁne a number of diﬀerent experiments parameterized
by
enc ∈{0,1}×{0,1}, dec ∈{0,1}, prf ∈{0,1}.
If enc = (d,e) ∈{0,1}×{0,1}, then given an encryption query ( m0,m1), the challenger encrypts
md under pk0 and me under pk1. If dec = f ∈{0,1}, then given a decryption query (ˆc0,ˆc1,ˆπ), the
challenger decrypts ˆcf under skf. If prf = 0, then the challenger generates proofs in encryption
queries as usual, but if prf = 0 it does so using simulated proofs. Analyze the following sequence
of experiments:
852
0. enc = (0,0), dec = 0, prf = 0
1. enc = (0,0), dec = 0, prf = 1
2. enc = (0,1), dec = 0, prf = 1
3. enc = (0,1), dec = 1, prf = 1
4. enc = (1,1), dec = 1, prf = 1
5. enc = (1,1), dec = 1, prf = 0
6. enc = (1,1), dec = 0, prf = 0
20.25 (A concrete instantiation based on DDH). Instantiate the construction in the pre-
vious exercise with the multiplicative ElGamal encryption scheme in Section 20.2, along with the
optimized Fiat-Shamir non-interactive proof system derived from the Sigma protocol of Exam-
ple 20.1. Describe the encryption scheme in detail, and verify that all of the assumptions of the
previous exercise are satisﬁed, so that the resulting encryption scheme is CCA secure under the
DDH assumption in the random oracle model.
20.26 (ℓ-special soundness implies soundness). Consider again the notion of ℓ-special sound-
ness, as deﬁned in Exercise 19.30. Prove the following generalization of Theorem 20.1: if Π is a
Sigma protocol with a large challenge space that provides ℓ-special soundness, then Π is sound (in
fact, statistically sound, as in Remark 20.1).
20.27 (Compressed n-wise Chaum-Pedersen). This exercise asks you to design and analyze
a compressed n-wise Chaum-Pedersen protocol (see Section 19.5.2). Let G be a group of prime
order q with generator g ∈G. Let n be a poly-bounded parameter, and let Rn ⊆(Zn
q) ×(G2n+1)
be the following relation:
Rn :=
{(
(β1,...,β n),(u,v1,w1,...,v n,wn)
)
: vi = gβi and wi = uβi for i= 1,...,n
}
.
Using the idea from Exercise 19.31, design and analyze a Sigma protocol for Rn that requires
only constant communication (in particular, independent of n), is special HVZK, and provides
(n+ 1)-special soundness.
Discussion: Combined with Theorems 20.2 and 20.3, and the result from Exercise 20.26, this
result shows that when we apply the Fiat-Shamir transform to this protocol, we get a sound
zero-knowledge non-interactive proof system for n-wise Chaum-Pedersen. This result illustrates
the utility of ℓ-special soundness in a setting where we only need soundness, and not a “proof of
knowledge”.
20.28 (Generalized “encrypt then prove”). This exercise generalizes Theorem 19.25. Let
(G,E,D ) be a semantically secure public-key encryption scheme with message space M. As in
Exercise 20.24, let us assume that E is a deterministic algorithm takes as input a public key pk, a
message m ∈M, and a randomizer s ∈S, so to encrypt a message m, one computes s ←R Sand
c←E(pk,m; s).
Let Π be a Sigma protocol for the relation
R=
{
( (m,s), (pk,c) ) : c= E(pk,m; s)
}
.
853
Assume that Π has a large challenge space, has unpredictable commitments (see Deﬁnition 19.7),
is special cHVZK (see Deﬁnition 20.6), and provides computational strong special soundness (see
Exercise 19.16).
Let Φ = ( GenPrf ,VrfyPrf ) be the non-interactive proof system built from Π via Fiat-Shamir (as
in Section 20.3.3).
We build a new encryption scheme ( G,E′,D′) as follows. To encrypt a message m under public
key pk, the encryption algorithm G′computes
s←R S, c←E(pk,m; s), π←GenPrf ((m,s),(pk,c0)),
and outputs the ciphertext c′ := (c,π). To decrypt such a ciphertext c′ under secret key sk, the
decryption algorithm E′checks that the proof πis valid and, if so, outputs D(sk,c), and otherwise,
outputs reject.
Show that if we model the hash in Fiat-Shamir as a random oracle, then ( G,E′,D′) is parallel
1CCA secure, which is a security notion we introduced in Section 19.9.2.2. Recall that this notion
of CCA security is deﬁned by a restriction of the 1CCA attack game (see Deﬁnition 12.2) in which
all of the adversary’s decryption queries are submitted in a single batch. You may use the result
of Exercise 19.32.
854
Chapter 21
Authenticated Key Exchange
Suppose Alice and Bob wish to communicate securely over an insecure network. Somehow, they
want to use a secure channel. In Chapter 9, we saw how Alice and Bob could do this if they already
have a shared key; in particular, we looked at real-world protocols, such as IPsec and the TLS
record protocol, which provide authenticated encryption of packets, and which also guarantee that
packets are delivered in order and without duplicates. However, this begs the question: how do
Alice and Bob establish such a shared key to begin with? Protocols that are used for this purpose
are called authenticated key exchange (AKE) protocols, and are the subject of this chapter.
Roughly speaking, an AKE protocol should allow two users to establish a shared key, called a
session key. At the end of a successful run of such a protocol, a user, say P, should have a clear
idea of which user, say Q, he is talking to, that is, with which user he has established a shared
session key (this may be determined either before the protocol runs, or during the course of the
execution of the protocol). A secure AKE protocol should ensure that P’s session key is eﬀectively
a fresh, random key that is known only to Q.
Use of a TTP. Typically, to realize an AKE protocol, we shall need to assume the existence of
a trusted third party (TTP) , whose job it is to facilitate communication between users who
have no prior relationship with each other. Initially, each user of the system must perform some
kind of registration protocol with the TTP; at the end of the registration protocol, the user has
established his own long term secret key . If the TTP is oﬄine, no further communication with the
TTP is necessary, and users do not need to share any secret information with the TTP. Here we
will primarily focus on protocols that make use of such an oﬄine TTP. The role of the TTP in
these protocols is that of a Certiﬁcate Authority, or CA, a notion we introduced in Section 13.8.
Recall that a CA issues certiﬁcates that bind a real-world identity to a public key.
In Section 21.12, we discuss AKE protocols that use an online TTP, which is involved in every
run of the AKE protocol, and which shares secret information with users. In general, an oﬄine
TTP is preferable to an online TTP. Nevertheless, the advantage of an online TTP is that such
protocols can be built using only symmetric key primitives, without public-key tools. In addition,
key revocation is relatively simple with an online TTP. However, there are many disadvantages to
online TTP protocols, as discussed in Section 21.12.
Multiple user instances and freshness of keys. A given user may run an AKE protocol
many times. We shall refer to each such run as an instance of that user. While a given user has
855
only a single long-term secret key, we expect that each run of the AKE protocol produces a fresh
session key.
For example, a user may wish to set up a secure channel with his bank on Monday, with his
oﬃce ﬁle server on Tuesday, and again with his bank on Wednesday. Freshness guarantees that
even if the user’s oﬃce ﬁle server is hacked, and an adversary is able to retrieve the session key from
Tuesday, this should not compromise the session key from Monday or Wednesday. The adversary
should learn nothing about the Monday and Wednesday keys. Moreover, freshness guarantees that
certain methods of realizing secure channels maintain their security across multiple sessions. For
example, suppose that a stream cipher is used to maintain the secrecy of the data sent through
the secure channel between the user and his bank. If the same key were used to encrypt two
diﬀerent streams, an adversary can mount a “two time pad” attack to obtain information about
the encrypted data, as discussed in Section 3.3.1. Freshness ensures that keys used in diﬀerent
sessions are eﬀectively independent of one another.
Security properties: an informal introduction. Secure AKE protocols turn out to be rather
tricky to design: there are many subtle pitfalls to avoid. Indeed, part of the problem is that it is
challenging to even formally specify what the security goals even should be.
First, let us consider the powers of the adversary. Of course, an adversary may eavesdrop on
messages sent between user instances running the protocol. Typically, these messages will include
certiﬁcates issued by the CA, and so we should assume that such certiﬁcates are public and freely
available to any adversary. We shall also assume that an adversary may be able to modify messages,
and indeed, may be able to inject and delete messages as well. So essentially, we shall allow the
adversary to have complete control over the network . Of course, this is an overly pessimistic point
of view, and a typical real-world adversary will not have this much power, but as usual, in analyzing
security, we want to take the most pessimistic point of view.
In addition, some users in the system may register with the CA, but these users may be corrupt,
and not follow the protocol. Such corrupt users may even collude with one another. For our
purposes, we shall just assume that all such corrupt users are under the control of a single adversary.
The remaining users of the system are honest users, who follow the protocol properly.
We have already hinted at some of the properties we want any secure AKE protocol to satisfy.
Let us try to make these just a bit more precise.
Suppose an instance of an honest user P has successfully terminated a run of the AKE protocol,
thinking he is talking to an instance of user Q, and holding a session key k. On the one hand, if
Q happens to be a corrupt user, the key k is inherently vulnerable, and we might as well assume
that k is known to the adversary. On the other hand, if Qis an honest user, we want the following
guarantees:
authenticity: the key k, if it is shared with anyone, is shared with an instance of userQ; moreover,
this instance of user Q should think he is talking to an instance of user P;
secrecy: from the adversary’s point of view, the key k is indistinguishable from a random key;
moreover, this should hold even if the adversary sees the session keys from other user instances.
Later in this chapter (in Section 21.9), we shall make the above security requirements much
more precise. In fact, we will consider several levels of security, depending on the exact powers of
the adversary. In the weakest security deﬁnition, the adversary never compromises the long-term
secret key of any honest user. A stronger security notion, called “perfect forward secrecy,” defends
856
against an adversary that is able to compromise long-term keys of honest users. An even stronger
notion, called “HSM security,” defends against an adversary that can read the ephemeral random
bits generated by honest users. The issues involved will become clearer after looking at a number
of example protocols.
21.1 Identiﬁcation and AKE
One can think of AKE as a combination of identiﬁcation, discussed in Chapters 18 and 19, and
anonymous key exchange, discussed in Section 10.1. However, it is not enough to simply run such
protocols sequentially.
Consider the following protocol:
1. P identiﬁes himself to Q;
2. Q identiﬁes himself to P;
3. P and Q generate a shared key.
Here, steps 1 and 2 are implemented using an identiﬁcation protocol (as in Chapter 18), and step 3
is implemented using an anonymous key exchange protocol (as in Section 10.1).
To attack this protocol, an adversary might wait until steps 1 and 2 are complete, and then
“hijack” the session. Indeed, suppose that after step 2, the adversary steps in between P and
Q, runs one anonymous key exchange protocol with P, obtaining a shared key k1, and another
anonymous key exchange protocol with Q, obtaining a shared key k2.
If the session key is used to implement a secure channel, then after the protocol completes
the adversary can easily play “man in the middle”: whenever P encrypts a message under k1,
the adversary decrypts the resulting ciphertext, and then re-encrypts the message, possibly after
modifying it in some way, under k2; similarly, messages from Qto P can be decrypted and then re-
encrypted. Thus, the adversary is able to read the entire conversation between P and Q, modifying
messages at will.
To foil the above attack, one might consider the following protocol:
1. P and Q generate a shared key, and use this key to implement a secure channel;
2. P identiﬁes himself to Q inside the channel;
3. Q identiﬁes himself to P inside the channel.
Here, step 1 is implemented using an anonymous key exchange protocol. This key can then be
used to implement a secure channel, and then steps 2 and 3 are implemented by an identiﬁcation
protocol, with each protocol message encrypted under the shared key a symmetric cipher that
provides authenticated encryption.
However, an adversary can also easily attack this protocol by playing “man in the middle”:
1. The adversary generates a shared key k1 with P, and other shared key k2 with Q;
2. During each run of the identiﬁcation protocol, whenever P sends a message to Q, which
is encrypted under k1, the adversary decrypts the corresponding ciphertext, and then re-
encrypts the message under k2, sending the corresponding ciphertext to Q.
857
3. Similarly, whenever Q sends a message to P, which is encrypted under k2, the adversary
decrypts the corresponding ciphertext, and then re-encrypts the message under k1, sending
the corresponding ciphertext to P.
When the attack completes, the adversary can simply continue playing “man in the middle.”
Thus, these simple-minded approaches to designing a secure AKE protocol do not work. To
build a secure AKE protocol, one must carefully intertwine the processes of identiﬁcation and
anonymous key exchange.
21.2 An encryption-based protocol
In this section, we present an AKE protocol, called AKE1. As we shall eventually see (in Sec-
tion 21.9.2), protocol AKE1 does indeed satisfy our most basic notion of security, called static
security, in which the adversary never compromises the long-term secret key of any honest user.
However, it is vulnerable to more powerful attacks that will be discussed later, and which are
modeled by stronger security deﬁnitions.
Certiﬁcate authority. Protocol AKE1, like all the protocols in the chapter up through Sec-
tion 21.10, makes use of a CA, which issues certiﬁcates that bind identities to public keys. For a
user P, we shall write idP to denote P’s identity, and let CertP be a certiﬁcate that binds idP to
a public key. Here, idP is an arbitrary bit string, unique to this user, and we assume that CertP
encodes idP, as well as P’s public key pkP, and a signature on a message of the form “ idP’s public
key is pkP”, under the CA’s public key. We shall assume that all users have access to the CA’s
public key, so that they can verify certiﬁcates.
For this particular protocol, the public key pkP for a user P consists of the public key for a
CCA-secure public key encryption scheme, and the public key for a signature scheme. The long-
term secret for user P consists of the corresponding secret keys for the encryption and signature
schemes. When P registers with the CA, he presents idP and pkP, along with any credentials
needed to convince the CA that P’s identity “really is” idP (what these credentials are, and how
they are checked, is outside the scope of our description of this protocol). If the CA is happy with
these credentials, the CA issues a certiﬁcate CertP, which P retains.
Note that for this protocol and all the other protocols we discuss in this chapter, we do not
assume that the CA does anything else besides checking a user’s credentials. In particular, the CA
does not do anything to ensure that the user’s public key satisﬁes any particular property, or that
the user “knows” the corresponding secret key.
Notation. To describe protocol AKE1, we use the following notation:
• CertP denotes P’s certiﬁcate, binding his identity idP to his public keys for encryption and
signing;
• EncP(m) denotes an encryption of the message m under P’s public encryption key;
• SigP(m) denotes a signature on the message m under P’s public veriﬁcation key;
• Kdenotes the set of session keys;
858
P Q
r, CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
k
Q c:= EncP(k,idQ), σ:= SigQ(r,c,idP), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−k
P
Figure 21.1: Protocol AKE1
• Rdenotes a large set, which will be used to generate random nonces.
When executed by users P and Q, protocol AKE1 runs as described in Fig. 21.1. Here, r is
chosen at random by P from the set R, and k is chosen at random by Q from the set K. Also,
each user veriﬁes the certiﬁcate it receives; in addition, P veriﬁes the signature σ it receives, and
also veriﬁes that c decrypts to a message of the form ( k,idQ).
In Fig. 21.1 we have used the notation
k
Q
to indicate that when the protocol ﬁnishes, a user holds the session key k, and thinks he is talking
to user Q.
Here is a more detailed description of the protocol:
1. P computes r←R R, and sends ( r,CertP) to Q;
2. Qveriﬁes CertP; if the certiﬁcate is invalid, Qaborts; otherwise, Qextracts the identity idP
from CertP, along with P’s public encryption key, and then computes
k←R K, c←R EncP(k,idQ), σ←R SigQ(r,c,idP),
and sends (c,σ,CertQ) to P; in addition, Q terminates successfully, and outputs the session
key k, and partner identity id P;
3. P veriﬁes CertQ; if the certiﬁcate is invalid, P aborts; otherwise, P extracts the identity idQ
from CertQ, along with Q’s public veriﬁcation key, and then veriﬁes thatσis a valid signature
on the message ( r,c,idP) under Q’s public veriﬁcation key; if not, P aborts; otherwise, P
decrypts the ciphertext c, and veriﬁes that c decrypts to a message of the form ( k,idQ) for
some k ∈K; if not, P aborts; otherwise, P terminates successfully, and outputs the session
key k, and partner identity idQ.
Remarks. A number of comments are in order, which apply to any AKE protocol:
1. When a user runs this protocol, it either aborts or terminates successfully; when it terminates
successfully, the protocol outputs a session key k and a partner identity id. When we say “ P
thinks he is talking toQ”, we really mean thatP runs the protocol to a successful termination,
and outputs the partner identity idQ.
2. As we have described this protocol, a user can start the protocol without necessarily knowing
the identity of his partner, obtaining this identity (and certiﬁcate) along the way. Of course,
859
P Q
(public key = uP = gαP ) (public key is in CertQ)
r, CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
k:= H(uP,v,v αP ,idQ)
Q v:= gβ, σ:= SigQ(r,v, idP), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−k:= H(uP,v,u β
P,idQ)
P
Figure 21.2: Protocol AKE1eg
in many situations, a user might know in advance who he plans on talking to, and may
abandon the protocol if the partner identity obtained during the run of the protocol does not
match his expectations. A user might also abandon the protocol if it “times out.”
3. The protocol is inherently asymmetric: the role played by P is quite diﬀerent from that played
by Q. Two users running the protocol will have to establish a convention to decide who plays
which role.
4. When a single user runs multiple instances of the protocol, some mechanism is used to route
protocol messages to the appropriate instance of the protocol. This routing mechanism is
not required to provide any security guarantees, and our description of the protocol does not
include any description of this mechanism.
Choice of encryption scheme. We will prove the static security of this protocol in Sec-
tion 21.9.2. The purpose of encrypting the identity idQ along with the session key k is to bind this
identity to the ciphertext c. CCA-secure encryption is needed to ensure that this binding cannot
be broken. If we wish, we could reduce the length of the ciphertext by encrypting a collision-
resistant hash of idQ instead of idQ itself. We saw a similar usage of binding public information
to a ciphertext in Section 12.2.3. In fact, instead of encrypting idQ (or a hash thereof), we could
use a CCA-secure public-key encryption scheme with associated data, as in Section 12.7, treating
idQ as the associated data in this application. Since we are just encrypting a random key with
associated data, we could get by with a key encapsulation mechanism (KEM) with associated data
(see Exercise 12.18).
If we use the KEM corresponding to the ElGamal encryption scheme EEG (see Section 12.4), we
get the key exchange protocol AKE1eg shown in Fig. 21.2. Here, G is a cyclic group of prime order
q generated by g∈G, and H : G3 ×IDSpace →K is a hash function, where user identities belong
to IDSpace. User P’s public key is uP ∈G and secret key is αP ∈Zq. On each run of the protocol,
P generates r∈R at random and Qgenerates β ∈Zq at random. At the end of the protocol, both
users compute the session key k:= H
(
gαP,gβ,gαPβ,idQ
)
(we have added uP = gαP to the hash for
a tighter security reduction). For CCA security of the KEM we model H as a random oracle, and
assume that ICDH, deﬁned in Section 12.4, holds for G. As discussed in Remark 12.1, P should
check that v is in G. The description of the group G, including the generator g, is viewed as a
shared system parameter.
Instead of ElGamal, one could use any other CCA-secure encryption scheme, such as ERSA.
860
Erasing ephemeral data. We are assuming (for now) that the user’s long-term keys are never
compromised. However, in a secure implementation of any session key protocol, it is important that
the participants securely erase any ephemeral data they generated during the protocol. If any of this
data ends up stored in memory which an adversary can read at some later time, then the adversary
may be able to break the system. For example, if we use ElGamal encryption as in protocol AKE1eg,
then it is important that Qsecurely erases the value β — if this leaks, the adversary can obviously
recover the session key k. Similarly, any random bits that go into generating signatures should
also be securely erased. It is easy to see that if the random bits that go into generating a Schnorr
signature (see Section 19.2) are leaked, then the adversary can trivially compute the long-term
signing key. This attack is potentially even more devastating, since instead of just obtaining a
single session key, the adversary can impersonate a user at any time, as often as he likes, to any
user. This is another reason to derandomize signature schemes, as discussed in Exercise 13.6, so as
to avoid this problem altogether.
Implicit authentication. Protocol AKE1 only provides implicit authentication, in the follow-
ing sense. When P ﬁnishes the protocol, he can be conﬁdent that Q was “alive” during the run of
the protocol (since Q must have signed the message containing P’s random nonce); moreover, P
can in fact be conﬁdent that some instance of Q ﬁnished the protocol and is holding a matching
session key. However, when Q ﬁnishes the protocol, he has no such guarantee: not only may there
not be an instance of P with a matching session key, but P may not have even been “alive” during
the execution of the protocol. Nevertheless, Qcan be sure of this: if anyone at all eventually shares
his session key, then that someone is an instance of P (who thinks he is talking to Q).
21.2.1 Insecure variations
To appreciate why this protocol is designed the way it is, it is instructive to consider minor variations
that are susceptible to various attacks, and illustrate how these attacks might be exploited in the
real world. These attacks serve to illustrate the types of vulnerabilities any secure AKE should
avoid, and demonstrate why each and every piece of protocol AKE1 is essential to achieve security.
Variation 1: do not sign c — a key exposure attack
Suppose we modify protocol AKE1 so that the message signed by Qdoes not include the ciphertext
c; likewise, the logic of P is modiﬁed accordingly. The resulting protocol runs as follows:
P Q
r, CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
k
Q c:= EncP(k,idQ), σ:= SigQ(r,idP), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−k
P
This modiﬁed protocol can be attacked as follows:
• the adversary intercepts the message ( c,σ,CertQ) from Q to P;
• the adversary computes c′ ←R EncP(k′,idQ), where k′ is a session key of his choosing, and
sends the message ( c′,σ,CertQ) to P.
861
The following diagram illustrates the attack:
P Q
r, CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
∥
c:= EncP(k,idQ), σ:= SigQ(r,idP), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−k
P
k′
Q c′:= EncP(k′,idQ), σ, CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−∥
In the diagram, we write ∥←to indicate a message blocked by the adversary, and ←∥to indicate
a message generated by the adversary. At the end of the attack, Q is holding the session key k,
which is unknown to the adversary; however, P is holding the session key k′, which is known to the
adversary.
This type of attack, where the adversary is able to recover (or in this case, even choose) a session
key, is called a key exposure attack , and certainly violates the secrecy property. However, let
us consider a more concrete scenario to illustrate why this attack is dangerous, even though the
adversary knows only k′, but not k. Suppose that after the AKE protocol is run, the session key
is used to secure a conversation between P and Q, using authenticated encryption. If P sends
the ﬁrst message in this conversation, then the adversary can obviously decrypt and read this
message. Alternatively, if P receives the ﬁrst message in the conversation, the adversary can make
this message anything he wants.
Let us ﬂesh out this attack scenario even further, and consider a hypothetical electronic banking
application. Suppose that one user is a bank and the other a customer. Further, suppose that
the conversation between the bank and customer is a sequence of request/response pairs: the
customer sends a transaction request, the bank executes the transaction, and sends a response to
the customer.
On the one hand, suppose P is the customer and Q is the bank. In this case, the ﬁrst request
made by the customer can be read by the adversary. Obviously, such a request may contain private
information, such as a credit card or social security number, which the customer obviously does not
want to share with an adversary. On the other hand, suppose P is the bank and Qis the customer.
In this case, the adversary can send the bank a request to perform some arbitrary transaction on
the customer’s account, such as to transfer money from the customer’s account into some bank
account controlled by the adversary.
Variation 2: do not sign r — a replay attack
Suppose we modify protocol AKE1 so that the message signed by Q does not include the random
nonce r; likewise, the logic of P is modiﬁed accordingly. The resulting protocol runs as follows:
P Q
CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
k
Q c:= EncP(k,idQ), σ:= SigQ(c,idP), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−k
P
In this new protocol, r is not really used at all, so we leave it out.
This new protocol is susceptible to the following attack:
862
• ﬁrst, the adversary eavesdrops on a conversation between P and Q; suppose P sent the
message CertP to Q, who responds with the message (c,σ,CertQ); these messages are recorded
by the adversary;
• at some later time, the adversary, initiates a new run of the protocol with P; P sends out
the message CertP; the adversary intercepts this message, throws it away, and sends P the
message (c,σ,CertQ), recorded from the previous run of the protocol.
The following diagram illustrates the attack:
P Q
CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
k
Q c:= EncP(k,idQ), σ:= SigQ(c,idP), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−k
P
···
P —
CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→∥
k
Q c, σ←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−∥
At the end of the attack, the second instance of user P thinks he is talking to Q, but the session key
of the second instance of P is exactly the same as the session key k of the ﬁrst instance of P. Note
that the adversary does not obtain any direct information about k, nor is there a new instance of
Q that shares this key. This type of attack, where the adversary is able to force a user instance to
re-use an old session key, is called a replay attack, and it also violates the secrecy property.
Even though the adversary obtains no direct information about k from the attack, this attack
can still be exploited. Suppose, for example, that k is used to implement a secure channel that
uses a stream cipher in its implementation. In this way, the adversary might be able to get P to
encrypt two diﬀerent messages, using a stream cipher, under the same secret key. As discussed in
Section 3.3.1, this might allow the adversary to obtain information about the encrypted data, via
a “two time pad” attack.
Another way this replay attack might be exploited is to simply replay some part of the ﬁrst
conversation between P and Q. Indeed, returning to our bank example, suppose P is the bank and
Q is the customer. Then if in the ﬁrst conversation, the customer requested a certain amount of
money to be transferred to a third party’s account, the adversary could simply replay this request,
and cause the bank to transfer the same amount of money a second time.
Variation 3: do not sign idP — an identity misbinding attack
Suppose we modify protocol AKE1 so that the message signed by Q does not include the identity
idP; likewise, the logic of P is modiﬁed accordingly. The resulting protocol runs as follows:
P Q
r, CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
k
Q c:= EncP(k,idQ), σ:= SigQ(r,c), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−k
P
Here is a rather subtle attack on this protocol:
863
• after obtaining P’s public key by some means, the adversary registers a new user R with the
CA, obtaining a certiﬁcate CertR that binds R’s identity, idR, to P’s public key;
• at some later time, P and Q engage in the AKE protocol; when P sends the message
(r,CertP), the adversary intercepts this message, and instead delivers the message ( r,CertR)
to Q;
• when Q sends the message ( c,σ,CertQ), the adversary delivers this message to P.
The following diagram illustrates the attack:
P Q
r, CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→∥
∥ r, CertR−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
k
Q c:= EncP(k,idQ), σ:= SigQ(r,c), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−k
R
At the end of the attack, P and Q share the session key k, which is unknown to the adversary;
however, P thinks he is talking to Q, while Q thinks he is talking to R. This type of attack is
called an identity misbinding attack, and it violates the authentication property.
Note that to carry out the attack, R needs to “hijack” P’s public key; that is, the adversary
registers the user R with the CA, but using P’s public key. Recall that we are assuming here that
although the CA checks R’s credentials (i.e., he is who he says he is), the CA does not necessarily
require R to prove that he has the corresponding secret key (which he could not do in this case).
It is perhaps not so easy to exploit an identity misbinding attack, but here is one semi-plausible
scenario. Nowadays, one can buy plastic “voucher cards” at stores, and these voucher cards can be
redeemed on the Internet in various ways, for example, to add credit to a prepaid cell phone account.
To redeem a voucher card, a customer logs in to his account, and then types in a serial number that
appears on the voucher card, and the value of the voucher card is added to the customer’s account.
Now, suppose that the above protocol is used to allow users to log into their accounts, and that
Q represents the cell phone company, and that P and R are customers. In the above misbinding
attack, the phone company thinks he is talking to R, when really, he is talking to P. So when the
unsuspecting customer P redeems a voucher card, Q credits the value of the voucher card to R’s
account, rather than to P’s account.
Variation 4: do not encrypt idQ — a replay attack
Suppose we modify protocol AKE1, so that Qdoes not encrypt his identity. The new protocol runs
as follows:
P Q
r, CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
k
Q c:= EncP(k), σ:= SigQ(r,c,idP), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−k
P
864
The following diagram illustrates a simple replay attack on this protocol:
P Q
r, CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
k
Q c:= EncP(k), σ:= SigQ(r,c,idP), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−k
P
···
P —
r′, CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→∥
k
R c, σ′:= SigR(r′,c, idP), CertR←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−∥
Here, Ris a corrupt user, under control of the adversary. However, we assume thatRhas registered
with the CA as usual, and so has a certiﬁcate that binds his identity to a public key for which
he has a corresponding secret key. Thus, in the last ﬂow, the adversary may easily generate the
required signature SigR(r′,c, idP).
The end result of this replay attack is essentially the same as the replay attack we saw against
Variation 2, except that in this case, the second instance of user P thinks he is talking to R, instead
of to Q.
Variation 5: encrypt r instead of idQ — an identity misbinding attack
Suppose that Q encrypts r instead of idQ. The new protocol runs as follows:
P Q
r, CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
k
Q c:= EncP(k,r), σ:= SigQ(r,c,idP), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−k
P
As in Variation 3, this protocol is susceptible to an identity misbinding attack:
P Q
r, CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
∥
c:= EncP(k,r), σ:= SigQ(r,c,idP), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−k
P
k
R c, σ′:= SigR(r,c,idP), CertR←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−∥
At the end of this attack, P and Q share the session key k, which is unknown to the adversary;
however, P thinks he is talking to R, while Q thinks he is talking to P.
As in Variation 4, R is a corrupt user, under the control of the adversary, but we assume that
R has registered with the CA as usual — unlike Variation 3, R does not need to “hijack” another
user’s public key.
865
Variation 6: The need for CCA secure encryption
Suppose we use an encryption scheme that is semantically secure, but not necessarily CCA secure.
There are a number of types of attack that may be possible, depending on the scheme.
For example, suppose that we use the encryption scheme ETDF, based on a trapdoor function,
as discussed in Section 11.4. This scheme makes use of a semantically secure cipher, and we shall
assume that this is a stream cipher. With these assumptions, given a ciphertext c that encrypts
some unknown bit stringm, and given an arbitrary bit string ∆, one can easily compute a ciphertext
c′ that encrypts m⊕∆ (see Section 3.3.2). Now, suppose that m = k ∥idQ is the encoding of
the pair ( k,idQ), where k is an ℓ-bit string. Then setting ∆ := 0ℓ ∥(idQ ⊕idR), we can easily
transform an encryption c of k ∥idQ into an encryption c′ of k ∥idR, without any knowledge of
k. Because of this, we can easily modify the replay attack on Variation 4, so that it works on the
original protocol AKE1, as follows:
P Q
r, CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
k
Q c:= EncP(k,idQ), σ:= SigQ(r,c,idP), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−k
P
···
P —
r′, CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→∥
k
R c′:= EncP(k,idR), σ′:= SigR(r′,c′,idP), CertR←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−∥
Another avenue of attack is to send “trick” ciphertexts to P, so that the decryptions of the
trick ciphertexts reveal secret information. For example, an attacker could use the Bleichenbacher
attack on PKCS1 from Section 12.8.3 to recover a secret session key. The adversary could record
the ciphertext c sent from Q to P during a run of the protocol between Q and P. Then, by later
sending to P “trick” ciphertexts derived from c, as in Bleichenbacher’s attack, the attacker could
learn the decryption of c. This will expose the secret session key between P and Q.
21.2.2 Summary
We have presented the AKE protocolAKE1, and have illustrated how several variants of this protocol
are insecure. In particular, we illustrated three basic types of attack:
• a key recovery attack, in which an adversary is able to recover (or even choose) a session
key;
• a replay attack, in which an adversary is able to force a user instance to re-use an old session
key;
• an identity misbinding attack, in which an adversary is able to make two user instances
share a key, but these two user instances have conﬂicting views of who is talking to whom.
866
P Q
pk, σ1 := SigP(pk), CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
k
Q c:= E
(
pk, (k,idQ)
)
, σ2 := SigQ(pk,c, idP), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−k
P
Figure 21.3: Protocol AKE2
21.3 Perfect forward secrecy and a protocol based on ephemeral
encryption
If an adversary obtains a user’s long-term secret key, the adversary may impersonate that user
going forward, and cause a great deal of damage. However, it would be nice if the damage could
be limited to the time after which the user’s key was compromised, so that at least session keys
generated before the compromise remain secret. This additional security property is called perfect
forward secrecy. If a protocol satisﬁes this property, we say that it is PFS secure.
Protocol AKE1 in Section 21.2 certainly is not PFS secure. Indeed, if a user’s long-term decryp-
tion key is obtained by an adversary, then all previous session keys encrypted under that user’s
encryption key become available to the adversary.
In this section, we present another AKE protocol, calledAKE2, that is PFS secure. This protocol
makes use of a CCA-secure public-key encryption scheme E = ( G,E,D ) along with a signature
scheme. The public key for each user is a veriﬁcation key for the signature scheme, and the long-
term secret key is the corresponding secret signing key. A new, “ephemeral” key pair for the
encryption scheme is generated with every run of the protocol.
When executed by users P and Q, protocol AKE2 runs as described in Fig. 21.3. Here, user P
generates a key pair ( pk,sk) every time he runs the protocol. In addition, each user veriﬁes the
certiﬁcates and signatures it receives.
Here is a more detailed description of protocol AKE2:
1. P computes
(pk,sk) ←R G(), σ1 ←R SigP(pk),
and sends (pk,σ1,CertP) to Q;
2. Qveriﬁes CertP; if the certiﬁcate is invalid, Qaborts; otherwise, Qextracts the identity idP
from CertP, along with P’s public veriﬁcation key; Q veriﬁes that σ1 is a valid signature on
pk under P’s public veriﬁcation key; if not, Q aborts; otherwise, Q computes
k←R K, c←R E(pk,(k,idQ)), σ2 ←R SigQ(pk,c, idP),
and sends (c,σ2,CertQ) to P; in addition, Qterminates successfully, and outputs the session
key k, and partner identity idP;
3. P veriﬁes CertQ; if the certiﬁcate is invalid, P aborts; otherwise, P extracts the identity idQ
from CertQ, along with Q’s public veriﬁcation key, and then veriﬁes thatσis a valid signature
867
P Q
u:= gα, σ1 := SigP(u), CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
k:= H(u,v,v α,idQ)
Q v:= gβ, σ2 := SigQ(u,v, idP), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−k:= H(u,v,u β,idQ)
P
Figure 21.4: Protocol AKE2eg
on the message ( pk,c, idP) under Q’s public veriﬁcation key; if not, P aborts; otherwise, P
decrypts the ciphertext c, and veriﬁes that c decrypts to a message of the form ( k,idQ) for
some k ∈K; if not, P aborts; otherwise, P terminates successfully, and outputs the session
key k, and partner identity idQ.
Forward secrecy. Intuitively, protocol AKE2 is PFS secure because user long-term keys are used
only for signing, not encrypting. So compromising a signing key should not allow the adversary to
decrypt any messages.
Choice of encryption scheme. Just as we did for protocol AKE1, we can make use of an
ElGamal-based KEM to implement the encryption. The resulting protocol, called AKE2eg, is shown
in Fig. 21.4. As before, G is a cyclic group of prime order q generated by g ∈G, and H : G3 ×
IDSpace →K is a hash function. User P generates α∈Zq at random, while user Qgenerates β ∈Zq
at random. At the end of the protocol, both users compute the session key k= H(gα,gβ,gαβ,idQ).
Again, H is modeled as a random oracle and we assume that ICDH holds for G (see Section 12.4).
As discussed in Remark 12.1, P should check that v is in G. It is not necessary for Qto check that
u is in G.
Just as for protocol AKE1, we could use ERSA instead of ElGamal. However, this is not very
practical, as key generation for RSA is much slower than for ElGamal, and the key generation
algorithm must be executed with every run of the protocol.
Erasing ephemeral data. As we discussed above, in a secure implementation of any session key
protocol, it is important that the participants securely erase any ephemeral data they generated
during the protocol. Again, if we use ElGamal encryption as in protocol AKE2eg, if either Q’s value
β or P’s value α is leaked, the adversary can obviously recover the session key. Worse, if α leaks,
the adversary can even do more damage: he can impersonate P at any time, as often as he likes,
to any user. This is because the adversary has both αand P’s signature on u= gα. This is all the
adversary needs to run the protocol and establish a shared session key with any user Q, causing Q
to think it is talking to P.
Insecure variations. Because of the similarity of protocol AKE2 to protocol AKE1, most of the
examples of insecure variations discussed in Section 21.2.1 can be easily adapted to protocols AKE2
and AKE2eg. See also Exercise 21.2.
868
21.3.1 Assuming only semantically secure encryption
We brieﬂy discuss the possibility of modifying protocol AKE2 so that it requires only a semantically
secure encryption scheme. Without CCA security, the protocol is vulnerable to a similar attack as
in Variation 6 of AKE1. Therefore, for this to have any chance of success, we have to assume that
one of the two users knows the identity of its partner before the protocol starts. So in the following
protocol, we assume P knows the identity idQ of its partner beforehand.
P Q
pk, σ1 := SigP(pk,idQ), CertP
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
k
Q c:= E(pk,k), σ2 := SigQ(pk,c, idP), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−k
P
While this protocol is statically secure, it is not PFS secure if we only assume that the encryption
scheme is semantically secure. It is instructive to see why this is the case. Suppose the adversary lets
P send the ﬁrst message ( pk,σ1,CertP) to Q, and then Q responds with ( c,σ2,CertQ). However,
suppose the adversary blocks this last message, but that Q uses the session key k to encrypt a
plaintext m1, sending the resulting ciphertext c1 out over the network for the adversary to see. At
this point in time, neither P nor Q is compromised, and so we expect that the adversary should
not be able to learn anything about m1. Moreover, PFS security should imply that this holds even
if P or Q is compromised in the future .
So suppose that at some later time, the adversary is able to obtain Q’s signing key. This allows
the adversary to send a message ( c′,σ′
2,CertQ) to P, where c′ ̸= c and σ′
2 is a valid signature on
(pk,c′,idP). This means that P will accept the signature and decrypt c′, obtaining some session
key k′ that may be diﬀerent from but related to k. For example, the adversary may be able to
make k′⊕k = ∆ for some ∆ ̸= 0 of the adversary’s choice. Now, suppose P encrypts a plaintext
m2 under k′, and sends the resulting ciphertext c2 out over the network for the adversary to see.
The adversary may now be able to carry out a related key attack on the symmetric cipher,
analyzing the ciphertexts c1 and c2, and exploiting the fact that they are produced using keys
whose XOR is ∆ to learn something new about the plaintext m1. Indeed, the standard deﬁnitions
of security for symmetric ciphers make no security guarantees when such related keys are used.
More generally, this attack violates our informal secrecy requirement, which says that learning
one session key (in this case P’s) should not reveal anything about a diﬀerent session key (in this
case Q’s).
21.4 HSM security
We emphasized a number of times that in a secure implementation of a session key protocol, it
is important that the participants securely erase all ephemeral data they generated during the
protocol. Consider again protocol AKE2 in Section 21.3. If the value sk generated by P during a
run of the protocol is somehow leaked to an adversary, the consequences are devastating: using sk
and P’s signature on pk, the adversary can impersonate P at any time, as often as he likes, to any
user.
Such ephemeral leakage could occur in a number of ways. The protocol could be poorly imple-
mented, and fail to securely erase this data. Alternatively, the user’s machine could be temporarily
infected with malware that is able to observe the machine’s memory while the protocol is running.
869
If such leakage occurs, some damage is unavoidable. It would be nice if protocols could be
designed so that the damage is limited to only those sessions where the leakage occurred.
One might object to this whole line of inquiry: if an adversary is able to read this ephemeral data,
what is to keep him from reading the user’s long-term secret key? Indeed, in many implementations
of the protocol, this objection is perfectly reasonable. However, in well-designed implementations,
special care is taken to ensure that the long-term key is carefully stored and not as easily accessed
as the ephemeral data or even the session key itself. In this situation, it is reasonable to demand
more from a key exchange protocol.
A good way to think about the attack model is in terms of a Hardware Security Module
(HSM). An HSM is a specialized piece of hardware that stores a user P’s long-term secret key
LTSP, and which can only be used as an “oracle” that computes a protocol-speciﬁc function
f(LTSP,x). That is, given x, the HSM computes and outputs the value f(LTSP,x). During a
limited time window, the adversary may have access to the HSM, and be able to evaluatef(LTSP,x)
for any xof its choice, but not to extract LTSP from the hardware. This should only compromise a
limited number of sessions, and only sessions in which the adversary had access to the HSM while
the key exchange protocol was running. Of course, as in PFS security, we also consider a permanent
compromise where the adversary permanently steals LTSP.
While an HSM may be implemented using special hardware, it may also be implemented as an
isolated “enclave” enforced by the processor [47]. Such enclaves are becoming ubiquitous.
HSM security. Very roughly speaking, HSM security means that if an instance of user Qruns
the protocol to completion, and thinks he shares a session key with P, then that session key should
be vulnerable only if
(i) P is a corrupt user,
(ii) P is an honest user, but LTSP was compromised at some time in the past, or
(iii) P is an honest user, but the adversary accessed P’s HSM during the (presumably short)
window of time that Q was running the protocol, and moreover, the number of other user
instances who think they are talking to P is less than the total number of times P’s HSM
was accessed.
Condition (i) corresponds to static security, conditions (i)–(ii) correspond to PFS security, and
conditions (i)–(iii) correspond to HSM security (so HSM security is at least as strong as PFS
security). Essentially, condition (iii) says that the sessions damaged by a single HSM query are
limited in both time and number. The formal deﬁnition is ﬂeshed out in Section 21.9.4.
HSM security is a very strong notion of security. It can be used to model leakage of ephemeral
data: if a run of the protocol leaks ephemeral data, then we treat that run of the protocol as if
the adversary ran the protocol, accessing the HSM just as the protocol itself would. However, it
can also model much stronger attacks, in which the adversary can actively probe the HSM with
arbitrary inputs, to try to learn something more about LTSP than could be gained by observing
honest runs of the protocol.
One might argue that we should just put the entirety of protocol AKE2 in the HSM, and thereby
trivially obtain HSM security. However, we would prefer the interface to the HSM be as simple
as possible. Moreover, we insist that the HSM is just an oracle for a simple function and is
completely stateless. This requirement rules out the possibility of encapsulating protocol AKE2 in
870
P Q
pk,CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
c:= E(pk,k), σ1 := SigQ(1,pk,c, idP), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
k
Q σ2 := SigP(2,pk,c, idQ)−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→k
P
Figure 21.5: Protocol AKE3
an HSM. Moreover, even without this restriction, AKE2 is not HSM secure: an adversary could
query P’s HSM on Monday to get the ﬁrst ﬂow of the protocol, interact with Q on Tuesday, and
then interact with P’s HSM on Wednesday to ﬁnish the protocol. This attack would contradict
the requirement that compromises based on HSM access should be limited in time. Our goal is to
construct an eﬃcient protocol that achieves HSM security, where the HSM stores a signing key,
and does nothing more than act as a stateless “signing oracle.”
In the following protocol, which we call AKE3, each user has a long-term public key that is a
public key for a signature scheme. The corresponding long-term signing key is stored in an HSM
that signs arbitrary messages. In addition, the protocol makes use of a semantically secure public-
key encryption scheme E = ( G,E,D ). As in protocol AKE2, a new, ephemeral key pair for the
encryption scheme is generated with every run of the protocol. On the plus side, we will only need
to assume that Eis semantically secure (instead of CCA secure). On the minus side, the protocol
consists of three ﬂows (instead of two).
When executed by users P and Q, protocol AKE3 runs as described in Fig. 21.5. Here, user P
generates a key pair ( pk,sk) every time he runs the protocol. In addition, each user veriﬁes the
certiﬁcates and signatures it receives.
Here is a more detailed description of protocol AKE3:
1. P computes (pk,sk) ←R G(), and sends ( pk,CertP) to Q;
2. Qveriﬁes CertP; if the certiﬁcate is invalid, Qaborts; otherwise, Qextracts the identity idP
from CertP, along with P’s public veriﬁcation key, and then computes
k←R K, c←R E(pk,k), σ1 ←R SigQ(1,pk,c, idP),
and sends (c,σ1,CertQ) to P;
3. P veriﬁes CertQ; if the certiﬁcate is invalid, P aborts; otherwise, P extracts the identity idQ,
along with Q’s public veriﬁcation key, and then veriﬁes thatσ1 is a valid signature on the mes-
sage (1,pk,c, idP) under Q’s public veriﬁcation key; if not, P aborts; otherwise, P computes
k ←D(sk,c); if k = reject; then P aborts; otherwise, P computes σ2 ←R SigP(2,pk,c, idQ),
and sends σ2 to Q; in addition, P terminates successfully, and outputs the session key k, and
partner identity idQ;
4. Qveriﬁes that σ2 is a valid signature on the message (2,pk,c, idQ) under P’s public veriﬁcation
key; if not, Qaborts; otherwise, Qterminates successfully, and outputs the session key k, and
partner identity idP.
871
P Q
u:= gα, CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
v:= gβ, σ1 := SigQ(1,u,v, idP), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
k:= H(u,v,v α)
Q σ2 := SigP(2,u,v, idQ)−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→k:= H(u,v,u β)
P
Figure 21.6: Protocol AKE3eg
Ensuring HSM security. A key property that is needed to prove HSM security is that both P
and Q get their peer to sign random challenges during the protocol. This ensures that the HSM
must have been accessed during the protocol to sign that particular random challenge — either
indirectly, by an honest user instance, or directly, by the adversary. This is essential to achieve
HSM security. It also means that every HSM secure protocol must have three ﬂows.
Choice of encryption scheme. As we did for protocols AKE1 and AKE2, we can implement
protocol AKE3 using ElGamal encryption. This is shown in Fig. 21.6. To prove security, either H
is modeled as a random oracle and we assume that CDH holds for G, or H is modeled as a secure
KDF and we assume that DDH holds for G (or we use the HDH assumption in Exercise 11.14).
Also note that since we do not require CCA security, it is not necessary for P to explicitly check
that v is in G (or for Q to explicitly check that u is in G).
21.4.1 A technical requirement: strongly unpredictable ciphertexts
To prove HSM security, we need to impose a non-standard, but perfectly reasonable, requirement
on the public-key encryption scheme. Namely, that it is hard to predict the output of the encryption
algorithm on a given public key and given message. Although semantic security implies that this
holds for honestly generated public keys (this follows from the result of Exercise 5.12, which is
easily adapted to the public key setting), we require that it holds even for adversarially chosen
public keys.
To formulate this property, we assume that the encryption algorithm may output error if it
detects that something is wrong with the public key (or the message, for that matter). We say
the encryption scheme has strongly unpredictable ciphertexts if for all pk, m, and c, with
c̸= error, the probability that E(pk,m) = c is negligible.
The reason for this technical requirement is that in protocol AKE3 (and other HSM secure
protocols we will examine), the ciphertext is being used as an unpredictable challenge.
Certainly, for ElGamal-based encryption, this requirement is already met. Other encryption
schemes can typically be easily adapted to ensure this requirement is met.
21.4.2 Insecure variations
As in Section 21.2.1, we consider minor variants, showing attacks on each, and thus demonstrating
that every piece of protocol AKE3 is essential. Insecure Variation 4 is the most interesting. It
872
demonstrates an attack in the HSM model, where the adversary makes a single oracle query to the
user’s long-term key and can subsequently compromise many sessions.
Variation 1: do not sign c in σ1 — a key exposure attack
Suppose Q does not sign c in σ1. The new protocol runs as follows:
P Q
pk, CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
c:= E(pk,k), σ1 := SigQ(1,pk,idP), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
k
Q σ2 := SigP(2,pk,c, idQ)−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→k
P
Here is a simple key exposure attack:
P Q
pk, CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
∥
c:= E(pk,k), σ1 := SigQ(1,pk,idP), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
c′:= E(pk,k′), σ1, CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−∥
k′
Q σ2 := SigP(2,pk,c′,idQ)−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→∥
Here, the adversary generates c′by encrypting a session key k′of his choosing under pk. At the end
of the protocol, P has the session key k′, which is known to the adversary; however, the adversary
cannot make Q terminate the protocol successfully.
Variation 2: do not sign idP in σ1 — an identity misbinding attack
Suppose Q does not sign idP in σ1. The new protocol runs as follows:
P Q
pk, CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
c:= E(pk,k), σ1 := SigQ(1,pk,c), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
k
Q σ2 := SigP(2,pk,c, idQ)−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→k
P
Here is an identity misbinding attack:
P Q
pk, CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→∥
∥ pk, CertR−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
c:= E(pk,k), σ1 := SigQ(1,pk,c), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
k
Q σ2 := SigP(2,pk,c, idQ)−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→∥
∥ σ′
2 := SigR(2,pk,c, idQ)−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→k
R
873
At the end of this attack, P and Q share the session key k, although P thinks he is talking to Q,
and Qthinks he is talking to R. To carry out this attack, the adversary needs the help of a corrupt
user R, who registers with the CA following the normal registration protocol.
Variation 3: do not sign pk in σ2 — a key exposure attack
Suppose P does not sign pk in σ2. The new protocol runs as follows:
P Q
pk, CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
c:= E(pk,k), σ1 := SigQ(1,pk,c, idP), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
k
Q σ2 := SigP(2,c, idQ)−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→k
P
Here is a rather subtle attack. Suppose Q’s signing key has been compromised and he is
unaware that this has happened, and continues participating in using the session key protocol with
this compromised key. Even though Q’s long-term signing key is compromised, we nevertheless
might expect that if Q runs the protocol with an honest user P, the session should remain secure
— after all, it is Q’s key that is compromised, not P’s. However, in this situation, the adversary
can carry out an attack as follows:
• the adversary intercepts the message ( pk,CertP) from P to Q;
• the adversary runs the key generation algorithm to obtain ( pk′,sk′) ←R G(), and sends the
message (pk′,CertP) to Q;
• when Q responds with a message ( c,σ1,CertP), where c := E(pk′,k) and σ1 :=
SigQ(1,pk′,c, idP), the adversary blocks this message and sends instead the message
(c,σ′
1,CertP), where σ′
1 := SigQ(1,pk,c, idP); it also decrypts c using sk′to obtain k;
• when P responds with a signature σ2 := SigP(2,c, idQ), the adversary simply forwards this
to Q.
The following diagram illustrates the attack:
P Q
pk, CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→∥
∥ pk′, CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
∥
c:= E(pk′,k), σ1 := SigQ(1,pk′,c, idP), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
c, σ′
1 := SigQ(1,pk,c, idP), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−∥
k′
Q σ2 := SigP(2,c, idQ)−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→k
P
At the end of the attack, P is holding the session key k′, which is unknown to the adversary;
however, Q is holding the session key k, which is known to the adversary.
874
For this attack to work, it must be the case that even though P decrypts c with the “wrong”
public key, the decryption still succeeds so that P generates the ﬁnal signature σ2. For typical
semantically secure encryption schemes, this decryption will always succeed (and it still might
succeed even using a CCA-secure scheme).
This particular type of vulnerability is called a key compromise integrity (KCI) vulnera-
bility. A similar notion in a diﬀerent context was discussed in Section 13.7.5.2. It is not entirely
clear that one should really worry about this type of vulnerability. But some people do, and since
it is easy mitigate against, it seems best to do so.
Variation 4: do not sign c in σ2 — a key exposure attack
Suppose P does not sign c in σ2. The new protocol runs as follows:
P Q
pk, CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
c:= E(pk,k), σ1 := SigQ(1,pk,c, idP), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
k
Q σ2 := SigP(2,pk,idQ)−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→k
P
Here is a key exposure attack that exploits the fact that the adversary can access P’s HSM.
First, the adversary runs ( pk,sk) ←R G(). It then somehow queries P’s HSM to get a signature
σ2 = SigP(2,pk,idQ). In practice, all the adversary needs to be able to do is to somehow get a
look at P’s ephemeral secret key during an ordinary run of the protocol between P and Q. Now
that the adversary has done this, he can run the following attack against Q at any time and any
number of times :
Q
∥ pk, CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
∥
c:= E(pk,k), σ1 := SigQ(1,pk,c, idP), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
∥ σ2−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→k
P
In the each run of the protocol, the adversary makes Q think he shares the key k with P, but in
fact, Q shares the key k with the adversary. The adversary can get k by decrypting c using sk.
This variation is also open to a key exposure attack via a KCI vulnerability, similar to that in
Variation 3. We leave this to the reader to verify.
Variation 5: do not sign idQ in σ2 — an identity misbinding attack
Suppose P does not sign idQ in σ2. The new protocol runs as follows:
P Q
pk, CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
c:= E(pk,k), σ1 := SigQ(1,pk,c, idP), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
k
Q σ2 := SigP(2,pk,c)−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→k
P
875
Here is an identity misbinding attack:
P Q
pk, CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
∥
c:= E(pk,k), σ1 := SigQ(1,pk,c, idP), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
c, σ′
1 := SigR(1,pk,c, idP), CertR
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−∥
k
R σ2 := SigP(2,pk,c)−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→k
P
At the end of this attack, P and Q share the session key k, although P thinks he is talking to R,
and Qthinks he is talking to P. To carry out this attack, the adversary needs the help of a corrupt
user R, who registers with the CA following the normal registration protocol.
Variation 6: do not sign the 1/2 values — a key exposure attack
The reader may be wondering why we have Qinclude a “1” in its signed message and P include a
“2” in its signed message. Suppose we leave these out, so that the protocol becomes:
P Q
pk,CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
c:= E(pk,k), σ1 := SigQ(pk,c, idP), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
k
Q σ2 := SigP(pk,c, idQ)−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→k
P
Here is a key exposure attack:
Q
∥ pk, CertQ
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
∥
c:= E(pk,k), σ1 := SigQ(pk,c, idQ), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
∥ σ1−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→k
Q
At the end of this attack, an instance of user Q thinks he shares a key with another instance of
user Q, while in reality, he shares a key with the adversary. In some settings, it may be reasonable
to assume that an instance of a user will not wish to share a key with another instance of itself,
but this may not always be the case: for example, a person’s phone and laptop computer may talk
to each other, using the same certiﬁcate.
21.5 Identity protection
In this section, we consider an additional security requirement: identity protection.
Very roughly speaking, identity protection means that an adversary cannot learn the identity of
either one or both the users that are running the AKE protocol. Here, the adversary could either
be a passive observer, or even an active participant in the protocol.
876
P Q
pk−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
c:= E
(
pk, (k,k1,k2)
)
, c1 := Es
(
k1, (SigQ(1,pk,c),CertQ)
)
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
k
Q c2 := Es
(
k2, (SigP(2,pk,c),CertP)
)
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→k
P
Figure 21.7: Protocol AKE4
In the case where the adversary is a passive observer, and the two users running the protocol are
honest, the goal is to prevent the adversary from learning the identity of either one or both users.
We call this eavesdropping identity protection. When the adversary is one of the participants,
the goal is a bit more subtle: obviously, we want each user to eventually learn the identity of the
other; however, the goal is to allow one user, say P, to withhold his identity until he is sure he is
talking to someone he trusts. We say that P enjoys full identity protection.
As an example, consider a network of mobile devices communicating with a number of base
stations. Identity protection should prevent an adversary from tracking the location of a given
mobile device. Certainly, identity protection against an eavesdropping adversary will help to prevent
this. However, a more aggressive adversary may try to interact with a mobile device, pretending
to be a base station: although the protocol will presumably end in failure, it may have proceeded
far enough for the adversary to have learned the identity of the mobile device.
In Fig. 21.7 we present a simple protocol that is HSM secure and provides identity protection,
which we call protocol AKE4. This protocol makes use of a public-key encryption scheme E =
(G,E,D ) and a symmetric encryption scheme Es = (Es,Ds).
Here is a more detailed description of protocol AKE4:
1. P computes (pk,sk) ←R G(), and sends pk to Q;
2. Q generates a random session key k and random keys k1,k2 for Es, and then computes
c←R E
(
pk, (k,k1,k2)
)
, σ1 ←R SigQ(1,pk,c), c1 ←R Es
(
k1, (σ1,CertQ)
)
and sends (c,c1) to P;
3. P decrypts c under the key sk; if decryption fails, P aborts; otherwise, P obtains k,k1,k2,
and decrypts c1 under k1; if decryption fails, P aborts; otherwise, P obtains σ1,CertQ; P
veriﬁes CertQ; if the certiﬁcate is invalid, P aborts; otherwise, P extracts the identity idQ,
along with Q’s public veriﬁcation key, and then veriﬁes that σ1 is a valid signature on the
message (1,pk,c) under Q’s public veriﬁcation key; if not, P aborts; otherwise, P computes
σ2 ←R SigP(2,pk,c), c2 ←R Es
(
k2, (σ2, CertP)
)
and sends c2 to Q; in addition, P terminates successfully, and outputs the session key k, and
partner identity idQ;
4. Qdecrypts c2 under the key k2; if decryption fails, Qaborts; otherwise, Qobtains σ2,CertP;
Qveriﬁes CertP; if the certiﬁcate is invalid, P aborts; otherwise, Qextracts the identity idP,
877
P Q
u:= gα
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
v:= gβ, c1 := Es
(
k1, (SigQ(1,u,v ), CertQ)
)
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
k
Q c2 := Es
(
k2, (SigP(2,u,v ), CertP)
)
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→k
P
(k,k1,k2) := H(gα,gβ,gαβ)
Figure 21.8: Protocol AKE4eg
along with P’s public veriﬁcation key, and then veriﬁes that σ2 is a valid signature on the
message (2,pk,c) under P’s public veriﬁcation key; if not, P aborts; otherwise, Qterminates
successfully, and outputs the session key k, and partner identity idP.
Let us return to the above application to get some intuition. In using this protocol, P plays
the role of a mobile device while Q plays the role of a base station. First, to an outside observer
watching an interaction between P and Q, no information about the identity of either P or Q is
revealed. Second, P need only reveal its identity to a base station whose identity it knows and
trusts. Note that Q’s identity is not protected; it is revealed to P before Qknows who P is. Hence,
both parties have eavesdropping identity protection, and P has full identity protection.
HSM security. The protocol is HSM secure (where the HSM is a signing oracle), assuming Eis
semantically secure, Es provides one-time authenticated encryption, and the underlying signature
schemes are secure. In fact, to prove HSM security, we only need to assume that Es provides one-
time ciphertext integrity. Semantic security for Es is only needed to achieve identity protection,
which is a notion that we shall not attempt to formally deﬁne.
Choice of encryption scheme. As we did for protocols AKE1–AKE3, we can implement protocol
AKE4 using ElGamal encryption. This is shown in Fig. 21.8. We have streamlined the protocol
somewhat, so that all of the necessary keys are derived directly from the hash function H. Again,
H is modeled as a random oracle and we assume that CDH holds for G, or H is modeled as a secure
KDF and we assume that DDH holds for G (or we use the HDH assumption in Exercise 11.14).
Just as for protocol AKE3, since we do not require CCA security, there is no need for either user to
perform any explicit group membership checks.
21.6 One-sided authenticated key exchange
Up to now, we have assumed that all users must register with a CA. In fact, in many practical
settings, this is too much to ask for. In this section we consider a setting in which only one of the
two users running the protocol has registered with a CA.
For example, consider the situation where a customer wishes to establish a secure channel with
an online bank. Here, the customer will typically not have registered with a CA, but the bank
has. To be more general, let us call a user (such as a customer) without a certiﬁcate a client,
878
P Q
pk−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
c:= E
(
pk, (k,k1,k2)
)
, c1 := Es
(
k1, (SigQ(1,pk,c),CertQ)
)
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
k
Q c2 := Es
(
k2, ( ∗, ∗)
)
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→k
∗
Figure 21.9: Protocol AKE4∗
and a user (such as a bank) with a certiﬁcate a server. As we shall see below, one can easily
construct key exchange protocols that eﬀectively allow a client and server to establish a one-
sided authenticated secure channel . Intuitively, when the client establishes such a channel,
he eﬀectively has a “data pipe” that connects securely to the server. For example, the client may
safely transmit sensitive information (e.g., a credit card number) through the channel, conﬁdent
that only the server will read this information; also, the client can be sure that any data received
on this channel originated with the server. However, from the server’s point of view, things are
diﬀerent, since a client has no certiﬁcate. When the server establishes such a channel, all he knows
is that he has a “data pipe” that connects to “someone,” but he has no idea who that “someone”
is.
Typically, if a client wants to establish a long-term relationship with the server, he will use a one-
sided authenticated secure channel to create a client account with the server, which includes, among
other things, the client’s user ID and password. The client can be sure that this password, and any
other sensitive information, can only be read by the server. Later, in a subsequent transaction with
the server, the client will set up a new one-sided authenticated secure channel with the server. To
identify himself to the server, the client will transmit his user ID and password over the channel.
From the client’s point of view, it is safe to transmit his password over the channel, since he
knows that only the server can read it. From the server’s point of view, once the client’s user
ID and password have been veriﬁed, the server can be (relatively) conﬁdent that this “data pipe”
connects securely to this client. At this point, the one-sided authenticated secure channel has been
essentially upgraded to a mutually authenticated secure channel (but see Section 21.11.1). While
the server may not know who the client “really is,” he at least knows it is the same client that
initially established a relation with the server using the given user ID.
21.6.1 A one-sided authenticated variant of AKE4
We present a one-sided authenticated variant of AKE4, which we call AKE4∗, in Fig. 21.9.
Here, P is the client and Q is the server. Protocol AKE4∗ is to be viewed as an extension of
protocol AKE4, so that some sessions provide one-sided authentication and others provide two-sided
authentication. Protocol AKE4∗ is identical until the last ﬂow, in which now the client sends an
encryption under k2 of a dummy message. When the server decrypts c2 and sees this dummy
message, the server assumes the client is unauthenticated.
Note that with the third ﬂow, the security still holds under semantic security. Note that if we
want a protocol that only supports one-sided authentication, the third ﬂow is not necessary. To
prove HSM security of protocol AKE4∗, we need to make a stronger assumption — namely, that the
879
public-key encryption scheme Eis CCA secure. The reason for this is much the same as in the case
of protocol AKE2 (see Section 21.3.1).
If we implement protocol AKE4∗ using ElGamal encryption, we get protocol AKE4eg, but with
the last ﬂow replaced by an encryption of a dummy message, as in protocol AKE4∗.
21.7 Deniability
Consider protocol AKE3 in Section 21.4. In that protocol user P generates a signature
SigP(2,pk,c, idQ). Anybody observing the protocol would see this signature, and could prove
to another party that P ran the key exchange protocol with Q. For example, suppose P is a
mobile device that communicates with a base station Q. From this signature, one could “prove”
to a judge that the mobile device was near the base station at some point in time. As discussed
at the beginning of Chapter 13, this “proof” might still be challenged in a court of law, as there
are other ways this signature could have been created — for example, P’s signing key could have
been leaked. The same observations apply to Q in protocol AKE3, since Q generates a signature
SigQ(1,pk,c, idP).
It would be nice if key exchange protocols would provide some form of “deniability”, so that
no information obtained from the execution of the protocol could be used to prove to a third party
that either one or both of the users involved actually participated in the protocol.
Now consider protocol AKE4. Since all messages in the protocol are encrypted, an outsider
observing the execution gets no information about the users involved, and in particular, no infor-
mation that could implicate either of them. However, one still has to consider the possibility that
one of the participants can implicate the other. In this protocol, neither P nor Q explicitly sign a
message that contains the other’s identity. In fact, it does indeed seem that this protocol provides
some level deniability to both users. However, we know of no way to argue this in any rigorous
way. Indeed, in this protocol, Q can implicate P to a certain degree, as follows. After running the
protocol with P, user Q can save all of the data it generated and collected during the protocol,
including the random bits r that Q used to generate the ciphertext c. At this point, Q can prove
to a third party that P signed the message (2 ,pk,c), where Q knows all of the inputs (including
r) used in the computation of c. Does this by itself prove that P ran the protocol with Q? Not
really, but to make Q’s evidence stronger, Qcould compute r, say, as a hash of SigQ(pk), which is
something that only Qcan compute. The fact that P signed a message that includes a ciphertext c
computed using this special r seems like strong evidence that P ran the AKE protocol with Q.
Now P could defend himself against this evidence by claiming that what actually happened
is that he ran the protocol with another user R, but Q collaborated with R to make it look like
P ran the protocol with Q. In particular, P could argue that R generated the ciphertext c using
randomness r that was supplied by Q. Hence, Q’s evidence that implicates P is not unassailable,
but it is still perhaps stronger than we would like.
Deniability. In this section, we will brieﬂy present a couple of protocols that provide a strong
form of deniability for one of the two participants of the protocol. Deniability for a user P is
ensured by a proof (in a fairly reasonable heuristic model) that when P engages in the protocol
with Q, whatever evidence user Qis able to gather that might implicate user P, user Qcould have
generated on its own, without ever talking to P. This ensures that Q’s evidence is unconvincing.
This property holds even for a malicious Q that does not follow the protocol.
880
P Q
(public key = gα) (public key = gβ)
gµ, CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
gν, k1, CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
k
Q k2−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→k
P
(k,k1,k2) := H
(
g(α+µ)(β+ν), gµν, gα,gµ,gβ,gν, idP,idQ
)
Figure 21.10: Protocol AKE5
The ﬁrst protocol we present provides deniability for P, but no identity protection. The second
protocol additionally provides identity protection where P only reveals its identity to Q after it
knows Q’s identity.
21.7.1 Deniability without identity protection
Our ﬁrst protocol that provides deniability for one of the two users is called protocol AKE5, and is
presented in Fig. 21.10. The protocol makes use of a cyclic group G of prime order q generated
by g∈G, along with a hash function H. The hash function takes as input several group elements
along with two user identities, and outputs ( k,k1,k2) ∈K×R×R . Here, Kis the set of session
keys, and Ris any super-poly-sized set. In the security analysis, we will model H as a random
oracle. We will also need to assume a variant of the ICDH assumption (see Section 12.4) for G.
Here is a more detailed description of protocol AKE5.
1. P chooses µ∈Zq at random and sends ( gµ,CertP) to Q;
2. Qveriﬁes CertP; if the certiﬁcate is invalid, Qaborts; otherwise, Qextracts the identity idP
from CertP, along with P’s public key gα; Q chooses ν ∈Zq at random and computes
(k,k1,k2) ←H((gαgµ)(β+ν), (gµ)ν, gα,gµ,gβ,gν, idP,idQ), (21.1)
where gβ is Q’s public key, and sends ( gν,k1,CertQ) to P;
3. P veriﬁes CertQ; if the certiﬁcate is invalid, P aborts; otherwise, P extracts the identity idQ
from CertQ, along with Q’s public key gβ; then P computes
(k,k1,k2) ←H((gβgν)(α+µ), (gν)µ, gα,gµ,gβ,gν, idP,idQ); (21.2)
then P compares its computed value of k1 to the value it received from Q; if these do not
match, P aborts; otherwise, P sends k2 to Q; in addition, P terminates successfully, and
outputs the session key k, and partner identity idQ;
4. Q compares its computed value of k2 to the value it received from P; if these do not match,
Q aborts; otherwise, Q terminates successfully, and outputs the session key k, and partner
identity idP.
881
To completely specify the protocol, we have to specify the interface for the HSM. P’s HSM
stores αand idP, takes as input µ, gβ, gν, and idQ, and outputs the hash value computed in (21.2).
Similarly, Q’s HSM stores β and idQ, takes as input ν, gα, gµ, and idP, and outputs the hash value
computed in (21.1). In fact, we assume that any given user may have user instances playing both
the role of P and the role of Q, so the HSM also takes as input the speciﬁed role and computes the
hash accordingly.
HSM security. The HSM security of protocol AKE5 can be proved under a variant of the ICDH
assumption. We will sketch some of the details later in Section 21.9.4.3.
Deniability. Since neither party signs anything, protocol AKE5 seems to provide some level of
deniability for both P and Q. However, we can make an even stronger case for P’s deniability. The
idea is that we can eﬃciently simulate everything thatQsees in its interaction withP, without using
P at all — this means that whatever evidence user Q is able to gather that might implicate user
P, it could have generated on its own, without ever talking to user P. To build such a simulator,
we need to assume that it is easy to recognize DH-triples in G — we can achieve this by using an
elliptic curve with a pairing as in Section 15.4. Our simulator will also work by modeling H as a
random oracle — in particular, our simulator must have the ability to observe Q’s random oracle
queries. The simulator works as follows. It begins by choosing µ ∈Zq at random and sending
(gµ,CertP) to Q. Next, when Q sends ( gν,k1,CertQ) to P, the simulator looks at Q’s random
oracle queries and sees if any of these output k1. If none exists, we can safely say that P would
abort (note that if some other user Rin the system made the relevant query, the input to the hash
would contain idR rather than idQ, and so P would also abort in this case). If it does exist, the
simulator checks that its input is of the right form (this is where we need the ability to recognize
DH-triples). If not, we can again safely say that P would abort. Otherwise, the simulator knows
the input to the hash function and therefore knows the output ( k,k1,k2). Therefore, the simulator
can generate the last ﬂow k1 as well as the session key k.
A few remarks about this simulator are in order:
• While it works in the random oracle model, it does not actively manipulate the random oracle
as in many of our security proofs, but rather, simply observesQ’s random oracle queries. This
is essential in order to achieve a meaningful notion of deniability — we are trying to argue
that Q could generate this view on its own, and Q does not have the ability to manipulate
the random oracle.
• The simulator must be able to simulate not only the conversation but also the session key.
This is because after completing the protocol, P might start using the session key. Any usage
of this key, together with the conversation, could potentially be used by Q to implicate P.
• The simulation proceeds in an “online” fashion, and works even in a concurrent, multi-user
environment where Q might also be interacting with other users who are completely honest
and are not collaborating with Q, as well as with many instances of P itself.
21.7.2 Deniability with identity protection
We now add identity protection to protocol AKE5. This new protocol is presented in Fig. 21.11,
and is called protocol AKE6. The main idea is that instead of sending gα in the clear, user P sends
882
P Q
(public key = gα) (public key = gβ)
gα+σ, gµ
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
gβ+τ, gν, c1 := Es
(
k1, (τ,CertQ )
)
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
k
Q c2 := Es
(
k2, (σ,CertP )
)
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→k
P
(k,k1,k2) := H(g(α+σ+µ)(β+τ+ν), gµν, gα+σ,gµ,gβ+τ,gν)
Figure 21.11: Protocol AKE6
a “blinded” value gα′
, where α′ = α+ σ, and then later sends the exponent σ along with CertP
(which contains P’s public key gα) in encrypted form; Q can then verify the blinding by checking
that gα ·gσ = gα′
. User Q carries out a symmetric strategy.
Here is a more detailed description of protocol AKE6. It makes use of a symmetric encryption
scheme Es = (Es,Ds).
1. P chooses σ,µ ∈Zq at random, sets α′:= α+ σ, and sends ( gα′
,gµ) to Q;
2. Q chooses τ,ν ∈Zq at random, sets β′:= β+ τ, and computes
(k,k1,k2) ←H((gα′
gµ)(β′+ν), (gµ)ν, gα′
,gµ,gβ′
,gν), (21.3)
c1 ←R Es
(
k1, (τ,CertQ )
)
,
and sends (gβ′
,gν,c1) to P;
3. P computes
(k,k1,k2) ←H((gβ′
gν)(α′+µ), (gν)µ, gα′
,gµ,gβ′
,gν), (21.4)
c2 ←R Es
(
k2, (σ,CertP )
)
;
P decrypts c1 using the key k1; if decryption fails, P aborts; otherwise, P obtains (τ,CertQ)
and veriﬁes CertQ; if the certiﬁcate is invalid, P aborts; otherwise, P extracts the identity
idQ from CertQ, along with Q’s public key gβ; P veriﬁes that gβ ·gτ = gβ′
; if this fails, P
aborts; otherwise, P sends c2 to Q; in addition, P terminates successfully, and outputs the
session key k, and partner identity idQ;
4. Qdecrypts c2 using the key k2; if decryption fails, Qaborts; otherwise, Qobtains (σ,CertP)
and veriﬁes CertP; if the certiﬁcate is invalid, Q aborts; otherwise, Q extracts the identity
idP from CertP, along with P’s public key gα; Q veriﬁes that gα ·gσ = gα′
; if this fails,
Q aborts; otherwise, Q terminates successfully, and outputs the session key k, and partner
identity idP.
To completely specify the protocol, we have to specify the interface for the HSM. P’s HSM
stores α, takes as input σ, µ, gβ′
, and gν, and outputs the hash value in (21.4). Similarly, Q’s HSM
stores β, takes as input τ, ν, gα′
, and gµ, and outputs the hash value in (21.3).
883
HSM security. The HSM security of protocol AKE6 can be proved under a variant of the I2CDH
assumption (see Section 13.7.4), and assuming Es provides one-time authenticated encryption. We
will sketch some of the details later in Section 21.9.4.4.
Deniability. We can also argue forP’s deniability using simulation strategy similar to that which
we used for protocol AKE5. Let us return to our example of a mobile device communicating with a
number of base stations, which we discussed in the context of identity protection in Section 21.5.
If a mobile device plays the role of P and the base station plays the role of Q, then protocol AKE6
provides very strong privacy guarantees for the mobile device:
• the mobile device’s identity is not visible to an outside observer;
• the mobile device only reveals its identity to the base station after the base station reveals
and authenticates its own identity;
• the mobile device can deny that it interacted with any particular base station.
On the limits of deniability. Deniability is a very slippery concept. In reality, many steps in
the conversation between P and Q may provide Q with evidence that it interacted with P. For
example, Q might ask P to supply information that is not publicly available, such as P’s bank
account number or birth date. Q could later use this information to argue to a third party that it
interacted with P. The point of protocols AKE5 and AKE6 is to show that the AKE protocol itself
need not give Q evidence that it interacted with P.
21.8 Channel bindings
Sometimes, it is helpful if a higher-level application using a session can refer to a session by a
globally unique name. In key exchange protocols, this is called a channel binding, although some
authors also call this a “session ID”.
To add this feature to a key exchange protocol, we require when instances of users P and Q
ﬁnish a successful run of a session key protocol, in addition to a session key k and partner IDs, the
key exchange protocol provides them with a channel binding.
The security property that a key exchange protocol with channel bindings should provide can
be roughly stated as follows:
Two user instances securely share a key if and only if they share the same channel
binding.
We can easily add secure channel bindings to all of the protocols discussed so far:
• Protocol AKE1: ( idP,idQ,r,c)
– Protocol AKE1eg: ( idP,idQ,r,v )
• Protocol AKE2: ( idP,idQ,pk,c)
– Protocol AKE2eg: ( idP,idQ,u,v )
• Protocol AKE3: ( idP,idQ,pk,c)
884
– Protocol AKE3eg: ( idP,idQ,u,v )
• Protocols AKE4 and AKE4∗: ( pk,c)
– Protocols AKE4eg and AKE4∗
eg: ( u,v)
• Protocol AKE5: ( idP,idQ,gµ,gν)
• Protocol AKE6: ( gα+σ,gµ,gβ+τ,gν)
We will brieﬂy discuss an application of channel bindings later in Section 21.11.1.
21.9 Formal deﬁnitions
Deﬁning security for AKE protocols is not so easy. In fact, there is currently no widely accepted
standard deﬁnition of security. Nevertheless, in this section, we present a deﬁnition of security that
captures the most basic elements of secure AKE in a reasonable way, and which is consistent in the
most essential aspects with various deﬁnitions in the literature. We start with the formal deﬁnition
of static security, which does not model either PFS or HSM security. Later, we discuss how to
modify the deﬁnition to model these notions.
The deﬁnition presented here works with either an oﬄine TTP (e.g., a CA), or an online TTP.
Intuitively, our deﬁnition of security captures the idea that each instance of a user should obtain a
fresh session key, which from an adversary’s point of view, should essentially appear to be uniformly
distributed over some speciﬁed key space K, and independent of all other session keys obtained by
other honest user instances (belonging to either this or other users). However, there are some
wrinkles which complicate things:
• The whole point of an AKE protocol is to generate a session key that is shared between two
user instances; therefore, the goal that every session key should be fresh is not quite right:
some pairs of session keys can and should be equal.
• A user may establish a session key directly with a corrupt user, in which case, this key cannot
possibly be expected to be fresh.
Syntactically, an AKE protocol speciﬁes a set Kof session keys, and three algorithms:
• The TTP algorithm, which dictates the logic of the TTP over the lifetime of the system. 1
• The user registration algorithm , which is an interactive protocol algorithm (see Sec-
tion 18.1) that takes as input a user ID. This algorithm speciﬁes an interactive subprotocol
that registers the named user with the TTP, and which establishes that user’slong-term secret
key.
• The session key establishment algorithm, which is an interactive protocol algorithm (see
Section 18.1) that takes as input a user’s ID and long-term secret key (as initialized by the
user registration algorithm ). This algorithm speciﬁes an interactive subprotocol that is used
to establish a session key with another user. To break symmetry, this algorithm also takes as
input a value role ∈{left,right}. Upon termination, this subprotocol outputs either abort, or
outputs (pid,k), where pid is a partner ID and k∈K is a session key.
1Formally, the TTP should be an eﬃcient interface, as in Deﬁnition 2.12.
885
Our goal is to present an attack game consisting of two experiments. Experiment 0 represents
a real attack, while Experiment 1 represents an idealization of an attack. As usual, we want these
two experiments to be indistinguishable from the point of view of an adversary. In each experiment,
the adversary is interacting with a challenger, which is slightly diﬀerent in each experiment.
The challenger plays the roles of the TTP and all the honest users. Formally speaking, the
adversary is completely arbitrary; however, one can think of the adversary as really playing three
distinct roles at once:
• the network,
• a higher level protocol, such as an email system, being run by honest users, and which makes
use of the session keys obtained by instances of honest users, and
• a truly malicious attacker, coordinating with corrupt users.
Because our formal adversary also plays the role of higher level protocols that use session keys, we
allow the adversary free access to session keys obtained by honest users, which may at ﬁrst seem
counter-intuitive, since one normally thinks of session keys as being hidden from the adversary. See
Section 21.9.1 for more about how to understand and use the deﬁnition.
Experiment 0. At the beginning of the attack, the challenger initializes the internal state of the
TTP. Now the adversary can make a number of queries to the challenger:
Register honest user: This query constructs a new honest user U, with an identity U.id speciﬁed
by the adversary. Behind the scenes, the challenger runs an instance of theregistration protocol
with the TTP. This protocol is run in a secure fashion: the adversary cannot see or inﬂuence
any messages sent between the honest user and the TTP. The TTP will update its internal
state, if necessary, and the challenger sets U.ltk to the user’s long-term secret key.
Register corrupt user: Here, the adversary essentially is allowed to run the registration protocol
directly with the TTP, using an identity of his choice.
Initialize honest user instance: This query constructs a new user instance I, which is associ-
ated with a previously registered honest user I.user = U, which is speciﬁed by the adversary.
The adversary also supplies a role I.role ∈{left,right}. The challenger initializes the internal
state of an honest user instance, using the ID I.user.id, the long-term secret key I.user.ltk,
and the given role I.role.
Deliver protocol message: The adversary speciﬁes a running honest user instance I along with
an incoming message min that is to be processed by that instance. The challenger processes
the message, updating the internal state of the instance, producing an outgoing message mout
along with a status value, which is one of
• (ﬁnished, I.pid, I.sk), indicating successful termination with partner ID I.pid and session
key I.sk,
• aborted, indicating unsuccessful termination,
• running, indicating not yet terminated.
886
Both mout and the status value — including the partner ID and session key, in the case of a
ﬁnished status — are handed to the adversary.
Deliver TTP message: This is only used in the online TTP setting. The adversary gives a
message min that is intended for the TTP. The challenger processes the message according
to the logic of the TTP. Any resulting message mout is given to the adversary.
There is one further restriction: the adversary is never allowed to register an honest user’s ID
as a corrupt user, and is never allowed to register an honest user ID more than once.
That completes the formal description of Experiment 0. Thus, the challenger maintains the
internal state of the TTP, the honest users, and all the honest user instances. The challenger
does not maintain any state information for corrupt users: this is the adversary’s responsibility.
However, the challenger does maintain a list of user IDs registered as corrupt users, and refuses
any registration requests that would register a given ID as both an honest and corrupt user.
Note that the adversary is never allowed to obtain the long-term secret key of an honest user or
the internal state of an honest user instance. Because of this restriction, this deﬁnition of security
does not capture the notions of PFS or HSM security. Later, we will show how to tweak the
deﬁnition to model these notions.
Before deﬁning Experiment 1, we have to introduce the notion of a partner function, which
will be required to establish security. Basically, a partner function is a mechanism which establishes
which user instances actually share a key, and which user instances hold session keys that must be
treated as inherently vulnerable. To be as ﬂexible as possible, the partner function may depend on
the protocol itself, but it must be eﬃciently computable as a function of thenetwork communication
log.
For technical reasons relating to protocol composability, this log does not include everything
the adversary sees. We shall deﬁne the log as a sequence of entries, generated as follows.
• For a register corrupt user query, the entry (corruptUser,id), where id is the ID of the corrupt
user.
• For an initialize honest user instance query, the entry (init,I,I. role) is appended to the log.
Note that the log entrydoes not include I.pid, as this value is not a part of the normal network
traﬃc.
• For a deliver protocol message query, the entry ( deliver,I,m in,mout,status) is appended to
the log, where status ∈{ﬁnished,aborted,running}. Note that the log entry does not include
I.pid or I.sk when status = ﬁnished, as these values are not a part of the normal network
traﬃc.
• For a deliver TTP message query, the entry (deliverTTP,min,mout) is added to the log. Recall
that in the oﬄine TTP setting of this chapter, there are no deliver TTP message queries, and
hence no log entries of this form.
These are the only entries in the log.
The partner function will be computed by the challenger in Experiment 1 each time a deliver
protocol message query to a running user instance I results in successful termination with a status
of (ﬁnished, I.pid, I.sk). The input to the partner function consists the communication log in the
attack game up to that point in time. The output of the partner function classiﬁes the user instance
as either
887
• vulnerable,
• fresh, or
• connected to J, where J is some ﬁnished honest user instance.
The meaning of this classiﬁcation will become clear momentarily.
Before deﬁning Experiment 1, we need one other concept. We call two ﬁnished honest user
instances I and J compatible if
• I.pid = J.user.id,
• J.pid = I.user.id, and
• I.role ̸= J.role.
Recall that our intuitive notion of authenticity translates into saying that if two users share a key,
they should be compatible.
Experiment 1. The challenger’s actions are precisely the same as in Experiment 0, except that
when a user instance I ﬁnishes with a session key I.sk, instead of giving the adversary I.sk, the
challenger instead gives the adversary an eﬀective session key I.esk, which is determined (in part)
by the classiﬁcation of I by the partner function.
vulnerable: If I.pid belongs to a corrupt user, then
I.esk ←I.sk;
that is, the eﬀective session key is set to the actual session key. Otherwise, I.esk ←error.
fresh: If I.pid belongs to some registered user (honest or corrupt), then
I.esk ←R K;
that is, the eﬀective session key is chosen at random. Otherwise, I.esk ←error.
connected to J: If J is compatible with I, and J is fresh and no other user instance previously
connected to it, then
I.esk ←J.esk;
that is, the eﬀective session key is set to that of this honest user instance’s “partner.” Oth-
erwise, I.esk ←error.
That ﬁnishes the description of Experiments 0 and 1. If Wb is the event that an adversary A
outputs 1 in Experiment b, we deﬁne A’s advantage with respect to a given AKE protocol Π and
partner function pf to be
sKEadv[A,Π,pf] :=
⏐⏐⏐Pr[W0] −Pr[W1]
⏐⏐⏐.
Deﬁnition 21.1 (statically secure authenticated key exchange). An AKE protocol Π is
statically secure if there exists an eﬃciently computable partner function pf such that for all
eﬃcient adversaries A, the value sKEadv[A,Π,pf] is negligible.
888
Remark 21.1. Note that in Experiment 1, the eﬀective session key is set to error if certain validity
conditions do not hold. However, since keys never take the value error in Experiment 0, security
implies that these validity conditions must hold with overwhelming probability in both experiments.
Also, for many protocols, these validity conditions are easily computable as a function of the
communication log. However, this is not always the case — for example, protocols that provide
identity protection, such as protocol AKE4 in Section 21.5. 2
Remark 21.2. For a secure protocol, there is typically very little, if any, choice in the deﬁnition of
a partner function. In the literature, this partnering is sometimes achieved by other means, whereby
a speciﬁc partner function is deﬁned that must work for all secure protocols. For example, some
authors use the notion of “matching conversations”, which roughly means that two user instances
are partners if their conversations match up bit-by-bit. This can sometimes be overly restrictive,
as it may require the use of strongly secure signatures to ensure that conversations are highly non-
malleable. Instead of matching conversations, some authors use a notion of “session IDs” to specify
a partner function. This can also be problematic, especially when deﬁning security of protocols
that provide only one-sided authentication, as in Section 21.6.1. 2
A correctness requirement. To be complete, in addition to deﬁning the security of an AKE
protocol Π with respect to a partner function pf, we should also deﬁne a correctness requirement.
Roughly speaking, such a requirement says that if an adversary interacts with the challenger as in
Experiment 0 above, then for any pair of honest user instances, if the adversary faithfully transmits
all protocol messages between these two instances (and the TTP, if necessary), then (with over-
whelming probability) these two honest user instances will both terminate the protocol successfully,
and one will be connected to the other via the partner function. Note that this correctness require-
ment, together with the security requirement, guarantees that (with overwhelming probability)
these two honest user instances must share the same session key.
21.9.1 Understanding the deﬁnition
Our formal security deﬁnition may seem a bit unintuitive at ﬁrst. For example, one might ask, why
is the adversary given the session keys when the goal of the protocol is supposedly to protect the
session keys?
To gain a better understanding of the deﬁnition, it is useful to see how to use the deﬁnition to
analyze the security of a higher-level protocol that uses a secure AKE protocol. We focus here on
the most important application of AKE protocols, namely, to establish secure channels.
So suppose that we use a secure AKE protocol as follows. Once a user instance ﬁnishes the
key exchange protocol, it uses its session key to implement a secure channel using authenticated
encryption, as in Chapter 9. If we want this channel to be bi-directional, we will need an authen-
ticated encryption for each one-directional channel. We can derive all of the necessary keys from
the session key by using the session key as a seed for a PRG or a key for a PRF. The user instance
may now send and receive messages on its bi-directional channel, using these keys.
To analyze the security of this “secure session protocol”, we can proceed as follows. We can
think of each user instance as using an abstract interface, similar to that in Section 9.3. After
the user instance starts the protocol, if the AKE protocol terminates successfully, the user instance
obtains a partner ID. Next, the user instance can place messages in its out-box and retrieve messages
from its in-box, as in Section 9.3, but where now, the channel is bi-directional, so this user instance
889
is both a sender and a receiver. In this implementation of the abstract interface, the logic of the
out-box and in-box is implemented using an authenticated encryption scheme and the keys derived
from the session key.
An attacker in this setting has complete control of the network, and can attempt to interfere
with the protocol messages used to implement the AKE protocol as well as the protocol messages
used to implement the secure channel.
Now, starting with this “real” implementation, we can work towards a more “ideal” implemen-
tation.
The ﬁrst step is to use the security property of the AKE protocol, which allows us to replace
real session keys with eﬀective session keys, according to the classiﬁcation of user instances. Some
user instances will be “vulnerable” if they attempt to communicate with a corrupt user. Each
remaining user instance will have a truly random session key, which is shared with its partner user
instance (if any). Let us call these user instances “safe”. In our classiﬁcation system, “safe” user
instances are either “fresh” or “connected”.
To justify this step, we need to apply our deﬁnition of a secure AKE. In this analysis, the
adversary Battacking the AKE protocol comprises not only our original attacker A, but also the
logic of the honest users, outside of the internals of the AKE protocol itself. This adversary Bdoes
see and use the session keys, but it is only an artifact of the proof, and does not correspond to any
“real world” attacker. (Also note that the original attacker Acan compute the partner function
based on the network communication log, so A“knows” who is talking to whom.)
The second step is to replace the real implementation of each channel connecting two “safe”
user instances by the ideal implementation discussed in Section 9.3. In this ideal implementation,
ciphertexts are just handles and messages magically jump from sender to receiver.
21.9.2 Security of protocol AKE1
We now consider the security of the AKE protocol AKE1.
Recall protocol AKE1:
P Q
r, CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
k
Q c:= EncP(k,idQ), σ:= SigQ(r,c,idP), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−k
P
An instance of P playing on the left-hand side has the left role, and an instance of Q playing on
the right-hand side has the right role. We will adopt this convention in analyzing all the protocols
in this chapter. When this protocol terminates, the instance of P has session key k and partner ID
idQ, and the instance of Q has session key k and partner ID idP.
Theorem 21.1. Protocol AKE1 is a statically secure authenticated key exchange protocol, assuming:
the size of the nonce space Ris super-poly, the underlying public-key encryption scheme is CCA
secure, and the underlying signature schemes used by the users and CA are secure.
Proof sketch. The ﬁrst step in the proof is to specify a partner function. In this and other proofs in
this chapter, it is convenient to deﬁne a “loosely matching” relation for cryptographic objects. We
say two certiﬁcates are loosely matching if their IDs are the same. We say two signatures are always
loosely matching. We say two nonces, public keys, or ciphertexts are loosely matching if they are
890
identical. Two tuples of cryptographic objects are loosely matching if each of their components are
loosely matching.
When a right instance J ﬁnishes, we look at the ID of the certiﬁcate it receives. If it belongs
to a corrupt user, we classify J as vulnerable. Otherwise, we classify J as fresh. (Note that this is
the only time in this chapter we need to use the corruptUser entries in the communication log.)
When a left instance I ﬁnishes, if the two ﬂows it sees loosely match the two ﬂows seen by some
right instance J, we classify I as connected to J. Otherwise, we classify I as vulnerable.
We now sketch why this partner function works.
First, observe that the classiﬁcation of a right instance J as vulnerable is always valid, since by
deﬁnition, J’s partner ID belongs to a corrupt user.
Next, consider a left instance I that successfully ﬁnishes the protocol. We consider two cases.
Case 1: I has a partner ID belonging to some honest user. We claim that there is a unique right
instance J who sees two ﬂows that loosely match those seen by I, and that I and J are compatible;
this follows from the security of the signature schemes and the fact that ciphertexts do not repeat
(all this happens with overwhelming probability, of course). This I will be classiﬁed as connected
to J, and this classiﬁcation will be valid; this follows from the fact that I and J are compatible,
and nonces do not repeat.
Case 2: Otherwise. We claim that ﬂows seen by I cannot loosely match the two ﬂows seen by
any right instance J — otherwise, I’s partner ID would match the ID of J, and we would be back
in Case 1. Thus, I is classiﬁed as vulnerable, and this is a valid classiﬁcation. We also argue that
the ciphertext decrypted by I could not have been generated by any fresh right instance J under
I’s public key. Indeed, if it were, then J’s ID would be embedded in the ciphertext, and since
that ID belongs to an honest user, by the logic of the protocol, we must be back in Case 1. This
last assertion, together with the CCA security of encryption, implies that fresh session keys can be
replaced by random keys without detection. 2
21.9.3 Modeling perfect forward secrecy
We now show how to modify our static security deﬁnition to model perfect forward secrecy, that
is, PFS security. The changes are actually quite minimal.
First, we add a new type of query:
Compromise user: The adversary speciﬁes an honest user U. The challenger gives the long-term
secret key U.ltk to the adversary. Although we still say that U is an honest user, we say U is
compromised from this point on.
This query models the compromise of an honest user’s long-term secret key.
The second change is to the computation of eﬀective session keys in Experiment 1. Speciﬁcally,
we change the rule for computing the eﬀective session key for avulnerable user instance I as follows:
vulnerable: If I.pid belongs to a corrupt user or a compromised honest user, then
I.esk ←I.sk.
Otherwise, I.esk ←error.
These are the only changes. We denote by pfsKE adv[A,Π,pf] an adversary A’s advantage
against a protocol Π in this modiﬁed attack game, with respect to a given partner function pf.
891
Deﬁnition 21.2 (PFS secure key exchange). An AKE protocol Π is PFS secure if there
exists an eﬃciently computable partner function pf such that for all eﬃcient adversaries A, the
value pfsKEadv[A,Π,pf] is negligible.
Remark 21.3. Even after an honest user is compromised, the adversary may continue delivering
messages to user instances belonging to that user. We must allow this, as the adversary does not
obtain the internal state of these user instances, and so cannot “take over” the execution of these
user instances. For consistency and simplicity, we also allow the adversary to continue to initialize
user instances belonging to a compromised user. 2
Remark 21.4. Observe that the vulnerable classiﬁcation of I is valid only if I.pid belongs to a
corrupt user or a compromised honest user. It is not valid if only I.user itself is compromised. This
means our deﬁnition of security implies security against so-called KCI (key compromise imperson-
ation) attacks (see Variation 3 on p. 874 in Section 21.4.2). 2
21.9.3.1 Security of protocol AKE2
We now consider the security of protocol AKE2. We shall prove that this protocol satisﬁes the
deﬁnition of security with forward secrecy presented in Section 21.9.3.
Recall protocol AKE2:
P Q
pk, σ1 := SigP(pk), CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
k
Q c:= E(pk, (k,idQ) ), σ2 := SigQ(pk,c, idP), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−k
P
Theorem 21.2. Protocol AKE2 is a PFS secure authenticated key exchange protocol assuming: the
underlying public-key encryption scheme is CCA secure, and the underlying signature schemes used
by the users and CA are secure.
Proof sketch. The ﬁrst step in the proof is to specify a partner function.
When a right instance J ﬁnishes, we classify it as fresh if the ﬁrst ﬂow it sees loosely matches
the ﬁrst ﬂow sent by some left instance (see proof of Theorem 21.1). Otherwise, we classify J as
vulnerable.
When a left instance I ﬁnishes, we see if the two ﬂows it sees loosely match the two ﬂows seen by
some right instance J. If so, we classify I as connected to J. Otherwise, we classify I as vulnerable.
To prove that this works, one has to prove two claims.
1. When some right instance J ﬁnishes with a partner ID that belongs to an uncompromised
honest user, then the ﬁrst ﬂow it sees loosely matches the ﬂow sent by some left instance I.
2. When some left instance I ﬁnishes with a partner ID that belongs to an uncompromised
honest user, then there is a unique right instance J for which the two ﬂows seen by both
loosely match.
Proving that both of these hold (with overwhelming probability) follows from the security of the
signature schemes. The rest of the proof is very similar to that of Theorem 21.1. 2
892
21.9.4 Modeling HSM security
We now show how to modify our PFS security deﬁnition so as to model HFS security. Again, the
changes are actually quite minimal.
Starting with the PFS security model, we add a new type of query to the PFS that models
adversarial access to the HSM:
Access HSM: The adversary speciﬁes an honest user U and a value x. The challenger responds
with f(U.ltk,x). Here f is the function deﬁning the interface to the HSM and U.ltk is the
long-term secret key of user U.
The second change is to the computation of eﬀective session keys in Experiment 1. Speciﬁcally,
we change the rule for computing the eﬀective session key for avulnerable user instance I as follows:
vulnerable: If I.pid belongs to a corrupt user or a compromised honest user, or both of the
following conditions hold:
(i) I.pid belongs to an honest userU whose HSM was accessed at some point in time between
when I was activated and when I ﬁnished, and
(ii) the total number of adversarial HSM accesses on user U is greater than the number of
other vulnerable user instances J with J.pid = I.pid,
then
I.esk ←I.sk.
Otherwise, I.esk ←error.
Conditions (i) and (ii) above correspond to high-level security goals for HSM security we intro-
duced in Section 21.4. Together, they say that a single HSM query can be used to classify only a
single user instance as vulnerable, and the query must happen while that user instance is running.
We denote by hsmKE adv[A,Π,pf] an adversary A’s advantage against a protocol Π in this
modiﬁed attack game, with respect to a given partner function pf.
Deﬁnition 21.3 (HSM secure authenticated key exchange). An AKE protocol Π is HSM
secure if there exists an eﬃciently computable partner function pf such that for all eﬃcient adver-
saries A, the value hsmKEadv[A,Π,pf] is negligible.
21.9.4.1 Security of protocol AKE3
Recall protocol AKE3:
P Q
pk,CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
c:= E(pk,k), σ1 := SigQ(1,pk,c, idP), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
k
Q σ2 := SigP(2,pk,c, idQ)−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→k
P
Theorem 21.3. Protocol AKE3 is an HSM secure authenticated key exchange protocol assuming:
the underlying public-key encryption scheme is semantically secure and has strongly unpredictable
ciphertexts (see Section 21.4.1), and the underlying signature schemes used by the users and CA
are secure.
893
Proof sketch. We ﬁrst deﬁne the partner function.
When a left instance I ﬁnishes, we classify it as fresh if the ﬁrst two ﬂows it sees loosely match
the ﬁrst two ﬂows seen by some right instance (see proof of Theorem 21.1). Otherwise, we classify
I as vulnerable.
When a right instance J ﬁnishes, we see if the ﬁrst two ﬂows it sees loosely match the ﬁrst two
ﬂows seen by some left instance I. If so, we classify J as connected to I. Otherwise, we classify J
as vulnerable.
To prove that this works, one has to prove two claims.
1. When some left instance I ﬁnishes with a partner ID that belongs to an uncompromised
honest user whose HSM has not been queried during the protocol execution to sign the
relevant message, then there is a unique right instance J for which the two ﬂows seen by both
loosely match.
2. When some right instance J ﬁnishes with a partner ID that belongs to an uncompromised
honest user whose HSM has not been queried during the protocol execution to sign the
relevant message, then there is a unique left instance I for which the two ﬂows seen by both
loosely match.
Proving that both of these hold (with overwhelming probability) follows from the security of the
signature schemes. From these two claims, the rest of the proof follows fairly easily. 2
21.9.4.2 Security of protocol AKE4
Recall protocol AKE4:
P Q
pk−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
c:= E
(
pk, (k,k1,k2)
)
, c1 := Es
(
k1, (SigQ(1,pk,c),CertQ)
)
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
k
Q c2 := Es
(
k2, (SigP(2,pk,c),CertP)
)
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→k
P
Theorem 21.4. Protocol AKE4 is an HSM secure authenticated key exchange protocol assuming:
the underlying public-key encryption scheme is semantically secure and has strongly unpredictable
ciphertexts (see Section 21.4.1), the underlying symmetric encryption scheme provides one-time
ciphertext integrity, and the underlying signature schemes used by the users and CA are secure.
Proof sketch. We ﬁrst deﬁne the partner function.
When a left instance I ﬁnishes, we classify it as fresh if the ﬁrst two ﬂows it sees exactly match
the ﬁrst two ﬂows seen by some right instance. Otherwise, we classify I as vulnerable.
When a right instance J ﬁnishes, we see if the ﬁrst two ﬂows it sees exactly match the ﬁrst two
ﬂows seen by some left instance I. If so, we classify J as connected to I. Otherwise, we classify J
as vulnerable.
To prove that this works, one has to prove two claims.
1. When some left instance I ﬁnishes with a partner ID that belongs to an uncompromised
honest user whose HSM has not been queried during the protocol execution to sign the
relevant message, then there is a unique right instance J for which the two ﬂows seen by both
exactly match.
894
2. When some right instance J ﬁnishes with a partner ID that belongs to an uncompromised
honest user whose HSM has not been queried during the protocol execution to sign the
relevant message, then there is a unique left instance I for which the two ﬂows seen by both
exactly match, and moreover, the third ﬂow seen by both exactly match as well.
From these two claims, the rest of the proof follows fairly easily. 2
21.9.4.3 Security of protocol AKE5
Recall protocol AKE5:
P Q
(public key = gα) (public key = gβ)
gµ, CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
gν, k1, CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
k
Q k2−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→k
P
(k,k1,k2) := H(g(α+µ)(β+ν), gµν, gα,gµ,gβ,gν, idP,idQ)
To prove security of AKE5, we need the ICDH assumption for G (see Section 12.4). Also, if an
instance of user P can establish a session key with another instance of user P (which is something
that we do allow in general), then we need an additional assumption. We can deﬁne this assumption
using a slight modiﬁcation of Attack Game 12.3 — namely, instead of choosingα,β ∈Zq at random,
the challenger chooses α ∈Zq at random and sets β ←α. We call this the ICDH+ assumption.
Intuitively, it means that it is hard to compute gα2
, given gα and access to “DH-decision oracle”
that recognizes DH-triples of the form ( gα,·,·).
Theorem 21.5. Protocol AKE5 is an HSM secure authenticated key exchange protocol under the
ICDH and ICDH + assumptions, if we model H as a random oracle (and if the set Rin which k1
and k2 lie is super-poly-sized).
Proof sketch. Unlike all of the other AKE protocols presented so far, protocol AKE5 does not use a
signature scheme for authentication.
We sketch why this authentication mechanism works. Suppose that a left user instanceIbelong-
ing to a user P that terminates successfully with a partner ID that belongs to an uncompromised
honest user Qwhose HSM has not been queried during the execution of the protocol with the value
gµ as an input. We want to show that the ﬁrst two ﬂows seen by I loosely match the ﬁrst two ﬂows
seen by a right instance J. If there is no such instance J, then the adversary must have himself
queried the random oracle at the relevant point
(g(α+µ)(β+ν), gµν, gα,gµ,gβ,gν, idP,idQ). (21.5)
We show how this adversary can be used to solve the CDH problem for the problem instance
(gβ,gµ). To this end, we run the attack game knowing α. Dividing the ﬁrst component of (21.5)
by the second, we can compute
gαβ+αν+µβ.
895
Since αis known, we can divide out the terms involvingα, which allows us to computegµβ. However,
we have to take into account the fact that the right user’s HSM may be accessed throughout this
attack (directly by the adversary, as well as by honest user instances). We can still use the adversary
to solve the CDH problem for the problem instance ( gβ,gµ), provided we also are given access to
an oracle that recognizes DH-triples of the form ( gβ, ·, ·) — using this DH-decision oracle, we
can manage the random oracle much as in the proof of Theorem 12.4. This is why we need the
ICDH assumption.
We have to take into account that we could have Q = P, in which case the above argument
has to be modiﬁed. One can make a similar argument as above, but now we use the adversary to
compute gα2
given gα as well as access to an oracle that recognizes DH-triples of the form (gα, ·, ·).
This is why we need the ICDH+ assumption. We leave the details of this to the reader.
The above argument shows that this mechanism ensures authenticity forP. A similar argument
shows that it provides authenticity for Q.
Once we have established these authenticity properties, we can also argue that replacing a real
session key by a random session key is not detectable, unless the adversary can compute gµν given
gµ and gν. This makes use of the ordinary CDH assumption.
2
21.9.4.4 Security of protocol AKE6
Recall protocol AKE6:
P Q
(public key = gα) (public key = gβ)
gα+σ, gµ
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
gβ+τ, gν, c1 := Es
(
k1, (τ,CertQ )
)
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
k
Q c2 := Es
(
k2, (σ,CertP )
)
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→k
P
(k,k1,k2) := H(g(α+σ+µ)(β+τ+ν), gµν, gα+σ,gµ,gβ+τ,gν)
Theorem 21.6. Protocol AKE6 is an HSM secure authenticated key exchange protocol under the
I2CDH and ICDH+ assumptions, if we model H as a random oracle, and if Es provides one-time
authenticated encryption.
Proof sketch. The main ideas of the proof are the same as in the proof of Theorem 21.5. We
need the stronger I 2CDH assumption to prove that the protocol provides authenticity for Q. In
the analysis, in using the adversary to compute gαν from gα and gν, we will have to be able to
recognize DH-triples of the form ( gν, ·, ·), in addition to DH-triples of the form ( gα, ·, ·). 2
21.9.5 Modeling one-sided authentication
We brieﬂy sketch how we can modify our various security deﬁnitions to accommodate one-sided au-
thentication. Formally speaking, there is a special honest user with the special user ID anonymous.
Any unauthenticated user instance is considered to be an instance belonging to this user. In addi-
tion, for any of the models (static, PFS, or HSM), we always allow a user instance I to be classiﬁed
896
as vulnerable if I.pid = anonymous. That is, we change the rule for computing the eﬀective session
key for a vulnerable user instance I as follows:
vulnerable: If I.pid = anonymous or ..., then
I.esk ←I.sk.
Otherwise, I.esk ←error.
Here, “... ” is the corresponding condition, which depends on the model (static, PFS, or HSM).
Theorem 21.4 holds for protocol AKE4∗as well. The partner function is identical, and the proof
outline is basically the same.
21.9.6 Modeling channel bindings
We introduced the notion of channel bindings in Section 21.8. All of our security models can be
easily accommodated to model this feature.
In Experiment 1 of the attack game, when computing an eﬀective session key for a user instance,
the challenger checks if the channel binding for this user instance would violate the following global
constraint:
Two user instances are classiﬁed as connected to each other if and only if they share
the same channel binding.
This is just a restatement of the informal constraint given in Section 21.9.6 in the language of our
formal model. If this constraint is violated, the eﬀective session key is set to error. Otherwise, it is
set using the normal rules.
The security theorems for all the protocols we have studied in this chapter carry over unchanged
if we use the channel bindings deﬁned in Section 21.8. Note that for all of the schemes that use
a public-key encryption scheme, we require that the scheme has strongly unpredictable cipher-
texts (see Section 21.4.1). This property ensures that the probability that multiple invocations of
the encryption algorithm output the same ciphertext twice is negligible, even if public keys are
adversarially chosen.
For technical reasons relating to protocol composability, one other technical requirement for
channel bindings is that a user instance should compute its channel binding based only on its role,
and on the the information it sends and receives over the network during the execution of the key
exchange protocol (all of this information is present in the network communication log). All of the
channel bindings deﬁned in Section 21.8 satisfy this requirement.
21.10 Case study: TLS session setup
In Section 9.8 we saw the TLS record protocol which is used to encrypt traﬃc between two parties
after they set up a secure session by generating shared secret keys. In this section we describe the
authenticated key exchange protocol used in TLS to set up a secure session. We only look at the
key exchange protocol used in TLS 1.3 which was introduced in 2017.
For consistency with the notation in this chapter, we let P play the role of the client and Q
play the role of the server. P and Q wish to set up a secure session.
897
P Q
u:= gα, N c, oﬀer−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
v:= gβ, N s, mode,
c1 := Es(ksh, CertReqest),
c2 := Es(ksh, CertQ),
c3 := Es
(
ksh, SigQ(u,N c,oﬀer,v, N s,mode,c1,c2)
)
,
c4 := Es
(
ksh, S
(
ksm,(u,N c,oﬀer,v, N s,mode,c1,c2,c3)
))
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
kc→s
ks→c
Q
c5 := Es(kch, CertP),
c6 := Es
(
kch, SigP(u,N c,oﬀer,v, N s,mode,c1,...,c 5)
)
,
c7 := Es
(
kch, S
(
kcm,(u,N c,oﬀer,v, N s,mode,c1,...,c 6)
))
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→kc→s
ks→c
P
where:
(ksh,ksm,kch,kcm) := H1(gαβ,u, N c,oﬀer,v, N s,mode)
(kc→s, ks→c) := H2(gαβ,u, N c,oﬀer,v, N s,mode,c1,...,c 4)
Figure 21.12: The TLS 1.3 key exchange protocol
TLS 1.3. The essence of the TLS key exchange protocol is shown in Fig. 21.12. The protocol
supports both one-sided authentication and mutual authentication. The ﬁgure shows TLS mutual
authentication. In the ﬁgure, (Es,Ds) is a symmetric encryption scheme that provides authenticated
encryption, such as AES-128 in GCM mode. Algorithm S refers to a MAC signing algorithm, such
as HMAC-SHA256. Algorithms SigP(·) and SigQ(·) sign the provided data using P’s or Q’s signing
keys, respectively. Finally, the hash functions H1,H2 are used to derive symmetric keys. They are
built from HKDF (Section 8.10.5) with a hash function such as SHA256.
The symmetric encryption scheme ( Es,Ds) and the hash function in HMAC and HKDF to use
is determined by a negotiation in the ﬁrst two messages of the protocol. TLS negotiates these
algorithms, rather than hard code a speciﬁc choice, because some countries and organizations
may prefer to use diﬀerent algorithms. Some may not trust the algorithms standardized by the
US National Institute of Standards (NIST). Nevertheless, all implementations are required, at a
minimum, to support AES-128 in GCM mode and SHA256, as well as a few other common ciphers.
The protocol. The TLS key exchange in Fig. 21.12 works as follows. In the ﬁrst ﬂow, the client
P sends to the server Qa group element u:= gα, a nonce N c, and an “oﬀer”. The oﬀer is a message
that speciﬁes the group and the symmetric encryption and hash schemes that the client is willing
and able to use. In fact, the client can provide several groups in his oﬀer, providing corresponding
group elements for each group. In TLS 1.3, the groups that may be used are constrained to be one
of several pre-deﬁned groups, which include both elliptic curves and subgroups of ﬁnite ﬁelds. The
symmetric encryption and hash scheme are also constrained to be one of a small number of pre-
898
deﬁned schemes. The oﬀer includes a list of encryption/hash scheme pairs, in order of preference.
After receiving the ﬁrst ﬂow, the server Q examines the “oﬀer” sent by the client. It veriﬁes
that the group (or groups) preferred by the client coincides with the group (or groups) that the
server is able and willing to use. It also selects an encryption scheme ( Es,Ds) and a hash function
from the oﬀer that it is willing and able to use, if any. If the server is unable to ﬁnd a compatible
group and encryption/hash schemes, the server may send a special “retry” request to the client,
but we will not discuss this aspect of the protocol here. Otherwise, the server responds to the client
with a ﬂow that consists of a group element v := gβ, a nonce N s, and a “mode” message which
indicates the parameter choices (group, encryption/hash scheme) made by the server. This ﬂow
also contains several encrypted messages:
• A special “certiﬁcate request” message, which is only sent if the server wishes to authenticate
the client. If present, this message speciﬁes the type of certiﬁcates the server will accept.
• The server’s certiﬁcate (which includes the server’s signature veriﬁcation key).
• A signature (under server’s signing key) on the conversation so far.
• A tag computed using HMAC (see Section 8.7) on the conversation so far.
The key ksh used to encrypt these messages and the key ksm used in applying HMAC are derived
from the data
gαβ, u= gα, N c, oﬀer, v= gβ, N s, mode (21.6)
using HKDF.
After receiving the second ﬂow, the client responds with a ﬂow that consists of several encrypted
messages:
• The client’s certiﬁcate (which includes the client’s signature veriﬁcation key). This message
is only sent if the server requested client authentication.
• A signature (under client’s signing key) on the conversation so far. This message is only sent
if the server requested client authentication.
• A tag computed using HMAC (see Section 8.7) on the conversation so far.
The key kch used to encrypt these messages and the key kcm used in applying HMAC are derived
from (21.6) using HKDF.
The session computed by both client and server consists of two keys, kc→s and ks→c, which are
derived from
gαβ, u, N c, oﬀer, v, N s, mode, c1,...,c 4
using HKDF. In the record protocol, kc→s is used to encrypt messages sent from the client to the
server, and ks→c is used to encrypt messages sent from the server to the client, as discussed in
Section 9.8. The client may “piggyback” record protocol messages along with the third ﬂow of the
key exchange protocol.
The TLS 1.3 protocol also allows the server to “piggyback” record protocol messages along
with the second ﬂow of the key exchange protocol. This is why c5,c6,c7 are not included in
the computation of the session keys kc→s and ks→c. Of course, any record protocol messages
piggybacked on the second ﬂow are being sent to an unauthenticated client. This mode of operation
falls out of the scope of our formal models of secure key exchange.
899
Identity protection. Notice that P’s and Q’s identities (contained in CertP and CertQ) are
encrypted and not visible to an eavesdropper. Moreover, P does not transmit its identity until
it veriﬁes Q’s identity. This is done to ensure eavesdropping identity protection for both parties
and full identity protection for P, as discussed in Section 21.5. Identity protection is a feature of
TLS 1.3 that was not present in earlier versions of the protocol.
We note, however, that identity protection is only of limited value in a Web environment. The
reason is that modern web browsers include Q’s identity (the DNS domain name of the web server)
in the clear in the ﬁrst message to the server. This data is included in a ﬁeld called server name
indication or SNI. It is needed when multiple domains are hosted on a single server. The SNI
ﬁeld tells the server what certiﬁcate CertQ to use in its response. Without the SNI ﬁeld, the server
would not know what domain the client is requesting and the connection will fail.
Security. TLS 1.3, as described in Fig. 21.12, is very similar to our protocol AKE4 (speciﬁcally,
its instantiation using ElGamal encryption, protocol AKE4eg) and its one-sided variant, protocol
AKE4∗(see Figures 21.7, 21.8, and 21.9). It can be proved to be HSM secure in the random oracle
model under roughly the same assumptions as we make for protocol AKE4 (in particular, the CDH
assumption), provided we restrict the protocol so that Q does not employ the special mode of
operation in which it starts using the session key before it receives the third ﬂow. To securely
support that mode of operation, instead of the CDH assumption, we need to invoke the stronger
ICDH assumption (for much the same reason as we need CCA security and the ICDH assumption
in the analysis of protocol AKE2eg).
Additional features. TLS 1.3 provides several features beyond basic key exchange. One feature,
called exported keys, is used by applications that need additional key material for encrypting
application data outside of the secure session. The TLS 1.3 key exchange provides an exporter
master secret that is known to both P and Q. It can be given to a higher-level application for its
own use. This key is eﬀectively independent of all the keys used to secure the TLS session, but
provides the same security guarantees.
Another feature is the ability to update traﬃc keys . In a long lived TLS session it may be
desirable to “roll” the session keys ks→c and kc→s forward, so that a compromise of these keys does
not expose earlier session data. At any time after the key exchange completes, either P or Q can
request a key update by sending a KeyUpdate message. Roughly speaking, this causes the following
update to take place
k′
s→c := H(ks→c) and k′
c→s := H(kc→s)
where H is implemented using HKDF. Both sides then delete the earlier keys ks→c and kc→s and
encrypt all subsequent traﬃc usingk′
s→c and k′
c→s. This mechanism provides some degree of forward
secrecy within the session.
21.10.1 Authenticated key exchange with preshared keys
TLS provides support for an abbreviated session setup protocol if P and Q already share a secret
key. This abbreviated key exchange, called a pre-shared key handshake, is more eﬃcient than a
full key-exchange. It skips some of the computationally expensive key exchange steps in Fig. 21.12.
Usually, a pre-shared key is the result of a previous TLS key exchange between P and Q. A
pre-shared key can be generated at anytime after a TLS key exchange completes. Server Qsends a
900
new-session-ticket message to client P, over the secure channel, and this message tellsP to compute
and store a pre-shared key psk. Using the notation in Fig. 21.12, this pre-shared key is computed
as:
psk := H3(gαβ,u, N c,oﬀer,v, N s,mode,c1,...,c 7,N t),
where H3 is a key derivation function based on HKDF, and N t is a random nonce from Qprovided
in the new-session-ticket message. The server can send multiple new-session-ticket messages, each
with a fresh nonce N t, causing the client to calculate and store multiple pre-shared keys.
A pre-shared key can also be generated by an out-of-band mechanism, such as manually loading
a random key into the client and server. For example, if the client is a mobile device, say a car,
this key can be loaded into the car at the time of sale, and also loaded into the cloud service that
the car connects to.
Every pre-shared key has an identity, which is a bit string that identiﬁes the key. This identity
is either provided manually, in case the pre-shared key is loaded manually, or it is provided as part
of the new-session-ticket message from Q.
When a client P wants to establish a TLS session with Q using a pre-shared key, it includes
the key’s identity in the oﬀer that it sends (in the clear) in the ﬁrst ﬂow in Fig. 21.12. In fact,
the client can include several identities corresponding to multiple pre-shared keys that it is willing
to use. Server Q can choose to reject all them and do a full handshake as in Fig. 21.12, or it can
choose one of the provided identities and include it in the mode that it sends in its response to the
client.
If the server selects one of the provided identities, thenP and Qdo an abbreviated key exchange.
The key exchange proceeds as in Fig. 21.12, except that the ciphertexts c1,c2,c3,c5,c6 are not
computed, not sent, and not included in the HMAC and HKDF computations. Instead, the derived
keys are computed as
(ksh,ksm,kch,kcm) := H4(psk, gαβ,u, N c,oﬀer,v, N s,mode)
(kc→s, ks→c) := H5(psk, gαβ,u, N c,oﬀer,v, N s,mode,c4)
(21.7)
where H4 and H5 are key derivation functions based on HKDF. Notice that this abbreviated key
exchange is much faster than a full key exchange because no signatures are computed by either
party. This is a signiﬁcant performance savings for a busy server.
Forward secrecy. A key exchange based on a pre-shared key (optionally) includes the quantities
u:= gα and v:= gβ, as in Fig. 21.12. They are used along with gαβ in the symmetric key derivation
steps in (21.7). This ensures forward secrecy in case the pre-shared key is leaked to an adversary
at some future time. This optional forward secrecy for session resumption is a feature of TLS 1.3
that was not present in earlier versions of TLS.
Tickets. To establish a secure session using a pre-shared key, both P and Q need to remember
the key and its identity. This can be diﬃcult on a busy server that interacts with many clients.
Instead, the server can oﬄoad storage of the pre-shared key to the client. To do so, the server
computes an authenticated encryption c of the server’s TLS state with the client, which includes
all the computed pre-shared keys. This c is called a ticket and the encryption is done using a
secret key known only to the server. Then, in the new-session-ticket message sent to the client,
the server sets the key’s identity to c. When the client later suggests to use this pre-shared key
in a TLS key exchange, it sends the key’s identity c to the server as part of the oﬀer. The server
901
decrypts c and obtains the pre-shared key. This way the server does not store any long-term state
per client. It greatly simpliﬁes the use of pre-shared keys in a large system.
Zero round trip data. TLS 1.3 introduced a dangerous mechanism called 0-RTT that is de-
signed to improve performance. 0-RTT allows the client to send encrypted application data in the
very ﬁrst ﬂow of Fig. 21.12. In a Web context, 0-RTT lets the client send an HTTP request in the
ﬁrst ﬂow. This can greatly improve page load time, because the server can send the response in
the second ﬂow of Fig. 21.12. Hence, 0-RTT saves a full round trip when loading a web page.
This mechanism is only allowed when the client requests a key exchange using a pre-shared key.
The client appends the encrypted application data to the ﬁrst ﬂow, encrypted with a key kce that
is derived from
psk, u, N c, oﬀer.
The server derives the same key kce from the received data and uses it to decrypt the ciphertext
appended to the ﬁrst ﬂow.
The trouble with 0-RTT is that the encrypted application data is vulnerable to replay. An
adversary can record the ﬁrst ﬂow from the client to the server and replay it at a later time.
Unless the server takes extra steps to prevent a replay attack, the result can be very harmful to
the client P. For example, suppose the ﬁrst ﬂow from P contains a query for P’s bank account
balance. The adversary can replay this ﬁrst ﬂow at any time and count the number of bytes in
the server’s response. Because the response length is correlated with the number of digits in P’s
account balance, the adversary can monitorP’s account balance by repeatedly replaying the request
from P. Even worse, if the request from P is a bill payment, the adversary can replay the request
and cause the same bill to be paid multiple times.
The reason that 0-RTT data is not protected from a replay attack is that the encryption keykce
cannot depend on the server nonce N s. This key must be derived before the client sees N s. All
other keys in TLS depend on N s and this prevents replay attacks on data encrypted with those
keys. Data encrypted with kce is not protected this way.
With some eﬀort, server Q can defend against replay attacks on 0-RTT data. As a ﬁrst line
of defense, every pre-shared key has a limited lifetime, speciﬁed in the new-session-ticket message
when the pre-shared key is ﬁrst created. The maximum lifetime is seven days. The server will
reject any connection attempt using an expired pre-shared key. This limits the replay window, but
does not fully prevent replays.
To prevent replays the server can store the client nonce from every 0-RTT request it receives. If
the server ever sees a request with a duplicate client nonce, it can safely reject that request. Note
that the client nonce only needs to be stored for a limited amount of time; it can be deleted once
the corresponding pre-shared key expires. In practice, this defense is not easy to implement in a
large distributed web application. The adversary can record a client request in North America and
replay it in Asia. Since large systems do not typically synchronize state across geographic regions
in real time, the repeated client nonce will not be detected in Asia and the replay request will be
accepted.
The end result is that if a server chooses to support 0-RTT, clients can beneﬁt from faster page
load times, but they are also at risk due to replay attacks. If the beneﬁt is not worth the risk, the
server can signal to the client that it is choosing to ignore the 0-RTT data, in which case the client
will retransmit the data in the third ﬂow, after the secure session is properly established.
902
21.11 Password authenticated key exchange
In Section 21.6 we discussed one-sided authenticated key exchange protocols, where the server has
a certiﬁcate and the client does not.
As we discussed there, a client can establish a one-sided authenticated secure channel with
a server, and then identify himself to the server within the channel, using, perhaps, some simple,
password-based identiﬁcation protocol. This approach is widely used today. However, this approach
has some serious security problems. In this section, we explore these problems in some detail, and
then examine a new type of key exchange protocol, called password based key exchange, which
mitigates these problems to some degree.
21.11.1 Phishing attacks
Suppose a client has an account with a server, and that an adversary wants to discover the client’s
password. Typically, the client logs into his account using a web browser, entering his user ID
and password into some ﬁelds on a special “secure login page” belonging to the server. Normally,
this is all done using a one-sided authenticated secure channel, as discussed above. However, in
a phishing attack, an adversary bypasses the secure channel by simply tricking the client into
entering his user ID and password on a fake login page that belongs to the adversary, rather than
the secure login page.
In practice, phishing attacks are not so hard to mount. There are two phases to a phishing
attack: ﬁrst, to trick the client into visiting his fake login page, rather than the secure login page,
and second, to make the fake login page look and feel like the secure login page, so that the client
enters his user ID and password.
• One common approach used to trick a client into visiting the fake login page is to send the
client an email, telling the client that there is some compelling reason that he should log in to
his account at the server (“verify your account” or “conﬁrm billing information”). The email is
designed very nicely, perhaps with an oﬃcial-looking logo, and for the client’s “convenience,”
contains an embedded link that will take the client’s web browser to the secure login page.
However, the embedded link is really a link to the fake login page. This approach, though
fairly crude, actually works with a good number of unsuspecting clients.
Because of such attacks, careful clients know better than to follow any links in such email
messages. However, there are more sophisticated strategies that trick even the most careful
clients: using attacks that exploit security weaknesses in the Internet routing mechanism, it
is possible for a client to directly enter one web address in the address bar of their browser,
but end up at a web site controlled by the adversary.
• Once the phisher has brought the client to his fake login page, he has to make sure that his
fake login page is a convincing replica of the secure login page. This is typically not too hard
to do. Of course, the adversary has to design the page so that the content displayed is very
similar to the content displayed on the secure login page. This is usually trivial to do. There
might be other clues that indicate the client is at the wrong web page, but many clients may
not notice these clues:
– the address of the web page may not be that of the server; however many clients may
not even look carefully at this address, or even know what it really means; moreover,
903
even a more discerning client may be easily fooled if, for example, the adversary controls
a domain called somesite.com, and then directs the client to http://www.yourbank.
com.somesite.com, instead of http://www.yourbank.com;
– the web browser may not display the usual signal (e.g., a little padlock) that is used to
indicate a “secure web page,” but again, a casual client may not notice;
– the web browser may indeed display a “secure web page” signal, but almost no client
will bother checking the details of the certiﬁcate, which in this case, may be a perfectly
valid certiﬁcate that was issued by the CA to the adversary, rather than to the server;
in fact, unless the client has taken a course in security or cryptography, he probably has
no idea what a certiﬁcate even is.
To attempt to foil a phishing attack, instead of using a simple password identiﬁcation protocol,
one might use a challenge-response identiﬁcation protocol, such as the following:
P Q
r←R R
r←−−−−−−−−−−−−−−−−
v←H(pw,r)
v−−−−−−−−−−−−−−−−→v ?= H(pw,r)
Here, P is the client, Q is the server, pw is the password. Also, r is a random nonce, and H is a
hash function (which we model as a random oracle). The server Q sends P the random nonce r,
P computes v as H(pw,r), and sends v to Q, and Q veriﬁes that v = H(pw,r). (Note that this
protocol is a variant of the password-based challenge-response protocol discussed in Section 18.6.1,
with the MAC and key derivation all rolled in to the hash function.)
If the client uses this protocol, then at least a phishing attack will not lead directly to the
complete exposure of the client’s password. However, there are other ways a phishing attack can
be exploited.
First of all, the client identiﬁes himself to the server, but not vice versa, so if the client is tricked
into visiting the fake login page, the adversary may not get his password, but may be able to cause
other trouble, since the client thinks he is securely logged in to the server, and so may be tricked
into revealing other sensitive information.
Worse, the adversary could mount a man-in-the-middle attack. Again, the adversary sets up
a channel with the client, via phishing, and simultaneously sets up a perfectly normal one-sided
authenticated secure channel with the server. Now the adversary simply plays “man in the middle,”
forwarding the messages in the identiﬁcation protocol from the server to client, and vice versa. Now,
not only can the adversary try to obtain sensitive information from the client, as above, but since
the adversary is now logged into the server under the client’s user ID, he can also cause damage
on the server side. For example, if the server is a bank, and the client is a customer, the adversary
can transfer money from the customer’s account to a bank account controlled by the adversary.
904
One might also consider using the following mutual challenge-response identiﬁcation protocol:
P Q
r←R R
r←−−−−−−−−−−−−−−−−
s←R R,v ←H(pw,0,r,s)
s,v−−−−−−−−−−−−−−−−→
v ?= H(pw,0,r,s),w ←H(pw,1,r,s)
w←−−−−−−−−−−−−−−−−
w ?= H(pw,1,r,s)
Unfortunately, this protocol is subject to the same man-in-the-middle phishing attack as above.
This type of man-in-the-middle attack can be avoided, however, if we use the channel binding
feature that can be provided by key exchange protocols. We brieﬂy introduced this notion in
Section 21.8. The security property for channel bindings guarantees that the client and server have
the same channel bindings if and only if they share a key. So to protect against a man-in-the-middle
attack, we can modify the above mutual authentication protocol so that the channel binding is
included in the hash. That is, the hashes are computed as H(pw,chb,0,r,s) and H(pw,chb,1,r,s),
where chb is the channel binding. Now, the man-in-the-middle attacks fails, because the two
participants will be computing the hashes with diﬀerent channel binding inputs, so it does no good
to forward a hash from one participant to the other. This even protects against phishing attacks,
but also provides some security even when the server’s long-term key secret used in the key exchange
has been compromised (as in the PFS or HSM attack models).
Unfortunately, even with this patch, this challenge-response protocol is subject to an oﬄine
dictionary attack (see Section 18.3.1). Indeed, suppose that the client’s password is weak, and
belongs to a relatively small dictionary Dof common passwords. Also suppose that the adversary
establishes a channel with the client, via phishing. Playing the role of server, the adversary sends
a nonce r to the client, who responds with s,v := H(pw,chb,0,r,s). At this point, the adversary
quits the protocol. Having obtained v, the adversary now performs a brute-force search for the
client’s password, as follows: for each pw′∈D, the adversary computes H(pw′,r), and tests if this
is equal to v. If he ﬁnds such a pw′, it is very likely that pw′ = pw, and so the adversary has
obtained the client’s password.
By the way, reversing the roles of client and server in the above mutual identiﬁcation protocol
makes matters even worse: now the adversary can simply set up a normal one-sided authenti-
cated secure channel with the server, and the ﬁrst thing the server does is to send the value
H(pw,chb,0,r,s) to the adversary. Now the adversary can carry out an oﬄine dictionary attack
without even having to ﬁrst do any phishing.
Finally, we remark that even without phishing, the adversary can always perform an online
dictionary attack, by simply attempting to log in to the server many times, using passwords chosen
from some dictionary of common passwords. As discussed in Section 18.3.1, a server can usually
take simple countermeasures that limit the number of failed login attempts, so that such online
dictionary attacks are not nearly as dangerous as an oﬄine attack.
905
21.11.2 PAKE: an introduction
We have discussed one-sided authenticated key exchange protocols, and how these can be combined
with simple password-based identiﬁcation protocols to establish a secure channel between a client
and a server, where the server has a certiﬁcate, and the client has no certiﬁcate but shares a
password with the server. We also discussed how this approach to establishing a secure channel is
not very secure in practice: via a phishing attack, an adversary can trick a client into divulging
his password to the adversary; moreover, we saw that even if a challenge-response identiﬁcation
protocol is used, a phisher can still obtain enough information from the client so that the adversary
can still obtain the client’s password using an oﬄine dictionary attack.
These security problems are the motivation for password authenticated key exchange
(PAKE) protocols. Here is the basic idea of a PAKE protocol. We assume that every pair of users
that wish to establish a shared session key have a shared password. We make no other assumptions:
there are no certiﬁcates, and there is no CA or any other type of TTP.
Ideally, passwords are strong, that is, chosen at random from a large set. In this case, the security
goals for a PAKE protocol are essentially the same as for an AKE protocol; indeed, although we
do not spell out a formal security model, it is essentially the same as in Section 21.9, except that
now, shared, strong passwords are used for authentication purposes, in place of a TTP.
Unfortunately, in practice, a PAKE protocol may very well be used with weak passwords, and
we have to relax our security expectations accordingly. Indeed, with weak passwords, any PAKE
protocol is inherently vulnerable to an online dictionary attack : an adversary can always guess a
password, and engage a user in the protocol and see if its guess is correct. Typically, the guess is
incorrect if and only if either the key exchange protocol itself fails, or some higher level protocol
that uses the session key fails. However, we might at least hope that this is the worst type of attack
possible; in particular, we might hope that an adversary cannot mount an oﬄine dictionary attack.
21.11.3 Protocol PAKE0
Consider the following protocol PAKE0, which is described in Fig. 21.13. Here, P and Q are users
with a shared password pw, and H is a hash function, which we model as a random oracle. In
this protocol, P and Q exchange random nonces r and s, and then compute the session key as
k = H(pw,idP,idQ,r,s). In describing this, and other, PAKE protocols, we leave out the details
of how P and Q communicate their identities idP and idQ to one another, and how they retrieve
the corresponding password.
Suppose that pw is a strong password. Then in this case, protocol PAKE0 is quite secure (in
particular, it would satisfy the deﬁnition of security in Section 21.9, appropriately modiﬁed, mod-
eling H as a random oracle). Notice that this protocol does not provide mutual, or even one-sided,
identiﬁcation: an instance of P may run the protocol, and not share a session key with anyone;
however, if he shares a session key with someone, he shares it with an instance of Q.
Unfortunately, if pw is a weak password, then an eavesdropping adversary can easily carry out
an oﬄine dictionary attack , as follows.
Assume that pw belongs to some relatively small dictionary Dof common passwords. Also
assume that after P runs the protocol, it encrypts a publicly known plaintext munder the session
key, using a symmetric cipher E= (E,D), and sends the resulting ciphertext out on the network.
Our adversary eavesdrops on a run of the protocol between P and Q, obtaining the values r
and s. At this point, P computes the session key as k = H(pw,idP,idQ,r,s), and sends out an
906
shared secret password: pw
P Q
r←R R r−−−−−−−−−−−−−−−−→
s←R R
k←H(pw,idP,idQ,r,s)
k←H(pw,idP,idQ,r,s) s←−−−−−−−−−−−−−−−−
session key: k
Figure 21.13: Protocol PAKE0
encryption c of m under the key k. The adversary intercepts c, and then does the following:
for all pw′∈D do
k′←H(pw′,idP,idQ,r,s)
m′←D(k′,c)
if m′= m then
output pw′and halt
In all likelihood, the output pw′is equal to the password pw.
Of course, the above attack will work with many other types of partial information about the
session key that may be leaked to the adversary, besides a plaintext/ciphertext pair. For example,
the key may be used as a MAC key, and used to authenticate publicly known messages.
21.11.4 Protocol PAKE1
As we saw, if weak passwords are used, then protocol PAKE0 is vulnerable to an oﬄine dictionary
attack by an eavesdropping adversary. We next present a PAKE protocol that does not suﬀer from
this vulnerability.
This protocol, which we call PAKE1, makes use of a cyclic group G of prime order q generated
by g ∈G, and a hash function H, which we model as a random oracle. The protocol is described
in Fig. 21.14. Here, both users compute the value w = gαβ, and then compute the session key as
k= H(pw,idP,idQ,u,v,w ).
If the password pw is strong, then this protocol is quite secure. The interesting case is what
happens when the password pw is weak. First, we claim that under the CDH assumption for G,
and modeling H as a random oracle, then protocol PAKE1 is not vulnerable to a dictionary attack
by an eavesdropping adversary.
We shall give an intuitive argument for this. But ﬁrst, we introduce some notation, and we
recall the CDH assumption. For s,t ∈G, if s= gµ and t= gν, then we deﬁne
[s,t] := gµν.
The CDH problem is this: given random s,t ∈G, compute [s,t]. The CDH assumption asserts that
there is no eﬃcient algorithm that can solve the CDH problem with non-negligible probability.
907
shared secret password: pw
P Q
α←R Zq,u ←gα u−−−−−−−−−−→
β ←R Zq, v←gβ, w←uβ
k←H(pw,idP,idQ,u,v,w )
w←vα
k←H(pw,idP,idQ,u,v,w )
v←−−−−−−−−−−
session key: k
Figure 21.14: Protocol PAKE1
Suppose an adversary eavesdrops on a conversation between P and Q. He obtains random
group elements uand v, while P and Qcompute the session key as k= H(pw,idP,idQ,u,v, [u,v]).
Intuitively, for a dictionary attack to succeed, the adversary will have to query the random oracle
H at points of the form (pw′,idP,idQ,u,v, [u,v]) for various values of pw′. Let us call such a point
relevant. Indeed, it is only by querying the random oracle at a relevant point for some pw′ that
the adversary can tell if pw′= pw: for example, by using the value k′ of the oracle at that point
to decrypt a given encryption of a known plaintext under k.
We claim that under the CDH assumption, the probability that he queries the random ora-
cle at any relevant point is negligible. Indeed, if an adversary can make a relevant query with
non-negligible probability, then we could use this adversary to solve the CDH problem with non-
negligible probability, as follows. Given a challenge instance ( s,t) of the CDH problem, set u:= s
and v := t, and give u and v to our eavesdropping adversary. Now, the adversary will make a
number of random oracle queries. As usual, we process random oracle queries using a lookup ta-
ble, and collect a list of all queries of the form ( pw′,idP,idQ,u,v,w ′), where pw′ is an arbitrary
password, and w′is an arbitrary group element. Finally, we select one of the queries in this list at
random, and output the corresponding w′. If our selected query is relevant, then w′is a solution to
the CDH problem. Note that because recognizing solutions to the CDH problem is in general hard
(this is the DDH assumption), we cannot easily recognize relevant queries, and so we are forced to
employ this guessing strategy; nevertheless, if the adversary has a non-negligible chance of making
a relevant query, we have non-negligible (though smaller) chance of solving the CDH problem.
Thus, we have shown that protocol PAKE1 provides security against an oﬄine dictionary attack
by an eavesdropping adversary. However, as we now illustrate, protocol PAKE1 does not provide
security against a dictionary attack by a active adversary, that is, an adversary that participates
directly in the protocol.
Assume that pw belongs to some relatively small dictionary Dof common passwords. Also
assume that after P runs the protocol, it encrypts a publicly known plaintext munder the session
key, using a symmetric cipher E= (E,D), and sends the resulting ciphertext out on the network.
Our adversary works as follows. First, he plays the role of Qin PAKE1. The honest user P sends
u to the adversary, who simply follows the protocol, computing
β ←R Zq, v ←gβ, w ←uβ,
908
public system parameters: a,b ∈G
shared secret password: pw
P Q
α←R Zq,u ←gαapw u−−−−−−−−−−→
β ←R Zq, v←gβbpw
w←(u/apw )β
k←H(pw,idP,idQ,u,v,w )
w←(v/bpw )α
k←H(pw,idP,idQ,u,v,w )
v←−−−−−−−−−−
session key: k
Figure 21.15: Protocol PAKE2
and sending v to P. At this point, P computes the session key as k= H(pw,idP,idQ,u,v,w ), and
sends out an encryption c of m under the key k. The adversary intercepts c, and then does the
following:
for all pw′∈D do
k′←H(pw′,idP,idQ,u,v,w )
m′←D(k′,c)
if m′= m then
output pw′and halt
In all likelihood, the output pw′is equal to the password pw.
21.11.5 Protocol PAKE2
As we saw, if weak passwords are used, then while protocol PAKE1 provides security against an
oﬄine dictionary attack by an eavesdropping adversary, it is vulnerable to an oﬄine dictionary
attack by an active adversary.
We now present a protocol, PAKE2, which does provide security against an oﬄine dictionary
attack, by both passive and active adversaries. Like PAKE1, protocol PAKE2 makes use of a cyclic
group G of prime order q generated by g∈G, and a hash function H, which we model as a random
oracle. The protocol has additional system parameters aand b, which are randomly chosen elements
of G. Furthermore, passwords are viewed as elements ofZq. This is not a limitation since, if needed,
passwords can be hashed into Zq using an appropriate hash function.
Protocol PAKE2 is described in Fig. 21.15. Just as in protocol PAKE1, both users compute the
value w = gαβ, and then compute the session key as k = H(pw,idP,idQ,u,v,w ). The only
diﬀerence is that now P “blinds” the value gα by multiplying it by apw , and Qblinds the value gβ
by multiplying it by bpw .
We now give an informal argument that protocol PAKE2 provides security against dictionary
attacks by either an eavesdropping or active adversary, under the CDH assumption, and modeling
H as a random oracle.
909
First, consider an adversary that eavesdrops on a run of the protocol between honest user P
and honest user Q. He obtains a conversation ( u,v). The session key computed by P and Q is
k= H(pw,idP,idQ,u,v, [u/apw ,v/bpw ]). (21.8)
Intuitively, the adversary’s goal is to query the random oracle at as manyrelevant points as possible,
where here, a relevant point is one of the form
(pw′,idP,idQ,u,v, [u/apw′
,v/bpw′
]), (21.9)
where pw′ ∈Zq. The following lemma shows that under the CDH assumption, he is unable to
make even a single relevant query:
Lemma 21.7. Under the CDH assumption, the following problem is hard: given random a,b,u,v ∈
G, compute γ ∈Zq and w∈G such that w= [u/aγ,v/bγ].
Proof. We ﬁrst make some simple observations about the “Diﬃe-Hellman operator” [ ·,·]. Namely,
for all x,y,z ∈G and all µ,ν ∈Zq, we have
[x,y] = [y,x], [xy,z] = [x,z][y,z], and [ xµ,yν] = [x,y]µν.
Also, note that [ x,gµ] = xµ, so given any two group elements x and y, if we know the discrete
logarithm of either one, we can eﬃciently compute [ x,y].
Now suppose we have an adversary that can eﬃciently solve the problem in the statement of
the lemma with non-negligible probability. We show how to use this adversary to solve the CDH
problem with non-negligible probability. Given a challenge instance (s,t) for the CDH problem, we
compute
µ←R Zq, a ←gµ, ν ←R Zq, b ←gν,
and then we give the adversary
a, b, u := s, v := t.
Suppose now that the adversary computes for us γ ∈Zq and w ∈G such that w = [u/aγ,v/bγ].
Then we have
w= [u,v][u,b]−γ[a,v]−γ[a,b]γ2
. (21.10)
Since we know the discrete logarithms of a and b, each of the quantities
w, [u,b], [a,v], [a,b], γ
appearing in (21.10) are either known or easily computed from known values, and so we can easily
solve (21.10) for [u,v], which is the same as [ s,t]. 2
Next, consider an active adversary that engages in the protocol with an honest user. We consider
the case where the adversary plays the role of Q, and the honest user is P — the argument in the
other case is similar.
Now, in the adversary’s attack, he obtains the ﬁrst message u from P, which is just a random
group element. Next, the adversary computes a group element v in some way, and sends this to P
— the adversary may compute vin any way he likes, possibly in some devious way that depends on
u. As usual, P now computes the session key as in (21.8), and the adversary’s goal is to evaluate
the random oracle H at as many relevant points, as in (21.9), as possible. Of course, an adversary
that simply follows the protocol using some guess pw′ for password can always make one relevant
query. What we want to show is that it is infeasible to make more than one relevant query. This
is implied by the following lemma:
910
Lemma 21.8. Under the CDH assumption, the following problem is hard: given random a,b,u ∈G,
compute γ1,γ2 ∈Zq and v,w1,w2 ∈G such that γ1 ̸= γ2 and wi = [u/aγi,v/bγi] for i= 1,2.
Proof. Suppose that we are given an instance ( s,t) of the CDH problem. Then we compute
µ←R Zq, a ←gµ,
and give the adversary
a, b := s, u := t.
The adversary computes for us γ1,γ2 and w1,w2 such that γ1 ̸= γ2, and
wi = [u/aγi,v/bγi] = [u,v][u,b]−γi[a,v]−γi[a,b]γ2
i (i= 1,2).
Then we have
w2/w1 = [u,b]γ1−γ2[a,v]γ1−γ2[a,b]γ2
2−γ2
1 . (21.11)
Since we know the discrete logarithm of a, each of the quantities
w1, w2, [a,v], [a,b], γ1, γ2
appearing in (21.11) is either known or easily computed from known values; moreover, sinceγ1−γ2 ̸=
0, we can eﬃciently solve (21.11) for [ u,b], which is the same as [ s,t]. 2
21.11.6 Protocol PAKE+
2
Often, users play very distinct roles. One user may be a client, which obtains the password by
keyboard entry, while the other is aserver, which is a machine that keeps apassword ﬁle, containing
information for each client who is authorized to access the server. A type of attack that we would
like to provide some defense against is a server compromise, in which an adversary obtains the
server’s password ﬁle. Given the password ﬁle, the adversary can certainly impersonate the server;
however, we would like to make it as hard as possible for the adversary to impersonate a client,
and gain unauthorized access to the server.
Given the password ﬁle, an adversary can always mount an oﬄine dictionary attack to discover
a given client’s password: the adversary can just run both the client and server side of the protocol,
using a guess for the password on the client’s side, and using the data stored in the password ﬁle
on the server’s side, and see if the protocol completes successfully. Ideally, this would be all the
adversary could do. Recall that such an oﬄine dictionary attack can made more diﬃcult for the
adversary by using a slow hash function (Section 18.4.3).
Consider protocol PAKE2, which as we argued, provides security against oﬄine dictionary attacks
by both eavesdropping and active adversaries. The roles of the two users in that protocol are quite
symmetric, but for concreteness, let us say that P is the client, and Q is the server. In the most
obvious implementation, Qwould explicitly store the passwordpw in the password ﬁle. Clearly, this
implementation is undesirable, as an adversary that compromises the server immediately obtains
the password.
We now present protocol PAKE+
2 , which has the property that if the server is compromised,
the best an adversary can do to impersonate a client is an oﬄine dictionary attack. Like PAKE2,
protocol PAKE+
2 makes use of a cyclic group G of prime order q generated by g∈G, group elements
a,b ∈G, and a hash function H, which we model as a random oracle. In addition, the protocol
911
public system parameters: a,b ∈G
password: pw, ( π0,π1) := H′(pw,idP,idQ)
P Q
secret: π0,π1 secret: π0,c := gπ1
α←R Zq,u ←gαaπ0 u−−−−−−−−−−→
β ←R Zq, v←gβbπ0
w←(u/aπ0)β, d←cβ
k←H(π0,u,v,w,d )
w←(v/bπ0)α, d←(v/bπ0)π1
k←H(π0,u,v,w,d )
v←−−−−−−−−−−
session key: k
Figure 21.16: Protocol PAKE+
2
employs another hash function H′, which has range Zq×Zq, and which we also model as a random
oracle.
Let pw be the password shared between client P and server Q, which is an arbitrary bit string.
The protocol is described in Fig. 21.16. Here, the client stores ( π0,π1), while the server stores
(π0,c), where c:= gπ1, where
(π0,π1) := H′(pw,idP,idQ) ∈Zq ×Zq.
Of course, the client can derive ( π0,π1) from pw. Both users compute the values w = gαβ and
d= gπ1β, and then compute the session key as k= H(π0,u,v,w,d ).
It is not hard to argue that protocol PAKE+
2 oﬀers the same level of security as protocol PAKE2
under normal conditions, when the server is not compromised. However, consider what happens
if the server Q is compromised in protocol PAKE+
2 , and the adversary obtains π0 and c. At this
point, the adversary could attempt an oﬄine dictionary attack, as follows: evaluate H′ at points
(pw′,idP,idQ) for various passwords pw′, trying to ﬁnd pw′ such that H′(pw′,idP,idQ) = (π0,·).
If this succeeds, then with high probability, pw′ = pw, and the adversary can easily impersonate
the client.
The key property we want to prove is the following: if the above dictionary attack fails, then
under the CDH assumption, the adversary cannot impersonate the client.
To prove this property, ﬁrst suppose that an adversary compromises the server, then attempts
a dictionary attack, and ﬁnally, attempts to log in to the server. Compromising the server means
that the adversary obtains π0 and c= gπ1. Now suppose the dictionary attack fails, which means
that the adversary has not evaluated H′ at the point ( pw,idP,idQ). The value π1 is completely
random, and the adversary has no other information about π1, other than the fact that c = gπ1.
When he attempts to log in, he sends the server Qsome group element u, and the server responds
with v:= gβbπ0 for random β ∈Zq. Now, the adversary knows π0, and therefore can compute the
value e:= gβ. However, to successfully impersonate the client, he must evaluate the random oracle
H at the point ( π0,u,v, [u/aπ0,e],[c,e]), which means he has to compute [ c,e]. But since c and e
912
are random group elements from the adversary’s point of view, computing [ c,e] is tantamount to
solving the CDH problem.
The complication we have not addressed in this argument is that the adversary may also interact
with the client P at some point, giving an arbitrary value v to P, who raises v/bπ0 to the power
π1, and derives a session key from this value. Because of this, P acts to a certain degree as a DDH
oracle, essentially giving the adversary an oracle for recognizing DH-triples of the form ( gπ1,·,·).
The issues are much the same as in the proof of Theorem 12.4. At ﬁrst glance, it might appear that
we need to make use of the interactive CDH assumption (see Deﬁnition 12.4) to prove security;
however, a closer examination shows that this is not the case. This is because in deriving the
session key, P also passes the value w:= (v/bπ0)α to the function H, and so P acts as an oracle for
recognizing 2DH-tuples (Exercise 12.32) of the form (gπ1,gα,·,·,·), where αis generated at random
by P. Using the trapdoor test in Exercise 12.32, we can prove security under the CDH assumption.
21.11.7 Explicit key conﬁrmation
As it is now, if an adversary runs protocol PAKE2 or PAKE+
2 with an honest user, using a guess at
the password that turns out to be wrong, then the adversary will have the wrong session key, but
the honest user will have no immediate indication that something went wrong. While higher level
protocols will most likely fail, possibly raising some suspicions, from a system design point of view,
it is perhaps better if the key exchange protocol itself raises the alarm. This is easy to do using a
simple form of what is called explicit key conﬁrmation. Instead of just deriving a session key k
from the hash, both users P and Q can derive keys k0 and k1, and then:
• P sends k0 to Q,
• Q sends k1 to P,
• P checks that the value ˜k1 it receives is equal to its computed value k1,
• Q checks that the value ˜k0 it receives is equal to its computed value k0.
If an online dictionary attack is underway, the protocol will be immediately alerted to this, and
can take defensive measures (see Section 18.3.1). Thus, in using PAKE protocols such as PAKE2 or
PAKE+
2 , it is highly recommended to augment them with an explicit key conﬁrmation step.
21.11.8 Phishing again
PAKE protocols provide some protection against phishing attacks (see Section 21.11.1). However,
a phishing adversary can still attempt to bypass the PAKE protocol entirely. For example, an
adversary may lure a client to a fake login page on his web browser, and the client enters his
password into the web browser in such a way that the password gets transmitted directly to the
adversary, rather than being processed by a PAKE protocol. This problem can be defended against
by an appropriate user interface design, so that the web browser presents an easy-to-identify and
hard-to-fake “safe area,” into which passwords should be entered, to be processed using a PAKE
protocol. Admittedly, designing such a user interface is not easy.
PAKE protocols can be combined to useful eﬀect with a one-sided AKE protocol (see Sec-
tion 21.6), despite the susceptibility of such protocols to phishing attacks.
913
First, consider the problem of how a client establishes a shared password with the server in
the ﬁrst place. Using a secure channel set up using a one-sided AKE protocol might be the only
reasonable way to do this, short of using a more expensive or less convenient type of secure channel.
Second, once the client and server have a shared password, it cannot hurt to run a PAKE
protocol through a secure channel set up using a one-sided AKE protocol. This way, an adversary’s
ability to attack the PAKE protocol will be limited by his ability to mount a successful phishing
attack.
21.11.9 Case study: PAKE used in the WiFi WPA3 protocol
To be written.
21.12 Key exchange using an online trusted third party
So far in this chapter we looked at key exchange protocols where the trusted third party (TTP) is
active only during the registration phase. If the the TTP is allowed to play an active role during
both registration and key exchange then it is possible to construct secure key exchange protocols
using only symmetric ciphers. There is no need for public-key cryptography. A TTP that plays an
active role in a key exchange protocol is sometimes called a key distribution center, or KDC,
for short.
While a key exchange protocol with an online TTP is very eﬃcient for clients, it can generate a
heavy load on the TTP. This is especially true in settings where a large number of clients engage in
many key exchanges every second. The online TTP must play an active role in all these exchanges,
making it expensive to build a scalabe and reliable system. Moreover, a compromise of the TTP
compromises all key exchanges, past and future. These scalability and security concerns make these
protocols diﬃcult to use for the global Internet. However, for corporate networks, a trusted online
TTP may be acceptable and quite practical.
21.12.1 A key exchange protocol with an online TTP
We now present a secure key exchange protocol with an online TTP calledOnlineTTP. The protocol
uses a CPA-secure symmetric cipher and a secure MAC. Let Ke be the cipher key space and Km be
the MAC key space. The registration protocol with the TTP is straightforward, but requires a
private channel to the TTP. User P with identity idP who wishes to register with the TTP does
the following:
1. Send the message idP to the TTP to indicate that the user is requesting to be registered.
2. The TTP generates a random long term secret key
kP ←R (kenc,P, kmac,P) ∈Ke ×Km
and stores the pair ( idP, kP) in a private table. The TTP sends the message kP to P.
User P now has a long term key kP shared with the TTP. Clearly the adversary should not be
allowed to eavesdrop or modify these messages. We note that the TTP need not store all user
keys kP. Instead, the TTP need only store one PRF master secret key kTTP and re-generate kP as
needed by evaluating kP ←F(kTTP,idP). If needed, one can prepend a random client nonce to the
914
P Q TTPrP, idP−−−−−−−−−−−−−→ rP, rQ, idP, idQ
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
k
P
cQ := EncQ(k),
tQ := MacQ(idP,rP,rQ,cQ)←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
k
Q
cP := EncP(k), idQ, r Q,
tP := MacP(idQ,rP,rQ,cP)←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
Figure 21.17: Key exchange with an online TTP
idP input to the PRF so that the user can refresh their shared key with the TTP at any time by
generating a new client nonce.
Key exchange. Next, we describe the key exchange protocol between users P and Q. We use
the following notation:
• idP and idQ denote the identities of the users P and Q;
• For a user X (either P or Q) we let EncX(m) denote the encryption of message musing the
secret key kenc,X shared between X and the TTP;
• For a user X we let MacX(m) denote a MAC of the message m using the secret key kmac,X
shared between X and the TTP.
• Kdenotes the set of session keys;
• Rdenotes a large set, which will be used to generate random nonces.
As before, we use the notation
k
Q
to indicate that user P terminated the protocol successfully, P obtained a session key k, and P
thinks he is talking to Q.
The key exchange protocol OnlineTTP between parties P and Qis illustrated in Fig. 21.17 and
runs as follows:
1. P computes rP ←R R, and sends ( rP, idP) to Q;
2. Q computes rQ ←R R, and sends ( rP, rQ, idP, idQ) to the TTP;
3. TTP checks that it has a secret key kP for P and kQ for Qand aborts if not; otherwise TTP
computes
k←R K,
cQ ←R EncQ(k), t Q ←R MacQ
(
idP,rP,rQ,cQ
)
cP ←R EncP(k), t P ←R MacP
(
idQ,rP,rQ,cP
) .
915
TTP sends (cQ, tQ) to Q and sends (cP, tP, idQ, rQ) to P.
4. Qveriﬁes that tQ is a valid tag on the message (idP,rP,rQ,cQ) and aborts if not; otherwise, Q
decrypts the ciphertext cQ and veriﬁes that cQ decrypts to a message k∈K; if not, Qaborts;
otherwise, Qterminates successfully, and outputs the session key k, and partner identity idP.
5. P veriﬁes that tP is a valid tag on the message (idQ,rP,rQ,cP) and aborts if not; otherwise, P
decrypts the ciphertext cP and veriﬁes that cP decrypts to a message k∈K; if not, P aborts;
otherwise, P terminates successfully, and outputs the session key k, and partner identity idQ.
The term idQ is sent in the last ﬂow from the TTP to P so that P knows the peer’s identity. The
term rQ is sent in this last ﬂow to enable P to verify the MAC tP.
Party P in this protocol sends a message to Q and, in response, receives a message from the
TTP. If needed, protocol OnlineTTP can be easily modiﬁed so that P only communicates with Q
by simply routing the ﬁnal message to P through Q. The information from the TTP intended for
P but routed through Q, is called a ticket. In more detail, the last ﬂow in OnlineTTP can be
changed to
P Q TTP
ticketQ,P := (cP,tP,idQ,rQ)←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−k
Q ticketQ,P
←−−−−−−−−−−−−−
Now P only communicates with Q, as required.
Channel bindings. Recall that in Section 21.8 we discussed the notion of a channel binding,
which is a globally unique name by which a higher-level application can refer to a session. For
this protocol, we can optionally add such a channel binding. An appropriate channel binding is
(idP,idQ,rP,rQ).
21.12.2 Insecure variations of protocol OnlineTTP
To get a feel for why protocolOnlineTTPis designed the way it is, we examine several variants of the
protocol and show that they are vulnerable to attack. In particular we show that removing elements
from the MACs computed by the TTP leads to insecure protocols. These attacks demonstrate why
each and every piece of protocol OnlineTTP is essential to achieve security.
916
Insecure variation 1: TTP does not MAC the ciphertext cP — key exposure attack
Suppose we modify protocol OnlineTTP so that the MAC in the message from the TTP to P does
not include the ciphertext cP. The resulting protocol runs as follows:
P Q TTPrP, idP−−−−−−−−−−−−−→ rP, rQ, idP, idQ
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
k
P
cQ := EncQ(k),
tQ := MacQ(idP,rP,rQ,cQ)←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
k
Q
cP := EncP(k), idQ, r Q,
tP := MacP( idQ,rP,rQ )
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
This modiﬁed protocol is vulnerable to a severe key exposure attack as follows:
• First, the adversary registers a new user Rwith the TTP and initiates a conversation with P.
The adversary obtains cR := EncR(k′) from the TTP and decrypts it to obtaink′. In addition,
the adversary eavesdrops on the message from the TTP to P and obtains c′
P := EncP(k′).
• At some time later, P and Q engage in a key exchange protocol. When the TTP sends the
message (cP, tP, idQ, rQ) to P, the adversary intercepts it and instead delivers the message
(c′
P, tP, idQ, rQ) to P.
The following diagram illustrates step 2 of the attack, after the adversary obtained k′and c′
P.
P Q TTP
rP, idP−−−−−−−−−−−−−→ rP, rQ, idP, idQ
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
k
P
cQ := EncQ(k),
tQ := MacQ(... )←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
∥
cP := EncP(k), idQ, r Q,
tP := MacP
(
idQ,rP,rQ
)
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
k′
Q c′
P := EncP(k′), idQ, r Q,
tP←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−∥
P will not detect the adversary’s interference since the TTP does not MAC the ciphertext cP in
its message to P. At the end of the attack, Q is holding the session key k, which is unknown to
the adversary; however, P is holding the session key k′, which is known to the adversary. Once the
protocol completes, P will likely begin using k′to encrypt private information. The adversary can
read everything P sends. We previously referred to this as a key exposure attack.
917
Key exposure attack on Q. A similar key exposure attack is possible if the MAC in the message
from the TTP to Q does not include the ciphertext cQ. To attack Q the adversary replaces cQ by
c′
Q where c′
Q := EncQ(k′).
Insecure variation 2: TTP does not MAC the nonce rP — replay attack
Suppose we modify protocol OnlineTTP so that the MAC in the message from the TTP to P does
not include the nonce rP. The resulting protocol runs as follows:
P Q TTPrP, idP−−−−−−−−−−−−−→ rP, rQ, idP, idQ
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
k
P
cQ := EncQ(k),
tQ := MacQ(idP,rP,rQ,cQ)←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
k
Q
cP := EncP(k), idQ, r Q,
tP := MacP( idQ,rQ,cP )
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
This new protocol is susceptible to the following replay attack, which is similar to the attack
discussed in Variation 2 in Section 21.2:
• ﬁrst, the adversary eavesdrops on a conversation between P and Q; suppose P sent the
message ( r′
P,idP) to Q, who then sends the appropriate message to the TTP. The TTP
responds with a message to Q and a message ( c′
P,t′
P,idQ,r′
Q) to P where c′
P := EncP(k′);
these messages are recorded by the adversary;
• at some later time, P initiates a new run of the protocol with Q; P sends out the mes-
sage (r,idP); the adversary intercepts this message, throws it away, and sends the message
(c′
P,t′
P,idQ,r′
Q) to P recorded from the previous run of the protocol.
The following diagram illustrates the second step in the attack. The shaded elements are replayed
by the adversary from the old conversation between P and Q:
P
rP, idP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→∥
k′
Q
c′
P := EncP(k′), idQ, r′
Q
t′
Q := MacP(idQ, r′
Q, c′
P)
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−∥
At the end of the attack, user P thinks he is talking to Q, but the session key k′ is the same key
used in a previous conversion with Q.
918
Replay attack on Q. A similar replay attack on Qis possible if the MAC on the message from
the TTP to Qdoes not include the nonce rQ. In the second step in the attack the adversary initiates
a key exchange with Q and sends the message ( r′
P,idP) to Q. It then intercepts the message from
the TTP to Qand modiﬁes it to contain ( c′
Q,t′
Q). User Qsuccessfully completes the protocol, but
is holding a session key k′from a previous conversation.
Insecure variation 3: TTP does not MAC the identity idP — identity misbinding
attack
Suppose we modify protocol OnlineTTP so that the MAC in the message from the TTP to P does
not include idQ. The resulting protocol runs as follows:
P Q TTPrP, idP−−−−−−−−−−−−−→ rP, rQ, idP, idQ
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
k
P
cQ := EncQ(k),
tQ := MacQ(idP,rP,rQ,cQ)←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
k
Q
cP := EncP(k), idQ, r Q,
tP := MacP( rP,rQ,cP )
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
The attack on this protocol is an identity misbinding attack, similar to the attack discussed in
Variation 3 in Section 21.2. The attack causes Qto think he is speaking with P while P thinks he
speaking with R. Here is how the attack works:
• ﬁrst, the adversary registers a new user R;
• at some time later, P and Q engage in a key exchange protocol. When the TTP sends the
ﬁnal message ( cP, tP, idQ, rQ) to P, the adversary intercepts this message, and instead
delivers the message ( cP, tP, idR, rQ) to P.
919
The following diagram illustrates the attack:
P Q TTP
rP, idP−−−−−−−−−−−−−→ rP, rQ, idP, idQ
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
k
P
cQ := EncQ(k),
tQ := MacQ(... )←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
∥
cP := EncP(k), idQ, r Q,
tP := MacP
(
rP,rQ,cP
)
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
k
R
cP := EncP(k), idR, r Q,
tP := MacP
(
rP,rQ,cP
)
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−∥
Both sides complete the protocol successfully and share the same key k. The adversary has no
direct information on k. Nevertheless, the parties are misbound.
Identity misbinding attack on Q. A similar misbinding attack on Qis possible if the MAC on
the message from the TTP to Qdoes not include idP. The adversary intercepts the initial message
(rP,idP) from P to Q and delivers instead the message ( rP,idR) to Q. Next, the adversary
intercepts the message ( rP,rQ,idR,idQ) from Q to the TTP and delivers instead the message
(rP,rQ,idP,idQ) to the TTP. Now both parties complete the protocol successfully, but Q thinks
he is talking to R and P thinks he is talking to Q.
Insecure variation 4: TTP does not MAC the nonce rQ
Suppose we modify protocol OnlineTTP so that the MAC in the message from the TTP to P does
not include rQ. The resulting protocol runs as follows:
P Q TTPrP, idP−−−−−−−−−−−−−→ rP, rQ, idP, idQ
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
k
P
cQ := EncQ(k),
tQ := MacQ(idP,rP,rQ,cQ)←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
k
Q
cP := EncP(k), idQ, r Q,
tP := MacP( idQ,rP,cP )
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
Note that rQ sent to P in the last ﬂow appears to be redundant and can be removed. However,
this is not the case, if the protocol is to provide secure channel bindings.
920
The attack works as follows: The adversary waits for P and Q to engage in a key exchange
protocol. When the TTP sends the ﬁnal message ( cP, tP, idQ, rQ) to P, the adversary intercepts
this message, and instead delivers the message (cP, tP, idQ, 0) to P. In other words, the adversary
changes rQ to 0.
At the end of the attack, P thinks he is talking to Q and Q thinks he is talking to P, as
required. Similarly both sides agree on the same session key k and k is unknown to the adversary,
as required. However, they would disagree on their channel bindings. P’s channel binding would
be (idP,idQ,rP,0) and Q’s channel binding would be ( idP,idQ,rP,rQ).
21.12.3 Security for protocol OnlineTTP
Protocol OnlineTTP clearly satisﬁes the correctness requirement in Section 21.9. It also can be
proved to satisy the deﬁnition of static security in Section 21.9 (and to provide secure channel
bindings, if these are implemented). We state the following theorem without proof.
Theorem 21.9. Protocol OnlineTTP is a statically secure key exchange protocol (optionally pro-
viding secure channel bindings), assuming the nonce space Ris large, the underlying cipher is CPA
secure, and the underlying MAC system is secure.
Note that protocol OnlineTTP is trivially not PFS secure. Once the adversary learns either P’s
key, Q’s key, or the TTP’s key, all past sessions between P and Q are exposed.
21.13 A fun application: establishing Tor channels
To be written.
21.14 Notes
Citations to the literature to be added.
21.15 Exercises
21.1 (Station to station). The station to station (STS) protocol runs as follows:
P Q
u:= gα
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
v:= gβ, c1 := E(k,SigQ(u,v)), CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
k
Q c2 := E(k,SigP(u,v)),CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→k
P
Here, G is a cyclic group of prime order q generated by g∈G, and α,β ∈Zq are chosen at random.
The session key k is computed as k ←H(w), where w = gαβ and H is hash function. Also,
E= (E,D) is a symmetric cipher.
921
(a) Suppose the signatures SigQ(u,v) and SigP(u,v) are not encrypted. Show that an adversary
can easily carry out an identity misbinding attack, where Q thinks he is talking a corrupt
user R. However, Q shares a key with P, and P thinks he is talking to Q.
(b) The STS protocol uses the session key k itself within the protocol to encrypt the signatures
SigQ(u,v) and SigP(u,v). Suppose that a higher-level communication protocol uses the
session key to encrypt messages using E and the key k, and that the adversary can force
either party to encrypt a message of its choice. Show how to carry out the same identity
misbinding attack from part (a), even when the signatures are encrypted.
(c) Suppose that we ﬁx the protocol so that it derives two keys ( k′,k) ←H(w), where k′is used
to encrypt the signatures and k is used as the session key. Show that an adversary can still
carry out the same identity misbinding attack.
Hint: The adversary does not follow the usual protocol for registration with the CA.
(d) Suppose we ﬁx the protocol just as in part (c). Show another identity misbinding attack in
which P and Q share a key, but P thinks he is talking to Q, while Q thinks he is talking to
another instance of himself.
21.2 (An attack on an AKE2 variant). Show that protocol AKE2 may not be secure if the
second signature does not include pk. To do this, you should start with a CCA-secure public-key
encryption scheme and then “sabotage” it in an appropriate way so that it is still CCA secure, but
when AKE2 is instantiated with this sabotaged encryption scheme, there is a KCI vulnerability that
leaves it open to a key exposure attack.
Hint: Assume P’s signing key is exposed. The attack should expose P’s session key, even though
P thinks he is talking to an honest user Q whose signing key has not been exposed.
21.3 (An attack on an AKE4 variant). Show that protocol AKE4 may not be secure if the
symmetric cipher does not provide ciphertext integrity. Speciﬁcally, assuming the symmetric cipher
is a stream cipher, show that protocol AKE4 is vulnerable to an identity misbinding attack.
21.4 (Strongly unpredictable ciphertexts). This exercise illustrates why the assumption that
the encryption scheme in protocol AKE3 has strongly unpredictable ciphertexts is necessary. To do
this, you should start with a semantically secure public-key encryption scheme and then “sabotage”
it in an appropriate way so that it is still semantically secure, but when AKE3 is instantiated with
this sabotaged encryption scheme, it is open to an attack of the following type. After making a
couple of queries to user P’s HSM, the adversary can impersonate P at will to any user Q, and
the adversary will know the low-order bit of each resulting session key. Assume session keys are bit
strings of some ﬁxed length.
21.5 (An insecure variant of AKE5). Suppose that we leave the group element gµν out of the
hash in protocol AKE5. Show that this variant is not statically secure. In particular, show that it
does not provide authenticity.
922
21.6 (Implicit authentication). Consider the following variant of protocol AKE5:
P Q
(public key = gα) (public key = gβ)
gµ, CertP−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→
k
Q gν, CertQ
←−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−k
P
k:= H(g(α+µ)(β+ν), gµν, gα,gµ,gβ,gν, idP,idQ)
Show that this protocol is not PFS secure.
Discussion: This protocol relies solely on implicit authentication: while each user needs to know
their long term secret key to compute the session key, they do not need to know it to run the
protocol. That is, neither user has to explicitly prove to the other that they know their long term
secret key. In fact, any protocol with these properties cannot be PFS secure.
Note: This protocol does, in fact, satisfy our deﬁnition of static security, under appropriate
assumptions.
21.7 (Insecure variants of AKE1). For each of the insecure variants of AKE1 described in Sec-
tion 21.2.1, show how the formal deﬁnition of security (Deﬁnition 21.1) is violated.
21.8 (Security proof for AKE2). Prove claims 1 and 2 in the proof of Theorem 21.2.
21.9 (Security proof for AKE3). Prove claims 1 and 2 in the proof of Theorem 21.3.
21.10 (Security proof for AKE4). Prove claims 1 and 2 in the proof of Theorem 21.4.
21.11 (TLS 1.3 is not a secure PAKE). In Section 21.10.1 we described the TLS 1.3 protocol
for establishing a secure session between P and Q using a pre-shared key psk. Suppose psk is a
password chosen at random from some small dictionary D. Show that an active adversary can
recover psk after a single attempted key exchange with Q. You may assume that the adversary
already has CertQ. Your attack shows that one should not use TLS 1.3 as a PAKE. Speciﬁcally,
psk should not be a human generated password.
21.12 (Non-interactive key exchange). Let G be a group of prime order q generated by
g ∈G and let H : G ×Z2
q →K be a hash function. Consider a system with n users where, for
i = 1 ,...,n , user i chooses a random αi ←R Zq, computes hi := gαi, and publishes ( i,hi) on a
public bulletin board. Assume that no one can update i’s entry on the board. Now, every pair of
users 1 ≤i ≤j ≤n can establish an authenticated shared key ki,j := H(gαiαj,i,j ) without any
interaction, other than reading the bulletin board. This is called non-interactive key exchange
or NIKE. Our goal is to prove that this approach is secure.
(a) Adapt the deﬁnition of HSM security from Section 21.9.4 to the NIKE settings of this protocol.
(b) Prove that the NIKE protocol above is secure assuming ICDH holds in G, when H is modeled
as a random oracle.
923
Chapter 22
Threshold cryptography
In this chapter we explore threshold cryptography , an important technique used to protect
secret keys in a public key cryptosystem. We will see how to apply this technique to protect the
decryption key in a public key encryption scheme, and how to protect the signing key in a signature
scheme. We begin with threshold decryption, which is a way to protect a decryption key.
Threshold decryption. Let ( pk,sk) be a key pair in a public key encryption scheme. If an
attacker steals sk, then all ciphertexts ever encrypted under pk can be decrypted by the attacker.
For this reason, important secret keys are often stored in a special hardware device, called a
hardware security module, or HSM, that responds to decryption requests, but never exports the
secret key in the clear from the device. An attacker who compromises the computing environment
can temporarily use the key by sending requests to the HSM, but cannot steal the key and use it,
or sell it, later.
A diﬀerent approach to protecting a secret key is to split it into a number of pieces, called
shares. Each share is stored on a diﬀerent machine, called a decryption server. All decryption
servers must participate in a protocol in order to decrypt a ciphertext, and decryption fails if even
one decryption server does not participate. In particular, to steal the secret key, an attacker must
break the security of all the decryption servers, which can be harder than breaking the security
of a single decryption server. Furthermore, if each decryption server stores its share in an HSM,
then an attacker must compromise all of these HSMs to recover the key, which is much harder than
compromising a single HSM.
In what follows, we use N to denote the total number of decryption servers, each of which stores
a share of the secret key. Requiring all N decryption servers to participate in every decryption
hurts availability: if even a single decryption server is unavailable for a period of time, decryption
becomes impossible during that time; worse, if a single decryption server loses its share (say,
because of a hardware failure), then decryption becomes permanently impossible. For this reason,
we often require that decryption can proceed if tof the N decryption servers are available, for some
parameter t in the range 1 ,...,N . For security, even if t−1 decryption servers are compromised,
and an attacker steals the shares stored by these compromised servers, the attacker should not
learn anything useful about the secret key sk, and in particular, should not be able to decrypt
ciphertexts on its own. Such a scheme is called a t-out-of-N threshold decryption scheme .
For example, in a 3-out-of-5 scheme, stealing just two shares reveals nothing useful to the adversary,
yet decryption can proceed even if two servers crash or otherwise lose their shares. Note that some
924
sk1 sk2 sk3
combiner
combiner
ciphertext c
c
c′
1
c c
c′
3
plaintext m
(a) Threshold decryption using two responses
from three decryption servers. The combiner
sends the ciphertext c to all three decryption
servers, two servers respond, and the combiner
obtains the plaintext m.
sk1 sk2 sk3
combiner
combiner
message m
m
σ′
1
m m
σ′
3
signature σ
(b) Threshold signing using two responses
from three signing servers. The combiner sends
the message m to all three signing servers, two
servers respond, and the combiner obtains the
signature σ.
Figure 22.1: Threshold decryption and threshold signing
applications require larger values for t and N.
During the decryption protocol, the secret key sk should never be reconstituted in a single
location, as doing so would create a single point of failure that an attacker can target to steal the
key. In this chapter we will focus on a type of decryption protocol that is particularly simple and
practical, in the sense that it requires minimal interaction. In this type of protocol, in addition to
the N decryption servers, there is an additional entity called a combiner, which orchestrates the
decryption protocol. One could also think of the combiner as a “decryption client”. The combiner
takes as input a ciphertextcto decrypt, and forwardscto all the decryption servers. Every available
server applies its key share to c, and sends back a decryption share. Once the combiner receives
t responses from the decryption servers, it can construct the complete decryption of c. The entire
process is illustrated in Fig. 22.1a.
In Section 22.3 we will construct threshold decryption systems that are chosen-ciphertext secure.
Moreover, the schemes we construct are also designed to be robust against decryption servers that
behave incorrectly: if a decryption server returns an incorrect decryption share, the combiner
can detect that the decryption server has misbehaved (in which case the combiner can report the
misbehaving server and/or seek a correct decryption share from some other decryption server).
Threshold signatures. The same idea can be used to protect the signing key sk in a signature
scheme. In a t-out-of-N threshold signature scheme, the signing key sk is split into N shares.
Each share is stored on a diﬀerent machine, called a signing server, and t signing servers must
participate to sign a message. Even if t−1 signing servers are compromised, and an attacker steals
the shares stored by these compromised servers, the attacker should not learn anything useful about
the signing key sk, and in particular, should not be able to sign messages on its own. Moreover, if
925
(N−t) servers crash, or lose their key shares, the remaining tservers can continue to sign messages.
Just as for threshold decryption, it is essential that the signing key sk is never reconstituted in a
single location. Fig. 22.1b illustrates the process of threshold signing.
In Section 22.2 we will construct threshold signature schemes that are designed to be secure
and robust.
Distributed key generation. At this point the reader is probably wondering how the decryption
and signing servers obtained the key shares sk1,..., skN in the ﬁrst place? A simple solution
involves a trusted key generator that generates a key pair (pk,sk), and then splits sk into N shares
sk1,..., skN. For i = 1 ,...,N , the key generator sends share number i to decryption/signing
server i, and then deletes sk. Often the machine that runs the key generator meets a violent end
— such as melting its motherboard — to ensure that residual data on the machine does not expose
sk.
This solution is a bit disappointing. We took special care to never reconstitute the key sk in
a single location, and yet it is in a single location when generated. If the single key generator is
attacked while it is generating the key, then sk will be compromised.
Fortunately, there is a better solution calleddistributed key generation or DKG. For all the
threshold systems we describe, there are practical DKG protocols in which the N servers exchange
messages so that at the end of the DKG protocol a public key pk is publicly known, and each
server knows its own key share. Even if t−1 of the servers are already compromised while they are
running the DKG protocol, the system still remains secure. We will discuss such DKG protocols
in more detail in Section 22.4.
22.1 Shamir’s secret sharing scheme
Many threshold cryptography schemes are built from an elegant general technique for splitting a
secret into multiple shares. We ﬁrst describe this technique, and in the next two sections show how
to use it for threshold decryption and for threshold signatures.
Suppose Alice has a secret α∈Z, where Z is some ﬁnite set. She wishes to generate N shares
of α, each belonging to some ﬁnite set Z′, and denoted α1,...,α N ∈Z′, so that the following
property is satisﬁed:
any tof the N shares are suﬃcient to reconstruct α, but every set of t−1 shares reveals
nothing about α.
This sharing lets Alice give one share of her secret αto each of her N friends, so that any tfriends
can help her recover α, but t−1 friends learn nothing. A scheme satisfying this property is called
a secret sharing scheme.
Deﬁnition 22.1. A secret sharing scheme over Z is a pair of eﬃcient algorithms (G,C):
• G is a probabilistic sharing algorithm that is invoked as (α1,...,α N) ←R G(N,t,α ), where
0 <t ≤N and α∈Z, to generate a t-out-of-N sharing of α.
• C is a deterministic combining algorithm that is invoked as α←C(J,{αj}j∈J), where J
is a subset of {1,...,N }of size t, to recover α.
926
• Correctness: we require that for every α ∈Z, for every possible output (α1,...,α N) of
G(N,t,α ), and every t-size subset J of {1,...,N }, we have
C(J,{αj}j∈J) = α.
Threshold secret sharing schemes often impose a bound on the parameter N. This bound
can be modeled as a system parameter. Some schemes may impose additional constraints on the
relationship between the parameters N and t. For any particular scheme, we will refer toallowable
parameters as those (N,t) pairs that are allowed by the scheme.
Intuitively, a secret sharing scheme is secure if every set of t−1 shares output by G(N,t,α )
reveals nothing about α. To deﬁne this notion formally, it will be convenient to use the following
notation: for a set J ⊆{1,...,N }, we denote by G(N,t,α )[J] the collection of shares {αj}j∈J,
where the output of G(N,t,α ) is (α1,...,α N).
Deﬁnition 22.2. A secret sharing scheme (G,C) over Z is secure if for all allowable parameters
0 < t≤N, for every α,α′ ∈Z, and every subset J of {1,...,N }of size t−1, the distribution
G(N,t,α )[J] is identical to the distribution G(N,t,α ′)[J].
The deﬁnition implies that by looking at t−1 shares, one cannot tell if the secret is α or α′,
for all α and α′ in Z. Hence, looking at only t−1 shares reveals nothing about the secret. The
deﬁnition is unconditional: even an inﬁnitely powerful adversary cannot tell if the secret is αor α′.
22.1.1 Shamir secret sharing
An elegant secret sharing scheme over Zq, where q is prime, is due to Shamir. This scheme makes
use of the following general fact about polynomial interpolation: a polynomial of degree at most
t−1 is uniquely determined bytpoints on the polynomial. For example, two points determine a line,
and three points determine a parabola. This general fact not only holds for the real and complex
numbers, but over any algebraic domain in which all non-zero elements have a multiplicative inverse.
Such a domain is called a ﬁeld. When q is prime, Zq is a ﬁeld, and so this general fact holds here
as well.
Shamir’s scheme ( Gsh,Csh) is a t-out-of-N secret sharing scheme over Zq that requires that
q >N, and works as follows:
• Gsh(N,t,α ): choose random κ1,...,κ t−1 ←R Zq and deﬁne the polynomial
ω(x) := κt−1xt−1 + κt−2xt−2 + ... + κ1x+ α ∈Zq[x].
Notice that ωhas degree at most t−1 and that ω(0) = α.
For i= 1,...,N compute αi ←ω(i) ∈Zq.
Output the N shares α1,...,α N.
• Csh(J,{αj}j∈J): since every polynomial of degree at most t−1 is determined by its value
at tpoints, the tshares αj for j ∈J completely determine the polynomial ω. Algorithm Csh
interpolates the polynomial ωand outputs the constant term α:= ω(0).
The description of algorithmCsh needs a bit more explanation, which we provide by the following
more general lemma, and which will be useful in other contexts as well. The lemma essentially says
that the value of a polynomial of degree at most t−1 at an arbitrary point is a linear combination
of the value of the polynomial on any set of t points.
927
Lemma 22.1 (Linearity of interpolation). Let q be a prime, let J⊆ Zq be a set of size t, and
let j∗∈Zq. Then there is a collection of values {λj}j∈J, with each λj ∈Zq, such that the following
holds:
for every polynomial ω∈Zq[x] of degree at most t−1, we have
ω(j∗) =
∑
j∈J
λjω(j).
Moreover, the values λj for j ∈J are eﬃciently computable given J and j∗.
Proof. This can be easily derived from the Lagrange interpolation formula, which states that
for every polynomial ω∈Zq[x] of degree at most t−1, we have
ω=
∑
j∈J
ω(j)
∏
j′∈J\{j}
x−j′
j−j′.
Indeed, one can verify that if g is the polynomial of degree at most t−1 on the right hand side of
this formula, then ω(j) = g(j) for all j ∈J; from this, and the fact that a polynomial of degree at
most t−1 is uniquely determined by t points on the polynomial, we see that ω= g.
Now just plug in j∗for x in this formula, and we obtain
ω(j∗) =
∑
j∈J
ω(j)
∏
j′∈J\{j}
j∗−j′
j−j′,
and the lemma follows by setting
λj :=
∏
j′∈J\{j}
j∗−j′
j−j′ for j ∈J. 2
The values λj in the above lemma are called Lagrange interpolation coeﬃcients.
To implement Csh(J,{αj}j∈J), we can apply the above lemma with the given setJand j∗:= 0
to compute the Lagrange interpolation coeﬃcients {λj}j∈J, which satisfy
ω(0) =
∑
j∈J
λjω(j)
for every polynomial ω∈Zq[x] of degree at most t−1. It follows that
α=
∑
j∈J
λjαj.
Hence, Csh(J,{αj}j∈J) operates in two steps: (i) compute the Lagrange interpolation coeﬃcients
{λj}j∈J, and (ii) compute the secret α as α:= ∑
j∈Jλjαj.
Besides implementing algorithm Csh, Lemma 22.1 has other applications as well. For example,
it allows us to do “interpolation in the exponent”, as the following corollary shows:
928
Corollary 22.2 (Interpolation in the exponent). Let G be a group of prime order q. Let
J ⊆Zq be a set of size t, and let j∗∈Zq. Given J, j∗, as well as a collection of group elements
of the form {hω(j)}j∈J, where h∈G and ω∈Zq[x] is a polynomial of degree at most t−1, we can
eﬃciently compute the group element hω(j∗).
Proof. To compute hω(j∗), we ﬁrst compute the Lagrange interpolation coeﬃcients λj for j ∈J ,
as in Lemma 22.1. Then we have
hω(j∗) = h
∑
j∈Jλjω(j) =
∏
j∈J
(hω(j))λj.
Thus, we compute each of the powers ( hω(j))λj for j ∈J, and multiply these together. 2
We stress that the algorithm in this corollary is not given as input the polynomial ω, or even
the group element h. This corollary will be essential in several of our threshold schemes in this
chapter, both in terms of implementing the schemes and in proving their security.
22.1.2 Security of Shamir secret sharing
It remains to show that Shamir’s secret sharing scheme is secure, as in Deﬁnition 22.2.
Theorem 22.3. Shamir’s secret sharing scheme (Gsh,Csh) is secure. In particular, if 0 <t ≤N <
q, then for every α ∈Zq, any collection of t−1 shares {αℓ}ℓ∈L is a mutually independent family
of random variables, with each share uniformly distributed over Zq,
Proof. Let α be a ﬁxed value in Zq and let j′
1,...,j ′
t be ﬁxed, distinct, non-zero values in Zq.
Consider the map that sends
(κ1,...,κ t−1) ∈Zt−1
q to ( α′
1,...,α ′
t−1) ∈Zt−1
q ,
where α′
ℓ := ω(j′
ℓ) for ℓ= 1,...,t −1 and ω:= α+ κ1x+ ··· + κt−1xt−1.
Claim: this map is one-to-one.
The theorem follows from the claim, since if ( κ1,...,κ t−1) is chosen uniformly over Zt−1
q , then
(α′
1,...,α ′
t−1) must also be uniformly distributed over Zt−1
q .
To prove the claim, suppose by way of contradiction that this map is not one-to-one. This
would imply the existence of two distinct polynomials g,h ∈Zq[x] of degree at most t−2, such
that the polynomials α+ xg and α+ xh agree at the t−1 non-zero points j′
1,...,j ′
t−1. But then
this implies that g and h themselves agree at these same t−1 points, which contradicts our basic
fact about polynomial interpolation. 2
Remark 22.1. In presenting Shamir’s secret sharing scheme, we have streamlined the notation a
bit by using the values j = 1,...,N as the points at which we evaluate the polynomial ω∈Zq[x]
— we are implicitly identifying the integer j with its image in Zq under the natural map from Z
onto Zq. More generally, we could use an arbitrary, ﬁxed sequence (β1,...,β N) of distinct, non-zero
values in Zq as evaluation points. Everything we present in this chapter can be trivally adapted
to such general sequences of evaluation points. Indeed, Shamir secret sharing works perfectly well
over ﬁnite ﬁelds other than Zq, and in such settings, one might have to use a more general sequence
of evaluation points. 2
929
22.2 Threshold signatures
In this section, we turn to the problem of designing threshold signature schemes. We begin by
deﬁning the basic syntax of a threshold signature scheme.
Deﬁnition 22.3. A threshold signature scheme (G,S,V,C ) is a tuple of four eﬃcient algo-
rithms:
• G is a probabilistic key generation algorithm that is invoked as
(pk,pkc,sk1,..., skN) ←RG(N,t)
to generate a t-out-of-N shared key. It outputs a public key pk, a combiner public key
pkc, and N signing key shares , sk 1,..., skN.
• S is a (possibly) probabilistic signing algorithm that is invoked as σ′
i ←R S(ski,m), where
ski is one of the key shares generated by G, m is a message, and σ′
i is a signature share for
m using sk i.
• V is a deterministic veriﬁcation algorithm as in a signature scheme, invoked as V(pk,m,σ )
and outputs either accept or reject.
• C is a deterministic combiner algorithm that is invoked as σ ←C(pkc,m, J,{σ′
j}j∈J),
where pkc is the combiner public key, m is a message, J is a subset of {1,...,N }of size
t, and each σ′
j is a signature share for m. The algorithm either outputs a signature σ, or
outputs a special message blame(J∗), where J∗ is a nonempty subset of J.
Intuitively, the message blame(J∗) indicates that the provided signature shares σ′
j for j ∈J∗,
are invalid.
• Correctness: as usual, the veriﬁcation algorithm should accept a properly constructed sig-
nature; speciﬁcally, for all possible outputs (pk,pkc,sk1,..., skN) of G(N,t), all messages m,
and all t-size subsets J of {1,...,N }, we have
Pr
[
V
(
pk, m, C(pkc, m, J, {S(skj,m)}j∈J)
)
= accept
]
= 1.
In the above deﬁnition, messages lie in some ﬁnite message space Mand signatures in some
ﬁnite signature space Σ. We say that the signature scheme is deﬁned over ( M,Σ). Also, just as for
a secret sharing scheme, a threshold signature scheme may impose constraints on the parameters
N and t (such as an upper bound on N and on the relationship between N and t). For any
particular scheme, we will refer to allowable parameters as those ( N,t) pairs that are allowed
by the scheme.
Note that Deﬁnition 22.3 does not require that the ski’s are actually shares of any particular
signing key sk, for some non-threshold signature scheme, but for many threshold signature schemes,
this is indeed the case.
Let’s map these four algorithms back to Fig. 22.1b. Algorithm G(N,t) is used to provision
the signing servers with their signing key shares sk1,..., skN. Next, when the combiner requests a
signature on some message m, each signing server examines the message and decides if it wants to
sign it. If server i is willing to sign, it computes σ′
i ←R S(ski,m) and sends the resulting signature
930
share σ′
i to the combiner. Once the combiner receives t signature shares from distinct signing
servers, it runs algorithm C to (hopefully) obtain either a valid signature on m or a set of indices
J∗ that identiﬁes the misbehaving servers that contributed invalid shares. Note that the above
deﬁnition does not actually require that a signature produced by the combiner algorithm is valid
or that the set of indices J∗ actually identiﬁes misbehaving servers — such a requirement is a
robustness property that will be formally deﬁned below. If a scheme is robust in this sense, then
if the t shares collected by the combiner do not yield a valid signature, the set J∗may be used to
discard invalid shares from misbehaving servers, and the combiner may then seek out valid shares
from other servers. This process may continue until a valid signature is obtained, provided there
are at least t signing servers that are available and behaving correctly.
Later, after we look at some constructions, we will formally deﬁne what it means for a threshold
signature scheme to be secure. However, let us give an informal deﬁnition of security here. We
will assume that an adversary has compromised t−1 signing servers, obtaining their signing key
shares. During the attack, the adversary may ask any of the uncompromised servers for signature
shares on any messages of its choice. Security will mean that the adversary cannot forge a valid
signature on any other message.
Note that this security deﬁnition rules out a broad range of attacks. First, it rules out a key-
recovery attack: if an attacker steals the signing key shares from at most t−1 servers, the attacker
cannot reconstruct the signing key itself. Second, it also rules out attacks in which an attacker
corrupts the behavior of some (but fewer than t) of the signing servers, even if it does not obtain
their signing key shares. Indeed, in any application, the signing servers will be implementing some
particular policy that determines which messages should be signed under what circumstances, and
will only issue signature shares if this policy is satisﬁed. Security means that if at most t−1 servers
are corrupted, in the sense that either (a) their signing key shares have been stolen or (b) they do
not implement the signing policy correctly, then an attacker still cannot get a valid signature on a
message unless at least one of the uncompromised servers determines that the policy is satisﬁed and
issues a signature share. We should note, conversely, that if an attacker can get t−1 compromised
servers to issue signature shares on a given message (either by stealing their signing key shares or
by subverting their signing policy), then the attacker needs to get just one of the uncompromised
servers to issue a signature share on that message in order to obtain a valid signature on that
message.
For example, a certiﬁcate authority (CA) is supposed to validate certain credentials before
issuing a certiﬁcate (see Section 13.8). Performing this validation correctly is just as important as
securely storing the signing key. To improve the security of the CA, we may implement it as a
distributed system using a threshold signature scheme, where each signing server stores one signing
key share and independently validates credentials before issuing a signature share. Security means
that if at most t−1 servers are corrupted, in the sense that either (a) their signing key shares
have been stolen or (b) they do not implement credential validation correctly, then an attacker still
cannot get a valid certiﬁcate unless it presents valid credentials to at least one of the uncompromised
servers.
Remark 22.2. Deﬁnition 22.3 requires that tand N be speciﬁed at key generation time. However,
the schemes in this section can be extended so that both t and N can be changed after the secret
key shares are generated, without changing the public key pk. See Exercise 22.4. 2
931
22.2.1 A generic threshold signature scheme
Every secure signature scheme ( G,S,V ) can be trivially transformed into a threshold signature
scheme (G′,S′,V ′,C′) as follows:
• During setup, the key generator creates N key pairs ( pki,ski) ←R G(), for i= 1,...,N , and
gives key ski to signing server i. The public veriﬁcation key is pk := ( pk1,..., pkN), and
the combiner public key is the same, pkc := pk. More precisely, we deﬁne threshold key
generation G′(N,t) as an algorithm that outputs ( pk,pk,sk1,..., skN).
• To sign a message m, signing server i, for i ∈{1,...,N }, outputs the signature share σ′
i ←R
S(ski,m).
• The combiner collects t signature shares from the signing servers, and outputs all t of
them as the full signature. In more detail, algorithm C′(pkc,m, J,{σ′
j}j∈J), where pkc =
(pk1,..., pkN) and |J|= t, ﬁrst checks that each σ′
j is a valid signature on m under pkj. If
not, it outputs reject(J∗), where J∗is the set of indices j for which σ′
j is invalid. Otherwise,
it outputs the signature σ:= (J,{σ′
j}j∈J).
• Finally, algorithm V′(pk,m,σ ), where pk = (pk1,..., pkN) and σ = (J,{σ′
j}j∈J), outputs
accept if |J|= t and each σ′
j is a valid signature on m under pkj.
This scheme satisﬁes the security and robustness requirements for a threshold signature scheme
(see Exercise 22.3). However, one major drawback of this scheme is that its performance degrades
as t grows. In particular, the size of the signature grows linearly in t, as does the time to verify a
signature. Our goal is to construct a threshold signature scheme where the size of the signature,
and the time to verify it, are independent of the parameters N and t.
Another drawback of this scheme is that the thresholdtis publicly known. Anyone can examine
the public veriﬁcation algorithm V′and learn t. As a result, an adversary knows exactly how many
signing servers it needs to corrupt in order to steal the secret signing key sk. Generally, it is
desirable that the threshold scheme not reveal the threshold t to the public. The scheme that we
construct next has the property that t is only known to the combiner, and to no one else.
Remark 22.3 (Decentralized key provisioning). This scheme has the additional beneﬁt that
we do not need a trusted key generator, and thus can eliminate that single point of failure. Indeed,
server i can generate the key pair ( pki,ski) on its own; moreover, corrupt servers are allowed to
generate their public keys in an arbitrary fashion (not necessarily by running G, and possibly
depending on the public keys of the honest servers). We call this type of setup decentralized key
provisioning. 2
22.2.2 BLS threshold signing
The BLS signature scheme (Section 15.5.1) supports a very eﬃcient threshold signing mechanism,
even for large t and N. A threshold signature looks like a regular (non-threshold) BLS signature.
First, let us brieﬂy recall how the BLS signature scheme works. Let G be a group of prime
order q with generator g ∈G. In addition, we assume there is an eﬃcient algorithm ODDH that
solves the decision Diﬃe-Hellman (DDH) problem in G. In particular, for all α,β ∈Zq algorithm
ODDH satisﬁes
ODDH(gα,gβ,gγ) =
{
accept if γ = αβ;
reject otherwise.
932
Recall that in Section 15.4 we saw that a symmetric pairing in a group G gives an eﬃcient algo-
rithm for DDH in G. In particular, (15.8) gives an eﬃcient implementation for ODDH. Then in
Section 15.5.1 we presented the BLS scheme using a pairing. Here we present BLS more abstractly,
using a generic algorithm for DDH in G.
The BLS signature scheme ( G,S,V BLS) uses a hash function H : M→ G and works as follows:
• G() →(pk,sk): choose a random α ←R Zq, compute u ←gα, and output pk := u ∈G and
sk := α.
• S(sk,m) →σ: output σ:= H(m)α ∈G, where sk = α.
• VBLS(pk,m,σ ): output ODDH(pk, H(m), σ).
Indeed, for a public key pk = gα, a message m∈M, and a valid signature σ = H(m)α, the tuple(
pk = gα, H(m) = gβ, σ= gαβ)
is a DDH tuple and VBLS outputs accept. In Section 15.5.1 we
showed that the scheme is secure when CDH holds in G and H is modeled as a random oracle.
Remark 22.4. In Section 15.5.1 we presented the BLS scheme using an asymmetric pairing.
While our discussion of threshold BLS will use the abstract description of BLS presented above,
the construction generalizes almost verbatim to threshold BLS using an asymmetric pairing. 2
22.2.2.1 The BLS threshold signature scheme
To make BLS into a threshold signature scheme we will apply Shamir’s secret sharing scheme
(Gsh,Csh) to the secret key α∈Zq. We will show that a signature can be generated without ever
reconstituting the secret key α at a single location.
The scheme. We describe the BLS threshold signature scheme SthBLS = ( G′,S′,V ′,C′) using
the same group G and hash function H : M→ G as in BLS. The scheme uses the BLS signature
veriﬁcation algorithm VBLS and works as follows.
• G′(N,t) →(pk,pkc,sk1,..., skN):
– choose a random α←R Zq and compute pk ←gα;
– run Gsh(N,t,α ) to obtain N shares α1,...,α N ∈Zq;
Note: recall that for some polynomialω∈Zq[x] of degree at mostt−1, we haveω(0) = α
and ω(i) = αi for i= 1,...,N ;
– for i= 1,...,N , compute ui ←gαi, and set ski := αi and pki := ui;
– set pkc := (pk1,..., pkN);
– output (pk, pkc, sk1,..., skN).
• S′(ski,m) →σ′
i, where ski = αi: output the signature share σ′
i ←H(m)αi ∈G.
Note: observe that σ′
i is a regular BLS signature on mwith respect to the public key ui = gαi.
• C′(pkc, m, J, {σ′
j}j∈J) →σ, where J is a subset of {1,...,N }of size t, and
pkc = (pk1,..., pkN) = (u1,...,u N) :
933
– step 1: verify that all t signature shares are valid: let J∗ be the set of all j ∈J such
that
VBLS(uj,m,σ ′
j
)
= reject;
if J∗is nonempty, output blame(J∗) and abort;
Note: assuming all the shares are valid, then for j ∈J, we have σ′
j = H(m)ω(j), where
ω∈Zq[x] is the polynomial of degree at most t−1 used to share the secret key α;
Note: Remark 15.5 shows that one can verify the tsignature shares as a batch far more
eﬃciently than verifying them one by one;
– step 2: compute the signature: interpolate in the exponent by applying Corollary 22.2
with the given set J and j∗ := 0 to compute the signature σ := H(m)α = H(m)ω(0)
from the t group elements σ′
j = H(m)ω(j) for j ∈J; output σ as the signature on m;
more explicitly, the signature σ is computed as
σ←
∏
j∈J
(σ′
j)λj ∈G
where the exponents {λj}j∈J are the Lagrange interpolation coeﬃcients, which depend
only on the the set J, as computed in Lemma 22.1.
• V′(pk,m,σ ): output VBLS(pk,m,σ ).
The size of a signature σ and the time to verify it are independent of the parameters N and t.
Moreover, the veriﬁcation algorithm V′is independent of t, so that the public learns nothing about
the threshold t.
Further enhancements. The BLS threshold signature scheme can be strengthened in several
ways. First, the system easily generalizes to more ﬂexible access structures than strict threshold.
For example, it is easy to extend the scheme to support the following access structure: signing
is possible if signing server number 1 participates, and at least t of the remaining N −1 signing
servers participate. We explore more general access structures in Section 22.5 and Exercise 22.2.
Another enhancement, called proactive security, further strengthens the system by forcing
the adversary to break into t signing servers within a short period of time , say ten minutes [87].
Otherwise, the adversary gets nothing. This is done by having the key servers proactively refresh
the sharing of their secret key every ten minutes, without changing the public key. We discuss this
in Exercise 22.5.
The shared secret signing key αcan be generated using a distributed key generation (DKG)
protocol to ensure that αis never found in a single location, not even during key generation — see
Section 22.4.
Finally, as discussed in Remark 22.4, the BLS threshold signature scheme can easily be adapted
to the asymmetric pairing setting. Using the notation from Section 15.5.1, the public key and
the combiner public key for threshold BLS would be in the group G1, while the signatures and
signature shares would be in the group G0. All of the results we prove below for threshold BLS in
the symmetric setting hold as well in the asymmetric setting.
934
22.2.3 Threshold signature security
We next prove that the BLS threshold scheme is secure. To do so we must ﬁrst deﬁne what it means
for a t-out-of-N threshold signature scheme to be secure. The security model gives the adversary
two capabilities:
• We allow the adversary to completely control up tot−1 of the signing servers. The adversary
must declare at the beginning of the game a setLof up to t−1 servers that it wants to control.1
• In addition, the combiner is not a trusted party, and may be controlled by the adversary. This
means that a scheme that temporarily reconstitutes the signing key at the combiner would be
insecure. To capture the idea that the combiner may be controlled by the adversary, we allow
the adversary to request signatures on messages of its choice, and we then give the adversary
all N signature shares generated by the signing servers in response to these requests.
Even with control of the combiner, and up to t−1 servers, the adversary should not be able to
break the signature scheme. In particular, it should be unable to create an existential forgery under
a chosen message attack. Formally, we deﬁne security using the following attack game.
Attack Game 22.1 (threshold signature security). For a given threshold signature scheme
S = ( G,S,V,C ), deﬁned over ( M,Σ), and a given adversary A, we deﬁne the following attack
game.
• Setup: the adversary sends poly-bounded allowable parameters N and t, where 0 < t≤N,
and a subset L⊆{ 1,...,N }of size t−1 to the challenger. The challenger runs
(pk,pkc,sk1,..., skN) ←R G(N,t),
and sends pk, pkc, and {skℓ}ℓ∈Lto the adversary.
• Signing queries: for j = 1,2,..., signing query j from Ais a message mj ∈M. Given mj,
the challenger computes all N signature shares σ′
j,i ←R S(ski,mj) for i = 1,...,N . It sends(
σ′
j,1,...,σ ′
j,N
)
to the adversary.
• Forgery: eventually Aoutputs a candidate forgery pair ( m,σ) ∈M× Σ.
We say that the adversary wins the game if the following two conditions hold:
• V(pk,m,σ ) = accept, and
• m is new, namely, m̸∈{m1,m2,... }.
We deﬁne A’s advantage with respect to Sdenoted thSIGadv[A,S], as the probability that Awins
the game. 2
Deﬁnition 22.4 (secure threshold signatures). We say that a threshold signature scheme S
is secure if for all eﬃcient adversaries A, the quantity thSIGadv[A,S] is negligible.
1This is what is known as a static corruption model . A stronger model, called the adaptive corruption
model, allows the adversary to adaptively choose which signing servers to corrupt as the attack game proceeds.
935
22.2.3.1 Robust threshold signatures
Beyond security for the signature scheme, a threshold signature scheme should also be robust
against a malicious signing server that is trying to disrupt the signature generation process. We
deﬁne two such robustness properties that any practical threshold signature scheme should provide.
Suppose a combiner receives a collection {σ′
j}j∈J of t signature shares on a given message m.
By Deﬁnition 22.3, the combiner will output either a signature σ, or a special message blame(J∗),
where J∗is a nonempty subset of J.
• If the output of the combiner is blame(J∗), then we would like it to be the case that all of the
signature shares σ′
j for j ∈J∗are “bad”, in the sense that they were incorrectly generated by
misbehaving signing servers. A threshold signature scheme that guarantees that this always
happens is said to provide accurate blaming.
• Otherwise, the output of the combiner is a signature σ on the given message m. A threshold
signature scheme is said to be consistent if no eﬃcient adversary can cause the combiner to
output a signature that is invalid on m, with non-negligible probability. When the combiner
outputs a valid signature we say that the given collection of shares {σ′
j}j∈J is valid for m.
A threshold signature scheme is robust if it satisﬁes both properties.
In practice, robustness allows a combiner who is trying to construct a valid signature on m to
proceed as follows. If the combiner algorithm outputs blame(J∗), then the combiner can discard
the “bad” shares in J∗, and seek out t−|J∗|“good” shares from among the remaining signing
servers. As long as there are t correctly behaving servers available, the accurate blaming property
guarantees that the combiner can repeat this process until it gets a valid collection of shares, and
the consistency property guarantees that when this happens, the resulting signature must be valid
(with overwhelming probability).
We now deﬁne these two properties formally.
Deﬁnition 22.5 (accurate blaming). We say that a threshold signature scheme S= (G,S,V,C )
provides accurate blaming if the following holds:
for all possible outputs (pk,pkc,sk1,..., skN) of G(N,t), all messages m, all t-size
subsets J of {1,...,N }, and all collections of signature shares {σ′
j}j∈J:
C
(
pkc,m, J,{σ′
j}j∈J
)
= blame(J∗) = ⇒ Pr
[
S(skj,m) = σ′
j
]
= 0 for all j ∈J∗.
In other words, if the combiner algorithm outputs blame(J∗), then σ′
j, for j ∈J∗, cannot be an
output of S(skj,m). This ensures that an honest server that outputs S(skj,m) cannot be blamed.
The requirement is unconditional: not even an inﬁnitely powerful adversary can cause an honest
server to be blamed.
The consistency property is deﬁned using an attack game.
Attack Game 22.2 (consistent threshold signatures). For a given threshold signature scheme
S= (G,S,V,C ), deﬁned over (M,Σ), and a given adversaryA, we deﬁne the following attack game.
• The adversary sends to the challenger poly-bounded allowable parameters N and t, where
0 <t ≤N.
936
• The challenger runs (pk,pkc,sk1,..., skN) ←R G(N,t) and sends all this data to the adversary.
• The adversary outputs a message m, a subset J of {1,...,N }of size t, and a collection
{σ′
j}j∈J of signature shares.
• We say the adversary wins the game if C(pkc,m, J,{σ′
j}j∈J) outputs a signature σ (rather
than a “blame” message), but V(pk,m,σ ) = reject.
We deﬁne A’s advantage with respect to S, denoted conSIG adv[A,S], as the probability that A
wins the game. 2
Deﬁnition 22.6 (consistent threshold signatures). We say that a threshold signature scheme
Sis consistent if for all eﬃcient adversaries A, the quantity conSIGadv[A,S] is negligible.
At ﬁrst it may appear that every threshold signature scheme ( G,S,V,C ) can be easily made
consistent: construct a new scheme ( G,S,V,C ′) where C′(pkc,m, J,{σ′
j}j∈J) works as follows: (i)
run C on the same input, (ii) if C outputs a signature σ, check that σ is valid for m, (iii) if it is
valid, then output σ; otherwise abort. This ensures that if C′ outputs a signature, the signature
must be valid for m. However, this does not work because C cannot simply abort: if it does not
output a signature, it is required to ﬂag at least one of the provided signature shares as invalid.
However, when C′obtains an invalid signature from C, it does not know which signature share to
blame. Consequently, we must require consistency as an explicit property.
Deﬁnition 22.7 (robustness). We say that a threshold signature scheme is robust if it provides
accurate blaming and is consistent.
Remark 22.5. While we require accurate blaming to be perfect and unconditional, we only require
consistency against an eﬃcient adversary. The reason is that some threshold schemes, such as the
threshold RSA signature scheme (Exercise 22.10), do not satisfy perfect unconditional consistency.
We note that the BLS threshold signature scheme does provide perfect and unconditional consis-
tency: if the combiner outputs a signature, we are guaranteed that the signature is always valid for
the message being signed. 2
22.2.4 Security of threshold BLS
Next, we argue that the BLS threshold signature scheme is secure and robust. Security follows
directly from the security of the BLS signature scheme. Robustness holds unconditionally.
Theorem 22.4. If SBLS is a secure signature scheme, then SthBLS is a secure threshold signature
scheme.
In particular, for every adversary Athat attacks SthBLS as in Attack Game 22.1, there exists
an adversary B, where Bis an elementary wrapper around A, that attacks SBLS as in Attack
Game 13.1, such that
thSIGadv[A,SthBLS] = SIGadv[B,SBLS].
Proof. We design Bto play the role of challenger to A. Recall that Abegins by outputting the
parameters N and t, and a subset Lof size t−1 of {1,...,N }. Now, when Breceives pk = gα from
its own challenger, Bneeds to provide to Anot only pk, but also t−1 key shares corresponding to
L, as well as pkc.
937
To do this, Bsets αℓ ←R Zq for each ℓ ∈L, and sends to Athe t−1 key shares skℓ = αℓ for
ℓ∈L. By Theorem 22.3, these key shares have the same distribution as in an actual run of Attack
Game 22.1.
We know that there is a unique polynomialω∈Zq[x] of degree at most t−1 such that ω(0) = α
and ω(ℓ) = αℓ for each ℓ ∈L. Moreover, for i ∈{1,...,N }\L , the key share ski = αi satisﬁes
ω(i) = αi.
Adversary Balso has to give pkc to A. Recall that pkc = (u1,...,u N), where ui = gαi = gω(i)
for i = 1,...,N . For ℓ ∈L, adversary Bcan compute uℓ = gαℓ directly, since Bknows the value
αℓ. For each i∈{1,...,N }\L, adversary Bcan compute ui via “interpolation in the exponent”,
by applying Corollary 22.2 with J:= L∪{0}and j∗:= ito compute ui = gω(i) from the collection
of group elements {gω(j)}j∈J; more explicitly, Bcomputes ui as
ui ←pkλi0 ·g
∑
ℓ∈Lαℓ·λiℓ,
where the λij’s are the Lagrange interpolation coeﬃcients computed as in Lemma 22.1.
Next, when Aissues a signing query for a message m ∈M, adversary Bforwards this query
to its own challenger and gets back σ = H(m)α = H(m)ω(0). Now Bneeds to send to Aall the
signature shares for m. That is, Bneeds to construct σ′
1,...,σ ′
N, without knowing the secret key
α. Recall that for i = 1,...,N , we have σ′
i = H(m)αi = H(m)ω(i). For ℓ ∈L, adversary Bcan
compute σ′
ℓ = H(m)αℓ directly, since it knows the value αℓ. For i∈{1,...,N }\L, adversary Bcan
compute σ′
i again via “interpolation in the exponent”, by again applying Corollary 22.2, with J
and j∗= 0 as in the previous paragraph, to compute σ′
i = H(m)αi = H(m)ω(i) from the collection
of group elements {H(m)ω(j)}j∈J; more explicitly, Bcomputes σ′
i as
σ′
i ←σλi0 ·H(m)
∑
ℓ∈Lαℓ·λiℓ,
where the λij’s are the same Lagrange interpolation coeﬃcients used above.
When eventually Aoutputs a signature forgery (m,σ), our adversary Boutputs the same (m,σ).
Now Bhas the same advantage in its attack game that Ahas in its attack game, which completes
the proof. 2
Theorem 22.5. SthBLS is a robust threshold signature scheme. In particular, every adversary has
advantage zero in Attack Game 22.2.
Proof. For accurate blaming, it is easy to see that a correctly generated signature share will never
be ﬂagged as invalid. Conversely, for consistency, it is easy to see that any valid collection of
signature shares will combine to yield a valid signature. 2
This completes our discussion of the BLS threshold signature scheme. We showed that the
scheme is secure and robust.
22.2.5 Accountability versus privacy
Secure threshold signatures used in practice come in two ﬂavors. One is called accountable threshold
signatures and the other is called private threshold signatures.
• An accountable threshold signature scheme is a threshold signature scheme where a valid
signature identiﬁes a set of parties that must have participated in generating the signature.
938
For example, consider a 3-out-of-5 threshold setup, and let σ be a valid signature on a
message m. Then anyone should be able to examine σ and learn the identity of three parties
that must have participated in generating σ. For security we require that a set of signers
cannot generate a signature σ that falsely makes it look as if some other signer, not in the
set, participated in generating σ.
• A private threshold signature scheme is the opposite: it is a threshold signature scheme
where a valid signature reveals nothing about the set of parties that participated in generating
the signature. Moreover, the signature should reveal nothing about the total number of parties
or the threshold.
We will deﬁne these concepts more precisely momentarily.
An accountable threshold signature scheme is frequently needed in ﬁnancial applications. Con-
sider again a 3-out-of-5 threshold setup, where a valid signature is needed to move funds from Alice’s
account to Bob’s account. Now, suppose Alice complains that funds were fraudulently moved out
of her account. The ﬁnancial institution could then examine the signature that authorized the
transfer, and identify the three signing parties that participated in the fraud. Those three parties
will have some explaining to do. In general, accountable threshold signatures can be used to hold
the signing parties accountable for their actions.
A private threshold signature scheme is needed when an organization prefers to keep its internal
organizational structure private. For example, suppose a certiﬁcate authority (CA) splits its signing
key across ﬁve servers so that any three can sign a certiﬁcate request. The CA prefers that the
public not know the number of servers nor the threshold, lest that information help an attacker
who is trying to compromise the CA.
We have already seen examples of both ﬂavors of threshold signatures:
• The generic threshold signature scheme from Section 22.2.1 is accountable. This scheme is
used in the Bitcoin system for precisely this reason. Bitcoin transactions that are signed this
way are called multisig transactions.
• The BLS threshold signature scheme from Section 22.2.2.1 is private.
The trouble with the generic threshold construction is that signatures can be long — their length
is proportional to the threshold. Later in this section we will construct an accountable threshold
signature scheme with much shorter signatures: each signature is a single group element along with
a compact description of the signing set, no matter the threshold or the number of signing parties.
First, we deﬁne privacy and accountability more precisely.
22.2.5.1 Accountable threshold signatures: deﬁnition
We begin by deﬁning accountable threshold signatures, or ATS. An ATS scheme is a threshold
signature scheme with an additional tracing algorithm T.
Deﬁnition 22.8. An accountable threshold signature scheme , or ATS, is a tuple of ﬁve
eﬃcient algorithms (G,S,V,C,T ) where (G,S,V,C ) is a threshold signature scheme, and
• T is a deterministic tracing algorithm that is invoked as J ←T(pk,m,σ ), where pk is a
public key obtained by running G(N,t). Algorithm T outputs a set J ⊆{1,...,N }of size t
that indicates the set of parties that participated in generating σ.
939
• Correctness: for all possible outputs (pk,pkc,sk1,..., skN) of G(N,t), all messages m, and
all t-size subsets J of {1,...,N }, we have
Pr
[
T
(
pk, m, C(pkc, m, J, {S(skj,m)}j∈J)
)
= J
]
= 1.
ATS security. An ATS is secure if no set of parties can frame an innocent party. In particular,
even if all but one of the parties collude, they cannot frame the one remaining party. We capture
this in the following game.
Attack Game 22.3 (ATS security). For a given ATS scheme S= (G,S,V,C,T ), and a given
adversary A, we deﬁne the following attack game.
• Setup: the adversary sends poly-bounded allowable parameters N and t, where 0 < t≤N,
and a target victim v∈{1,...,N }to the challenger. The challenger runs
(pk,pkc,sk1,..., skN) ←R G(N,t),
and sends pk, pkc, and {skℓ}ℓ̸=v to the adversary.
• Signing queries: for j = 1,2,..., signing query j from Ais a message mj. Given mj, the
challenger computes σ′
j ←R S(skv,mj) and sends σ′
j to the adversary.
• Forgery: eventually Aoutputs a candidate forgery pair ( m,σ).
We say that the adversary wins the game if the following conditions hold:
• V(pk,m,σ ) = accept,
• m is new, namely, m̸∈{m1,m2,... }, and
• v∈T(pk,m,σ ) meaning that T incorrectly says that v participated in generating σ.
We deﬁne A’s advantage with respect to Sdenoted ATSadv[A,S], as the probability that Awins
the game. 2
Deﬁnition 22.9 (secure ATS). We say that an ATS scheme S is secure if for all eﬃcient
adversaries A, the quantity ATSadv[A,S] is negligible.
One might consider a seemingly stronger deﬁnition where the adversary is allowed to adaptively
choose the target victim v ∈{1,...,N }after seeing the public key, some secret keys, and some
signatures. However, security under Deﬁnition 22.9, where the adversary has to commit to v ahead
of time, implies security under a deﬁnition that allows the adversary to choose v adaptively. We
leave this as a good exercise. The key observation is that an adversary Athat chooses v adaptively
can be converted into an adversary Bthat chooses v in advance by having Bguess v∈{1,...,N }
at the beginning of the game. This Bhas a 1 /N chance of correctly guessing A’s choice for the
victim v to attack.
940
The generic threshold scheme is accountable. Let us brieﬂy verify that the generic threshold
signature scheme ( G′,S′,V ′,C′) from Section 22.2.1 is accountable by giving a tracing algorithm
T′so that (G′,S′,V ′,C′,T′) is a secure ATS. Recall that the generic scheme is built from a regular
(non-threshold) signature scheme (G,S,V ). The public key is pk := (pk1,..., pkN), and a threshold
signature on a message m is a collection σ := (J,{σ′
j}j∈J) of t valid signatures by t of the public
keys in pk.
The tracing algorithm T′(pk,m,σ ), where σ = (J,{σ′
j}j∈J), simply outputs J. It is a simple
exercise to show that if ( G,S,V ) is a secure signature scheme, then ( G′,S′,V ′,C′,T′) is a secure
ATS.
Remark 22.6 (Trusted key generation). Our description of an ATS assumes that a trusted
party, Tracy, runs algorithm G(N,t) to generate the signing keys for the parties. Clearly Tracy
can frame an innocent signing party since she has all the signing keys. This is clearly undesirable.
Note that the generic ATS ( G′,S′,V ′,C′,T′) described above can be securely implemented using
decentralized key provisioning (see Remark 22.3), and as such are not susceptible to this type
of attack. For other schemes, we may need to use a more expensive DKG protocol (such as in
Section 22.4) to avoid this type of attack. Later in this section we will see an ATS based on the
BLS signature scheme. This ATS can also be implemented using decentralized key provisioning. 2
22.2.5.2 Private threshold signatures: deﬁnition
A private threshold signature scheme, or PTS, is the opposite of an ATS: a valid signature should
reveal nothing about the set that generated it. Moreover, a signature should reveal nothing about
the threshold or the total number of parties. To capture this property we require that signatures
output by the combiner are indistinguishable from signatures generated by a regular (non-threshold)
signature scheme, even if the adversary gets to see the public key and many signatures on messages
of its choice. This is captured by the following attack game.
Attack Game 22.4 (PTS security). For a given threshold signature scheme S0 =
(G0,S0,V0,C0), and a regular (non-threshold) signature scheme S1 = (G1,S1,V1), we deﬁne two
experiments.
Experiment b (for b= 0,1):
• the adversary sends poly-bounded allowable parameters N and t, where 0 < t≤N, to the
challenger. The challenger computes
(pk0,pkc0,sk′
1,..., sk′
N) ←R G0(N,t) and ( pk1,sk1) ←R G1()
and sends pkb to the adversary.
• The adversary submits a sequence of queries to the challenger. For i= 1,2,..., the ith query
is a pair ( mi,Ji) where mi is a message and Ji is a t-size subset of {1,...,N }. In response,
the challenger computes the signature σi as follows:
– if b= 0, σi ←R C0
(
pkc0,mi,Ji,
{
S0(sk′
j,mi)
}
j∈Ji
)
;
– if b= 1, σi ←R S1(sk1,mi).
The challenger sends σi to the adversary.
941
• Finally, the adversary outputs a bit ˆb∈{0,1}.
Let Wb be the event that Aoutputs 1 in Experiment b. Deﬁne A’s advantage with respect to
(S0,S1) as
PTSadv[A,S0,S1] :=
⏐⏐⏐Pr[W0] −Pr[W1]
⏐⏐⏐. 2
Deﬁnition 22.10. A threshold signature scheme S0 is private, or a PTS, if there exists a reg-
ular (non-threshold) signature scheme S1 such that for all eﬃcient adversaries A, the quantity
PTSadv[A,S0,S1] is negligible.
The fact that an adversary cannot distinguish between a sequence of signatures generated by the
threshold scheme, and a sequence of signatures generated by the regular (non-threshold) scheme,
means that the threshold signatures and the public key reveal nothing about N, t, and J.
Deﬁnition 22.10 captures privacy for N, t, and J from the public’s point of view, where only
pk and a sequence of signatures are visible. This is suﬃcient to ensure that a sequence of threshold
signatures reveals nothing to the public about the inner workings of a signer such as a certiﬁcate
authority. The deﬁnition, however, does not ensure privacy against an insider. For example, one of
the N signers may be able to use its secret key to determine the set J that generated a particular
signature. It is not diﬃcult to deﬁne a game that prevents this. See Exercise 22.7.
The BLS threshold scheme is private. Let us brieﬂy verify that the BLS threshold signature
scheme ( G′,S′,V ′,C′) from Section 22.2.2.1 is private. Referring back to the notation in that
section, recall that the key generation algorithm samples a random α ←R Zq, and then generates
Shamir secret shares of this α. The public key is pk := gα. It is a simple exercise to show that for
all messages m, all N, t, and all t-size subsets J ⊆{1,...,N }, the resulting threshold signature
is σ := H(m)α. Hence, the threshold signature is always the same as a regular (non-threshold)
BLS signature with secret key α. It follows that no adversary can distinguish between a sequence
of threshold BLS signatures and a sequence of regular (non-threshold) BLS signatures. Hence, the
deﬁnition of privacy is satisﬁed.
22.2.6 BLS accountable threshold signatures
So far, our only example of an accountable threshold signature scheme, or ATS, is the generic
scheme from Section 22.2.1. The trouble with that scheme is that it generates long signatures,
where signature length is proportional to the threshold.
To construct a better ATS, with shorter signatures, we need not look far. In fact, the BLS
aggregate signature scheme from Section 15.5.3.2 directly gives an ATS. Let’s see how. As usual,
let G be a group of prime order q with generator g∈G.
The BLS accountable threshold scheme Sbls-ats = (G,S,V,C,T ) works as follows:
• G(N,t) →(pk,pkc,sk1,..., skN):
– choose random α1,...,α N ←R Zq and compute ski ←αi and pki ←gαi for i= 1,...,N ;
– set pk := (pk1,..., pkN) and pkc := pk;
– output (pk, pkc, sk1,..., skN).
• S(ski,m) →σi, where ski = αi: output the signature share σi ←H(m)αi ∈G.
942
• C
(
pkc,m, J,{σj}j∈J
)
→σ, where J⊆{ 1,...,N }is of size t, and pkc = (pk1,..., pkN):
– step 1: verify that for all j ∈J the signature share σj is a valid BLS signature on m
with respect to the public key pkj; let J∗be the set of all j ∈J for which this does not
hold; if J∗is nonempty, output blame(J∗) and abort;
– step 2: compute the aggregate signature σ ←∏
j∈Jσj ∈G and output ( J,σ) as the
signature on m.
• V
(
pk,m, (J,σ)
)
:
– step 1: compute the aggregate public key apk := ∏
j∈Jpkj;
– step 2: Verify that |J|= tand that σ is a valid BLS signature on mwith respect to the
public key apk. If so output accept; otherwise output reject.
• T
(
pk,m, (J,σ)
)
: output J.
A signature ( J,σ) is a single group elements in σ ∈G along with a compact description of the
signing set J. This is much shorter than a signature produced by the generic scheme. We next
prove that the scheme is a secure ATS as in Deﬁnition 22.9.
Theorem 22.6. If the BLS signature scheme is secure in the group G, then Sbls-ats is a secure
ATS.
Proof idea. Let Abe an ATS adversary attacking Sbls-ats. We build an adversary Bthat plays the
role of challenger to Aand attacks the BLS signature scheme. Bbegins by receiving a challenge
public key pk′from its BLS challenger and receiving N, t, and v from A. Our Bnow does:
• sample random α1,...,α N ←R Zq and compute pki ←gαi for i= 1,...,N ;
• set pkv ←pk′; (this embeds the challenge public key in position v)
• set pk := (pk1,..., pkN) and pkc := pk;
• send pk, pkc, and {skℓ}ℓ̸=v to A.
Next, Aissues a sequence of signature queries for pkv = pk′, which Banswers by querying its BLS
challenger. Eventually Aoutputs a forgery (m,(J,σ)), where (J,σ) is a valid signature on mand
where v ∈J . Because the signature is valid, we know that σ = ∏
j∈JH(m)αj. Moreover, since
v∈J, we can rewrite this as H(m)αv = σ/∏
j∈J,j̸=vH(m)αj. Our Bcan compute the right hand
side by itself and obtain σ′:= H(m)αv. This σ′is the BLS signature on the message m under pk′.
Finally, Bsends (m,σ′) to its challenger as a valid BLS forgery. 2
Remark 22.7 (Trusted key generation). Since Sbls-ats assumes a trusted key generator, we
do not have to defend against a rogue key attack as in Section 15.5.2.1. However, as discussed in
Remark 22.6, it is desirable to eliminate this trusted key generator as a single point of failure. Just
as for the generic ATS scheme, Sbls-ats can be implemented using decentralized key provisioning,
but doing so exposes the scheme to a rogue public key attack. Therefore, when generating keys
for Sbls-ats in this way, it is important to require that every party publish a proof of possession of
the secret key, as discussed in Section 15.5.3.2, or alternatively to use message augmentation, as
discussed in Section 15.5.3.1. A third option with some beneﬁts is presented in [33]. 2
This completes our discussion of threshold signature schemes.
943
22.3 Threshold decryption schemes
We now change topics and turn to the problem of designing threshold decryption schemes where
the decryption key is shared among N parties so that tof them are needed to decrypt a ciphertext.
We begin by deﬁning the basic syntax of a threshold decryption scheme. Such a scheme will make
use of the concept of associated data, as discussed in Section 12.7. Recall that associated data is
public data that is supplied as an additional input to the encryption algorithm. In order to recover
the encrypted plaintext, the identical associated data must be supplied during decryption.
Deﬁnition 22.11. A public-key threshold decryption scheme E= (G,E,D,C ) is a tuple of
four eﬃcient algorithms:
• G is a probabilistic key generation algorithm that is invoked as
(pk,pkc,sk1,..., skN) ←RG(N,t)
to generate a t-out-of-N shared key. It outputs a public key pk, a combiner public key
pkc, and N decryption key shares sk1,..., skN.
• E is a probabilistic encryption algorithm that is invoked as c←R E(pk,m,d ), where pk is
a public key output by G, m is a message, and d is associated data.
• D is a deterministic decryption algorithm that is invoked as c′
i ←D(ski,c,d ), where sk i
is one of the decryption key shares output by G, c is a ciphertext, d is associated data, and c′
i
is a decryption share for c using sk i.
• C is a deterministic combiner algorithm that is invoked as m←C(pkc,c,d, J,{c′
j}j∈J),
where pkc is the combiner public key, c is a ciphertext, d is associated data, J is a subset of
{1,...,N }of size t, and each c′
j is a decryption share of c. The algorithm either outputs a
plaintext m, the special symbol reject, or a special message blame(J∗), where J∗is a nonempty
subset of J.
Intuitively, the message blame(J∗) indicates that the provided decryption shares c′
j for j ∈J∗
are invalid.
• Correctness: as usual, decryption should correctly decrypt a properly constructed ciphertext;
speciﬁcally, for all possible outputs (pk,pkc,sk1,..., skN) of G(N,t), all messages m, all
associated data d, all possible outputs c of E(pk,c,d ), and all t-size subsets J of {1,...,N },
we have
C
(
pkc, c, J, {D(skj,c,d )}j∈J
)
= m.
In the above deﬁnition, messages lie in some ﬁnite message space M, ciphertexts in some ﬁnite
ciphertext space C, and associated data in some ﬁnite space D. We say that E is deﬁned over
(M,D,C). Also, just as for a secret sharing scheme, a threshold decryption scheme may impose
constraints on the parameters N and t (such as an upper bound on N and on the relationship
between N and t). For any particular scheme, we will refer to allowable parameters as those
(N,t) pairs that are allowed by the scheme.
Remark 22.8. Deﬁnition 22.11 requires thattand N be speciﬁed at key generation time. However,
all the schemes in this section can be extended so that both tand N can be changed after the secret
key shares are generated, without changing the public key pk. 2
944
Note that Deﬁnition 22.11 does not require that the ski’s are actually shares of any particular
decryption key sk for a non-threshold scheme, but for many threshold decryption schemes, this is
indeed the case.
Let’s map these four algorithms back to Fig. 22.1a. Algorithm G(N,t) is used to provision
the decryption servers with their decryption key shares sk1,..., skN. Next, when the combiner
requests a decryption of some ciphertext c, along with associated data d, each decryption server
examines the ciphertext and associated data, and it decides whether or not it wants to perform the
decryption. If server iis willing to perform the decryption, it computes c′
i ←R D(ski,c,d ) and sends
the resulting decryption share c′
i to the combiner. Once the combiner receives t decryption shares
from distinct signing servers, it runs algorithm C to (hopefully) obtain either a plaintext m or a
set of indices J∗that identiﬁes the misbehaving servers that contributed invalid shares.
Just as we did for threshold signature schemes, we will formally deﬁne an appropriate robust-
ness property for threshold decryption schemes. Intuitively, this property says two things: ﬁrst, if
the combiner algorithm outputs blame(J∗), then the set of indices J∗identiﬁes only misbehaving
servers; second, if the combiner outputs m∈M∪{ reject}, then the value of m is independent of
which servers contributed decryption shares. If a scheme is robust in this sense, then if the tshares
collected by the combiner do not yield m ∈M∪{ reject}, the set J∗ may be used to discard in-
valid shares from misbehaving servers, and the combiner may then seek out valid shares from other
servers. This process may continue until some m∈M∪{ reject}is obtained, provided there are at
least tdecryption servers that are available and behaving correctly. Moreover, it is guaranteed that
this value of m is independent of which t servers ultimately contributed valid decryption shares.
After we look at some constructions, we will formally deﬁne what it means for a threshold
decryption scheme to be secure. However, let us give an informal deﬁnition of security here. We
will assume that an adversary has compromised t−1 decryption servers, obtaining their decryption
key shares. During the attack, the adversary may ask any of the uncompromised servers for
decryption shares on any ciphertext/associated data pairs of its choice. Security will mean that
the adversary will not glean any useful information about any message mthat was encrypted using
associated data d to yield a ciphertext c, so long as it does not ask any of the uncompromised
servers for decryption shares on a ciphertext/associated data pair (ˆc, ˆd) where ˆd= d.
This security deﬁnition corresponds to the nontion of AD-only CCA security for non-threshold
schemes (as deﬁned in Section 12.7.1), and it rules out a broad range of attacks. First, it rules out
a key-recovery attack: if an attacker steals the decryption key shares from at most t−1 servers,
the attacker cannot reconstruct the decryption key itself. Second, it also rules out attacks in which
an attacker corrupts the behavior of some (but not too many) of the decryption servers, even if
it does not obtain their decryption key shares. Indeed, in any application, each decryption server
will be implementing some particular policy that determines which ciphertexts should be decrypted
under what circumstances, based on the associated data and other data presented to the server,
and will only issue a decryption shares if this policy is satisﬁed. Suppose a message mis encrypted
using associated data d, resulting in a ciphertext c. Security means that if at most t−1 servers are
corrupted, in the sense that either (a) their decryption key shares have been stolen or (b) they do
not implement the decryption policy correctly, then an attacker still cannot get any information
about m unless at least one of the uncompromised servers determines that the policy is satisﬁed
and issues a decryption share for a ciphertext/associated data pair (ˆ c, ˆd) where ˆd= d. We should
note, conversely, that if an attacker can get t−1 compromised servers to issue decryption shares
for the ciphertext/associated data pair ( c,d) (either by stealing their decryption key shares or by
945
subverting their decryption policy), then the attacker needs to get just one of the uncompromised
servers to issue a decryption share for ( c,d) in order to obtain m.
As a more concrete example, consider the key-escrow application discussed in Sections 12.2.3
and 12.7. In that application, a user’s ﬁle encryption key was encrypted under the public key of
an escrow service. Moreover, some metadata md associated with the ﬁle was used as associated
data in the computation of the ciphertext cES. At a later time, some requesting entity may present
the pair (cES,md) to the escrow service to obtain the ﬁle encryption key. The escrow service may
impose some type of access control policy, based on the given metadata, along with the identity
or credentials of the requesting entity. Enforcing this access control policy is just as important
as securely storing the decryption key. To improve the security of the escrow service, we may
implement it as a distributed system using a threshold decryption scheme, where each decryption
server stores one decryption key share and independently enforces the access control policy. Security
means that if at most t−1 servers are corrupted, in the sense that either (a) their decryption key
shares have been stolen or (b) they do not correctly enforce the access control policy, then an
attacker still cannot get any information about the escrowed ﬁle encryption key unless at least one
of the uncompromised servers determines that the access control policy is satisﬁed and issues a
decryption share for (ˆcES,md) for some ciphertext ˆcES.
Note that one can naturally deﬁne a stronger notion of security, corresponding to full CCA
security, rather than AD-only CCA security. Here, security means that the adversary will not
glean any useful information about any message m that was encrypted using associated data d to
yield a ciphertext c, so long as it does not ask any of the uncompromised servers for decryption
shares on the ciphertext/associated data pair ( c,d). This is a stronger notion of security, as the
adversary is less restricted in its attack. We focus on the weaker notion of AD-only CCA security
for three reasons:
• for most applications of threshold decryption, AD-only CCA security is suﬃcient,
• AD-only CCA-secure schemes are somewhat less complicated and more eﬃcient than fully
CCA-secure schemes, and
• any AD-only CCA-secure threshold decryption scheme can be easily converted into a fully
CCA-secure scheme using the simple technique described in Exercise 14.13 of “wrapping”
the ciphertext with a strongly one-time secure signature (see also Remark 22.10 and Exer-
cise 22.11).
22.3.1 A generic threshold decryption scheme
Any AD-only CCA-secure public key encryption scheme (G,E,D ) can be transformed into an AD-
only CCA-secure threshold decryption scheme ( G′,E′,D′,C′) by applying secret sharing to the
plaintext.
For parameters 0 <t ≤N, the t-out-of-N threshold decryption scheme works as follows:
• During setup, the key generator creates N key pairs ( pki,ski) ←R G(), for i= 1,...,N , and
gives key ski to signing server i. The public encryption key is pk := ( pk1,..., pkN), and
the combiner public key is the same, pkc := pk. More precisely, we deﬁne threshold key
generation G′(N,t) as an algorithm that outputs ( pk,pk,sk1,..., skN).
• Algorithm E′(pk,m,d ): for a public key pk = (pk1,..., pkN), we encrypt a message m∈Zq
for pk using associated data d as follows:
946
– run a t-out-of-N secret sharing on m to obtain N shares m1,...,m N,
– for i= 1,...,N compute ci ←R E(pki,mi,d),
– output c:= (c1,...,c N).
• Algorithm D′(ski,c,d): output c′
i := D(ski,ci,d).
• The combiner collects tdecryption shares and combines them using the combining algorithm
of the secret sharing scheme to recover the plaintext m.
As we will see, this construction satisﬁes the security requirements for a threshold decryption
scheme. However, one major drawback with this scheme is that its performance degrades as N
grows. In particular, the size of the ciphertext size grows linearly with N, as does the time to
encrypt a message. Our goal is to construct a threshold decryption scheme where ciphertext size
and encryption time are independent of the parameters N and t.
Another major drawback of this scheme is that it does not provide robustness.
• As one example of what could go wrong, the scheme does not give the combiner any way
of detecting invalid decryption shares — a misbehaving decryption server could give the
combiner an incorrect share of the plaintext, causing the combiner to output the wrong
plaintext.
• As another example of what could go wrong, even if all of the decryption servers behave
correctly, a party could generate a ciphertext ( c1,...,c N) where c1,...,c N do not encrypt
consistent shares of any one plaintext, and the combiner could end up outputting diﬀerent
plaintexts depending on which subset of servers it obtained decryption shares from.
There are various techniques that could be employed to make this scheme robust, but we will not
discuss them here.
Remark 22.9 (Decentralized key provisioning). Just as we observed in Remark 22.3 for
the generic threshold signing scheme, the generic threshold decryption scheme can be securely
implemented using decentralized key provisioning. Thus, for this scheme, do we not need a trusted
key generator, and thus can eliminate that single point of failure. 2
22.3.2 An insecure threshold decryption scheme
Recall that the ElGamal encryption scheme EEG, introduced in Section 11.5, uses a group G of
prime order q with generator g∈G, a symmetric cipher Es = (Es,Ds), deﬁned over (K,M,C), and
a hash function H : G2 →K. The secret key is an element α∈Zq, the public key isu:= gα ∈G. To
encrypt a plaintext m∈M, we ﬁrst compute β ←R Zq, v←gβ, w←uβ, and then k←H(v,w) and
e←R Es(k,m); the ciphertext is ( v,e) ∈G ×C. Such a ciphertext is decrypted by ﬁrst computing
w←vα, and then k←H(v,w) and m←Ds(k,e).
In Section 11.5 we showed that if we model H as a random oracle, then this scheme is seman-
tically secure under the CDH assumption for G. In Section 12.4 we showed that if we model H
as a random oracle, then this scheme is CCA secure under the stronger ICDH assumption for G.
As discussed in Exercise 12.17, we can add support for associated data by passing associated data
d ∈D as an additional input to the hash function H to derive the symmetric key in both the
encryption and decryption algorithms as k←H(v,w,d ).
947
To convert this into at-out-of-N decryption scheme, we might be tempted to proceed as follows.
The key generation algorithm ﬁrst generates a t-out-of-N Shamir secret sharing of the ElGamal
decryption key α∈Zq. The resulting shares αi for i= 1,...,N are the decryption key shares, and
each decryption server is given one share. Given a ciphertext ( v,e), the ith decryption server uses
its decryption key share to generate the decryption share wi := vαi.
Recall that for some polynomial ω ∈Zq[x] of degree at most t−1, we have ω(0) = α and
ω(i) = αi for i = 1,...,N . The design of the combiner algorithm is based on this fact. Given a
ciphertext (v,e), associated data d, and a collection of t decryption shares {wj}j∈J, the combiner
works as follows:
• First, the combiner performs “interpolation in the exponent”, by applying Corollary 22.2
with the given set J and j∗:= 0 to compute the group element w:= vα = vω(0) ∈G. More
explicitly, w is computed as
w←
∏
j∈J
wλj
j ∈G
where the exponents {λj}j∈J are the Lagrange interpolation coeﬃcients, which depend only
on the set J, as computed in Lemma 22.1.
• Once the combiner has computedw, it then computes the symmetric secret keyk←H(v,w,d )
and outputs plaintext m←Ds(k,e).
This threshold decryption scheme is neither robust nor CCA secure. While the scheme could
be augmented without too much diﬃculty to make it robust, it is inherently insecure against CCA
attacks.
To see why this scheme is inherently insecure against CCA attacks, observe that a decryption
server does not use the associated data d in generating its decryption share. Recall that in appli-
cations, a decryption server should be able to enforce some kind of decryption policy based on the
associated data, but there is no way to do that here.
As a more concrete example, suppose Alice encrypts a message m with associated data d,
generating a ciphertext (v,e). Perhaps dsays that her friend Bob should have the right to decrypt
this ciphertext. However, the ciphertext ( v,e) falls into the hands of Carol, who presents to the
decryption servers the ciphertext (v,e), along with associated data d′that says Carol has the right
to decrypt this ciphertext, as well as any credentials that are required by the decryption servers
to validate Carol’s identity. Now the decryption servers will send Carol the decryption shares that
she needs to decrypt the ciphertext ( v,e).
Without making much stronger assumptions, this scheme does not even satisfy a weaker notion
of CCA security, analogous to that considered in Exercise 12.28, in which the attacker has unfettered
access to the decryption servers before getting its hands on the ciphertext it really wants to decrypt.
Indeed, such an attacker may submit an arbitrary ciphertext (ˆv,ˆe) to the decryption servers, and
from the resulting decryption shares, it can reconstruct the group element ˆvα. Thus, access to the
decryption servers essentially gives the adversary an oracle for computing ˆ vα for arbitrary group
elements ˆv. This implies security in this weaker sense depends on an assumption at least as strong
as the static DH assumption , discussed in Section 16.1.4. The precise attack game is the same as
Attack Game 16.1, except that after making a sequence of SDH queries, the challenger presents
the adversary with a random challenge v∗∈G and the adversary wins the game if it can compute
948
(v∗)α. As we saw in Section 16.1.4, in some settings, there are attacks on static DH that are much
more eﬃcient than CDH or ICDH.
While this threshold decryption scheme is not CCA secure, it does in fact satisfy a notion of
CPA security, which is explored in Exercise 22.8.
22.3.3 A secure threshold decryption scheme based on CDH
It is not too diﬃcult modify the insecure threshold scheme discussed in Section 22.3.2 to obtain a
secure threshold scheme. The conceptually easiest way to do this makes use of an eﬃcient algorithm
ODDH that solves the decision Diﬃe-Hellman (DDH) problem in the group G, we we did for BLS
signatures in Section 22.2.2.
We shall present here a threshold decryption scheme that makes use of such an algorithmODDH.
This can be implemented using a pairing, and one can also devise a variation of the scheme that
uses an asymmtric pairing. However, unlike the sitiation with BLS, it turns out that we do not
really need to use a pairing at all — we can instead use a non-interactive proof system for proving
that a given triple is a DH-triple. We shall describe this in Section 22.3.6.
22.3.3.1 The GS encryption scheme
We begin by presenting an ordinary (non-threshold) encryption scheme, which we call the GS
encryption scheme, also denoted EGS.
This scheme has a message spaceMand associated data space D, and makes use of the following
components:
• a cyclic group G of prime order q generated by g ∈ G; we shall assume that the CDH
assumption holds in G;
• a symmetric cipher Es = (Es,Ds) deﬁned over ( K,M,Cs); we shall assume that Es is seman-
tically secure;
• a hash function HK: G ×G →K; we shall model HKas a random oracle;
• a hash function HG : G ×D→ G; we shall model HG as a random oracle;
• an eﬃcient algorithm ODDH that on input ( gα,gβ,gγ) that outputs accept if γ = αβ, and
otherwise outputs reject.
Key generation. The key generation algorithm computes
α∈Zq, u←gα,
and outputs the public key u and the secret key α.
Encryption. To encrypt a message m ∈M with associated data d ∈D under the public key
u∈G, the encryption algorithm computes
β ←R Zq, v←gβ, w←uβ, k←H(v,w), e←R Es(k,m),
u←HG(v,d), w←uβ,
output c:= (v,e,w ).
949
Decryption. To decrypt a ciphertext c = (v,e,w ) with associated data d under the secret key
α, the decryption ﬁrst checks that
ODDH(u,v,w ) = accept, where u:= HG(v,d). (22.1)
If this check passes, we say c is valid with respect to d, and the decryption algorithm computes
w←vα, k←H(v,w), m←Ds(k,e)
and outputs m. If this check does not pass, the decryption algorithm outputs reject.
Discussion. This scheme is basically the same as the normal ElGamal encryption scheme, except
that the encryption algorithm essentially proves that it knowsβby raising a challenge group element
u:= HG(v,d) to the power β. The decryption algorithm veriﬁes that this was done by performing
the validity check in (22.1).
The scheme EGS was in fact presented in Exercise 12.20, where you were asked to show that it
AD-only CCA-secure under the assumptions listed above. In fact, in that exercise, you were asked
to prove security against a stronger form of CCA attack in which when an adversary submits a
decryption query (ˆc, ˆd), where ˆc = (ˆv,ˆe,ˆw) is valid with respect to ˆd, the challenger returns the
group element ˆw := ˆuα. This stronger security property is crucial to converting this scheme to a
threshold scheme. We shall refer to this security property it as AD-only strong-CCA security.
22.3.3.2 Converting the GS scheme into a threshold decryption scheme
To make the GS scheme support threshold decryption, we will apply Shamir’s secret sharing scheme
(Gsh,Csh) to the secret key α ∈ Zq. We will show how to decrypt a ciphertext without ever
reconstituting the value α at a single location.
The scheme. We now present the threshold GS decryption scheme, also denoted EthGS =
(G,E,D,C ).
• G(N,t) →(pk,pkc,sk1,..., skN):
– compute
α←R Zq, u←gα,
and set pk := u;
– run Gsh(N,t,α ) to obtain N shares α1,...,α N in Zq;
Note: recall that for some polynomialω∈Zq[x] of degree at mostt−1, we haveω(0) = α
and ω(i) = αi for i= 1,...,N ;
– for i= 1,...,N , compute ui ←gαi;
– for i= 1,...,N , set ski := αi;
– set pkc := (u1,...,u N);
– output (pk, pkc, sk1,..., skN).
• E(pk,m,d ) →c, where pk = u:
950
– set c:= (v,e,w ), where
β ←R Zq, v←gβ, w←uβ, k←H(v,w), e←R Es(k,m),
u←HG(v,d), w←uβ.
Note: this encryption algorithm is identical to that in EGS.
• D(ski,c,d ) →c′
i, where ski = αi and c= (v,e,w ): check if c is valid with respect to d, as in
(22.1); if not, output c′
i := reject; otherwise, compute wi ←vαi and output c′
i := wi ∈G.
• C(pkc,c,d, J,{c′
j}j∈J) → m, where pkc = ( u1,...,u N), c = ( v,e,w ), J is a subset of
{1,...,N }of size t, and where c′
j = wj ∈G or c′
j = reject for j ∈J:
– Step 1: validate the ciphertext and the decryption shares:
∗ check that c is valid with respect to d, as in (22.1): if not, output reject and halt;
∗ check if for some j ∈J, we have either c′
j = reject or
c′
j = wj and ODDH(uj,v,w j) = reject;
if so, output blame(J∗) and halt, where J∗is the set of all such j ∈J;
Note: assuming all the shares are valid, then for j ∈J, we have c′
j = wj = vαj = vω(j),
where ω∈Zq[x] is the polynomial of degree at most t−1 used to share the secret key
α;
– Step 2: compute the ephemeral key w: “interpolate in the exponent” by applying Corol-
lary 22.2 with the given setJand j∗:= 0 to compute the ephemeral keyw:= vα = vω(0);
more explicitly, w is computed as
w←
∏
j∈J
wλj
j ∈G
where the exponents {λj}j∈J are the Lagrange interpolation coeﬃcients, which depend
only on the set J, as computed in Lemma 22.1;
– Step 3: compute k←HK(v,w) and output m←Ds(p,e).
22.3.4 Threshold decryption security
Before analyzing the GS threshold decryption scheme (and its variants), we deﬁne what it means
for a t-out-of-N threshold decryption scheme to be CCA secure. The security model gives the
adversary two capabilities:
• We allow the adversary to completely control up to t−1 of the decryption servers. The
adversary must declare at the beginning of the game a set Lof up to t−1 servers that it
wants to control.2
2Just as for threshold signatures, this is a static corruption model.
951
• In addition, the combiner is not a trusted party, and may be controlled by the adversary. This
means that a scheme that temporarily reconstitutes the decryption key at the combiner would
be insecure. To capture the idea that the combiner may be controlled by the adversary, we
allow the adversary to request decryptions on ciphertexts of its choice, and we then give the
adversary all N decryption shares generated by the decryption servers in response to these
requests.
Even with control of the combiner, and up to t−1 servers, the adversary should not be able to
break the AD-only CCA security of the scheme. Formally, we deﬁne security using the following
attack game.
Attack Game 22.5 (threshold AD-only CCA security). For a public-key threshold decryp-
tion scheme E= (G,E,D,C ) deﬁned over ( M,D,C), and for a given adversary A, we deﬁne two
experiments.
Experiment b (b= 0,1):
• Setup: the adversary sends poly-bounded allowable parameters N and t, where 0 < t≤N,
and a subset L⊆{ 1,...,N }of size t−1 to the challenger. The challenger runs
(pk,pkc,sk1,..., skN) ←R G(N,t),
and sends pk, pkc, and {skℓ}ℓ∈Lto A.
• Athen makes a series of queries to the challenger. Each query can be one of two types:
– Encryption query: for i = 1 ,2,..., the ith encryption query consists of a triple
(mi0,mi1,di) ∈ M2 ×D, where the messages mi0,mi1 are of the same length. The
challenger computes ci ←R E(pk,mib,di) and sends ci to A.
– Decryption query: for j = 1,2,..., the jth decryption query consists of a pair (ˆcj, ˆdj) ∈
C×D such that ˆdj is not among the associated data values d1,d2,... submitted in
previous encryption queries. The challenger computes the N decryption shares c′
j,i ←
D(ski,ˆcj, ˆdj), for i= 1,...,N , and sends these to A.
• At the end of the game, Aoutputs a bit ˆb∈{0,1}.
If Wb is the event that Aoutputs 1 in Experiment b, deﬁne A’s advantage with respect to Eas
thCCAadoadv[A,E] :=
⏐⏐⏐Pr[W0] −Pr[W1]
⏐⏐⏐. 2
Deﬁnition 22.12 (threshold AD-only CCA security). A public-key threshold decryption
scheme Eis AD-only CCA secure if for all eﬃcient adversaries A, the value thCCAadoadv[A,E]
is negligible.
Remark 22.10 (from AD-only to full CCA security). As already discussed at the beginning
of Section 22.3, AD-only CCA security is suﬃcient for many applications of threshold decryption.
However, one can also deﬁne a notion of (full) CCA security for threshold decryption schemes.
The only diﬀerence is that in the attack game, when submitting a decryption query (ˆ cj, ˆdj), the
restriction is that the pair (ˆ cj, ˆdj) must not appear among the ciphertext/associated data pairs
952
(c1,d1),(c2,d2),... corresponding to previous encryption queries. Any AD-only CCA-secure thresh-
old decryption scheme can be easily converted into a fully CCA-secure scheme using the simple
technique described in Exercise 14.13 of “wrapping” the ciphertext with a strongly one-time secure
signature. You are asked to work out the details of this in Exercise 22.11. 2
22.3.4.1 Robust threshold decryption
Beyond security for the scheme, a threshold decryption scheme should also be robust against a
malicious decryption server that is trying to disrupt the decryption process. We deﬁne two such
robustness properties that any practical threshold decryption scheme should provide.
Suppose a combiner receives a collection {c′
j}j∈J of t decryption shares on a given cipher-
text/associated data pair ( c,d). By Deﬁnition 22.11, the combiner will output either a plaintext
m∈M∪{ reject}, or a special message blame(J∗), where J∗is a nonempty subset of J.
• If the output of the combiner is blame(J∗), then we would like it to be the case that all of the
decryption shares c′
j for j ∈J∗are “bad”, in the sense that they were incorrectly generated
by misbehaving decryption servers. A threshold signature scheme that guarantees that this
always happens is said to provide accurate blaming.
• Otherwise, if the output of the combiner is a plaintext m, we say that the collection of shares
{c′
j}j∈J is valid for (c,d). We say that the threshold decryption scheme is consistent if it
is infeasible to come up with any other valid collection of decryption shares that combines to
yield a plaintext m′̸= m.
For many threshold decryption schemes, including the robust GS scheme, shares are indi-
vidually validated, but there is some non-zero, but negligible, probability that an incorrectly
generated share passes the validity test, and so the consistency property is violated with
negligible probability.
A threshold decryption scheme is robust if it satisﬁes both properties.
In practice, robustness allows a combiner who is trying to decrypt a ciphertext to proceed as
follows. If the combiner algorithm outputs blame(J∗), then the combiner can discard the “bad”
shares in J∗, and seek out t−|J∗|“good” shares from among the remaining decryption servers. As
long as there are t correctly behaving servers available, the accurate blaming property guarantees
that the combiner can repeat this process until it gets a valid collection of shares, and the consistency
property guarantees that when this happens, the resulting plaintext is the same that it would get
from any other valid collection of decryption shares.
Also note that the consistency property, together with the correctness property in Deﬁni-
tion 22.11, guarantees that if a ciphertext is a correctly generated encryption of a message, then
any valid collection of decryption shares when combined will yield the original message.
We now deﬁne these two properties formally. The accurate blaming property for threshold
decryption is almost identical to the corresponding property for threshold signatures.
Deﬁnition 22.13 (accurate blaming). We say that a threshold decryption scheme E =
(G,E,D,C ) provides accurate blaming if the following holds:
for all possible outputs (pk,pkc,sk1,..., skN) of G(N,t), all ciphertext/associated data
pairs (c,d), all t-size subsets J of {1,...,N }, and all collections of decryption shares
953
{c′
j}j∈J:
C(pkc,c,d, J,{c′
j}j∈J) = blame(J∗) =⇒ Pr
[
D(skj,c,d ) = c′
j
]
= 0 for all j ∈J∗.
The consistency property is deﬁned using an attack game.
Attack Game 22.6 (consistent threshold decryption). For a given threshold decryption
scheme E= (G,E,D,C ), deﬁned over (M,D,C) and a given adversary A, we deﬁne the following
attack game.
• The adversary sends to the challenger poly-bounded allowable parameters N and t, where
0 <t ≤N.
• The challenger runs (pk,pkc,sk1,..., skN) ←R G(N,t) and sends all this data to the adversary.
• The adversary outputs
(c,d, J1,{c′
1j}j∈J1,J2,{c′
2j}j∈J2),
where c∈C, d∈D, J1 and J2 are subsets of {1,...,N }of size t, and {c′
1j}j∈J1 and {c′
2j}j∈J2
are collections of decryption shares.
• We say the adversary wins the game if
– C(pkc,c,d, J1,{c′
1j}j∈J1) = m1 ∈M∪{ reject},
– C(pkc,c,d, J2,{c′
2j}j∈J2) = m2 ∈M∪{ reject}, and
– m1 ̸= m2.
We deﬁne A’s advantage with respect to S, denoted conPKE adv[A,E], as the probability that A
wins the game. 2
Deﬁnition 22.14 (consistent threshold decryption). We say that a threshold decryption
scheme E is consistent if for all eﬃcient adversaries A, the quantity conPKEadv[A,E] is neg-
ligible.
To exercise the deﬁnition, the reader should show that the generic threshold decryption scheme
from Section 22.3.1 is not consistent.
Deﬁnition 22.15 (robustness). We say that a threshold decryption scheme is robust if it pro-
vides accurate blaming and is consistent.
22.3.5 Security of threshold GS
Next, we argue that the GS threshold decryption scheme is AD-only CCA secure. Security follows
directly from the AD-only strong-CCA security of the non-threshold GS scheme, which you were
asked to establish in Exercise 12.20. Let us denote by sCCA adoadv[A,EGS] the advantage of an
adversary Ain attacking EGS in the strengthened AD-only CCA attack game Section 22.3.3.1, in
which a decryption query yields the group element from which the symmetric decryption key is
derived, rather than just a plaintext.
Theorem 22.7. If EGS is AD-only strong-CCA secure, then EthGS is an AD-only CCA-secure
threshold decryption scheme.
954
In particular, for every adversary Athat attacks EthGS as in Attack Game 22.5, there exists an
adversary B, where Bis an elementary wrapper around A, that attacks EGS as in the strengthened
AD-only CCA attack game, such that
thCCAadoadv[A,EthGS] = sCCAadoadv[B,EGS].
Proof. We design Bto play the role of challenger to A. Recall that Abegins by outputting the
parameters N and t, and a subset Lof size t−1 of {1,...,N }. Now, when Breceives pk = u,
where u= gα, from its own challenger, Bneeds to provide to Anot only pk, but also the key shares
{αℓ}ℓ∈L, as well as pkc = (u1,...,u N), where ui = gαi for i= 1,...,N .
To do this, Bsets αℓ ←R Zq for each ℓ∈L. By Theorem 22.3, these key shares have the same
distribution as in an actual run of Attack Game 22.5. We know that there is a unique polynomial
ω∈Zq[x] of degree at most t−1 such that ω(0) = α and ω(i) = αi for i= 1,...,N .
For ℓ ∈ L, adversary B can compute uℓ = gαℓ directly, since it knows the value αℓ. For
i ∈{1,...,N }\L, adversary Bcan compute ui via “interpolation in the exponent”, by applying
Corollary 22.2 with J := L∪{0}and j∗:= i to compute ui = gω(i) from the collection of group
elements {gω(j)}j∈J; more explicitly, Bcomputes ui as
ui ←uλi0 g
∑
ℓ∈Lαℓ·λiℓ, (22.2)
where the λij’s are the Lagrange interpolation coeﬃcients computed as in Lemma 22.1.
Next, whenever Asubmits an encryption query, adversary Bforwards this query to its own
challenger, and passes back to Awhatever ciphertext it generates.
Whenever Aissues a decryption query (ˆc, ˆd), adversary Bforwards this query to its own chal-
lenger, and gets back ˆw ∈G ∪{reject}. Adversary Bneeds to send to Aall the corresponding
decryption shares, without knowing the secret key α.
If ˆw= reject, all decryption shares are set to reject.
Otherwise, ˆw∈G; moreover, if ˆc= (ˆv,ˆe,ˆw), then we have ˆw= ˆvα = ˆvω(0).
For i = 1,...,N , the ith decryption share is ˆwi = ˆvαi = ˆvω(i). For ℓ ∈L, adversary Bcan
compute ˆwℓ = ˆvαℓ directly, since it knows the value αℓ. For i ∈{1,...,N }\L , adversary Bcan
compute ˆwi as
ˆwi ←ˆwλi0 ˆv
∑
ℓ∈Lαℓ·λiℓ,
where the λij’s are precisely the same Lagrange interpolation coeﬃcients as used in (22.2).
When eventually Aoutputs a bit ˆb, our adversary Boutputs the same bit. One sees that Bhas
the same advantage in its attack game that Ahas in its attack game, which completes the proof.
2
The next theorem establishes the robustness of EthGS.
Theorem 22.8. EthGS is a robust threshold decryption scheme. In particular, every adversary has
advantage zero in Attack Game 22.6.
Proof. For accurate blaming, it is easy to see that a correctly generated decryption share will
never be ﬂagged as invalid. Conversely, for consistency, it is easy to see that any valid collection
of decryption shares will combine to yield the same plaintext as would be output by the regular
(non-threshold) GS decryption scheme. 2
955
22.3.6 A pairing-free version of EthGS
As already mentioned, we do not really need ODDH (or pairings) to get an eﬃcient and secure
threshold decryption scheme. In fact, we can easily modify EthGS, replacing the each use of ODDH
by a non-interactive zero-knowledge proof that a given triple is a DH-triple.
22.3.6.1 E†
GS: a pairing-free version of EGS
Let us begin by describing a variant of EGS, denoted E†
GS, which is a regular (non-threshold) encryp-
tion scheme.
Just like EGS, the scheme E†
GS has a message space Mand associated data space D, and makes
use of the following components:
• a cyclic group G of prime order q generated by g ∈ G; we shall assume that the CDH
assumption holds in G;
• a symmetric cipher Es = (Es,Ds) deﬁned over ( K,M,Cs); we shall assume that Es is seman-
tically secure;
• a hash function HK: G ×G →K; we shall model HKas a random oracle;
• a hash function HG : G ×D→ G; we shall model HG as a random oracle.
The scheme E†
GS also makes use of a non-interactive proof system Φ = ( GenPrf ,VrfyPrf ) for
the relation
R= {( β, (u,v,w )) ) ∈ Zq ×G3 : v= gβ, w= uβ };
We presented the details of such a proof system in Section 20.3.6 based on the application of
Fiat-Shamir transform to the Chaum-Pedersen protocol (see Section 19.5.2). As discussed in Sec-
tion 20.3.6, this proof system (as well as the optimized version) provides both soundness (as in
Deﬁnition 20.4) and zero knowledge (as in Deﬁnition 20.5).
Key generation. The key generation algorithm computes
α∈Zq, u←gα,
and outputs the public key u and the secret key α. This is exactly the same as the key generation
algorithm for EGS.
Encryption. To encrypt a message m ∈M with associated data d ∈D under the public key
u∈G, the encryption algorithm computes
β ←R Zq, v←gβ, w←uβ, k←H(v,w), e←R Es(k,m),
u←HG(v,d), w←uβ,
π←R GenPrf ( β, (u,v,w ) )
output c:= (v,e,w ,π).
This is the same as the encryption algorithm for EGS, except for the additional proof element π in
the ciphertext that proves that ( u,v,w ) is a DH-triple.
956
Decryption. To decrypt a ciphertext c= (v,e,w ,π) with associated data dunder the secret key
α, the decryption ﬁrst checks that
VrfyPrf ( (u,v,w ), π) = accept, where u:= HG(v,d). (22.3)
If this check passes, we say c is valid with respect to d, and the decryption algorithm computes
w←vα, k←H(v,w), m←Ds(k,e)
and outputs m. If this check does not pass, the decryption algorithm outputs reject. This is the
same as the decryption algorithm for EGS, except that rather than invoking ODDH, the proof π is
veriﬁed using VrfyPrf to ensure that ( u,v,w ) is a DH-triple.
Discussion. The scheme E†
GS was in fact presented in Exercise 20.19, where you were asked to to
show that it is AD-only strong-CCA secure under the assumptions listed above.
22.3.6.2 Converting E†
GS to a pairing-free threshold decryption scheme
We can easily convert E†
GS to a pairing-free threshold decryption scheme, which we denote E†
thGS =
(G,E,D,C ). The scheme is very similar to the scheme EthGS presented in Section 22.3.3.2. The
only realy diﬀerence is that we use the proof system Φ rather than ODDH to verify that certain
triples are DH-triples.
• G(N,t) →(pk,pkc,sk1,..., skN):
– compute
α←R Zq, u←gα,
and set pk := u;
– run Gsh(N,t,α ) to obtain N shares α1,...,α N in Zq;
Note: recall that for some polynomialω∈Zq[x] of degree at mostt−1, we haveω(0) = α
and ω(i) = αi for i= 1,...,N ;
– for i= 1,...,N , compute ui ←gαi;
– for i= 1,...,N , set ski := (αi,ui);
– set pkc := (u1,...,u N);
– output (pk, pkc, sk1,..., skN).
Note: this key generation algorithm is identical to that in EthGS, except that ski includes ui;
this is not strictly necessary, as ui could also be computed from αi.
• E(pk,m,d ) →c, where pk = u:
– set c:= (v,e,w ), where
β ←R Zq, v←gβ, w←uβ, k←H(v,w), e←R Es(k,m),
u←HG(v,d), w←uβ,
π←R GenPrf ( β, (u,v,w ) ).
957
Note: this encryption algorithm is identical to that in E†
GS.
• D(ski,c,d ) →c′
i, where ski = αi and c= (v,e,w ,π): check if c is valid with respect to d, as
in (22.3); if not, output c′
i := reject; otherwise, compute
wi ←vαi, π′
i ←GenPrf ( αi, (ui,v,w i) ),
where ui := gαi, and output c′
i := (wi,π′
i).
Note: this decryption algorithm is identical to that in EthGS, except for the validity test (22.3),
which veriﬁes the proof π, rather than using ODDH to verify that (u,v,w ) is a DH-triple, and
except for the addition of the proof π′
i to prove that (ui,v,w i) is a DH-triple.
• C(pkc,c,d, J,{c′
j}j∈J) →m, where pkc = ( u1,...,u N), c = ( v,e,w ,π), J is a subset of
{1,...,N }of size t, and where c′
j = wj ∈G or c′
j = reject for j ∈J:
– Step 1: validate the ciphertext and the decryption shares:
∗ check that c is valid with respect to d, as in (22.3): if not, output reject and halt;
∗ check if for some j ∈J, we have either c′
j = reject or
c′
j = (wj,πj) and VrfyPrf ( (uj,v,w j), π′
j ) = reject;
if so, output blame(J∗) and halt, where J∗is the set of all such j ∈J;
Note: assuming all the shares are valid, then for j ∈J , we have c′
j = ( wj,πj), and
with overwhelming probability (based on the soundness of the proof system π), we have
wj = vαj = vω(j), where ω ∈Zq[x] is the polynomial of degree at most t−1 used to
share the secret key α;
– Step 2: compute the ephemeral key w: “interpolate in the exponent” by applying
Corollary 22.2 with the given set J and and j∗ := 0 to compute the ephemeral key
w:= vα = vω(0);
more explicitly, w is computed as
w←
∏
j∈J
wλj
j ∈G
where the exponents {λj}j∈J are the Lagrange interpolation coeﬃcients, which depend
only on the set J, as computed in Lemma 22.1;
– Step 3: compute k←HK(v,w) and output m←Ds(p,e).
Note: this combiner algorithm is identical to that in EthGS, except that it uses π to verify
that (u,v,w ) is a DH-triple and π′
j to verify that (uj,v,w j) is a DH-triple, rather than using
ODDH.
We next state a theorem on the security of E†
thGS. The security of E†
thGS relies on the security
of E†
GS as well as the zero-knowledge property of the proof system Φ, which is used in the de-
cryption algorithm of the threshold scheme. We have formulated the notion of zero knowledge in
Section 20.3.5 explicitly in the random oracle model. Moreover, the proof system Φ is also used
in the implementation of E†
GS. Therefore, in the following theorem, we state things in terms of the
security of E†
GS in the random oracle model.
958
Theorem 22.9. If E†
GS is AD-only strong-CCA secure in the random oracle model, then E†
thGS is
an AD-only CCA-secure threshold decryption scheme in the random oracle model.
In particular, if Sim be the zero knowledge simulator for Φ (as in Theorem 20.3), then for every
adversary Athat attacks E†
thGS as in the random oracle version of Attack Game 22.5, there exists
adversaries BGS and Bzk, such that are elementary wrappers around A, where BGS attacks E†
GS
as in the random oracle model of the strengthened AD-only CCA attack game, and Bzk attacks
the zero-knowledge property of Φ as in Attack Game 20.3, where
thCCAro
adoadv[A,E†
thGS] ≤sCCAro
adoadv[BGS,E†
GS] + 2·niZKadv[Bzk,Φ,Sim].
The next two theorems establish the robustness of EthGS.
Theorem 22.10. E†
thGS provides accurate blaming.
Proof. This follows directly from the fact that each non-interactive proof generated by a signing
server is always valid. 2
Theorem 22.11. E†
thGS is consistent, assuming Φ is existentially sound.
Proof. This follows directly from the existential soundness of the non-interactive proofs generated
by the signing servers. Indeed, to the extent that the validity of these proofs ensures that the
corresponding statements are true, the output of the combiner algorithm will be the same as that
of the regular (non-threshold) GS decryption scheme. 2
22.4 Distributed key generation
22.4.1 Deﬁning the problem
In our discussion of threshold cryptography so far we assumed that a trusted party runs the key
generation process to generate secret keys. The trusted party then hands out secret keys to the
signing parties in a threshold signature scheme, or to the decryption parties in a threshold de-
cryption scheme. In this section we show how the parties can engage in a distributed protocol to
generate the secret keys themselves, without the help of a trusted party.
Throughout the remainder of Section 22.4, the following notation is ﬁxed:
• q is a prime;
• G is a group of order q generated by g∈G;
• N,t,L are integers with 0 ≤L<t ≤N <q.
As usual, N represents the number of parties, and t represents the reconstruction threshold, i.e.,
the number of shares that need to be combined to reconstruct a secret. The value L is new: it
represents a bound on the number of corrupt parties. Up until now, we have been assuming that
L= t−1, but in this section, we take a somewhat more general view. All of these values are viewed
as system parameters, which are available to any protocol or algorithm.
959
Additional notation. We introduce some notation that we will use throughout Section 22.4.
Suppose ω = κ0 + κ1x+ ··· + κt−1xt−1 ∈Zq[x] is some polynomial of degree less than t with
coeﬃcients in Zq. We deﬁne the corresponding vector of group elements
gω := (gκ0,...,g κt−1) ∈Gt. (22.4)
In addition, if U = (u0,...,u t−1) ∈Gt is a vector of group elements, and β ∈Zq, we deﬁne the
group element
U(β) :=
t−1∏
j=0
uβj
j . (22.5)
With this notation, we have the relation
(gω)(β) = gω(β).
The dealer algorithm. For both the threshold signature scheme in Section 22.2.2 and the thresh-
old decryption schemes in Sections 22.3.3 and 22.3.6, the core of the trusted key generation process
can be implemented in terms of following dealer algorithm.
Algorithm dealer() takes no input, and outputs an N-tuple (out1,..., outN), computed
as follows:
compute κ0,...,κ t−1 ←R Zq
set ω:= κ0 + ··· + κt−1xt−1 ∈Zq[x] and U := gω ∈Gt
for i= 1,...,N : set outi := (U,ω(i))
Note that the ﬁrst component of each outi value is the same, namely, U := gω, while the second
component ω(i) is party Pi’s share of the secret ω(0) = κ0 ∈Zq. Note also that given U and any
β ∈Zq, the group element gω(β) can be eﬃciently computed as U(β). We shall sometimes denote
by Pub(outi) the ﬁrst component of outi, the “public part” of outi, which is the value U.
What we want is a protocol Π that allows N parties P1,...,P N to implement the logic of
the dealer algorithm in a secure distributed fashion, without relying on a trusted party. Such a
protocol Π should work reliably and securely even if L parties are corrupt. We shall also assume
that N −L≥t. Note that this implies that the honest parties themselves have enough shares to
reconstruct the secret, which is essential if we want the protocol to succeed without the help of the
corrupt parties. Indeed, the protocol we present does guarantee this.
The assumption that N−L≥talso implies that N >2L. However, for reasons to be discussed
below, we may need to additionally assume N >3L in order to implement a critical subprotocol.
We shall also need to rely on a weak form of trusted system setup. Namely, we will assume
decentralized key provisioning, by which we mean the following:
• Each honest party Pi generates their own public-key/private key pair ( pki,ski), which gen-
erally speaking may be keys for an encryption scheme and/or a signature scheme (or several
such schemes).
• Each corrupt party Pℓ generates its public key pkℓ in an arbitrary fashion, possibly depending
on the public keys of the honest parties.
• Each honest party is then provisioned with the tuple of public keys ( pk1,..., pkN).
960
We have already used this notion previously in the chapter, but without deﬁning it very carefully
(see, for example, Remark 22.3). One way to realize semi-trused key provisioning is to use a
certiﬁcate authority (CA). Alternatively, it can be done by some ad hoc manual process.
During the execution of protocol Π, if one honest party sends a message to another honest
party, we shall assume that this message is eventually delivered; however, we shall not assume
any particular bound on the amount of time it takes for a message to be delivered. Such an
asynchronous network model is the most realistic for a protocol distributed over a wide area
network. However, it does make the design of such a protocol more challenging, since it is impossible
for a party Pi who is waiting a message from Pj to distinguish between the case that the network
is just very slow (and the message will eventually arrive) and the case that Pj is corrupt (and may
never send the message).
After the protocol starts running, each honest party Pi should eventually output the value outi.
Because of the asynchronous network model, the honest parties will not produce their outputs all
at the same time. The precise timing and ordering of the outputs is dictated by the protocol Π, by
the behavior of the corrupt parties, and by the behavior of the communication network. We will
think of the corrupt parties and the communication network as being under the control of a single
adversary, who is coordinating an attack.
We will state the security requirements more formally in a moment. However, the salient
properties are that:
• the outputs of all parties should have essentially the same joint distribution as if they had
been generated for them by dealer();
• the corrupt parties should jointly learn essentially nothing more than the outputs that would
be generated for them by dealer().
22.4.1.1 Using a DKG protocol
At a high level, the idea is that we want to use a secure DKG protocol Π as a “drop in” replacement
for the key generation algorithm Gused in Deﬁnitions 22.3 and 22.11 when instantiating these deﬁ-
nitions with the BLS threshold signature scheme in Section 22.2.2 and the GS threshold decryption
schemes in Sections 22.3.3 and 22.3.6 (respectively). For concreteness, we shall focus here on the
BLS threshold signature scheme; however, everything we say translates directly to the GS threshold
decryption schemes, and indeed applies to any threshold scheme with a similar structure.
While this high-level idea is generally accurate, there are a few wrinkles.
• Instead of viewing N and t as inputs to G, we now view them (as well as L) as system
parameters. The reason is that all of the participants of protocol Π need to agree in advance
on these values, and the easiest way to do this is to view these values as system parameters.
• The public key pk and combiner public key pkc must be available to the relevant entities.
The public key pk is needed by any entity who needs to verify a signature. The combiner
public key pkc is needed by any entity who needs to play the role of combiner.
In our original formulation, we assumed a trusted party that ran algorithm Gand then either
securely distributed these keys to all relevent entities, or securely published them in some
location accessible by all relevent entities. But now, these keys are computed by the parties
P1,...,P N running the DKG protocol Π. Speciﬁcally each party Pi computes and outputs
961
protocol messages
<latexit sha1_base64="AK0D+ptQzRO/kWfiXzxiv2maX1g=">AAACEXicbVDLSsNAFJ3UV62vqks3g0XoqiQi2mXBje4q2Ac0IUymk3boZBJmboQS8gtu/BU3LhRx686df+Ok7UJbD8xwOOde7r0nSATXYNvfVmltfWNzq7xd2dnd2z+oHh51dZwqyjo0FrHqB0QzwSXrAAfB+oliJAoE6wWT68LvPTCleSzvYZowLyIjyUNOCRjJr9bdDLsRgTGHLE4h9zl2cz8zP5dzgxKR3ea5X63ZDXsGvEqcBamhBdp+9csdxjSNmAQqiNYDx07Ay4gCTgXLK26qWULohIzYwFBJIqa9bHZRjs+MMsRhrMyTgGfq746MRFpPo8BUFjvqZa8Q//MGKYRNL+MySYFJOh8UpgJDjIt48JArRkFMDSFUcbMrpmOiCAUTYsWE4CyfvEq65w3nsmHfXdRazUUcZXSCTlEdOegKtdANaqMOougRPaNX9GY9WS/Wu/UxLy1Zi55j9AfW5w8b9p3L</latexit>
{ out i } i 2 I
<latexit sha1_base64="9GAr40ZFhWZLz6ilPXPS3BX5TCE=">AAAB8nicbVBNS8NAFHypX7V+VT16WSyCp5KIqMeqF48VrC2koWy223bpZhN2X4QS+jO8eFDEq7/Gm//GTZuDtg4sDDPvsfMmTKQw6LrfTmlldW19o7xZ2dre2d2r7h88mjjVjLdYLGPdCanhUijeQoGSdxLNaRRK3g7Ht7nffuLaiFg94CThQUSHSgwEo2glvxtRHDEqs+tpr1pz6+4MZJl4BalBgWav+tXtxyyNuEImqTG+5yYYZFSjYJJPK93U8ISyMR1y31JFI26CbBZ5Sk6s0ieDWNunkMzU3xsZjYyZRKGdzCOaRS8X//P8FAdXQSZUkiJXbP7RIJUEY5LfT/pCc4ZyYgllWtishI2opgxtSxVbgrd48jJ5PKt7F3X3/rzWuCnqKMMRHMMpeHAJDbiDJrSAQQzP8ApvDjovzrvzMR8tOcXOIfyB8/kDcfSRXA==</latexit>
A
<latexit sha1_base64="aPNzPnlWRy+iTB3tZxTtB/uW8b8=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF48V7Qe0oWy2m3bpZhN2J0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekEhh0HW/ncLa+sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCajhUijeRIGSdxLNaRRI3g7GtzO//cS1EbF6xEnC/YgOlQgFo2ilh15D9MsVt+rOQVaJl5MK5Gj0y1+9QczSiCtkkhrT9dwE/YxqFEzyaamXGp5QNqZD3rVU0YgbP5ufOiVnVhmQMNa2FJK5+nsio5ExkyiwnRHFkVn2ZuJ/XjfF8NrPhEpS5IotFoWpJBiT2d9kIDRnKCeWUKaFvZWwEdWUoU2nZEPwll9eJa2LqleruveXlfpNHkcRTuAUzsGDK6jDHTSgCQyG8Ayv8OZI58V5dz4WrQUnnzmGP3A+fwAgw42y</latexit>
⇧
signing queries
signing 
oracle
<latexit sha1_base64="FZ/noyR5XjEAGG8kmF4Fe85HlOo=">AAACH3icbVDLSgMxFM3UV62vqks3wSLUTZkRqV0W3Oiugn1ApwyZNG1DM5khuSOUMH/ixl9x40IRcde/MX0stPVA4OSce7n3njARXIPrTp3cxubW9k5+t7C3f3B4VDw+aek4VZQ1aSxi1QmJZoJL1gQOgnUSxUgUCtYOx7czv/3ElOaxfIRJwnoRGUo+4JSAlYJi1TfYjwiMVGQaaZiV5x8OJk4hC/gl9rPAcOxzuSijRJj7LAuKJbfizoHXibckJbREIyh++/2YphGTQAXRuuu5CfQMUcCpYFnBTzVLCB2TIetaKknEdM/M78vwhVX6eBAr+yTgufq7w5BI60kU2srZjnrVm4n/ed0UBrWe4TJJgUm6GDRIBYYYz8LCfa4YBTGxhFDF7a6YjogiFGykBRuCt3ryOmldVbxqxX24LtVryzjy6AydozLy0A2qozvUQE1E0TN6Re/ow3lx3pxP52tRmnOWPafoD5zpDzhXo6w=</latexit>
{ Pub( out i ) } i 2 I
<latexit sha1_base64="2Ji0Rmt/aBrUWDtAU5m8DRpEg7U=">AAAB8nicbVDLSsNAFL2pr1pfVZdugkVwVRIR7bLgxoWLCvYBbSiT6aQdOpkJMzdCCf0MNy4UcevXuPNvnLRZaPXAwOGce5lzT5gIbtDzvpzS2vrG5lZ5u7Kzu7d/UD086hiVasraVAmleyExTHDJ2shRsF6iGYlDwbrh9Cb3u49MG67kA84SFsRkLHnEKUEr9QcxwQklIrubD6s1r+4t4P4lfkFqUKA1rH4ORoqmMZNIBTGm73sJBhnRyKlg88ogNSwhdErGrG+pJDEzQbaIPHfPrDJyI6Xtk+gu1J8bGYmNmcWhncwjmlUvF//z+ilGjSDjMkmRSbr8KEqFi8rN73dHXDOKYmYJoZrbrC6dEE0o2pYqtgR/9eS/pHNR96/q3v1lrdko6ijDCZzCOfhwDU24hRa0gYKCJ3iBVwedZ+fNeV+Olpxi5xh+wfn4Bn+pkV0=</latexit>
L
Figure 22.2: Attack Game 22.1 with a DKG protocol Π in place of G
outi = (U,ω(i)) where U = gω = Pub(outi). As already noted, given U and any β ∈Zq, any
entity can eﬃciently compute the group element gω(β) as U(β). This means that any entity
that knows U can eﬃciently compute pk and pkc.
Now, if one of the parties Pi needs pk or pkc, they can compute it themselves as soon as
it ﬁnishes protocol Π. However, suppose that an entity Q external to P1,...,P N needs to
obtain pk or pkc. Entity Q can simply ask for this information from P1,...,P N; however,
this must be done with some care, since we are assuming that L of the parties P1,...,P N
may be corrupt, and Q does not know who these corrupt parties are. Moreover, we are also
assuming an asynchronous network, and so Qcannot distinguish between a very slow network
and a party Pi that is corrupt and non-responsive. So Q can request this information from
all parties P1,...,P N, and wait for L+ 1 responses that all agree:
– since any set of L+ 1 parties must contain an honest party, Q can be sure that this
common value is correct;
– since N >2L, then N−L≥L+ 1 honest parties will eventualy respond to Q’s request,
so Q will not be left waiting forever.
Of course, all of this assumes that Qcan verify the authenticity of messages coming from each
party Pi. To achieve this, we may need to extend the decentralized key provisioning setup
assumption to include provisioning all relevent entities Q with the public key of each Pi.
The above discussion focused on the logistics of deploying a threshold signature or decryption
scheme that uses a DKG protocol Π. We also need to discuss brieﬂy how using a secure DKG
protocol relates to Deﬁnitions 22.4 and 22.12 for secure threshold signature and decryption schemes.
Again, for concreteness, we focus on secure threshold signature schemes, but as above, our remarks
apply to threshold decryption schemes, and more generally, to any threshold schemes with a similar
structure.
Fig. 22.2 shows how Attack Game 22.1 is modiﬁed to allow for a DKG protocol Π. Just as
in Attack Game 22.1, we have an adversary Aand a challenger; however, the attack game and
the challenger are split into two components: one component corresponds to the execution of the
962
protocol messages
<latexit sha1_base64="AK0D+ptQzRO/kWfiXzxiv2maX1g=">AAACEXicbVDLSsNAFJ3UV62vqks3g0XoqiQi2mXBje4q2Ac0IUymk3boZBJmboQS8gtu/BU3LhRx686df+Ok7UJbD8xwOOde7r0nSATXYNvfVmltfWNzq7xd2dnd2z+oHh51dZwqyjo0FrHqB0QzwSXrAAfB+oliJAoE6wWT68LvPTCleSzvYZowLyIjyUNOCRjJr9bdDLsRgTGHLE4h9zl2cz8zP5dzgxKR3ea5X63ZDXsGvEqcBamhBdp+9csdxjSNmAQqiNYDx07Ay4gCTgXLK26qWULohIzYwFBJIqa9bHZRjs+MMsRhrMyTgGfq746MRFpPo8BUFjvqZa8Q//MGKYRNL+MySYFJOh8UpgJDjIt48JArRkFMDSFUcbMrpmOiCAUTYsWE4CyfvEq65w3nsmHfXdRazUUcZXSCTlEdOegKtdANaqMOougRPaNX9GY9WS/Wu/UxLy1Zi55j9AfW5w8b9p3L</latexit>
{ out i } i 2 I
<latexit sha1_base64="W8OA4gYu7czb409045tuqIpBqlY=">AAAB8nicbVBNS8NAFHypX7V+VT16WSyCp5KIqMeiF48VbC2moWy223bpJht2X4QS+jO8eFDEq7/Gm//GTZuDtg4sDDPvsfMmTKQw6LrfTmlldW19o7xZ2dre2d2r7h+0jUo14y2mpNKdkBouRcxbKFDyTqI5jULJH8LxTe4/PHFthIrvcZLwIKLDWAwEo2glvxtRHDEqs8dpr1pz6+4MZJl4BalBgWav+tXtK5ZGPEYmqTG+5yYYZFSjYJJPK93U8ISyMR1y39KYRtwE2SzylJxYpU8GStsXI5mpvzcyGhkziUI7mUc0i14u/uf5KQ6ugkzESYo8ZvOPBqkkqEh+P+kLzRnKiSWUaWGzEjaimjK0LVVsCd7iycukfVb3Luru3XmtcV3UUYYjOIZT8OASGnALTWgBAwXP8ApvDjovzrvzMR8tOcXOIfyB8/kDl/GRdQ==</latexit>
Z
<latexit sha1_base64="aPNzPnlWRy+iTB3tZxTtB/uW8b8=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF48V7Qe0oWy2m3bpZhN2J0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekEhh0HW/ncLa+sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCajhUijeRIGSdxLNaRRI3g7GtzO//cS1EbF6xEnC/YgOlQgFo2ilh15D9MsVt+rOQVaJl5MK5Gj0y1+9QczSiCtkkhrT9dwE/YxqFEzyaamXGp5QNqZD3rVU0YgbP5ufOiVnVhmQMNa2FJK5+nsio5ExkyiwnRHFkVn2ZuJ/XjfF8NrPhEpS5IotFoWpJBiT2d9kIDRnKCeWUKaFvZWwEdWUoU2nZEPwll9eJa2LqleruveXlfpNHkcRTuAUzsGDK6jDHTSgCQyG8Ayv8OZI58V5dz4WrQUnnzmGP3A+fwAgw42y</latexit>
⇧
challenger
(a) The real world
<latexit sha1_base64="AK0D+ptQzRO/kWfiXzxiv2maX1g=">AAACEXicbVDLSsNAFJ3UV62vqks3g0XoqiQi2mXBje4q2Ac0IUymk3boZBJmboQS8gtu/BU3LhRx686df+Ok7UJbD8xwOOde7r0nSATXYNvfVmltfWNzq7xd2dnd2z+oHh51dZwqyjo0FrHqB0QzwSXrAAfB+oliJAoE6wWT68LvPTCleSzvYZowLyIjyUNOCRjJr9bdDLsRgTGHLE4h9zl2cz8zP5dzgxKR3ea5X63ZDXsGvEqcBamhBdp+9csdxjSNmAQqiNYDx07Ay4gCTgXLK26qWULohIzYwFBJIqa9bHZRjs+MMsRhrMyTgGfq746MRFpPo8BUFjvqZa8Q//MGKYRNL+MySYFJOh8UpgJDjIt48JArRkFMDSFUcbMrpmOiCAUTYsWE4CyfvEq65w3nsmHfXdRazUUcZXSCTlEdOegKtdANaqMOougRPaNX9GY9WS/Wu/UxLy1Zi55j9AfW5w8b9p3L</latexit>
{ out i } i 2 I
protocol
messagescontrol
messages
<latexit sha1_base64="W8OA4gYu7czb409045tuqIpBqlY=">AAAB8nicbVBNS8NAFHypX7V+VT16WSyCp5KIqMeiF48VbC2moWy223bpJht2X4QS+jO8eFDEq7/Gm//GTZuDtg4sDDPvsfMmTKQw6LrfTmlldW19o7xZ2dre2d2r7h+0jUo14y2mpNKdkBouRcxbKFDyTqI5jULJH8LxTe4/PHFthIrvcZLwIKLDWAwEo2glvxtRHDEqs8dpr1pz6+4MZJl4BalBgWav+tXtK5ZGPEYmqTG+5yYYZFSjYJJPK93U8ISyMR1y39KYRtwE2SzylJxYpU8GStsXI5mpvzcyGhkziUI7mUc0i14u/uf5KQ6ugkzESYo8ZvOPBqkkqEh+P+kLzRnKiSWUaWGzEjaimjK0LVVsCd7iycukfVb3Luru3XmtcV3UUYYjOIZT8OASGnALTWgBAwXP8ApvDjovzrvzMR8tOcXOIfyB8/kDl/GRdQ==</latexit>
Z
<latexit sha1_base64="r7XrCAbzUV5LzGLqKSi/dLRWHts=">AAAB+nicbVDLSsNAFJ34rPWV6tLNYBHqpiQi2mXBjcsK9gFtKJPJTTt0MgkzE6XEfoobF4q49Uvc+TdO2iy09cDA4Zx7uWeOn3CmtON8W2vrG5tb26Wd8u7e/sGhXTnqqDiVFNo05rHs+UQBZwLammkOvUQCiXwOXX9yk/vdB5CKxeJeTxPwIjISLGSUaCMN7cogInrMdBYA4SBntfOhXXXqzhx4lbgFqaICraH9NQhimkYgNOVEqb7rJNrLiNSMcpiVB6mChNAJGUHfUEEiUF42jz7DZ0YJcBhL84TGc/X3RkYipaaRbybzoGrZy8X/vH6qw4aXMZGkGgRdHApTjnWM8x5wwCRQzaeGECqZyYrpmEhCtWmrbEpwl7+8SjoXdfeq7txdVpuNoo4SOkGnqIZcdI2a6Ba1UBtR9Iie0St6s56sF+vd+liMrlnFzjH6A+vzBy6rk+k=</latexit>
dealer ()
challenger
(b) The ideal world
<latexit sha1_base64="YUu/ggKmwmyVKWdKcuO7nZoTWDE=">AAAB83icbVBNS8NAFHypX7V+VT16WSyCp5KIqMeiF48VrS00oWy223bpbhJ2X4QS+je8eFDEq3/Gm//GTZuDtg4sDDPv8WYnTKQw6LrfTmlldW19o7xZ2dre2d2r7h88mjjVjLdYLGPdCanhUkS8hQIl7ySaUxVK3g7HN7nffuLaiDh6wEnCA0WHkRgIRtFKvq8ojgRm90JNe9WaW3dnIMvEK0gNCjR71S+/H7NU8QiZpMZ0PTfBIKMaBZN8WvFTwxPKxnTIu5ZGVHETZLPMU3JilT4ZxNq+CMlM/b2RUWXMRIV2Ms9oFr1c/M/rpji4CjIRJSnyiM0PDVJJMCZ5AaQvNGcoJ5ZQpoXNStiIasrQ1lSxJXiLX14mj2d176Lu3p3XGtdFHWU4gmM4BQ8uoQG30IQWMEjgGV7hzUmdF+fd+ZiPlpxi5xD+wPn8AX3Fkfs=</latexit>
Sim
<latexit sha1_base64="2Ji0Rmt/aBrUWDtAU5m8DRpEg7U=">AAAB8nicbVDLSsNAFL2pr1pfVZdugkVwVRIR7bLgxoWLCvYBbSiT6aQdOpkJMzdCCf0MNy4UcevXuPNvnLRZaPXAwOGce5lzT5gIbtDzvpzS2vrG5lZ5u7Kzu7d/UD086hiVasraVAmleyExTHDJ2shRsF6iGYlDwbrh9Cb3u49MG67kA84SFsRkLHnEKUEr9QcxwQklIrubD6s1r+4t4P4lfkFqUKA1rH4ORoqmMZNIBTGm73sJBhnRyKlg88ogNSwhdErGrG+pJDEzQbaIPHfPrDJyI6Xtk+gu1J8bGYmNmcWhncwjmlUvF//z+ilGjSDjMkmRSbr8KEqFi8rN73dHXDOKYmYJoZrbrC6dEE0o2pYqtgR/9eS/pHNR96/q3v1lrdko6ijDCZzCOfhwDU24hRa0gYKCJ3iBVwedZ+fNeV+Olpxi5xh+wfn4Bn+pkV0=</latexit>
L
<latexit sha1_base64="2Ji0Rmt/aBrUWDtAU5m8DRpEg7U=">AAAB8nicbVDLSsNAFL2pr1pfVZdugkVwVRIR7bLgxoWLCvYBbSiT6aQdOpkJMzdCCf0MNy4UcevXuPNvnLRZaPXAwOGce5lzT5gIbtDzvpzS2vrG5lZ5u7Kzu7d/UD086hiVasraVAmleyExTHDJ2shRsF6iGYlDwbrh9Cb3u49MG67kA84SFsRkLHnEKUEr9QcxwQklIrubD6s1r+4t4P4lfkFqUKA1rH4ORoqmMZNIBTGm73sJBhnRyKlg88ogNSwhdErGrG+pJDEzQbaIPHfPrDJyI6Xtk+gu1J8bGYmNmcWhncwjmlUvF//z+ilGjSDjMkmRSbr8KEqFi8rN73dHXDOKYmYJoZrbrC6dEE0o2pYqtgR/9eS/pHNR96/q3v1lrdko6ijDCZzCOfhwDU24hRa0gYKCJ3iBVwedZ+fNeV+Olpxi5xh+wfn4Bn+pkV0=</latexit>
L
<latexit sha1_base64="3BUsjQeZbDLZI8P3r6l1SH/+mVM=">AAACI3icbVDLSsNAFJ3UV62vqEs3g0VwISUR0eKq4MaFiwr2AU0Jk+m0HTqZhJkboYT8ixt/xY0Lpbhx4b84abuorQcGzpxzL/feE8SCa3Ccb6uwtr6xuVXcLu3s7u0f2IdHTR0lirIGjUSk2gHRTHDJGsBBsHasGAkDwVrB6C73W89MaR7JJxjHrBuSgeR9TgkYybdvvZDAkBKRPmQX2Evx9M8hjRLIfI8Jgb3MT3PicYkXqjPfLjsVZwq8Stw5KaM56r498XoRTUImgQqidcd1YuimRAGngmUlL9EsJnREBqxjqCQh0910emOGz4zSw/1ImScBT9XFjpSEWo/DwFTmO+plLxf/8zoJ9KvdlMs4ASbpbFA/ERginAeGe1wxCmJsCKGKm10xHRJFKJhYSyYEd/nkVdK8rLjXFefxqlyrzuMoohN0is6Ri25QDd2jOmogil7QG/pAn9ar9W5NrK9ZacGa9xyjP7B+fgG1TqWI</latexit>
L , { out ` } ` 2 L
Figure 22.3: The real and ideal worlds for a secure DKG protocol
protocol Π, which replaces the “setup” phase of Attack Game 22.1, and the other component
corresponds to the signing queries in Attack Game 22.1.
Just as in Attack Game 22.1, the adversary Astarts out by selecting a subset Lof L:= t−1
corrupt parties (as already discussed above, the values N and t are now system parameters, and
are not chosen by A). But now, instead of running G, the challenger runs the protocol Π on behalf
of the honest parties Pi for i ∈I := {1,...,N }\L . Somewhat more precisely, as indicated in
Fig. 22.2, the adversary sends various protocol messages to the honest parties, and their responses
are sent back to the adversary. Thus, the communication network is essentially under the complete
control of the adversary.
As indicated in Fig. 22.2 by the lines labeled {outi}i∈I and {Pub(outi)}i∈I, when an honest
party Pi ﬁnishes the protocol Π, its outputouti is sent to a “signing oracle”, while the corresponding
public part Pub( outi) is given to the adversary. The security of protocol Π will ensure that all of
these public parts are actually the same; however, to keep things simple, let us deﬁne the public
key of the system to be pk = U(0), where U is public part output by the ﬁrst honest party that
ﬁnishes protocol Π.
Once an honest party Pi that has ﬁnished protocol Π, the adversary may make any number of
corresponding signing queries, obtaining Pi’s signature share on any message of its choosing. This
is indicated in Fig. 22.2 by the line labeled “signing queries”.
At the end of the attack game, adversary Aattempts to output a forgery pair ( m,σ), where
m is a message that was never submitted as a signing query to any honest party, and σ is a valid
signature on m with respect to the public key pk. The whole scheme, consisting of the threshold
signature scheme combined with the DKG protocol Π, is considered secure if for every eﬃcient
adversary A, its probability of successfully outputting such a forgery pair is negligible.
22.4.1.2 Formal deﬁnition of security
The security of a DKG protocol Π is deﬁned via simulation. We will say Π is secure if there is
a simulator Sim, such that no eﬃcient adversary Zcan eﬀectively distinguish between the “real
world” execution of protocol Π and an “ideal world” in which Zinstead interacts with dealer() via
the simulator Sim. We now describe the real world and the ideal world more carefully.
963
The real world. We model execution of protocol Π in the “real world” as in Fig. 22.3(a). On the
right side of this ﬁgure is an adversaryZ, which interacts with a challenger who among other things
is responsible for executing the logic of the honest parties running protocol Π. At the beginning of
Z’s attack, it sends a subset L⊆{ 1,...,N }of size at most L, indicating the set of corrupt parties.
We let I:= {1,...,N }\L be the set of honest parties. Next the challenger and the adversary
interact with message exchanges corresponding to
• the steps of the decentralized key provisioning (discussed above);
• messages sent to/from the honest parties from/to other parties (honest or corrupt);
• queries made to any random oracles or other types of idealized object.
In the ﬁgure, these messages are referred to as “protocol messages”. As the adversary delivers such
protocol messages, the honest parties will eventually (and at diﬀerent times) generate outputs outi
for i∈I. The challenger delivers these to the adversary, as indicated.
The ideal world. Fig. 22.3(b) shows the “ideal world”, which includes the adversary Z, the
simulator Sim, and a challenger. The key diﬀerence is that now, instead of running the protocol
Π on behalf of honest parties, the challenger now runs the dealer algorithm. In more detail, in
the ideal-world execution, the adversary Zbegins just as in the real world by giving the challenger
the set Lof corrupt parties. Also, just as in the real world, the adversary Zexchanges protocol
messages, but with the simulator Sim, and not the challenger. It is the job of Sim to “cook up”
realistic looking protocol messages to make the adversary “think” it is still in the real world. The
only assistance that Sim gets are the outputs outℓ for ℓ∈L from dealer() for the corrupt parties
— these outputs are given to Sim right away, in response to providing the challenger with the set
L. Also, the challenger will give to Z, but not Sim, the outputs outi for i∈I from dealer() for the
honest parties; however, in order to continue making the adversary “think” it is running the real
world, the simulator tells the challenger via “control messages” exactly when these outputs should
be delivered to the adversary. A well-designed simulator will coordinate these “control messages”
with the “protocol messages” so that Zwill receive outputs and protocol messages in the same
relative order in the ideal world as it would in the real world.
Attack Game 22.7 (Secure DKG). For a DKG protocol Π, adversary Z, and a simulator Sim,
we deﬁne two experiments. Experiment 0 is the “real world” protocol execution involving Π and
Zdescribed above, and Experiment 1 is the “ideal world” execution involving dealer, Sim, and Z,
also described above. We assume that at the end of each experiment, Zoutputs a bit. If Wb is the
event that Zoutputs 1 in Experiment b, deﬁne Z’s advantage with respect to Π and Sim as
DKGadv[Z,Π,Sim] :=
⏐⏐⏐Pr[W0] −Pr[W1]
⏐⏐⏐. 2
Deﬁnition 22.16 (Secure DKG). A DKG protocol Π is secure if there exists an eﬃcient sim-
ulator Sim such that for every eﬃcient adversary Z, the value DKGadv[Z,E,Sim] is negligible.
22.4.1.3 Connection to threshold signing and decryption
We now discuss the connection between the above deﬁnition of a secure DKG protocol and the
variation of the threshold signature attack game discussed in Section 22.4.1.1, and illustrated in
964
(b) The ideal world(a) The real world
protocol messages
<latexit sha1_base64="AK0D+ptQzRO/kWfiXzxiv2maX1g=">AAACEXicbVDLSsNAFJ3UV62vqks3g0XoqiQi2mXBje4q2Ac0IUymk3boZBJmboQS8gtu/BU3LhRx686df+Ok7UJbD8xwOOde7r0nSATXYNvfVmltfWNzq7xd2dnd2z+oHh51dZwqyjo0FrHqB0QzwSXrAAfB+oliJAoE6wWT68LvPTCleSzvYZowLyIjyUNOCRjJr9bdDLsRgTGHLE4h9zl2cz8zP5dzgxKR3ea5X63ZDXsGvEqcBamhBdp+9csdxjSNmAQqiNYDx07Ay4gCTgXLK26qWULohIzYwFBJIqa9bHZRjs+MMsRhrMyTgGfq746MRFpPo8BUFjvqZa8Q//MGKYRNL+MySYFJOh8UpgJDjIt48JArRkFMDSFUcbMrpmOiCAUTYsWE4CyfvEq65w3nsmHfXdRazUUcZXSCTlEdOegKtdANaqMOougRPaNX9GY9WS/Wu/UxLy1Zi55j9AfW5w8b9p3L</latexit>
{ out i } i 2 I
<latexit sha1_base64="9GAr40ZFhWZLz6ilPXPS3BX5TCE=">AAAB8nicbVBNS8NAFHypX7V+VT16WSyCp5KIqMeqF48VrC2koWy223bpZhN2X4QS+jO8eFDEq7/Gm//GTZuDtg4sDDPvsfMmTKQw6LrfTmlldW19o7xZ2dre2d2r7h88mjjVjLdYLGPdCanhUijeQoGSdxLNaRRK3g7Ht7nffuLaiFg94CThQUSHSgwEo2glvxtRHDEqs+tpr1pz6+4MZJl4BalBgWav+tXtxyyNuEImqTG+5yYYZFSjYJJPK93U8ISyMR1y31JFI26CbBZ5Sk6s0ieDWNunkMzU3xsZjYyZRKGdzCOaRS8X//P8FAdXQSZUkiJXbP7RIJUEY5LfT/pCc4ZyYgllWtishI2opgxtSxVbgrd48jJ5PKt7F3X3/rzWuCnqKMMRHMMpeHAJDbiDJrSAQQzP8ApvDjovzrvzMR8tOcXOIfyB8/kDcfSRXA==</latexit>
A
<latexit sha1_base64="aPNzPnlWRy+iTB3tZxTtB/uW8b8=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF48V7Qe0oWy2m3bpZhN2J0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekEhh0HW/ncLa+sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCajhUijeRIGSdxLNaRRI3g7GtzO//cS1EbF6xEnC/YgOlQgFo2ilh15D9MsVt+rOQVaJl5MK5Gj0y1+9QczSiCtkkhrT9dwE/YxqFEzyaamXGp5QNqZD3rVU0YgbP5ufOiVnVhmQMNa2FJK5+nsio5ExkyiwnRHFkVn2ZuJ/XjfF8NrPhEpS5IotFoWpJBiT2d9kIDRnKCeWUKaFvZWwEdWUoU2nZEPwll9eJa2LqleruveXlfpNHkcRTuAUzsGDK6jDHTSgCQyG8Ayv8OZI58V5dz4WrQUnnzmGP3A+fwAgw42y</latexit>
⇧
signing queries
signing 
oracle
<latexit sha1_base64="AK0D+ptQzRO/kWfiXzxiv2maX1g=">AAACEXicbVDLSsNAFJ3UV62vqks3g0XoqiQi2mXBje4q2Ac0IUymk3boZBJmboQS8gtu/BU3LhRx686df+Ok7UJbD8xwOOde7r0nSATXYNvfVmltfWNzq7xd2dnd2z+oHh51dZwqyjo0FrHqB0QzwSXrAAfB+oliJAoE6wWT68LvPTCleSzvYZowLyIjyUNOCRjJr9bdDLsRgTGHLE4h9zl2cz8zP5dzgxKR3ea5X63ZDXsGvEqcBamhBdp+9csdxjSNmAQqiNYDx07Ay4gCTgXLK26qWULohIzYwFBJIqa9bHZRjs+MMsRhrMyTgGfq746MRFpPo8BUFjvqZa8Q//MGKYRNL+MySYFJOh8UpgJDjIt48JArRkFMDSFUcbMrpmOiCAUTYsWE4CyfvEq65w3nsmHfXdRazUUcZXSCTlEdOegKtdANaqMOougRPaNX9GY9WS/Wu/UxLy1Zi55j9AfW5w8b9p3L</latexit>
{ out i } i 2 I
<latexit sha1_base64="9GAr40ZFhWZLz6ilPXPS3BX5TCE=">AAAB8nicbVBNS8NAFHypX7V+VT16WSyCp5KIqMeqF48VrC2koWy223bpZhN2X4QS+jO8eFDEq7/Gm//GTZuDtg4sDDPvsfMmTKQw6LrfTmlldW19o7xZ2dre2d2r7h88mjjVjLdYLGPdCanhUijeQoGSdxLNaRRK3g7Ht7nffuLaiFg94CThQUSHSgwEo2glvxtRHDEqs+tpr1pz6+4MZJl4BalBgWav+tXtxyyNuEImqTG+5yYYZFSjYJJPK93U8ISyMR1y31JFI26CbBZ5Sk6s0ieDWNunkMzU3xsZjYyZRKGdzCOaRS8X//P8FAdXQSZUkiJXbP7RIJUEY5LfT/pCc4ZyYgllWtishI2opgxtSxVbgrd48jJ5PKt7F3X3/rzWuCnqKMMRHMMpeHAJDbiDJrSAQQzP8ApvDjovzrvzMR8tOcXOIfyB8/kDcfSRXA==</latexit>
A
signing queries
signing 
oracle
protocol
messagescontrol
messages
<latexit sha1_base64="YUu/ggKmwmyVKWdKcuO7nZoTWDE=">AAAB83icbVBNS8NAFHypX7V+VT16WSyCp5KIqMeiF48VrS00oWy223bpbhJ2X4QS+je8eFDEq3/Gm//GTZuDtg4sDDPv8WYnTKQw6LrfTmlldW19o7xZ2dre2d2r7h88mjjVjLdYLGPdCanhUkS8hQIl7ySaUxVK3g7HN7nffuLaiDh6wEnCA0WHkRgIRtFKvq8ojgRm90JNe9WaW3dnIMvEK0gNCjR71S+/H7NU8QiZpMZ0PTfBIKMaBZN8WvFTwxPKxnTIu5ZGVHETZLPMU3JilT4ZxNq+CMlM/b2RUWXMRIV2Ms9oFr1c/M/rpji4CjIRJSnyiM0PDVJJMCZ5AaQvNGcoJ5ZQpoXNStiIasrQ1lSxJXiLX14mj2d176Lu3p3XGtdFHWU4gmM4BQ8uoQG30IQWMEjgGV7hzUmdF+fd+ZiPlpxi5xD+wPn8AX3Fkfs=</latexit>
Sim
<latexit sha1_base64="r7XrCAbzUV5LzGLqKSi/dLRWHts=">AAAB+nicbVDLSsNAFJ34rPWV6tLNYBHqpiQi2mXBjcsK9gFtKJPJTTt0MgkzE6XEfoobF4q49Uvc+TdO2iy09cDA4Zx7uWeOn3CmtON8W2vrG5tb26Wd8u7e/sGhXTnqqDiVFNo05rHs+UQBZwLammkOvUQCiXwOXX9yk/vdB5CKxeJeTxPwIjISLGSUaCMN7cogInrMdBYA4SBntfOhXXXqzhx4lbgFqaICraH9NQhimkYgNOVEqb7rJNrLiNSMcpiVB6mChNAJGUHfUEEiUF42jz7DZ0YJcBhL84TGc/X3RkYipaaRbybzoGrZy8X/vH6qw4aXMZGkGgRdHApTjnWM8x5wwCRQzaeGECqZyYrpmEhCtWmrbEpwl7+8SjoXdfeq7txdVpuNoo4SOkGnqIZcdI2a6Ba1UBtR9Iie0St6s56sF+vd+liMrlnFzjH6A+vzBy6rk+k=</latexit>
dealer ()
<latexit sha1_base64="W8OA4gYu7czb409045tuqIpBqlY=">AAAB8nicbVBNS8NAFHypX7V+VT16WSyCp5KIqMeiF48VbC2moWy223bpJht2X4QS+jO8eFDEq7/Gm//GTZuDtg4sDDPvsfMmTKQw6LrfTmlldW19o7xZ2dre2d2r7h+0jUo14y2mpNKdkBouRcxbKFDyTqI5jULJH8LxTe4/PHFthIrvcZLwIKLDWAwEo2glvxtRHDEqs8dpr1pz6+4MZJl4BalBgWav+tXtK5ZGPEYmqTG+5yYYZFSjYJJPK93U8ISyMR1y39KYRtwE2SzylJxYpU8GStsXI5mpvzcyGhkziUI7mUc0i14u/uf5KQ6ugkzESYo8ZvOPBqkkqEh+P+kLzRnKiSWUaWGzEjaimjK0LVVsCd7iycukfVb3Luru3XmtcV3UUYYjOIZT8OASGnALTWgBAwXP8ApvDjovzrvzMR8tOcXOIfyB8/kDl/GRdQ==</latexit>
Z
<latexit sha1_base64="W8OA4gYu7czb409045tuqIpBqlY=">AAAB8nicbVBNS8NAFHypX7V+VT16WSyCp5KIqMeiF48VbC2moWy223bpJht2X4QS+jO8eFDEq7/Gm//GTZuDtg4sDDPvsfMmTKQw6LrfTmlldW19o7xZ2dre2d2r7h+0jUo14y2mpNKdkBouRcxbKFDyTqI5jULJH8LxTe4/PHFthIrvcZLwIKLDWAwEo2glvxtRHDEqs8dpr1pz6+4MZJl4BalBgWav+tXtK5ZGPEYmqTG+5yYYZFSjYJJPK93U8ISyMR1y39KYRtwE2SzylJxYpU8GStsXI5mpvzcyGhkziUI7mUc0i14u/uf5KQ6ugkzESYo8ZvOPBqkkqEh+P+kLzRnKiSWUaWGzEjaimjK0LVVsCd7iycukfVb3Luru3XmtcV3UUYYjOIZT8OASGnALTWgBAwXP8ApvDjovzrvzMR8tOcXOIfyB8/kDl/GRdQ==</latexit>
Z
<latexit sha1_base64="FZ/noyR5XjEAGG8kmF4Fe85HlOo=">AAACH3icbVDLSgMxFM3UV62vqks3wSLUTZkRqV0W3Oiugn1ApwyZNG1DM5khuSOUMH/ixl9x40IRcde/MX0stPVA4OSce7n3njARXIPrTp3cxubW9k5+t7C3f3B4VDw+aek4VZQ1aSxi1QmJZoJL1gQOgnUSxUgUCtYOx7czv/3ElOaxfIRJwnoRGUo+4JSAlYJi1TfYjwiMVGQaaZiV5x8OJk4hC/gl9rPAcOxzuSijRJj7LAuKJbfizoHXibckJbREIyh++/2YphGTQAXRuuu5CfQMUcCpYFnBTzVLCB2TIetaKknEdM/M78vwhVX6eBAr+yTgufq7w5BI60kU2srZjnrVm4n/ed0UBrWe4TJJgUm6GDRIBYYYz8LCfa4YBTGxhFDF7a6YjogiFGykBRuCt3ryOmldVbxqxX24LtVryzjy6AydozLy0A2qozvUQE1E0TN6Re/ow3lx3pxP52tRmnOWPafoD5zpDzhXo6w=</latexit>
{ Pub( out i ) } i 2 I
<latexit sha1_base64="FZ/noyR5XjEAGG8kmF4Fe85HlOo=">AAACH3icbVDLSgMxFM3UV62vqks3wSLUTZkRqV0W3Oiugn1ApwyZNG1DM5khuSOUMH/ixl9x40IRcde/MX0stPVA4OSce7n3njARXIPrTp3cxubW9k5+t7C3f3B4VDw+aek4VZQ1aSxi1QmJZoJL1gQOgnUSxUgUCtYOx7czv/3ElOaxfIRJwnoRGUo+4JSAlYJi1TfYjwiMVGQaaZiV5x8OJk4hC/gl9rPAcOxzuSijRJj7LAuKJbfizoHXibckJbREIyh++/2YphGTQAXRuuu5CfQMUcCpYFnBTzVLCB2TIetaKknEdM/M78vwhVX6eBAr+yTgufq7w5BI60kU2srZjnrVm4n/ed0UBrWe4TJJgUm6GDRIBYYYz8LCfa4YBTGxhFDF7a6YjogiFGykBRuCt3ryOmldVbxqxX24LtVryzjy6AydozLy0A2qozvUQE1E0TN6Re/ow3lx3pxP52tRmnOWPafoD5zpDzhXo6w=</latexit>
{ Pub( out i ) } i 2 I
<latexit sha1_base64="2Ji0Rmt/aBrUWDtAU5m8DRpEg7U=">AAAB8nicbVDLSsNAFL2pr1pfVZdugkVwVRIR7bLgxoWLCvYBbSiT6aQdOpkJMzdCCf0MNy4UcevXuPNvnLRZaPXAwOGce5lzT5gIbtDzvpzS2vrG5lZ5u7Kzu7d/UD086hiVasraVAmleyExTHDJ2shRsF6iGYlDwbrh9Cb3u49MG67kA84SFsRkLHnEKUEr9QcxwQklIrubD6s1r+4t4P4lfkFqUKA1rH4ORoqmMZNIBTGm73sJBhnRyKlg88ogNSwhdErGrG+pJDEzQbaIPHfPrDJyI6Xtk+gu1J8bGYmNmcWhncwjmlUvF//z+ilGjSDjMkmRSbr8KEqFi8rN73dHXDOKYmYJoZrbrC6dEE0o2pYqtgR/9eS/pHNR96/q3v1lrdko6ijDCZzCOfhwDU24hRa0gYKCJ3iBVwedZ+fNeV+Olpxi5xh+wfn4Bn+pkV0=</latexit>
L
<latexit sha1_base64="2Ji0Rmt/aBrUWDtAU5m8DRpEg7U=">AAAB8nicbVDLSsNAFL2pr1pfVZdugkVwVRIR7bLgxoWLCvYBbSiT6aQdOpkJMzdCCf0MNy4UcevXuPNvnLRZaPXAwOGce5lzT5gIbtDzvpzS2vrG5lZ5u7Kzu7d/UD086hiVasraVAmleyExTHDJ2shRsF6iGYlDwbrh9Cb3u49MG67kA84SFsRkLHnEKUEr9QcxwQklIrubD6s1r+4t4P4lfkFqUKA1rH4ORoqmMZNIBTGm73sJBhnRyKlg88ogNSwhdErGrG+pJDEzQbaIPHfPrDJyI6Xtk+gu1J8bGYmNmcWhncwjmlUvF//z+ilGjSDjMkmRSbr8KEqFi8rN73dHXDOKYmYJoZrbrC6dEE0o2pYqtgR/9eS/pHNR96/q3v1lrdko6ijDCZzCOfhwDU24hRa0gYKCJ3iBVwedZ+fNeV+Olpxi5xh+wfn4Bn+pkV0=</latexit>
L
<latexit sha1_base64="3BUsjQeZbDLZI8P3r6l1SH/+mVM=">AAACI3icbVDLSsNAFJ3UV62vqEs3g0VwISUR0eKq4MaFiwr2AU0Jk+m0HTqZhJkboYT8ixt/xY0Lpbhx4b84abuorQcGzpxzL/feE8SCa3Ccb6uwtr6xuVXcLu3s7u0f2IdHTR0lirIGjUSk2gHRTHDJGsBBsHasGAkDwVrB6C73W89MaR7JJxjHrBuSgeR9TgkYybdvvZDAkBKRPmQX2Evx9M8hjRLIfI8Jgb3MT3PicYkXqjPfLjsVZwq8Stw5KaM56r498XoRTUImgQqidcd1YuimRAGngmUlL9EsJnREBqxjqCQh0910emOGz4zSw/1ImScBT9XFjpSEWo/DwFTmO+plLxf/8zoJ9KvdlMs4ASbpbFA/ERginAeGe1wxCmJsCKGKm10xHRJFKJhYSyYEd/nkVdK8rLjXFefxqlyrzuMoohN0is6Ri25QDd2jOmogil7QG/pAn9ar9W5NrK9ZacGa9xyjP7B+fgG1TqWI</latexit>
L , { out ` } ` 2 L
Figure 22.4: The real and ideal worlds in the threshold signing and decryption attack games
Fig. 22.2. As always, everything we say here applies as well to threshold decryption, as well as any
threshold schemes with a similar structure.
Roughly speaking, the adversary Zin Attack Game 22.7 plays the role of both the adversary
and the callenger in the attack game illustrated in Fig. 22.2. This is illustrated in Fig. 22.4(a).
shares corresponding to corrupt parties. The adversary Zin Fig. 22.4(a) will output 1 if adversary
Aforges a signature.
The security of the DKG protocol Π implies that we can eﬀectively replace Π by the dealer
algorithm plus a simulator Sim, as shown in Fig. 22.4(b), without changing the probability that
Z outputs a 1 by more than a negligible amount. Moreover, one sees that in Fig. 22.4(b), the
adversary B, consisting of Atogether with Sim, essentially corresponds to an attack on a version of
Attack Game 22.1 in which a trusted party generates all of the keys, and the analysis we did earlier
in the chapter implies that the advantage ofBin this game is negligible. All of this together implies
that the advantage of adversary Ain the attack game illustrated in Fig. 22.2 is also negligible.
22.4.2 A simple DKG protocol
In this section, we present a DKG protocol. While it is conceptually simple, it is rather expensive
from the point of view of computational and communication complexity.
Before presenting the protocol itself in Section 22.4.2.3, we shall ﬁrst present two cryptographic
tools that we shall need for the protocol in Sections 22.4.2.1 and 22.4.2.2.
22.4.2.1 Veriﬁable encryption of a secret sharing
Roughly speaking, a veriﬁably encrypted secret sharing scheme , or VESS scheme, is a
semantically secure public-key encryption scheme that allows one party, say Pk, to encrypt a t-
out-of-N Shamir sharing of a secret under public keys of parties P1,...,P N, so that any party can
eﬃciently verify that the ciphertext does indeed encrypt such a sharing.
965
In somewhat more detail, we assume that each party Pi generates a vector of public encryp-
tion keys pki = (pk(1)
i ,..., pk(N)
i ), along with a vector of corresponding private decryption keys
ski = (sk(1)
i ,..., sk(N)
i ), for a semantically secure public-key encryption scheme. We assume a de-
centralized key provisioning mechanism, as discussed above, so that all parties obtain the public key
vectors pk1,..., pkN, but corrupt parties may generate their public keys in an arbitrary fashion.
The idea is that a party Pk encrypts a message to Pi under the public encryption key pk(k)
i (but
see Remark 22.12 below).
Now, to veriﬁably encrypt a secret sharing, party Pk generates κ0,...,κ t−1 ∈Zq at random and
sets ω= κ0 + κ1x+ ···+ κt−1xt−1 ∈Zq[x]. Party Pk then generates a transcript (C,U,π), where
• C = ( c1,...,c N) is a vector of ciphertexts, where each ci is an encryption of Pi’s share
αi := ω(i) under the public key pk(k)
i ;
• U := gω ∈Gt;
• π is a non-interactive zero-knowledge proof that ci is indeed an encryption of αi ∈Zq under
pk(k)
i satisfying gαi = U(i) for i= 1,...,N .
While it is possible to deﬁne a VESS scheme as an abstract cryptographic primitive, to keep
things more concrete, we shall work with a VESS scheme as just described. Formally, such a scheme
consists of
• a semantically public key encryption scheme Ewith message space Zq;
• a non-intertactive proof system Φ (see Section 20.3) for an appropriate relation that pro-
vides existential soundness (see Section 20.3.4) and non-interactive zero knowledge (see Sec-
tion 20.3.5);
the relation must be chosen so that soundness implies (with overwhelming probability) that
given (C,U,π) along with public keys pk(k)
1 ,..., pk(k)
N as above, if C = (c1,...,c N) and the
proof π is valid, then for each honest party Pi, the ciphertext ci decrypts under the secret
key sk(k)
i to αi satisfying gαi = U(i).
Remark 22.11 (Implementing a secure VESS scheme). One approach to implementing a
secure VESS scheme is to base it on the ideas in Exercise 20.11. There, you were asked to work out
the details of using the multiplcative ElGamal encryption scheme to encrypt a discrete logarithm
bit by bit, and to design a Sigma protocol to prove that the encryptor does this correctly. One
can extend such a Sigma protocol using the AND-proof construction of Section 19.7 to encrypt all
the Shamir shares of a secret and prove that this was done correctly. Finally, one can convert this
Sigma protocol into a non-interactive zero-knowledge proof using the Fiat-Shamir heuristic, as in
Section 20.3. While this protocol works, there is plenty of room for practical improvements. 2
Remark 22.12 (Reducing the number of keys). Instead of a vector of public keys pki for each
party Pi, we could have just a single public key pki. However, in this case, we would require that
the encryption scheme be a CCA-secure scheme that supports associated data (as in Section 12.7).
Speciﬁcally, a party Pk would encrypt a message to Pi under the key pki with associated data k.
In fact, for this (and many similar secure distributed computations), it suﬃces that the encryption
scheme satisﬁes the weaker notion of AD-only CCA security discussed in Section 12.7.1. To this
end, the reader may wish to work out the details of the analog of Exercise 20.11 based on the
966
AD-only CCA-secure encryption scheme presented in Exercise 20.20, rather than multiplicative
ElGamal. 2
22.4.2.2 A secret bulletin board
Our DKG protocol will crucially depend on a sub-protocol that securely implements a secret
bulletin board, which essentially allows each party to privately submit a proposed message to the
sub-protocol, so that each party eventualy obtains as output the same collection of messages.
We will describe the security properties of such a sub-protocol in terms of a trusted party SBB
that provides a secret bulletin board as a service to all of the parties P1,...,P N running the DKG
protocol. In reality, there is no trusted party SBB, and the service provided by SBB will have to be
implemented by P1,...,P N themselves. However, we can modularly design a secure DKG protocol
using the trusted party SBB and then replace SBB by a sub-protocol that securely implements
SBB — this will result in a secure DKG protocol that does not rely on any trusted party.
One can securely implement the service provided by SBB using fairly eﬃcient protocols, even
over an asynchronous network, under our assumption that N > 3L. Moreover, these protocols
do not require any trusted setup beyond the type of decentralized key provisioning discussed in
Section 22.4.1. Below, in Section 22.4.2.4, we will see how to securely implement SBB in terms
of a public bulletin board, which itself a special case of the well-studied problem of Byzantine
Agreement. The formalities of secure composition of protocols will be discussed in much more
detail in Chapter 23 (in particular, see Section 23.6.1).
So let us deﬁne how SBB is supposed to behave. We assume we have N parties P1,...,P N. The
protocol is parameterized by the value L, which is the bound on the number of corrupt parties, as
well as a value s, where L < s≤N −L, which is the size of the collection of messages output by
the SBB. The idea is that each party Pi proposes a message mi for the bulletin board by sending
its proposal mi privately to SBB. Eventually, SBB will choose a collection C= {mk}k∈Kof s such
proposals (so |K|= s), and send ( K,C) to all parties. We assume an adversary chooses a set of at
most L parties to corrupt at the beginning of its attack, and may choose the proposals for all of
these corrupt parties. Note that the size parameter scannot be larger than N−Lsimply because
L corrupt parties may refuse to cooperate and propose any message to the bulletin board.
In more detail, SBB works as follows.
• Each party Pi (both honest and corrupt) sends its proposed message mi privately to SBB.
We assume that the channel on which Pi sends mi to SBB is perfectly secure; more precisely,
when a party sends mi to SBB, the adversary is informed of the index i of the party Pi that
submitted mi, but is not given any information about mi; moreover, the adversary cannot
alter the message mi.
• At some point in time after SBB has received proposals from a set K∗ of parties, where
|K∗|≥ s, the adversary chooses a subset K⊆K ∗of size s, and sends Kto SBB. At this time,
SBB sends (K,C), where C:= {mk}k∈Kto all parties.
Note that while each honest party will eventually receive SBB’s output (K,C), the adversary
will determine exacty when each individual party actually receives it by sending SBB an
appropriate control message.
The above description of SBB guarantees that every honest party sees the same collection C
of s proposed messages. Moreover, at least one of the messages in Cwas actually proposed by an
967
honest party, since by assumption, there are at most L<s corrupt parties.
The adversary has some very limited control overC. Indeed, the adversary can determine which
honest parties’ proposals are included in C, and can determine the proposals in Cfrom the corrupt
parties in any way it likes. However, the corrupt proposals will not depend on any of the honest
proposals.
22.4.2.3 The DKG protocol
Now we have all the ingredients we need to construct a simple yet secure DKG protocol. We need
• a VESS scheme, as described in Section 22.4.2.1 consisting of a semantically secure public-
key encryption scheme Ewith message space Zq, and a corresponding non-interactive proof
system Φ, and
• a secret bulletin board SBB, as described in Section 22.4.2.2, with the size parameter s :=
L+ 1.
The protocol, which we call Π sdkg, runs as follows.
1. In the decentralized key provisioning phase, each honest party Pi generates its own public-
key/secret-key vector pair (pki,ski) for the encryption scheme E, and is provisioned with the
public-key vectors ( pk1,..., pkN) of all parties. Recall that each pki is a vector of public
keys (pk(1)
i ,..., pk(N)
i ) and each ski is a vector of secret keys ( sk(1)
i ,..., sk(N)
i ).
2. Each honest party Pi locally generates a random polynomial ωi ∈Zq[x] of degree less than t,
and generates a corresponding transcript
(Ui,Ci,πi)
as described in Section 22.4.2.1. Recall that Ui = gωi.
3. Each party Pi submits its own transcript ( Ui,Ci,πi) to SBB.
4. Eventually, each honest Pi party obtains from SBB the pair (K,C), where
C= {(Uk,Ck,πk)}k∈K,
and Kis a subset of {1,...,N }of size L+ 1.
Party Pi then locally performs the following computation:
(a) compute
K′:=
{
k∈K : πk is a valid proof
}
;
(b) for k ∈ K′, decrypt cki under the secret key sk(k)
i to obtain αki ∈Zq, where Ck =
(ck1,...,c kN);
(c) compute
U ←
∏
k∈K′
Uk and αi ←
∑
k∈K′
αki.
Finally, Pi outputs outi := (U,αi).
968
Before analyzing the security of protocol Π sdkg, let us ﬁrst at least verify that it computes
correct values, at least when all parties are honest. For k ∈K′, we have Uk = gωk, and for each
i= 1,...,N , the ciphertext cki is an encryption of the plaintext αki = ωk(i) under the public key
pk(k)
i . Let ω= ∑
k∈K′ωk. Then one may easily verify that U = gω and αi = ω(i) for i= 1,...,N ,
as required.
Theorem 22.12. Protocol Πsdkg is a secure DKG protocol, assuming N−L≥t, and that we have
decentralized key provisioning, a secure VESS scheme, and a secret bulletin board SBB.
Proof sketch. We have to show that protocol Πsdkg satisﬁes the security deﬁnition in Section 22.4.1.2.
To that end, we have to show the existence of an eﬃcient simulatorSim so that no adversary Zcan
distinguish between the real world, as presented in Fig. 22.3(a), in which Zinteracts with Π sdkg,
and the ideal world, as presented in Fig. 22.3(b), in which Zinstead interacts with the dealer and
Sim.
Now, the real world protocol Π sdkg actually contains several idealized components, and the
“protocol messages” in the real world are really just the interactions between Zand these idealized
components.
• First, we are assuming decentralized key provisioning, as discussed in Section 22.4.1. This
means that the challenger in the real world generates public-key/secret-key pairs on behalf of
all of the honest parties for the encryption scheme used in the VESS scheme, and gives the
public keys to Z; the adversary Zthen generates public keys for the encryption scheme for
the corrupt parties and gives these to the challenger.
• Second, the VESS scheme consists of a non-interactive proof system Φ for an appropriate
relation that provides existential soundness and non-interactive zero knowledge. Our notion
of non-interactive zero knowledge (see Section 20.3.5) is explicitly deﬁned in terms of a random
oracle, and so even in the real world, the adversary Zmakes explicit queries to this random
oracle, which are processed by the challenger.
• Third, the secret bulletin board SBB used by Πsdkg is modeled here as a trusted party, so that
even in the real world, the adversary Zexplicitly interacts with SBB: Zexplicitly supplies to
SBB the proposals of the corrupt parties, and is explicitly informed by SBB when an honest
party supplies a proposal; Zalso supplies explicit control messages to SBB to indicate the
subset of output proposals and to schedule when each honest party receives these proposals.
All of these interactions between Zand SBB are processed by the challenger.
We will see later (see Section 22.4.2.4) how to securely implement SBB using a distributed
protocol, so that the only idealized components needed are a random oracle and decentralized
key provisioning.
We now sketch the design of the simulator Sim. Recall that Lis the set of indices of corrupt
parties. Let I:= {1,...,N }\L, which is the set of indices of the honest parties. Also recall that
Kis the set of indices supplied by the adversary Zto SBB, and K′ is the set of indices for which
the corresponding proposals consist of transcripts with valid proofs.
The ﬁrst thing that happens in the ideal world is that Zprovides Lto the challenger, who runs
the dealer algorithm and gives to Sim the collection of values {outℓ}ℓ∈L. So now Sim has U ∈Gt
and the secret shares αℓ for ℓ ∈L. The simulator Sim continues by running the decentralized
key provisioning logic just as in the real protocol. After this, the adversary Zwill give to Sim
969
transcripts from various corrupt parties — remember that all “protocol messages” that Zwould
give to the challenger in the real world are given directly to Sim in the ideal world. In addition,
whenever an honest party Pi is initialized, Sim will simply send to Zits index, as if from SBB in
the real world. Eventually, Zwill give to Sim the set K, which also determines the set K′. At this
point in time (and no earlier), Sim will “cook up” consistent-looking transcripts on behalf of the
honest parties in the set K. Note that Sim does not get to see the secret shares αi for i∈I, while
Zdoes get to see these shares. This means that the transcripts that Sim cooks up will be “fake”
transcripts that must be computed without the knowledge of these shares. However, based on the
security properties of the VESS scheme, and exploiting the fact that Sim knows U and {αℓ}ℓ∈L,
the simulator Sim can cook up this transcript in such a way that the adversary Zwill not notice
that it is indeed fake.
To provide further details of Sim and to prove that it works, we shall present a sequence of
games, Game 0, . . . , Game 5, where Game 0 is the real world, and Game 5 is equivalent to the
ideal world.
Game 1: In this game, we make a couple of small, essentially syntactic changes. First, we have
the challenger delay the generation of the transcripts for the honest parties until after the
set Khas been speciﬁed — in particular, until after the adversary Zhas already submitted
its proposals to SBB. Moreover, it is only necessary to generate a transcript ( Uk,Ck,πk) for
honest parties Pk with k∈K.
In addition, whenever the challenger decrypts a ciphertext cki from such an honest party Pk
on behalf of any honest party Pi, it does not decrypt the ciphertext at all, but simply uses
the plaintext value αki = ωk(i).
Because of the behavior ofSBB, and under the correctness property for the encryption scheme,
there is no detectable diﬀerence between Games 0 and 1.
Game 2: In this game, we play our “soundness card”, so that when the adversary speciﬁes the
set Kof indices to SBB, the challenger tests whether there is some k∈K′∩L such that the
given transcript from party Pk is of the form ( Uk,Ck,πk), where
• Ck = (ck1,...,c kN), but
• cki does not decrypt to the right value under secret key the secret key sk(k)
i for some
i∈I.
If so, the challenger immediately “aborts”, meaning that it responds to all subsequent queries
by the adversary Zwith an “aborted” message.
Under the assumption of the soundness of the underlying proof system Φ, such an “abort”
happens with negligible probability, and Zcannot eﬀectively distinguish between Games 1
and 2.
Game 3: Now we play our “zero knowledge card”, so that in the transcript (Uk,Ck,πk) generated
by each honest party Pk for k∈K, the proof πk is replaced by a simulated proof.
Under the zero-knowledge assumption for the proof system Φ,Zcannot eﬀectively distinguish
between Games 2 and 3.
970
Game 4: Now we play our “semantic security card”, so that for the transcript ( Uk,Ck,πk) gen-
erated by each honest party Pk for k ∈K, if Ck = (ck1,...,c kN), for each honest party Pi,
the ciphertext cki is replaced by an encryption of 0.
Under the semantic security assumption, and the fact that we have already replaced the
corresponding proof πi by a simulated proof, and the fact that we are not using any of the
relevant decryption keys sk(k)
i , adversary Zcannot eﬀectively distinguish between Games 3
and 4.
Let us pause and take stock of what is happening in Game 4. Speciﬁcally, let us look at the
transcript ( Uk,Ck,πk) for each k ∈ K′, assuming the challenger does not abort. Let us write
Uk = (uk,0,...,u k,t−1) and Ck = (ck1,...,c kN). Let Mconsist of 0 and the indices of t−|L|− 1
arbitrary honest parties (in the case where |L|= L= t−1, we have M= {0}).
• Suppose k∈L. Then the ciphertexts cki for i∈I all decrypt to correct values, that is, each
cki decrypts to αki ∈Zq that satisﬁes gαki = U(i)
k . Moreover, the assumption that N−L≥t
implies that the number of honest parties is at least t, which means that from the collection
of values {αki}i∈I, we can interpolate to recover the unique polynomial ωk ∈Zq[x] of degree
less than t that satisﬁes gωk = Uk.
• Suppose k ∈I. The ciphertexts ckℓ for ℓ ∈L encrypt elements αkℓ of Zq. Moreover, the
random variables αkℓ for ℓ∈L are uniformly distributed over Zq, and the random variables
U(i)
k for i∈M are uniformly distributed over G; in addition, these random variables αkℓ and
U(i)
k are mutually independent and together determine the group elements uk,0,...,u k,t−1 via
“interpolation in the exponent”. Finally, for i ∈I, the ciphertext cki and the proof πk are
“fake”.
To visualize this better, consider the following table:
ℓ∈L i∈M
k∈K′∩L ωk(ℓ) ωk(i)
k∈K′∩I αkℓ U(i)
k
The rows of this table correspond to transcripts generated by parties Pk for k ∈K′. The columns
corresppond to the polynomial evaluation points ℓ∈L and i∈M. As discussed above, in the rows
corresponding to k∈K′∩L, the values ωk(ℓ) and ωk(i) are all known to the challenger, and in the
rows corresponding to k∈K′∩I, the random variables αkℓ and U(i)
k are uniformly distributed and
mutually independent.
Let us deﬁne
ω∗:=
∑
k∈K′∩L
ωk ∈Zq[x].
As we saw above, the challenger in Game 4 can eﬃciently compute ω∗.
Game 5: In this game, the logic of the challenger is modiﬁed to use the dealer algorithm as in the
ideal world. The dealer chooses a random polynomial ω ∈Zq[x] of degree less than t, and
gives to the challenger U := gω = (u0,...,u t−1) and αℓ := ω(ℓ) for ℓ ∈L. Now, when the
adversary Zspeciﬁes the set Kof indices to SBB, the challenger will use the polynomial ω∗
deﬁned above to “cook up” appropriate transcripts for the honest parties Pk with k ∈K in
971
such a way that for each honest party Pi, the logic in Step 4 of protocol Π sdkg will compute
the public part of its output as U and its share of the secret key αi as ω(i).
To this end, for each ℓ∈L, the challenger generates {αkℓ}k∈K∩I at random from Zq, subject
to
ω∗(ℓ) +
∑
k∈K∩I
αkℓ = αℓ.
Also, for each i∈M, the challenger generates {U(i)
k }k∈K∩I at random from G, subject to
gω∗(i) ∏
k∈K∩I
U(i)
k = U(i).
Then, for each k ∈K∩I , the challenger can compute Uk from {αkℓ}ℓ∈L and {U(i)
k }i∈M
by “interpolation in the exponent”. The challenger can also compute the encryptions cℓ
of αkℓ for ℓ ∈ L, and combine these with encryptions ci of 0 for i ∈ I, thus obtaining
Ck = (ck1,...,c kN). Finally, the challenger can generate a “fake” proof πk, using the zero-
knowledge simulator for Φ.
One sees that this game is logically equivalent to Game 4. Moreover, one sees that this game
is also logically equivalent to the ideal world depicted in Fig. 22.3(b). We leave it to the
reader to extract from our description of this game the logic of the simulator Sim. 2
Remark 22.13. Although Theorem 22.12 only assumes N−L≥t, in order to implement SBB in
the asynchronous network communication model, we require L<N/ 3. 2
Remark 22.14. As presented, protocol Πsdkg is only intended to be executed just once. However,
with a minor modiﬁcation, it can be securely executed multiple times, using the same keys provided
by the decentralized key provisioning across all executions. Moreover, these executions can be
run either sequentially or concurrently with one another. To achieve this, we just need separate,
independent instances of SBB, one for each instance of protocol Π sdkg.
We have not formally deﬁned the security of a DKG protocol in this multi-instance setting, but
it is fairly straightforward to do so. Note that in such a model, the set Lof corrupt parties is the
same across all execution instances.
We also note that if the DKG protocol will actually be run only once using the provisioned
keys, then if we replace the vector pki of keys by a single key pki as discussed in Remark 22.12,
then the reader may verify that the only security property required of the public-key encryption
scheme is that it is non-adaptive CCA secure, as in Exercise 12.28. 2
22.4.2.4 Implementing a secret bulletin board
We brieﬂy sketch how to implement a secret bulletin board in terms of a public bulletin board.
We can deﬁne the service provided by a public bulletin board in terms of a trusted party PBB
that behaves almost identically to SBB, but with the following changes:
• When a party Pi sends a message mi to PBB, it is not sent privately; in fact, if Pi is an
honest party, PBB, explicitly sends mi to the adversary.
• In addition, PBB will only accept a message mi if it passes an application-dependent validity
test, which (in addition to the index i and the message mi) may take as input public data,
such as public keys.
972
Byzantine Agreement. The problem of implementing a public bulletin board is a special case
of a classic and well-studied problem called Byzantine Agreement. In a Byzantine Agreement
protocol, we have N parties, L of which may be corrupt and behave arbitrarily (in the literature
in this area, the corrupt parties are called Byzantine). The goal of such a protocol is to have all of
the honest parties agree on a common value. There are typically some constraints on the allowable
common values (as otherwise, the honest parties could just agree in some predetermined default
value). For example, for our public bulletin board, the common value would be a set of s triples
of the form ( i,mi,σi), where the i values are distinct, each mi satisﬁes an application-dependent
validity test, and each σi is a valid signature on mi under Pi’s public key.
Any Byzantine Agreement protocol should satisfy the following two properties:
Safety: no two honest parties produce diﬀerent values;
Liveness: all honest parties eventually produce some value.
Some protocols for this problem only work if the communication network is assumed to be
synchronous, which means that there is a bound δ such that all protocol messages sent from
one honest party to another are guaranteed to be delivered within time δ. These are the most
practical Byzantine Agreement protocols, and with some kind of PKI (such as the decentralized
key provisioning we are assuming in this section), they can withstand L<N/ 2 corruptions, which
is optimal (without a PKI, L<N/ 3 is optimal).
However, such a synchrony assumption is unrealistic in many settings, such as a globally dis-
tributed network of parties, which is why we have not made this assumption in this section. Instead,
we assume only an asynchronous network, where there is no a priori bound on message delivery
time, even though we do assume that all messages are eventually delivered. Liveness in this setting
means that
• the total number of messages generated by all honest parties is poly-bounded with overwhelm-
ing probability, and
• when all messages generated by honest parties have been delivered, then all honest parties
will produce a value.
There are theoretically eﬃcient protocols for asynchronous Byzantine Agreement that can with-
stand L < N/3 corruptions, which is optimal. However, these protocols are not very practical. A
reasonable tradeoﬀ that is often made is to insist that safety is guaranteed without any synchrony
assumption, but that liveness is only guaranteed when the network eventually becomes synchronous
(at least for some suﬃciently long period of time). There are very practical protocols that satisfy
these assumptions that can also withstand L<N/ 3 corruptions, which is optimal.
Implementing SBB using PBB. We next sketch one simple way to implement a secure SBB
using a PBB. The protocol we present makes use of an ( L+ 1)-out-of-N VESS scheme, and we
require that N >2L. Assume that messages mi can be encoded as bit strings of a ﬁxed length,
and let Mdenote the set of all such bit strings. We also need a hash function H : G →M, which
will be viewed as a random oracle.
When party Pi is ready to submit its message mi to the secret bulletin board, it generates a
transcript (Ui,Ci,πi), which encrypts a random polynomial ωi ∈Zq[x] of degree at most L. Party
Pi then computes χi ←H(ωi(0))⊕mi, and submits the message Mi := (Ui,Ci,πi,χi) to the public
973
bulletin board PBB. Recall that a public bulletin board applies an application-speciﬁc validity test
— in this application, the validity test checks that Mi has the correct syntactic form and that πi
is a valid proof.
Eventually,PBB sends (K,{Mk}k∈K), to all parties. Upon receiving this, each party Pi decrypts
the transcripts in {Mk}k∈Kto obtain the values ωk(i) for k∈K, and for each such k, it sends the
message (k,ωk(i)) to all parties. Then party Pi does the following for each k∈K:
• wait for L+ 1 messages of the form ( k,αkj), where gαkj = U(j)
k (so that αkj = ωk(j));
• interpolate to obtain ωk(0);
• compute mk ←H(ωk(0)) ⊕χk.
Finally, party Pi outputs (K,{mi}k∈K).
One can show that this protocol is secure in a simulation-based sense, analogous to Deﬁni-
tion 22.16, assuming the discrete logarithm problem in G is hard, and modeling H as a random
oracle. In the real world, the adversary interacts with with PBB and the random oracle H, while in
the ideal world, the simulator interacts with SBB. We will not go into any further details here. We
also note that this protocol will inherit the liveness properties of whatever Byzantine Agreement
protocol is used to implement PBB, in the following sense: if (K,{Mk}k∈K) is delivered to all honest
parties, and all of the messages ( k,ωk(i)) that they subsequently generate are also delivered to all
honest parties, then all honest parties will produce an output.
Using PBB instead of SBB. It is natural to ask, why not just directly use a public bulletin
board PBB in our DKG protocol, rather than a secret bulletin board SBB? This would clearly be
more eﬃcient, but is it secure? The short answer is, “it depends on the application”. To give a
longer answer, let us ﬁrst examine what additional advantage an adversary would have if we used
PBB directly in our DKG protocol. In our DKG protocol in Section 22.4.2.3, each honest party Pi
submits a transcript (Ui,Ci,πi) to the bulletin board. The adversary may also submit a transcript
(Uℓ,Cℓ,πℓ) to the bulletin board on behalf of each corrupt party Pℓ, and then choose which of
these honest and corrupt transcript will be included in the set of L+ 1 transcripts output by the
bulletin board. Since the bulletin board is public, the adversary may choose the corrupt transcripts
and the output set based on the honest transcripts. Because of this, the adversary can bias the
public key — it will no longer be a random element of G. For example, the adversary could pick
a set ˜Kof L honest transcripts, and then generate a single corrupt transcript ( Uℓ,Cℓ,πℓ) in some
arbitrary way that depends on the transcripts in ˜K, so that the resulting value
U := Uℓ ·
∏
k∈˜K
Uk,
and in particular, the resulting public key U(0), is biased in some particular way that could make
it easier to break a threshold cryptosystem.
Despite the ability of an adversary to bias the public key in this way, one can prove that any
non-threshold cryptosystem with secret key α ∈Zq and public key u := gα ∈Zq can be securely
deployed as a secure threshold scheme using this simpliﬁed DKG protocol with a public bulletin
board, provided the non-threshold cryptosystem remains secure under tweaked key generation,
by which we mean the following interactive key generation procedure:
974
• The challenger generates the initial secret key ˜α ∈Zq at random and gives the initial
public key ˜u:= g˜α ∈G to the adversary.
• The adversary gives to the challenger µ∈Z∗
q and ν ∈Zq.
• The challenger then computes thetweaked secret keyα:= µ˜α+νand the tweaked public
key u:= gα ∈G.
The rest of the attack game deﬁning the security of the non-threshold cryptosystem is then executed
as usual, except that we use the tweaked secret and public keys in place of the normal secret and
public keys.
One can show that the BLS signature scheme and the GS encryption scheme are secure with
respect to tweaked key generation, and hence could be securely deployed as threshold schemes using
the simpler DKG protocol with a public bulletin board. In constrast, it is not hard to show that
the ECDSA signature scheme (see Section 19.3) is insecure with respect to tweaked key generation.
22.5 Beyond threshold: monotone access structures
So far in this chapter we looked atthreshold secret sharing systems where a secret is shared amongN
parties, and can be reconstructed by any t of them. The secret is hidden from any subset that
is smaller than t. We used threshold secret sharing to distribute the ability to sign and decrypt
without ever reconstructing the secret key in a single location.
Simple threshold access to a secret is not suﬃcient in many real world settings. For example, a
bank may want to share a secret α among two groups of employees S1 and S2, so that at least t1
people from S1 and at least t2 people from S2 are needed to reconstruct the secret. Otherwise, the
secret α should remain hidden.
This simple example shows the need for more ﬂexible access structures beyond a simple thresh-
old. Consider a total of N parties, denoted S = {1,...,N }. An access structure is an explicit
collection of subsets of Sthat are allowed to reconstruct the secret. We will only consider access
structures that have the following natural property: if a set J⊆S can reconstruct the secret, then
so can any superset of J. An access structure that has this property is said to be monotone.
Deﬁnition 22.17. Let Sbe a ﬁnite set, and let A ⊆2S be a collection of subsets of S. We say
that A is monotone if J∈ A and J⊆J ′⊆S implies that J′∈A.
Deﬁnition 22.18. A pair (S,A) is called an access structure if S is ﬁnite and A ⊆ 2S is
monotone. Subsets of Sin A are called authorized and subsets not in A are called unauthorized.
For example, let At be the collection of all subsets of Scontaining t or more elements. Then
(S,At) is an access structure that implements a tthreshold access: all subsets of Scontaining tor
more elements are authorized, and all subsets containing fewer than t elements are unauthorized.
Our deﬁnition of threshold secret sharing (Deﬁnition 22.1) easily generalizes to arbitrary access
structures.
Deﬁnition 22.19. Let (S,A) be an access structure. A secret sharing scheme over Z that
implements (S,A) is a pair of eﬃcient algorithms (G,C) so that:
• G is a probabilistic sharing algorithm that is invoked as (α1,..., αN) ←RG(α), for α∈Z,
to generate a secret sharing of α.
975
• C is a deterministic combining algorithm that is invoked as α ←C(J,{αj}j∈J), where
J∈ A, to recover α. The algorithm outputs an element of Z.
• Correctness: we require that for every α ∈Z, for every possible output (α1,..., αN) of
G(α), and every set J∈ A we have
C(J,{αj}j∈J) = α.
The security deﬁnition generalizes too. It says that an unauthorized set cannot distinguish one
secret from another from the shares that they have.
Deﬁnition 22.20. A secret sharing scheme (G,C) over Z that implements (S,A) is secure if for
every α,α′∈Z, and every unauthorized subset J̸∈ A, the distribution G(α)[J] is identical to the
distribution G(α′)[J].
To exercise the deﬁnition, let us show that if ( G,C) over Z implements an access structure
(S,A), then the set A is eﬃciently recognizable: there is an eﬃcient randomized algorithm that
takes a set J⊆S as input, and decides if J∈ A. On input J, the decision algorithm does:
• choose a random α←R Z,
• compute (α1,..., αN) ←R G(α) and α′←C(J, {αj}j∈J),
• output accept if α= α′and output reject otherwise.
Correctness implies that when J ∈A, the algorithm always outputs accept. Security implies that
when J ̸∈A, the algorithm outputs accept with probability 1/|Z|. Hence, assuming |Z|≥ 2, this
eﬃcient randomized procedure decides if J is in A. If needed, it can be repeated several times to
reduce the error probability.
This observation shows that if an access structure ( S,A) is so complicated that A cannot be
eﬃciently recognized, then there no eﬃcient secret sharing scheme that implements it.
22.5.1 The generic secret sharing construction
Every access structure (S,A) can be securely implemented by a secret sharing scheme ( G,C) over
Zq, where the length of each share output by Gis linear in the size of A. To explain how the scheme
works, let us ﬁrst look at a simple example.
Example 22.1. Let S:= {1,2,3,4}and A :=
{
{1,2,3}, {1,3,4}, S
}
. One can verify that A is
monotone. For α ∈Zq, algorithm G(α) generates an independent secret sharing of α for each of
the 3-element sets in A:
• For the set {1,2,3}it chooses random ρ(1)
1 ,ρ(1)
2 ,ρ(1)
3 in Zq such that their sum is α.
• For the set {1,3,4}it chooses random ρ(2)
1 ,ρ(2)
3 ,ρ(2)
4 in Zq such that their sum is α.
The algorithm outputs the shares
α1 := (ρ(1)
1 ,ρ(2)
1 ), α2 := ρ(1)
2 , α3 := (ρ(1)
3 ,ρ(2)
3 ), α4 := ρ(2)
4 .
The reader can verify that all three authorized sets can reconstruct α, while unauthorized sets learn
nothing about α. 2
976
Construction 22.1 (generic construction). Let (S,A) be an access structure. For i= 1,...,N
let A(i) be the set of all authorized sets that contain i. That is, A(i) = {J ∈A : i ∈J} . The
generic construction works as follows:
• G(α): for every authorized set J= {j1,...,j ℓ}in A choose random ρ(J)
j1 ,...,ρ (J)
jℓ in Zq such
that their sum is equal to α. For example, choose the ﬁrst ℓ−1 elements at random in Zq and
set the last element to be αminus the sum of the ﬁrst ℓ−1 elements. Then, for i= 1,...,N
the share αi is
αi :=
{
ρ(J)
i
}
J∈A(i)
This αi contains one element in Zq for every authorized set that contains i. In particular, αi
contains |A(i)|elements. Output the list of shares ( α1,..., αN).
• C(J,{αj}j∈J): output ∑
j∈Jρ(J)
j ∈Zq.
In words, the algorithm picks out one element in Zq from each of the provided shares, and
outputs their sum.
We leave it to the reader to verify that the scheme is correct and secure. 2
There are many ways to optimize this scheme to obtain somewhat smaller shares. An example
is given in Exercise 2.20.
This generic construction works well when the number of authorized sets is small, but is unwork-
able as soon as the number of authorized sets is even mildly large. Even for a 10-out-of-20 threshold
access structure this generic construction requires each share to contain over a hundred thousand
elements in Zq. More generally, for an N-out-of-2N access structure, this scheme outputs shares
whose size is exponential in N. In contrast, Shamir’s secret sharing scheme from Section 22.1.1
implements an N-out-of-2N threshold access structure using much shorter shares: every share
contains a single element in Zq no matter how big N is.
22.5.2 Linear secret sharing schemes and monotone span programs
Recall that Shamir’s secret sharing scheme (Gsh,Csh) implements a threshold access structure. For
t≤N <q, a t-out-of-N sharing of α∈Zq is a set of shares α1,...,α N in Zq generated as follows:
choose a random polynomial ωin Zq[x] of degree less than tsuch that ω(0) = α, and set αi := ω(i)
for i= 1,...,N .
Shamir’s secret sharing scheme has three important properties that makes it especially useful
for constructing secure threshold signing and threshold decryption schemes.
Linearity of the secret: For every subset J⊆{ 1,...,N }of size t, the secret αcan be expressed
as a ﬁxed linear combination (depending only on J) of the shares αj for j ∈J.
Independence of the shares: Consider a subset L⊆{ 1,...,N }of size t−1. Then for every
secret α, the t−1 random variables αℓ for ℓ∈L are uniformly and independently distributed
over Zq.
Linearity of the shares: Consider a subset L⊆{ 1,...,N }of size t−1. For each i∈{1,...,N },
the share αi can be expressed as a ﬁxed linear combination (depending only on Land i) of α
and αℓ for ℓ∈L.
977
The ﬁrst property, linearity of the secret, follows directly from Lemma 22.1 (with j∗ = 0).
It is used to combine signature shares into a full signature, and combine decryption shares into
a full decryption. For example, in the threshold BLS signature scheme, linearity of the secret
enables the combiner to compute a signature σ:= H(m)α on a message m from t signature shares
{σ′
j := H(m)αj}j∈J via “interpolation in the exponent”. Indeed, by linearity of the secret, there
are eﬃciently computable scalars {λj}j∈J such that σ= ∏
j∈J(σ′
j)λj.
The second property, independence the shares, was proved in Theorem 22.3. It is used to prove
the security of threshold schemes. Speciﬁcally, in the reductions in the proofs of Theorems 22.4
and 22.7, we used this property to justify generating the adversary’s secret shares αℓ for ℓ∈L at
random.
The third property, linearity of the shares, also follows directly from Lemma 22.1 (with j∗ =
i). It is also used to prove the security of threshold schemes. Speciﬁcally, in the reductions in
the proofs of Theorems 22.4 and 22.7, we used this property to compute veriﬁcation keys and
signature/decryption shares via “interpolation in the exponent”.
Note that this third property also follows directly from the ﬁrst and second properties. Indeed,
for i /∈L, we can apply the ﬁrst property with J:= L∪{i}, writing
α= λiαi +
∑
ℓ∈L
λℓαℓ.
Moreover, by the second property, we must have λi ̸= 0, as otherwise, the secret key would be
completely determined by the t−1 shares αℓ for ℓ ∈L. This allows us to then express αi as a
linear combination of α and αℓ for ℓ∈L.
22.5.2.1 Monotone span programs
In this section we abstract the three properties of Shamir secret sharing by deﬁning the concept
of a linear secret sharing scheme. Linear secret sharing schemes are important because they can
be used as a “drop in” replacement for Shamir secret sharing in all the constructions presented
in this chapter. In particular, if an access structure ( S,A) can be implemented eﬃciently by a
linear secret sharing scheme, then all the constructions in this chapter for threshold signing and
threshold decryption can be adapted to support the access structure (S,A). We will see an example
momentarily.
We begin by deﬁning a somewhat odd looking computation model called a monotone span
program. As we will see, every access structure ( S,A) that is recognized by an eﬃcient monotone
span program has an eﬃcient linear secret sharing scheme.
Deﬁnition 22.21. A monotone span program P is a tuple
(
N,q, {Mi}N
i=1
)
, where N is a
positive integer, q is a prime, and each Mi is a di ×d matrix over Zq for some d,d1,...,d N.
• We say that the program P accepts a set J ⊆ {1,...,N }if and only if the row vector
e1 := (1,0,0,..., 0) ∈Zd
q is in the linear span of the rows of the matrices {Mi}i∈J.
• Let L(P) be the set of subsets of S:= {1,...,N }that are accepted by P. We say that P
recognizesthe access structure (S,A) if L(P) = A.
The set L(P) is clearly monotone: if e1 is in the row span of {Mi}i∈J then it is also in the row
span of {Mi}i∈J′ for every superset J′of J.
978
Example 22.2. Let’s show that a t-out-of-N threshold access structure (S,At) can be recognized
by a monotone span program P =
(
N,q, {Mi}N
i=1
)
, where q >N. For i= 1,...,N , the matrix Mi
is simply
Mi := (1,i,i 2,...,i t−1) ∈Z1×t
q .
The N ×t matrix formed by the rows of M1,...,M N is called a Vandermonde matrix. A
property of a Vandermonde matrix is that every t rows are linearly independent. Therefore, for
every set J of size at least t, the rows of {Mi}i∈J span all of Zt
q, and in particular, their span
contains e1. Hence J ∈L(P). Moreover, if Lis a set with fewer than t elements, then it is a
simple exercise to show that e1 is not in the span of the rows of {Mi}i∈L. Therefore, L(P) = At,
as required. 2
Example 22.3. Consider again the access structure ( S,A) where S = {1,2,3,4} and A ={
{1,2,3}, {1,3,4}, S
}
from Example 22.1. Let P = (4,q, {Mi}4
i=1) be the monotone span program
where q≥2 and the matrices M1,M2,M3,M4 are
M1 :=
⏐⏐⏐⏐
1 1 0 0 0
1 0 0 1 0
⏐⏐⏐⏐ ∈Z2×5
q
M2 :=
⏐⏐ 0 0 1 0 0
⏐⏐ ∈Z1×5
q
M3 :=
⏐⏐⏐⏐
0 −1 −1 0 0
0 0 0 0 1
⏐⏐⏐⏐ ∈Z2×5
q
M4 :=
⏐⏐ 0 0 0 −1 −1
⏐⏐ ∈Z1×5
q
The set {1,2,3}is accepted by the program P because the ﬁrst rows of M1,M2,M3 sum to e1.
The set {1,3,4}is accepted by P because bottom rows of M1,M3,M4 sum to e1. No other subset
of Sis accepted by P other than the set Sitself. Hence L(P) = A as required.
This example can be generalized to show that for every access structure ( S,A) there is a
monotone span program ( N,q, {Mi}N
i=1) that recognizes A, where for i = 1 ,...,N the matrix
Mi ∈Zdi×d
q for some d≤N ·|A|and di ≤|A(i)|. 2
22.5.2.2 The secret sharing scheme derived from a monotone span program
Let (S,A) be an access structure, where |S|= N. Suppose A is recognized by a monotone span
program P. Then the following secret sharing scheme implements ( S,A).
Construction 22.2. The secret sharing scheme ( G,C) derived from P =
(
N,q, {Mi}N
i=1
)
, where
Mi is a di ×d matrix over Zq, works as follows:
• G(α), for α∈Zq, works as follows:
– choose random ρ2,...,ρ d in Zq and let rbe the column vector r:= (α,ρ2,...,ρ d) ∈Zd
q.
– compute the column vector αi ←Mi ·r∈Zdi
q for i= 1,...,N .
– output the shares ( α1,..., αN).
• C(J, {αj}j∈J), for J∈ A, works as follows:
– Let d′:= ∑
i∈Jdi. Concatenate all the shares {αj}j∈J into one column vector α∈Zd′
q .
Similarly, concatenate all the matrices {Mj}j∈J into one d′×d matrix M over Zq.
979
– Find a row vector w ∈Zd′
q such that w·M = e1. We know that w exists because
J∈ A = L(P), and therefore e1 is in the row span of M.
– Output the inner product w·α∈Zq.
This completes the description of the scheme. Note that d1,...,d N aﬀect the size of the shares,
where as d aﬀects the running time of algorithms G and C. 2
Let us show that the scheme is correct. Let r be the vector constructed by algorithm G, and
let M,α,wbe the quantities constructed during an execution of C(J, {αj}j∈J). Then:
w·α= w·(M ·r) =
(
w·M
)
·r= e1 ·r= α.
Hence, C(J, {αj}j∈J) outputs α, as required.
As for security, we have:
Theorem 22.13. The secret sharing scheme (G,C) securely implements the access structure(S,A).
We shall defer the proof of this theorem until a bit later (see the discussion following Theo-
rem 22.14).
Deﬁnition 22.22. Let P be a monotone span program. The secret sharing scheme (G,C) obtained
from P via Construction 22.2 is called the linear secret sharing scheme derived from P.
Let’s see some familiar linear secret sharing schemes:
• The linear secret sharing scheme derived from the monotone span program in Example 22.2 is
identical to Shamir’s secret sharing scheme. To see the correspondence, observe that the vector
(α,ρ2,...,ρ d) in Construction 22.2 plays the same role as the vector ( α,κ1,...,κ t−1) that is
used to deﬁne the polynomial in Shamir’s secret sharing scheme. Hence, Theorem 22.13, on
the security of the general construction, is a generalization of Theorem 22.3, which proves
security of Shamir secret sharing.
• The linear secret sharing scheme derived from the monotone span program in Example 22.3
is the same as the generic secret sharing scheme from Example 22.1.
The main application for linear secret sharing schemes is that they can be used as a “drop
in” replacement for Shamir secret sharing in all the constructions presented in this chapter. In
particular, if an access structure ( S,A) can be implemented eﬃciently by a linear secret sharing
scheme, then all the constructions in this chapter for threshold signing and threshold decryption can
be adapted to support the access structure ( S,A). We will see an example of this in Section 22.5.3
and also in Exercise 22.2.
22.5.2.3 Properties of linear secret sharing schemes
Recall that Shamir secret sharing satisﬁes three important properties described at the beginning
of the section: (i) linearity of the secret, (ii) independence of the shares, and (iii) linearity of the
shares. We show that a linear secret sharing scheme derived from a monotone span program has
analogous properties. To generalize the second and third properties to a general access structure
we need the following useful concept.
980
Deﬁnition 22.23. Let P =
(
N,q, {Mi}N
i=1
)
and P′ =
(
N,q, {M′
i}N
i=1
)
be two monotone span
programs, and let (G,C) and (G′,C′) be the linear secret sharing schemes derived from P and P′
respectively. We say that P and P′ are equivalent if for all α ∈Zq the distributions G(α) and
G′(α) are identical.
The deﬁnition says that P and P′ are equivalent if the derived secret sharing schemes always
generate the same distribution of shares. We will see examples of equivalence in Lemma 22.15
below.
Properties of linear secret sharing schemes. The next theorem shows that a linear secret
sharing scheme has the same two properties as Shamir secret sharing.
Theorem 22.14. Let P =
(
N,q, {Mi}N
i=1
)
be a monotone span program that recognizes the access
structure (S,A), where N = |S|and where Mi is a di ×d matrix for all i= 1,...,N . Let (G,C)
be the linear secret sharing scheme derived from P.
(a) Linearity of the secret : There is an eﬃcient deterministic algorithm Lin that is invoked
as Lin(J), where J ∈A, and outputs vectors {λj ∈Zdj
q }j∈J. For all α∈Zq and all shares
(α1,..., αN) output by G(α), we have
α=
∑
j∈J
λj ·αj.
Here, λj ·αj is the dot product of the vectors λj and αj.
(b) Independence and linearity of the shares : For every unauthorized set L⊆S there is
an eﬃciently computable monotone span program PL=
(
N,q, {M′
i}N
i=1
)
that is (i) equivalent
to P, and (ii) for each ℓ∈L the left-most column of M′
ℓ is zero.
To understand part (b) better, let ( G′,C′) be the linear secret sharing scheme derived from
PL =
(
N,q, {M′
i}N
i=1
)
. Recall that for a secret α ∈ Zq, algorithm G′(α) generates shares by
computing
αi := M′
i ·r∈Zdi
q for i= 1,...,N,
where r is the column vector r := ( α,ρ2,...,ρ d) for ρ2,...,ρ d ←R Zq. Because P and PL are
equivalent, we know that the resulting shares are sampled from the same distribution as G(α).
Moreover, since the left-most column of M′
ℓ is zero for ℓ ∈L, the share αℓ := M′
ℓ ·r does not
depend on the secret α; it only depends on ρ2,...,ρ d. Hence, for an unauthorized set L, all the
shares {αℓ}ℓ∈Lcan be sampled from a distribution that does not depend at all on the secret α∈Zq
— these shares only depend on ρ2,...,ρ d. This corresponds to the “independence of the shares”
property for Shamir secret sharing, and clearly implies Theorem 22.13. Moreover, every share αi
can be expressed linearly in terms of α and ρ2,...,ρ d. This corresponds to the “linearity of the
shares” property for Shamir secret sharing.
The proof of Theorem 22.14 part (a) is immediate from the deﬁnition of algorithm
C(J, {αj}j∈J). The algorithm reconstructs the secret α by computing a linear combination
of the shares {αj}j∈J. To illustrate this concretely, consider again Example 22.1, and observe that
for the authorized set J:= {1,2,3}algorithm Lin(J) would output the vectors
λ1 := (1,0), λ2 := (1), λ3 := (1,0).
981
To prove Theorem 22.14 part (b), we ﬁrst state the following simple lemma, which gives a
suﬃcient condition for when two monotone span programs are equivalent.
Lemma 22.15. Let P =
(
N,q, {Mi}N
i=1
)
be a monotone span program where each Mi is a di ×d
matrix. Let R be an invertible d×d matrix over Zq where the top most row of R is e1. Let P′ be
the monotone span program P′=
(
N,q, {MiR}N
i=1
)
. Then P and P′ are equivalent.
Proof. Fix some secret α∈Zq and shares α1,..., αN. For (ρ2,...,ρ d) ∈Zd−1
q let rbe the column
vector r:= (α,ρ2,...,ρ d). For (ρ′
2,...,ρ ′
d) ∈Zd−1
q let r′be the column vector r′:= (α,ρ′
2,...,ρ ′
d).
The proof follows from the fact that there is a one-to-one correspondence between tuples
(ρ2,...,ρ d) ∈Zd−1
q such that αi = Mi ·r for all i = 1 ,...,N , and tuples ( ρ′
2,...,ρ ′
d) ∈Zd−1
q
such that αi = (MiR) ·r′for all i= 1,...,N . The correspondence is deﬁned by r= R·r′. 2
Proof of Theorem 22.14 part (b). Lemma 22.15 reduces the proof to a linear algebra question. Let
Lbe an unauthorized set. It suﬃces to show that there is a d×d matrix R such that (i) R is
invertible, (ii) the top most row of R is e1, and (iii) for all ℓ∈L the left-most column of Mℓ ·R is
zero. Then PL:=
(
N,q, {MiR}N
i=1
)
is the required monotone span program.
First, consider the subspace V of Zd
q spanned by the rows of all of the matricesMℓ for ℓ∈L. We
can choose an ordered basis for V, denoted vk+1,..., vd. We can extend this to an ordered basis for
Zd
q, denoted v1,..., vk,vk+1,..., vd. By assumption, e1 /∈V, and so without loss of generality, we
may assume v1 = e1. Deﬁne R to be a matrix that represents a change of basis from the standard
basis e1,..., ed to the basis v1,..., vd. More precisely, R is the d×d matrix that maps the row
vector a∈Zd
q onto the row vector ( b1,...,b d) := aR ∈Zd
q whose entries are the coordinates of a
on the basis v1,..., vd, namely a= b1v1 + ··· + bdvd. We claim that R satisﬁes properties (i)–(iii)
above. Property (i) is clear, since R represents a change of basis. Property (ii) is clear, since
v1 = e1, and hence e1R = e1. Property (iii) is clear, since for each ℓ ∈L, each row aof Mℓ lies
in V, and hence the coordinate vector aR is only nonzero in positions k+ 1,...,d . In particular,
since k≥1, the left-most entry of aR is zero. 2
Remark 22.15. A linear secret sharing scheme (G,C) obtained from Construction 22.2 has another
important linearity property. If ( α1,..., αN) ←R G(α) is a sharing of α, and (β1,..., βN) ←R G(β)
is a sharing of β, then ( α1 + β1,..., αN + βN) is a sharing of α+ β. Indeed, if J is authorized,
then C(J, {αj + βj}j∈J) must output α+ β. This property is often used to “compute” on data
that is secret shared among multiple parties. 2
22.5.2.4 Linear secret sharing from monotone formulas
Finally, we show that every access structure deﬁned by a monotone formula can be implemented
by a linear secret sharing scheme. We will explain these terms momentarily. The key point is
that if one can write down a monotone formula that captures the access structure that one has
in mind, then it is straightforward to derive a linear secret sharing scheme that implements this
access structure. This, in turn, lets us build signing and decryption schemes that support the access
structure deﬁned by the monotone formula, as long as the formula size is poly-bounded.
Monotone formulas. We ﬁrst brieﬂy review what is a monotone formula. At-out-of-N thresh-
old gate is a function gt : {0,1}N →{0,1}that takes N boolean inputs, and outputs ‘1’ if t or
more the inputs is equal to ‘1’. Note that a 1-out-of- N threshold gate computes the logical OR of
its N inputs. Similarly, an N-out-of-N threshold gate computes the logical AND of its N inputs.
982
𝑥1CEO𝑥2CFO𝑥2
𝑥3accountant #1𝑥4accountant #2𝑥5lawyer
AND
2
-
of
-
3
OR
AND𝜙(𝑥1,…,𝑥5)
Figure 22.5: An example monotone formula φ that deﬁnes an access structure. To reconstruct
the secret one needs a lawyer, and (both the CEO and CFO) or (two of CFO and two accountants).
A monotone formula φ with N inputs is a directed acyclic graph, a DAG, where every node
has at most one outgoing edge. Every input node (i.e., a node with in-degree zero) is labeled
with a variable xi for some i∈{1,...,N }. Every internal node that has in-degree k is labeled as
t-out-of-k, for some 0 <t ≤k, to indicate that it is a t-out-of-k threshold gate. Finally, there is a
single output node (i.e., a node with out-degree zero). An example monotone formula is shown in
Fig. 22.5.
A monotone formula φwith N inputs computes a function Fφ : {0,1}N →{0,1}. For an input
x ∈{0,1}N one evaluates all the threshold gates one by one from the inputs to the output in a
topological order. The output of the ﬁnal gate is deﬁned to be the value of Fφ(x).
A monotone formula φwith N inputs deﬁnes an access structure (S,Aφ) where |S|= N. To see
how, we say that a set J⊆S is authorized (i.e., namely J∈ Aφ), if and only if the characteristic
vector of J, denoted χ(J) ∈{0,1}N, satisﬁes Fφ(χ(J)) = 1. Recall that for J ⊆{1,...,N }, the
characteristic vector of J is a vector in {0,1}N that is ‘1’ at exactly the positions that correspond
to the elements of J. For example, the characteristic vector of {2,4}⊆{ 1,2,3,4}is 0101. The
reader can verify that Aφ is monotone.
The linear secret sharing scheme derived from a monotone formula. The following
theorem is the main point of this section. For a monotone formula φ we write |φ|for the total
number of input nodes to the formula. For example, the formula φ in Fig. 22.5 has ﬁve inputs
x1,...,x 5, but |φ|= 6 because x2 is provided as an input twice. We write |φ|i for the number of
times that xi appears as an input to φ. For the formula φ in Fig. 22.5 we have |φ|2 = 2.
Theorem 22.16. Let (S,Aφ) be an access structure deﬁned by a monotone formula φ with N
inputs. Then for every prime q >|φ|, there is a monotone span program Pφ = (N,q, {Mi}N
i=1) that
recognizes (S,Aφ) where the dimension of each matrix Mi is di ×d where di = |φ|i and d≤|φ|.
The linear secret sharing scheme ( Gφ,Cφ) derived from Pφ generates shares α1,..., αN, where
each αi is a vector over Zq of length |φ|i.
The proof of Theorem 22.16 is by induction on the number gates in φ. We will not prove
the theorem here, but instead give an example for how algorithm Gφ(α) works for the monotone
983
formula in Fig. 22.5. Algorithm Gφ(α) processes one gate at a time, starting from the output
gate, and working its way towards the inputs. The output gate in Fig. 22.5 is an AND gate, so
the algorithm chooses random τ1,τ2 ←R Zq such that τ1 + τ2 = α. Both τ1 and τ2 are needed to
reconstruct α, so this properly implements an AND gate. Next, it recursively processes each gate.
The next gate is an OR gate which the algorithm processes by setting τ3 := τ1 and τ4 := τ1. Either
one of τ3 or τ4 can reconstruct τ1, so this properly implements an OR gate. The next gate is an
AND gate so the algorithm chooses τ5,τ6 ←R Zq such that τ5 + τ6 = τ3. Finally, the 2-out-of-3 gate
is processed by calling ( τ7,τ8,τ9) ←R Gsh(3,2,τ4). The ﬁnal shares given to the ﬁve parties are:
α1 := τ5, α2 := (τ6,τ7), α3 := τ8, α4 := τ9, α5 := τ2. (22.6)
The reconstruction algorithm works from the leaves to the output, and can reconstruct αwhenever
the subset of shares satisﬁes the formula φ. It is not too diﬃcult to see how this procedure
generalizes to any monotone formula.
We can express this procedure as a monotone span program P = (5 ,q, {Mi}5
i=1) where the
matrices are
M1 :=
⏐⏐ 0 0 1 0
⏐⏐
M2 :=
⏐⏐⏐⏐
0 1 −1 0
0 1 0 1
⏐⏐⏐⏐
M3 :=
⏐⏐ 0 1 0 2
⏐⏐
M4 :=
⏐⏐ 0 1 0 3
⏐⏐
M5 :=
⏐⏐ 1 −1 0 0
⏐⏐
The derived share generation algorithm Gφ(α) operates by computing αi ←Mi·rfor i= 1,..., 5,
where r:= (α,ρ2,ρ3,ρ4) is a column vector and ρ2,ρ3,ρ4 ←R Zq. This gives the shares
α1 := ρ3, α2 := (ρ2 −ρ3, ρ2 + ρ4), α3 := ρ2 + 2ρ4, α4 := ρ2 + 3ρ4, α5 := α−ρ2.
which is equivalent to (22.6).
22.5.3 A signature scheme from linear secret sharing
Now that we understand linear secret sharing schemes and their properties, let us generalize the
BLS threshold signature scheme to work with any secure linear secret sharing scheme. The GS
threshold decryption scheme can be generalized similarly.
Let G be a group of prime order q with generator g ∈ G. Let P =
(
N,q, {Mi}N
i=1
)
be a
monotone span program that recognizes an access structure ( S,A). Let ( G,C) be the linear secret
sharing scheme derived from P. Then ( G,C) is a secret sharing scheme over Zq that implements
(S,A), where |S|= N. We will also need a hash function H : M→ G, and the BLS signature
veriﬁcation algorithm VBLS.
Construction 22.3. The SlinBLS = (G′,S′,V ′,C′) signature scheme for the access structure (S,A)
works as follows.
• G′() →(pk,pkc,sk1,..., skN):
984
– choose a random α←R Zq and compute pk ←gα;
– run G(α) to obtain N shares (α1,..., αN) where each αi ∈Zdi
q for i= 1,...,N ;
– for i= 1,...,N set ski := αi ∈Zdi
q and pki ←gαi :=
(
gαi,1,...,g αi,di
)
∈Gdi;
– set pkc := (pk1,..., pkN);
– output (pk, pkc, sk1,..., skN).
• S′(ski,m) →σ′
i, where ski = αi ∈Zdi
q , output the signature share
σ′
i ←H(m)αi :=
(
H(m)αi,1,...,H (m)αi,di
)
∈Gdi.
• C′(pkc, m, J, {σ′
j}j∈J) →σ, where J∈ A:
– step 1: verify that all the provided signature shares are valid:
with pkc := (pk1,..., pkN), let J∗be the set of all j ∈J such that
VBLS(pkj,u,m, σ′
j,u
)
= reject for some u= 1,...,d j;
if J∗is nonempty, output blame(J∗) and abort;
– step 2: compute the vectors {λj ∈Zdj
q }j∈J ←Lin(J), where Lin is the algorithm from
Theorem 22.14(a);
– step 3: output the signature
σ←
∏
j∈J
dj∏
u=1
(σ′
j,u)λj,u ∈G.
• V′(pk,m,σ ): output VBLS(pk,m,σ ).
This completes the description of the signature scheme. 2
To see that the scheme is correct, recall that for J ∈A the vectors {λj ∈Zdj
q }j∈J ←Lin(J)
satisfy ∑
j∈Jαj ·λj = α. Therefore, the signature σ output in step 3 of algorithm C′satisﬁes
σ=
∏
j∈J
dj∏
u=1
(σ′
j,u)λj,u =
∏
j∈J
dj∏
u=1
(
H(m)αj,u
)λj,u
=
∏
j∈J
H(m)αj·λj = H(m)α
as required.
Security. To argue that the scheme is secure we ﬁrst need to deﬁne signature security for a general
access structure (S,A). We do so by generalizing Deﬁnition 22.4. Recall that Deﬁnition 22.4 allows
the adversary to corrupt up to t−1 signing parties and learn their secret keys. When deﬁning
security for a general access structure, we allow the adversary to corrupt any unauthorized set of
parties. Security requires that even with so many secret keys at its disposal, the adversary cannot
forge a signature.
985
Attack Game 22.8 (signature security for an access structure). Let S= (G′,S′,V ′,C′)
be a signature scheme deﬁned over ( M,Σ) for an access structure ( S,A), where |S|= N. For a
given adversary A, we deﬁne the following attack game.
• Setup: the adversary sends an unauthorized subset L⊆S to the challenger. The challenger
runs
(pk,pkc,sk1,..., skN) ←R G′(),
and sends pk, pkc, and {skℓ}ℓ∈Lto A.
• Signing queries: for j = 1,2,..., signing query j from Ais a message mj ∈M. Given mj,
the challenger computes all N signature shares σ′
j,i ←R S′(ski,mj) for i= 1,...,N . It sends(
σ′
j,1,...,σ ′
j,N
)
to the adversary.
• Forgery: eventually Aoutputs a candidate forgery pair ( m,σ) ∈M× Σ.
We say that the adversary wins the game if the following two conditions hold:
• V′(pk,m,σ ) = accept, and
• m is new, namely, m̸∈{m1,m2,... }.
We deﬁne A’s advantage with respect to Sdenoted asSIGadv[A,S], as the probability that Awins
the game. 2
Deﬁnition 22.24 (secure signatures for an access structure). We say that a signature
scheme Sfor an access structure (S,A) is secure if for all eﬃcient adversaries A, the quantity
asSIGadv[A,S] is negligible.
Next, we argue that the signature scheme SlinBLS in Construction 22.3 is secure. Security
follows from the security of the BLS signature scheme, and is a direct generalization of the proof
of Theorem 22.4.
Theorem 22.17. Let P =
(
N,q, {Mi}N
i=1
)
be a monotone span program where each Mi is a di×d
matrix and where all d,d1,...,d N are poly-bounded. Let (S,A) be the access structure recognized by
P. If SBLS is a secure signature scheme, then SlinBLS from Construction 22.3 is a secure signature
scheme for (S,A).
In particular, for every adversary Athat attacks SlinBLS as in Attack Game 22.8, there exists
an adversary B, where Bis an elementary wrapper around A, that attacks SBLS as in Attack
Game 13.1, such that
asSIGadv[A,SlinBLS] = SIGadv[B,SBLS].
Proof sketch. Let us sketch the main idea for how Bworks. First, Bis given a BLS public key
pk := gα for some unknown α∈Zq.
Next, Aoutputs an unauthorized setL⊆S . Our Bneeds to respond with pk,pkc,and {skℓ}ℓ∈L.
Let PL=
(
N,q, {M′
i}N
i=1
)
be the monotone span program equivalent to P from Theorem 22.14(b).
Recall that for all ℓ∈L the left-most column of M′
ℓ is zero.
Bchooses random ρ2,...,ρ d ←R Zq. Now let r := (α,ρ2,...ρ d) ∈Zd
q. Although Bdoes not
know α, it can compute the secret keys {skℓ}ℓ∈L, where skℓ := αℓ = M′
ℓ ·r, because the left-most
column of all the matrices M′
ℓ for ℓ ∈L is zero. Moreover, Bcan compute pkc = (gα1,...,g αN)
986
because gαi = gM′
i·r and Bhas gα as well as ρ2,...,ρ d ∈Zq. Now Bsends pk,pkc, and {skℓ}ℓ∈L
to A. We know that these quantities are distributed as in the real scheme because the programs P
and PLare equivalent.
Next, Aissues signing queries. To respond to a query for m ∈M our Bqueries its own BLS
signing oracle to obtain σ ←H(m)α. It can now compute all the signature shares σ1,..., σN for
m because
σi = H(m)M′
i·r ∈Gdi for all i= 1,...,N .
Bcan compute these values because r= (α,ρ2,...ρ d) and Bhas H(m)α as well as ρ2,...,ρ d ∈Zq.
Now that Bsupplied Awith all the information it requires, Awill eventually produce a BLS
forgery (m,σ) with a certain probability. This is a valid BLS forgery because Anever queried for
a signature for m. Therefore, Bsucceeds in forging a BLS signature with the same probability as
A’s advantage against SlinBLS. 2
Remark 22.16 (Stronger notions of security for an access structure). We note that the
above deﬁnition of signature security for an access structure is not as strong as one might intuitively
expect. In a stronger deﬁnition, we could model an attack in which for a given message m, the
adversary might obtain signature shares from just a subset of servers Tm ⊆{1,...,N }, rather
than all of the servers, and so long as L∪T , the adversary should not be able to compute a
signature on m. Our security deﬁnition above is too crude to model this type of attack, because
in a signing query, the adversary always gets signing shares from all of the servers. In the next
section, we will develop a more reﬁned security model that does model this type of attack. See, in
particular, Remark 22.17 for a stronger deﬁnition of secure signatures for an access structure (as
well Remark 22.19 for a stronger deﬁnition of secure decryption for an access structure). The good
news is that the BLS threshold signature scheme based on linear secret sharing also satisﬁes this
stronger deﬁnition, albeit under a stronger (but reasonable) assumption (and the same holds true
for the GS threshold decryption scheme based on linear secret sharing). 2
22.6 Gap security for threshold cryptosystems
In discussing the security of t-out-of-N threshold schemes, we have assumed that there may be up
to t−1 corrupt parties that obtain shares of the secret key. Moreover, our formal deﬁnitions of
security only imply the following:
if an adversary is able to obtain a signature/decryption, then at least one honest party
must have generated a corresponding signature/decryption share.
In this section, we brieﬂy discuss a stronger, more nuanced notion of gap security that works
with two bounds: a bound ton the number of shares required to construct a signature/decryption,
and a bound L ≤t−1 on the number of corrupt parties. (Note that in Section 22.4, we already
modeled these two separate parameters for DKG protocols.)
Before getting into more details, let us ﬁrst develop some intuition. Consider the generic t-out-
of-N threshold signature scheme discussed in Section 22.2.1, where a threshold signature consists
of a collection of t ordinary signatures. Assume that at most L ≤t−1 corrupt parties obtain
secret key shares (i.e., ordinary signing keys). Now suppose an adversary, which collaborates with
or controls the corrupt parties, is able to construct a threshold signature on a particular message
m (i.e., constructs t ordinary signatures on m). The corrupt parties may generate at most L
987
signature shares (i.e., ordinary signatures) on m which means that at least n−L honest parties
must have generated signature shares (i.e., ordinary signatures) on m as well. This is a stronger
security property than is guaranteed by Deﬁnition 22.4. There are applications where this stronger
security property is necessary (for example, some protocols for Byzantine Agreement, which we
touched on Section 22.4.2.4). Does this mean that for such applications we must use the generic
threshold signature scheme, rather than more compact threshold signature schemes, such as BLS
from Section 22.2.2? The answer is no: we can prove that the BLS signature scheme provides
this stronger security property; however, to do so, we must make a somewhat stronger (but still
reasonable) computational assumption.
In this section, we sketch the changes that must be made to our deﬁnitions of security for thresh-
old signing and decryption to capture this notion of gap security, and the stronger assumptions
needed to prove that the BLS signature scheme and the GS decryption scheme (see Section 22.3.3)
satisfy these stronger deﬁnitions.
22.6.1 Gap security for threshold signature schemes
Consider Deﬁnition 22.4, which deﬁnes the security of a threshold signature scheme, and more
speciﬁcally, Attack Game 22.1. Here is how this attack game must be modiﬁed to capture the
notion of gap security.
Attack Game 22.9 (threshold signature gap security). For a given threshold signature
scheme S= (G,S,V,C ), deﬁned over ( M,Σ), and a given adversary A, we deﬁne the following
attack game.
• Setup: the adversary sends poly-bounded allowable parameters N and t, where 0 < t≤N,
and a subset L⊆{ 1,...,N }, where |L|<t, to the challenger.
The challenger does the following:
– initialize an associative array
Map : M→{ 1,...,N }\L,
initialized with Map[m] = ∅for all m∈M,
– run
(pk,pkc,sk1,..., skN) ←R G(N,t),
– send pk, pkc, and {skℓ}ℓ∈Lto A.
• Signing queries: for j = 1,2,..., signing query j from Aconsists of a message mj ∈M and
an index ij ∈{1,...,N }\L.
The challenger does the following:
– update Map[mj] ←Map[mj] ∪{ij},
– compute S(skij,mj) and send this to the adversary.
• Forgery: eventually Aoutputs a candidate forgery pair ( m,σ) ∈M× Σ.
We say that the adversary wins the game if the following two conditions hold:
988
• V(pk,m,σ ) = accept, and
• |L∪Map[m]|<t.
We deﬁne A’s advantage with respect to Sdenoted dthSIG adv[A,S], as the probability that A
wins the game. 2
Deﬁnition 22.25 (gap secure threshold signatures).We say that a threshold signature scheme
Sis gap secure if for all eﬃcient adversaries A, the quantity dthSIGadv[A,S] is negligible.
The essential diﬀerences between Attack Game 22.1 and Attack Game 22.9 are that
• in the latter, the adversary obtains one signature share at a time, rather than all at once;
• in the latter, the adversary wins if it obtained signature shares on m from fewer than t−L
honest parties, where L:= |L|.
One can easily show that the generic threshold signature scheme presented in Section 22.2.1
satisﬁes Deﬁnition 22.25, assuming the underlying non-threshold scheme is secure.
The threshold BLS signature scheme presented in Section 22.2.2 satisﬁes Deﬁnition 22.25, as-
suming a certain interactive variant of the CDH assumption holds in the group G. As usual, we
deﬁne this assumption via an attack game.
Attack Game 22.10 (Linear one-more DH). Let G be a group of prime order q generated by
g∈G. For a given adversary A, we deﬁne the following attack game.
• The challenger chooses µ1,...,µ k ∈Zq and ν1,...,ν ℓ ∈Zq at random, and gives {gµi}k
i=1
and {gνj}ℓ
j=1 to the adversary.
• The adversary makes a sequence of queries to the challenger, each of which is a vector over
Zq of the form {κi,j}i,j, to which the challenger responds with
∏
i,j
(
gµiνj
)κi,j
.
• To end the game, the adversary outputs a vector {λi,j}i,j over Zq and a group element h∈G,
and wins the game if
h=
∏
i,j
(
gµiνj
)λi,j
and the output vector {λi,j}i,j is not a Zq-linear combination of the query vectors.
We deﬁne A’s advantage in solving the linear one-more DH problem for G, denoted
lin1mDHadv[A,G], as the probability that Awins this game. 2
Deﬁnition 22.26 (Linear one-more DH assumption). We say that the one-more DH as-
sumption holds for G if for all eﬃcient adversaries Athe quantity lin1mDHadv[A,G] is negligible.
It is not diﬃcult to prove the gap security of threshold BLS under the linear one-more DH
assumption. One can also show that the same holds in the asymmetric pairing setting under a
variation of the linear one-more DH assumption in which Attack Game 22.10 is modiﬁed so that
(with notation as in Section 15.5.1):
989
• gµi is replaced by gµi
1 ,
• gνj is replaced by gνj
0 ,
• gµiνj is replaced by gµiνj
0 ,
• h is in G0.
Note that these security results also hold if we use a secure DKG protocol as in Section 22.4.
Remark 22.17 (Monotone access structures). All of the above deﬁnitions generalize to mono-
tone access structures, as in Section 22.5. To model gap security with respect to monotone access
structures, Attack Game 22.9 may be modiﬁed as follows:
• In the Setup stage, instead of requiring that |L|< t, the requirement is that Lis an unau-
thorized set.
• In deﬁning the winning ]ondition, instead of requiring that |L∪Map[m]|<t, the requirement
is that L∪Map[m] is an unauthorized set.
One can also show that the BLS threshold signature scheme based on linear secret sharing presented
in Section 22.5.3 satisﬁes this deﬁnition under the linear one-more DH assumption. 2
22.6.2 Gap security for threshold decryption schemes
Consider Deﬁnition 22.12, which deﬁnes the security of a threshold decryption scheme, and more
speciﬁcally, Attack Game 22.5. Here is how this attack game must be modiﬁed to capture the
notion of gap security.
Attack Game 22.11 (threshold AD-only CCA gap security). For a public-key threshold
decryption scheme E= (G,E,D,C ) deﬁned over (M,D,C), and for a given adversary A, we deﬁne
two experiments.
Experiment b (b= 0,1):
• Setup: the adversary sends poly-bounded allowable parameters N and t, where 0 < t≤N,
and a subset L⊆{ 1,...,N }, where |L|<t, to the challenger.
The challenger does the following:
– initialize an empty associative array
Map : D→{ 1,...,N }\L,
– run
(pk,pkc,sk1,..., skN) ←R G(N,t),
– send pk, pkc, and {skℓ}ℓ∈Lto A.
• Athen makes a series of queries to the challenger. Each query can be one of two types:
– Encryption query: for i = 1 ,2,..., the ith encryption query consists of a triple
(mi0,mi1,di) ∈M2 ×D, where the messages mi0,mi1 are of the same length.
The challenger does the following:
990
∗ compute ci ←R E(pk,mib,di),
∗ if di /∈Domain(Map), then initialize Map[di] ←∅,
∗ send ci to A.
– Decryption query: for j = 1,2,..., the jth decryption query consists of a triple
(ˆcj, ˆdj,ij) ∈C×D×{ 1,...,N }\L
such that either
∗ ˆdj /∈Domain(Map), or
∗ ˆdj ∈Domain(Map) and |L∪Map[ ˆdj] ∪{ij}|<t.
The challenger does the following:
∗ if ˆdj ∈Domain(Map), then update Map[ ˆdj] ←Map[ ˆdj] ∪{ij},
∗ compute D(skij,ˆcj, ˆdj) and send this to A.
• At the end of the game, Aoutputs a bit ˆb∈{0,1}.
If Wb is the event that Aoutputs 1 in Experiment b, deﬁne A’s advantage with respect to Eas
dthCCAadoadv[A,E] :=
⏐⏐⏐Pr[W0] −Pr[W1]
⏐⏐⏐. 2
Deﬁnition 22.27 (threshold AD-only CCA gap security). A public-key threshold decryp-
tion scheme E is AD-only CCA gap secure if for all eﬃcient adversaries A, the value
dthCCAadoadv[A,E] is negligible.
Remark 22.18 (from AD-only to full CCA gap security). Analogous to Remark 22.10, one
can also deﬁne a notion of (full) CCA gap security for threshold decryption schemes. To do model
this, in the above attack game, the domain of the associated array map needs to be extended from
Dto C×D; for an encryption query, the input to Map is (ci,di), and for a decryption query, (ˆcj, ˆdj).
Just as before, we can convert any AD-only CCA gap secure scheme to a CCA gap secure scheme
via “wrapping” with a strongly one-time secure signature, to the reader. 2
One can show that the generic threshold decryption scheme presented in Section 22.3.1 satisﬁes
Deﬁnition 22.27, assuming the underlying non-threshold scheme is secure.
One can show that the threshold GS decryption scheme presented in Section 22.3.3 (as well as
the pairing-free variant in Section 22.3.6) satisﬁes Deﬁnition 22.27, under the linear one-more DH
assumption.
Remark 22.19 (Monotone access structures). All of the above deﬁnitions generalize to mono-
tone access structures, as in Section 22.5. To model gap security with respect to monotone access
structures, Attack Game 22.11 may be modiﬁed as follows:
• In the Setup stage, instead of requiring that |L|< t, the requirement is that Lis an unau-
thorized set.
• In deﬁning the pre-condition on decryption queries, instead of requiring that |L∪Map[ ˆdj] ∪
{ij}|<t, the requirement is that L∪Map[ ˆdj] ∪{ij}is an unauthorized set.
One can also show that the GS threshold decryption scheme based on linear secret sharing satisﬁes
this deﬁnition under the linear one-more DH assumption. 2
991
22.7 A fun application: a randomness beacon
To be written.
22.8 Notes
Citations to the literature to be added.
22.9 Exercises
22.1 (Special thresholds). In Section 22.1.1 we saw Shamir’s t-out-of-N secret sharing scheme
over Zq which works for any 0 <t ≤N <q. At the two extremes, t= 1 and t= N, there are much
simpler secret sharing constructions.
(a) Consider the following N-out-of-N secret sharing scheme ( G,C) for a secret α∈Zq:
• algorithm G(N,N,α ) chooses random α1,...,α N−1 ←R Zq, sets αN := α−∑N−1
i=1 αi ∈Zq,
and outputs α1,...,α N as the shares of α∈Zq.
• algorithm C reconstructs the secret α∈Zq from α1,...,α N by computing α←∑N
i=1 αi.
Show that this is a secure N-out-of-N secret sharing scheme as in Deﬁnition 22.2.
(b) Construct a simple 1-out-of- N secret sharing scheme ( G1,C1) and prove it to be a secure
1-out-of-N scheme. Show that your scheme is the same as Shamir’s t-out-of-N secret sharing
scheme with t= 1.
22.2 (Access structures). Generalize the BLS threshold signature scheme of Section 22.2.2 to
the following settings: The N decryption servers are split into two disjoint groups S1 and S2, and
signing a message mshould be possible only if the combiner receives at least t1 responses from the
set S1, and at least t2 responses from the set S2, where t1 ≤|S1|and t2 ≤|S2|. A signature in
your scheme should be a single group element. Adapt the security deﬁnition to these settings, and
prove that your scheme is secure.
Discussion: Your construction is an example of adapting BLS signatures to more general access
structure than a simple threshold. It is a special case of the technique discussed in Section 22.5
22.3 (Generic threshold signing). Prove that the generic threshold signature scheme
(G′,S′,V ′,C′) from Section 22.2.1 is secure and robust, as deﬁned in Section 22.2.3, assuming
the underlying signature scheme ( G,S,V ) is secure.
22.4 (Updating N and t). In our description of the BLS signature scheme, the number of key
shares N and the threshold t were determined during key generation time. What if we want to
change N or t after the key shares have been generated, but without changing the public key pk?
(a) Increasing N: to increment N by one, the signing servers need to generate a new key share
for the new key server, and update pkc. Let’s design a protocol that lets t signing servers
provision a new server with a key share. Suppose that the t signing servers have somehow
generated a t-out-of-tsharing of zero: for i= 1,...,t server ihas ρi ∈Zq, and ρ1+... +ρt = 0.
Explain how each of the t key servers can send a single Zq element to the new server that
992
lets the new server construct its key share. The new server should learn nothing other than
its new key share.
(b) Decreasing t: a single signing server can decrease the threshold t by one by simply sending
its secret key share to all other servers. Show that the resulting scheme is ( t−1)-out-of-N
secure.
Discussion: Incrementing t is more problematic. All the signing servers have to be issued new
key shares, and they must be trusted to delete the old key shares, otherwise they can continue to
use the old key shares with a t threshold. We will come back to this question in Exercise 22.4.
22.5 (Proactive security). Let α ∈Zq be a secret and let ( α1,...,α N) ←R Gsh(N,t,α ) be a
Shamir secret sharing of α. The parties holding the shares are concerned that an adversary may
slowly steal the shares, one share at a time. Concretely, suppose that every week the attacker can
obtain one additional share. Then after t weeks the attacker will learn the secret α. To defend
against this slow leakage, the parties decide to use a technique called proactive security, where
every week the N parties engage in a proactive refresh protocol. The protocol generates new shares
(α′
1,...,α ′
N) for the parties, but without changing the shared secret α∈Zq.
(a) At the very least, a proactive refresh should ensure that an adversary who obtains t−1 shares
{αj}j∈J before the refresh, and obtains t−1 shares {α′
j}j∈J′ after the refresh, learns nothing
about α. Describe a security game that captures this requirement.
(b) For Shamir secret sharing, design a simple protocol among the N share holders to implement
a secure proactive refresh for a sharing of α ∈Zq. Prove that (i) your protocol results in a
new sharing of the same secret α, and (ii) the protocol satisﬁes the security requirement from
part (a).
Hint: Use the linearity of Shamir secret sharing from Remark 22.15. Your protocol can des-
ignate one party to generate a fresh sharing of zero by computing (α′′
1,...,α ′′
N) ←R Gsh(N,t, 0).
For i= 1,...,N , the party sends α′′
i to party i, and party i adds this α′′
i to its own share.
22.6 (Blind BLS threshold signing). In the BLS threshold signature scheme from Sec-
tion 22.2.2.1, the combiner C ﬁrst sends the message mto the signers who then return the signature
shares, as in Fig. 22.1b. The combiner then assembles the signature shares into a signature on m.
Show how the behavior of the combiner and the signers be modiﬁed so that the signers never see
the message m to be signed in the clear, and yet the combiner obtains a valid signature on m.
Hint: use BLS blind signatures from Exercise 15.7.
22.7 (Privacy against insiders). Deﬁnition 22.10 deﬁnes the concept of a private threshold
signature scheme, a PTS, where privacy holds against the public.
(a) Give an example of a PTS that satisﬁes Deﬁnition 22.10, but any one of the N signers can
use its secret key to examine a signature and determine the set of parties that generated it.
(b) Consider the following two experiments that ensure privacy against insiders. Experiment b,
for b = 0,1, proceeds as follows: (i) the adversary outputs N and t; (ii) the challenger runs
G(N,t) and sends all the resulting information to the adversary including the N secret keys;
(iii) the adversary issues a sequence of queries where query numberiis a triple (mi,J(0)
i ,J(1)
i )
containing a message and two t-size sets, and the challenger responds with a signature on mi
by obtaining signature shares from J(b)
i and combining them; (iv) the adversary outputs a
993
guess ˆb for b. We say that a PTS is insider private if no adversary can distinguish the two
experiments.
Show that the BLS threshold signature scheme is insider private.
22.8 (CPA secure threshold decryption). This exercise introduces a notion of security against
chosen plaintext attack (CPA security) for threshold decryption schemes. The attack game is the
same as Attack Game 22.5, except that decryption queries work as follows:
For j = 1,2,..., the jth decryption query consists of ( ˆmj, ˆdj) ∈M×D . The challenger
computes ˆcj ←R E(pk, ˆmj, ˆdj) along with the N decryption shares c′
j,i ←D(ski,ˆcj, ˆdj),
for i= 1,...,N , and sends ˆcj and all of the decryption shares c′
j,i to A.
A’s advantage with respect to E is denoted thCPA adv[A,E]. Note that in the CPA security
setting, associated data really plays no role, and we may assume there is none.
Now consider the threshold ElGamal decryption scheme introduced in Section 22.3.2, which is
derived from the ordinary ElGamal encryption scheme EEG introduced in Section 11.5. Show that
if EEG is semantically secure, then the threshold ElGamal decryption scheme is CPA secure.
22.9 (Threshold RSA decryption). Let us show how to enable simple threshold decryption for
the RSA public key encryption scheme of Section 11.4.1.
(a) Recall that the key generation algorithm generates numbers n,e,d , where n is the RSA
modulus, e is the encryption exponent, and d is the decryption exponent. We extend the
key generation algorithm with two more steps: choose a random integer d1 in [1,n2] and set
d2 = d1 −d∈Z. Then output the two key shares sk1 := (n,d1) and sk2 := (n,d2), and the
public key pk := (n,e). Explain how to use this setup to convert ERSA (see Section 11.4.1) to
a 2-out-of-2 threshold decryption scheme.
Hint: Show that the distribution of the key share d2 is statistically close to the uniform
distribution on {1,...,n 2}.
(b) Prove that your scheme from part (a) satisﬁes the security deﬁnition for CPA secure threshold
decryption from the previous exercise.
(c) Generalize the scheme to provide 2-out-of-3 threshold decryption, using the mechanism of
Exercise 2.20. Prove that the scheme is CPA secure.
22.10 (Threshold RSA signatures). In Exercise 22.9 we showed how a secret RSA decryption
key can be split into three shares, so that two shares are needed to decrypt a given ciphertext,
but a single share reveals nothing. In this exercise we show that the same can be done for RSA
signatures, namely two shares are needed to generate a signature, but one share reveals nothing.
(a) Use Exercise 22.9 to construct a 2-out-of-3 threshold RSA signature scheme.
(b) Prove that your scheme from part (a) satisﬁes Deﬁnition 22.4.
22.11 (From AD-only CCA security to CCA security). Attack Game 22.5 captures the
notion of AD-only CCA security for threshold decryption. In Remark 22.10, we outlined how to
modify this game game to capture full CCA security. The conversion from AD-only to full CCA
994
security in Exercise 14.13 can be easily adapted to the threshold setting, where the signature check
is performed by the decryption algorithm (as opposed to the combiner algorithm). Prove that this
conversion also coverts an AD-only CCA secure threshold decryption scheme to a fully CCA secure
threshold decryption scheme, assuming the signature scheme is strongly one-time secure.
22.12 (Threshold Schnorr signatures). Show that the Schnorr signature scheme supports t-
out-of-N threshold signing. To generate a signature we allow for three rounds of communication
between the combiner and the key servers. The servers communicate with the combiner, but
not with each other. This is quite diﬀerent from the threshold signature schemes presented in
this chapter, where signing required only a single message from each participating server to the
combiner.
22.13 (Threshold master key in identity based encryption). In Section 15.6.1 we con-
structed a number of IBE schemes. One way to protect the master key msk in an IBE scheme is
to secret share it among N parties so that t> 1 of the N parties must work together to generate
a secret key skid for an identity id. This way, no single party can misuse msk to generate skid .
(a) Recall that in the IBE scheme EBF from Section 15.6.3.1 the master key msk is a secret value
α∈Zq and skid := H0(id)α. Use t-out-of-N Shamir secret sharing, as deﬁned in Section 22.1,
to show how to secret share α among N parties so that skid = H0(id)α can be generated
without ever reconstructing α at a single location.
(b) Show that t−1 parties cannot generate skid , assuming CDH holds in G0, and H0 is modeled
as a random oracle.
22.14 (A CCA-secure threshold decryption scheme from pairings). In Section 15.6.4.1
we saw a direct construction for a CCA secure public key encryption scheme from identity based
encryption (IBE). The construction supports a public validity test for ciphertexts: anyone can
validate that a ciphertext is well formed by verifying a signature embedded in the ciphertext. In-
stantiating this general transformation using a pairing-based IBE gives a simple CCA-secure public
key threshold decryption system. In particular, consider the IBE system EBF from Section 15.6.3.1.
In Exercise 22.13 we showed that the master key in this IBE system can be secret shared among
N parties so that any t parties can generate a decryption key.
(a) Show how to apply the construction from Section 15.6.4.1 to the threshold IBE system from
Exercise 22.13 to obtain a public key threshold decryption system.
(b) Prove that your construction is secure as in Deﬁnition 22.12 and is robust.
22.15 (An alternative to Construction 22.1). Let (S,A) be an access structure as discussed
in Section 22.5. Construction 22.1 is an additive secret sharing scheme over Zq that implements
(S,A), albeit with secret shares that can be quite large. In this exercise we develop a diﬀerent
generic additive secret sharing scheme. Let U ⊆2S be the set of all unauhorized sets, namely U is
the complement of A. For i= 1,...,N , let U(i) ⊆U be all the sets in U that do not contain i. Let
us construct a secret sharing scheme ( G,C) that implements (S,A).
• G(α): for each L∈ U sample a random ρ(L) ←R Zq such that the entire collection {ρ(L)}L∈U
satisﬁes α= ∑
L∈U ρ(L). Then, for i= 1,...,N the share αi is
αi :=
{
ρ(L)
}
L∈U(i)
995
This αi contains one element in Zq for every unauthorized set that does not containi. Output
the list of shares ( α1,..., αN).
(a) Explain how algorithm C(J,{αj}j∈J) works to output α∈Zq when J∈ A. First, show that
for every authorized set J∈ A, the set {αj}j∈J must include all the elements in {ρ(L)}L∈U.
(b) Prove that the scheme ( G,C) is secure as in Deﬁnition 22.20.
Discussion: This scheme leads to smaller shares compared to Construction 22.1 when ( S,A) has
more authorized sets than unauthorized sets. The size of the secret shares can be further reduced
by generating a random ρ(L) in Zq only when L∈ U is a maximally unauthorized set (namely Lis
unauthorized, but every superset of Lis authorized).
22.16 (Revocation using threshold decryption). In this exercise we look at an unexpected
application for threshold decryption. Recall that in Section 5.6 we saw a revocation scheme that
enables a sender to broadcast an encrypted message to N recipients so that all but r of them
can decrypt. The ciphertext size was proportional to rlog2(N/r). In this exercise we develop a
revocation scheme using a threshold decryption system ( G,E,D,C ), where ciphertext size is only
proportional to r, which is better. For now, suppose there is an upper bound t on the maximum
number of recipients to revoke.
• Initialize a (t+ 1)-out-of-(N + t) threshold decryption scheme to get pk and sk1,..., skN+t.
For i = 1 ,...,N give recipient i the key ski. The sender keeps the encryption key ek :=
(pk,skN+1,..., skN+t) to itself.
• When no one is revoked, the sender encrypts a message m by ﬁrst computing c:= E(pk,m)
and then computing the decryption shares ci := D(skN+i,c) for i = 1 ,...,t . The sender
broadcasts (c,c1,...,c t). Every recipient computes the decryption share of c using its secret
key. It then uses its own decryption share plus the t decryption shares in the ciphertext to
recover m. An eavesdropper only has t decryption shares and cannot decrypt.
• Now, suppose recipients 1 and 2 are revoked. The sender operates as before, but uses keys
sk1,sk2,skN+1,..., skN+t−2 to compute the t decryption shares embedded in c. Now, recip-
ients 1 and 2 only have t decryption shares and cannot decrypt. Everyone else has t+ 1
decryption shares and can decrypt. Hence, we revoked users 1 and 2 and no one else.
(a) Exercise 5.21 deﬁnes the syntax for a broadcast encryption scheme ( G′,E′,D′). Using this
syntax, instantiate the idea above using ElGamal threshold decryption. You may assume that
tis a ﬁxed system parameter. Explain exactly how encryption E′(ek,m,S ) to recipients in a
set S works, where |S|≥ N −t, and how decryption D(ski,c) works. Show that ciphertext
size is linear in t.
(b) Exercise 5.21 deﬁnes a security model for a broadcast encryption scheme. Show that the
scheme from part (a) is secure assuming the threshold decryption scheme is secure, and the
adversary requests at most t secret keys.
(c) So far the scheme can only revoke up to an a-priori bound of t recipients. However, suppose
the sender sets up ⌈log2 N⌉such schemes, where in scheme number i the value of t is set to
t:= 2i. Show how this lets the sender revoke as many recipients as it wants, but ciphertext
size is only linear in the number of revoked recipients.
996
Chapter 23
Secure multi-party computation
Suppose several parties wish to participate in an auction. For concreteness, let us assume this is a
sealed-bid second-price auction, also called a Vickrey auction, where parties submit sealed bids, so
that no party knows how much the other parties have bid. The highest bidder wins the auction,
but the price paid is the second highest bid. Moreover, losing bids are not revealed. The reason
to use a second-price auction with sealed bids is that every bidder is then incentivized to bid their
true valuation of the item for sale, assuming no collusion among the bidders.
Now suppose the auction is to be conducted over the Internet. The auction house runs a trusted
server T that will collect bids from the various parties who wish to make bids. After collecting all
the bids, the auction house selects the highest bidder, and announces the winner and the amount
that the winner will pay. All other information from the bidders should remain secret.
It goes without saying that the communication links between the bidders and the server T
will be secured, for example, using the TLS protocol which is used to secure traﬃc between two
parties. Provided T is secure and follows the protocol, all should be well. But what if T itself is
compromised and/or does not follow the protocol? Consider the following failure scenarios:
Failure Scenario 1: T follows the protocol, collecting all of the bids and announcing the correct
winner. However, some time after running the protocol, a hacker breaks into T and retrieves
all of the bids — not just the winning bid, but all of the losing bids as well.
Even though the auction is over, the release of this information can have detrimental conse-
quences. For example, if there are subsequent auctions, this can reveal the bidding strategies
of some parties.
Failure Scenario 2: T does not follow the protocol. For example, T may collude with one of the
bidding parties P: after collecting bids from all other parties, it informs P of the highest bid
received so far, allowing P to submit a bid that just beats the current highest bid.
Failure Scenario 3: T may follow the protocol, but crash. Worse, T could crash after it has
reported the winner to some parties but not to others. Now if the parties re-run the auction,
some parties already know the value of the winning bid, and can change their bidding strategy.
The problem with using a trusted server T to implement the auction is that it is a single point
of failure. If T is hacked, the integrity of the auction may be compromised, as illustrated in the
failure scenarios outlined above.
997
f
P1 : x1,x2
P2 : no input
P3 : x3
P1 : y1,y2
P2 : y1
P3 : no output
Figure 23.1: Example inputs and outputs of three parties P1,P2,P3 to a function f
A technique that may be used to mitigate the risk of a single point of failure is to use several
servers in place of a single server. For example, the auction service could be implemented using
several servers instead of just one. The hope, then, is that if only one (or a small number) of these
servers is compromised, the integrity of the auction itself will not be compromised.
Note that simply replicating the trusted server T does not solve the problem. For example,
in Scenario 1 above, if the servers are just replicas of T, then by hacking in to any one of these
replicas, an attacker can retrieve the losing bids.
So the question of whether we can securely distribute the service provided byT to several servers
is an interesting and non-trivial question. It turns out that the answer is yes. The techniques used
to solve this type of problem go by the name multi-party computation or MPC. This is a
very rich and broad topic, and one can easily devote an entire book to it. In this chapter, we will
introduce the reader to some of the core MPC concepts and techniques.
Recall that in Section 8.12 we presented a simple auction protocol using a “commit and reveal”
approach. However, in that protocol, the server T had to reveal all the bids at the end of the
auction, so that that the parties could verify that the auction was run correctly. As explained in
Failure Scenario 1, all the non-winning bids should remain hidden. MPC techniques give us the
best of both worlds: the bidders can verify that the auction was run correctly, but the non-winning
bids remain hidden.
23.1 The basic idea of MPC
We can generalize the auction example as follows. Suppose we have a functionf that takes ninputs
and produces m outputs:
(y1,...,y m) = f(x1,...,x n).
We have N parties, P1,...,P N. Their goal is to run a protocol where each input value xi is
submitted by one of the parties, and each output yj is obtained by one or more of the parties.
Which party submits a particular input xi and which parties obtain a particular output yj is a part
of the protocol speciﬁcation. Note that an individual party need not submit any inputs or obtain
any outputs, as shown in Fig. 23.1.
23.1.1 Informal notions of security
It is challenging enough to even deﬁne secure multi-party computation, let alone to design and
analyze protocols that satisfy such deﬁnitions. We begin by discussing some informal security
requirements.
998
In an execution of a protocol, some parties may be honest and others may be corrupt. Some
essential security properties that any secure protocol should provide are privacy, soundness, and
input independence:
privacy: no party learns anything about any other party’s inputs (except for information that is
inherently revealed by the outputs);
soundness: honest parties compute correct outputs (if they compute any output at all);
input independence: all parties must choose their inputs independently of the other parties’
inputs.
In the auction example, all of these properties are clearly desirable.Privacy guarantees that only
the winning bid is revealed. Soundness guarantees that the highest bidder wins. Input independence
guarantees that no party can place a bid that depends on any other bid. For example, if Alice bidsx
dollars, it is not enough for the protocol to hide the value x from other parties; rather, it should
be infeasible for another party to use Alice’s bid to bid, say, x+ 1 dollars without knowledge of x.
Note that our informal deﬁnition of soundness says that for every honest party, if it computes
an output at all, then its output must be correct. This somewhat wishy-washy deﬁnition allows
for the possibility that corrupt parties can disrupt the protocol to such a degree that some honest
parties end up with no output at all. In some cases, this can mean that some (or all) corrupt
parties obtain their outputs, while some (or all) honest parties fail to do so. As we saw, this is not
acceptable in the auction example.
Therefore, we can deﬁne two other desirable properties.
Guaranteed output delivery: all honest parties are guaranteed to obtain their output.
Fairness: if any party (corrupt or honest) obtains their output, then all honest parties do so.
The guaranteed output delivery property is stronger than the fairness property. In the auction
example, guaranteed output delivery is important, and fairness even more so; however, in other
applications, fairness and guaranteed output delivery may not be essential. Moreover, it turns
out that achieving fairness or guaranteed output delivery typically comes at a cost in terms of
the complexity of the protocol. Therefore, it makes sense to consider diﬀerent types of protocols:
those that do not achieve fairness, those that achieve fairness but not guaranteed output delivery,
and those that achieve guaranteed output delivery. In this chapter, to keep things simple, we will
focus mostly on protocols that do not attempt to achieve fairness or guaranteed output delivery.
However, in Section 23.6.2, we will present a protocol that does achieve fairness.
What are the inputs contributed by corrupt parties? We need to address a subtle issue
regarding the inputs of corrupt parties. Even if a corrupt party is initialized with some particular
input, such a party may behave arbitrarily, and in particular, it can run the protocol using a
diﬀerent input. The notion of soundness allows for this, but it still requires that the behavior of a
corrupt party is such that, no matter what it does, it behaves as if it runs the protocol with some
speciﬁc (yet arbitrary) input. Moreover, the input independence property requires that this input
is chosen independently of the honest parties’ inputs.
23.1.2 Assumptions
To guarantee any of these security properties, protocols need to make certain assumptions.
999
Cryptographic assumptions. Not surprisingly, the security of a protocol will depend on various
cryptographic assumptions. Most protocols require secure point-to-point communication channels,
which can be implemented using the techniques in Chapter 9. Some protocols require nothing more
than that.
Number of corrupt parties. Some protocols require that a majority, or even a super-majority,
of parties are honest (a super-majority is when more than two thirds of the parties are honest).
Other protocols work no matter how many parties are corrupt. In this chapter, we will see various
protocols with diﬀerent assumptions on the number of corrupt parties.
Communication assumptions. Some protocols require that communication among parties is
synchronous, which essentially means that all messages sent from one honest party to another
honest party will be delivered within a ﬁxed amount of time. This implies that if an honest party
“times out” waiting for a message from another party, the honest party may safely conclude that
the other party is corrupt. Other protocols make no such assumptions, and work using a purely
asynchronous network, where messages may be arbitrarily delayed.
Protocols that work in an asynchronous communication model are clearly more robust than
those that rely on a synchronous communication model. In this chapter, we will focus exclusively
on protocols in the asynchronous communication model .
Types of corruption. Some protocols are secure even if the corrupt parties may behave in an
arbitrary malicious way, which includes the possibility of colluding with one another. We call
security in this sense security against malicious adversaries . This type of security protects
against Failure Scenarios 1 and 2 in the above auction example (as well as Failure Scenario 3 if the
protocol provides guaranteed output delivery, or at least fairness).
A weaker model of security that is sometimes considered is security against honest-but-
curious adversaries. This model of security only protects against the following type of attack:
while the protocol is running, all parties — even corrupt ones — faithfully follow the protocol;
however, corrupt parties may leak their internal state to an adversary. In this type of attack, the
main security property at issue is privacy: after seeing this internal state information (in addition
to all network traﬃc), the adversary should still not have any additional information about honest
parties’ inputs. (This type of security only protects against Failure Scenario 1 in the above auction
example.)
Protocols that are secure against malicious adversaries are clearly more robust than those
that are only secure against honest-but-curious adversaries. In this chapter, our main goal is
to describe protocols that are secure against malicious adversaries; however, when introducing a
new technique, we will typically start by describing a protocol that is only secure against honest-
but-curious adversaries, and then show how to enhance it to achieve security against malicious
adversaries. Presenting techniques in this way makes them a bit easier to understand.
We stress that security against honest-but-curious adversaries is typically not an acceptable
security model in the real world, and that our main reason for presenting such protocols is chieﬂy
as an aid to understanding, and as a stepping stone to protocols that are secure against malicious
adversaries.
Even with malicious adversaries, one can consider two types: those that decide which parties to
corrupt at the very beginning of the protocol execution, or those who use a more adaptive strategy,
1000
perhaps watching network traﬃc for a while, and based on this, corrupting one party, and then
based on information learned after that, corrupting another party, and so on. If the adversary
is of the ﬁrst type, we say the corruptions are static, and otherwise, we say the corruptions
are adaptive. Security against adaptive corruptions is stronger, and therefore, more desirable;
however, for simplicity, we will focus exclusively on static corruptions.
23.1.3 How to deﬁne security formally
The way to deﬁne security formally is to say that an attack on the protocol in the “real world” is
equivalent to some attack on the protocol in an “ideal world” in which no damage can be done.
In the ideal world, the protocol is implemented using a trusted party to which all parties (both
honest and corrupt) submit inputs, and from which all parties (both honest and corrupt) obtain
their designated outputs. The trusted party itself cannot be corrupted.
In somewhat more detail, in such a deﬁnition, we ﬁrst carefully describe the details of the real-
world protocol execution, including the powers of the adversary, as well as the characteristics of the
communication network. In terms of the assumptions discussed in Section 23.1.2, in the deﬁnition
we present, we will mainly be assuming an asynchronous communication network, and a malicious
adversary who statically corrupts various parties.
We then describe the details of the ideal-world execution, including the behavior of the trusted
party who evaluates the function.
The deﬁnition of security then basically says: for every eﬃcient adversary Ain the real world,
there exists an “equivalent” eﬃcient adversary Sin the ideal world (usually called a simulator).
Since there is no possible attack in the ideal world, there is no possible attack in the real world.
In particular, the informal notions of privacy, soundness, and input independence are implied by
this type of deﬁnition. Indeed, these informal notions are clearly attained in the ideal world, and
therefore, because the real world and ideal worlds are “equivalent”, they must also be attained in
the real world. Clearly, to make such an argument more rigorous, we need to have a more rigorous
deﬁnition of what “equivalent” means, and we will give one later in the chapter in Section 23.5.
The formal deﬁnition of security we present in Section 23.5 will not provide for any assurance of
guaranteed output delivery or fairness.
23.1.4 Other applications of MPC
Before going any further, we present a few more examples of where MPC can be helpful.
Elections. Instead of an auction service, we may consider another type of service, such as an
election service. Without MPC, parties submit votes to a trusted server T who tallies the votes
and announces the winner (without revealing any individual votes). Using MPC, the goal is to
implement T as a distributed system of servers that remains secure even if a small number of
these servers is hacked. Note that in Section 20.3.1 we considered some aspects of such an election
protocol. While the techniques presented there are useful, they do not completely solve the problem.
Note that like the auction example, guaranteed output delivery is important, otherwise a single
misbehaving server could prevent the results of the election from being produced.
Privacy-preserving mining of medical data. Several hospitals store medical data on a large
number of patients. A scientist wishes to carry out a medical study. The goal of the study is to ﬁnd
1001
out if there is a signiﬁcant correlation among several variables represented in the data stored by
the hospitals. Because of privacy concerns and regulation, the hospitals do not wish to share any
data with each other. One solution to this problem is to have the hospitals and the scientist run an
MPC protocol that allows the scientist to test her hypothesis, without any hospital revealing any
data to the scientist or to any other hospital. Note that unlike the auction and voting examples,
neither guaranteed output delivery nor fairness are essential in this example. In fact, because the
hospitals are believed to be honest, but are not allowed to see all the data, it may even be suﬃcient
to use a protocol that only provides security against honest-but-curious adversaries.
Privately detecting misbehavior in the ﬁnancial system. Banks are frequently tasked
with identifying misbehavior, such as money laundering. Abstractly, one can think of ﬁnancial
transactions as a graph, where the vertices are accounts, and every funds transfer is a directed
edge in the graph. Every bank only has a partial view of the graph: it only sees transfers that
originate or terminate in an account held at that bank. Detecting a money laundering pattern,
possibly spanning accounts at multiple banks in multiple countries, requires a global view of the
graph. Yet, due to privacy concerns and regulation, banks cannot share this data with each other,
or with any single government. Instead, the banks can run an MPC protocol with each other to
look for a global pattern in the transaction graph, without learning any other information about
the graph. Here guaranteed output delivery is important, since otherwise, a single corrupt bank
may prevent all honest banks from learning the results.
Protecting a secret key by splitting it. Consider a challenge-response identiﬁcation protocol
based on a message authentication code, such as the CRYPTOcard protocol described in Sec-
tion 18.6.1. For each user, the server has to store a secret key used for a message authentication
code. For concreteness, let us assume that the message authentication code is implemented as a
PRF F deﬁned over (K,C,T), where K= {0,1}ℓ. It is essential that the key k∈K is never revealed
to an attacker. To make this less likely, the server does not store the key — in fact, the key is never
stored in any one machine. Rather, the key is split into pieces, or “shares”, k1 and k2, so that
k = k1 ⊕k2. The server S only stores the share k1, and an auxiliary server, S′, stores the other
share k2. Now, to run the identiﬁcation protocol, the server S generates a random challenge c∈X,
sends this to the user and receives a response t ∈Y. To validate the user, S must now compute
F(k,c) and test whether this is equal to t. To do this, the two servers S and S′ will run an MPC
protocol that will allow S to obtain F(k1 ⊕k2,c), based on the input k1 provided by S, and the
input k2 provided to S′ — in this example, the value c can be viewed as a publicly known value,
rather than a private input. Note that unlike the auction and voting examples, neither guaranteed
output delivery nor fairness are essential in this example.
This application may seem familiar: it is very similar to threshold cryptography discussed in
Chapter 22. In fact, threshold cryptography is a special case of MPC — parties who are holding
shares of a secret key wish to use the key without revealing anything else about their shares. In
Chapter 22 we saw that for certain algebraic signature and decryption schemes, one can design
custom threshold protocols that are very simple and eﬃcient, without using the full machinery
of MPC. For a PRF such as AES, one needs MPC to securely distribute the key (although, see
Exercise 4.28).
1002
Federated machine learning. Consider an organization that wants to train a machine learning
(ML) model to recommend movies. For training data, it wants to use the combined preferences
from a large group of people. Many people, however, are unwilling to reveal their preferences, lest
they be judged for their taste in movies. Instead, the organization could run an MPC protocol with
the group of people so that the ﬁnal ML model is available to everyone, and nothing is revealed
about the participant’s individual preferences other than the model. This approach to training a
model is called federated machine learning. It enables a group of contributors to train a model
without revealing their personal data. Of course, one has to ensure that the resulting ML model
does not leak the underlying training data, and this is often addressed by an appropriate use of
statistical techniques to introduce noise into the model. It is a good reminder that MPC does not
solve all privacy problems in the world, and that often, additional techniques are needed.
Zero knowledge. In Chapter 20 we saw zero knowledge protocols, where the prover has a state-
ment x and a witness w, and the veriﬁer only has the statement x. The prover wants to convince
the veriﬁer that there exists a valid witness w for x, namely that R(w,x) = 1, for an agreed
upon relation R. This is a special case of two party computation, where the veriﬁer’s output is
R(w,x) ∈{0,1}and the prover’s output is nil. The protocol needs to be private to ensure that the
veriﬁer learns nothing about the witness, and maliciously secure to ensure that a cheating prover
cannot fool the veriﬁer.
23.2 Securely evaluating arithmetic circuits
Recall the general goal. We have a function f that takes n inputs and produces m outputs:
(y1,...,y m) = f(x1,...,x n).
We also have N parties, P1,...,P N. Their goal is to run a protocol where each input value xi is
contributed by one of the parties, and each output yj is obtained by one or more of the parties.
Ultimately, we want to design an eﬃcient protocol for this problem that provides privacy,
soundness, and input independence, as discussed in Section 23.1.1. Our ﬁrst examples of such
protocols will assume that the function f is represented in terms of an arithmetic circuit.
23.2.1 Arithmetic circuit evaluation
Let q be a prime. An arithmetic circuit for Zq is a circuit with four types of gates: addition,
multiplication, constant addition, and scalar multiplication.
• An addition gate takes two inputs, x,y ∈Zq and produces a single output z= x+ y∈Zq.
• A multiplication gate takes two inputs, x,y ∈Zq and produces a single output z= x·y∈Zq.
• A constant addition gate takes one input x∈Zq and produces a single output z= x+ c∈Zq,
where c∈Zq is a constant associated with the gate.
• A scalar multiplication gate takes one input x∈Zq and produces a single output z= cx∈Zq,
where c∈Zq is a constant associated with the gate.
1003
x 1
x 2
x 3
x 4
x 5
x 6
x 7⇥
+
⇥ 3
⇥
Figure 23.2: An arithmetic circuit
The input to a gate may be an input to the circuit or the output of some other gate. There is no
restriction on the “fan out” of a gate: the output of a gate may be used as an input in any number
of gates. The output of some gates are designated as circuit outputs.
Fig. 23.2 shows a simple arithmetic circuit. This particular circuit has three inputs, x1, x2,
and x3, and two outputs x6 and x7. For each input to the circuit, and each gate output, we have
drawn a “wire” on which the value input to the circuit or computed by the gate is carried. Besides
the three input wires and two output wires, there are two internal wires, which carry the internal
values x4 and x5. In general, such a circuit should contain no cycles. This guarantees that once the
values carried on the circuit’s input wires are determined, we can calculate the values carried on
all of the remaining wires in the circuit by evaluating the gates one at a time, in some ﬁxed order
(known as a “topological ordering”), so that by the time we evaluate a particular gate, the values
carried on its input wires have already been calculated.
In the example in Fig. 23.2, if we feed input values x1, x2, and x3 to the circuit, then the values
on the remaining wires may be computed as follows:
x4 ←x1 + x2
x5 ←x2 ·x3
x6 ←x4 ·x5
x7 ←3 ·x5
where x6 and x7 are the outputs of the circuit.
Every output of an arithmetic circuit can be written as a multivariate polynomial over Zq,
where the variables of the polynomial are the inputs to the circuit. For example, in Fig. 23.2 we
see that
x6 = f(x1,x2,x3) := (x1 + x2) ·x2 ·x3.
However, an arithmetic circuit is more than just a multivariate polynomial: the circuit gives a
speciﬁc recipe for evaluating the polynomial.
As we shall see, the eﬃciency of the protocols we design for secure arithmetic circuit evaluation
will depend on the size of the circuit — primarily on the number of multiplication gates in the
circuit.
Restricting ourselves to arithmetic circuits is not a substantive restriction. First, from a theoret-
ical point of view, any function that can be eﬃciently computed by an algorithm, can be translated
1004
into an arithmetic circuit whose size is polynomial in the running time of the algorithm and the
size of the input. Second, from a more practical point of view, once we construct an MPC protocol
for arithmetic circuits, there are a number of techniques that can leverage the protocol to eﬃciently
construct an MPC protocol for any polynomial time computation.
23.2.2 Beaver’s protocol: an honest-but-curious 2.5-party protocol
We shall present a protocol that allows two parties to securely evaluate an arithmetic circuit. As
presented, the protocol is not exactly a two-party protocol: it makes use of a pre-processing phase
that involves a third party, who we call the dealer. This pre-processing phase can be done before
either of the two parties receive their inputs to the protocol. The dealer provides some data to the
two parties that they will use when they eventually run the protocol; however, the dealer does not
otherwise participate in the protocol. Hence, we call this a “2.5-party” protocol.
If needed, one can modify the protocol to eliminate the dealer, so that the protocol becomes
a true 2-party protocol (see Exercise 23.3). However, it is easiest to describe the protocol using a
dealer.
We want to design a protocol that lets two parties, call them P1 and P2, securely evaluate an
arithmetic circuit (with the help of the dealer). We assume the circuit itself is ﬁxed and publicly
known. We assume the circuit takes a number of inputs x1,...,x n and produces some number
of outputs y1,...,y m, where each xi and yj is an element of Zq, for some prime q. Some inputs
will be supplied by P1 and the others will be supplied by P2. Some outputs will be obtained by
P1 and some will be obtained by P2 — some outputs may be obtained by both P1 and P2. The
goal is to design a protocol that allows P1 and P2 to securely evaluate the circuit. As discussed in
Section 23.1.1, this means that the protocol provides privacy, soundness, and input independence:
privacy: neither party learns anything about the other party’s inputs (except for information that
is inherently revealed by the outputs);
soundness: honest parties compute correct outputs (if they compute any output at all);
input independence: both parties must choose their inputs independently of the other’s.
Our ﬁrst attempt at such a protocol will only provide security against honest-but-curious ad-
versaries. That is, we assume that both parties correctly follow the protocol. In this case, the main
security concern is privacy.
High level description of the protocol. Here is a very high-level idea behind the protocol.
Imagine that we evaluated the circuit directly, using inputs as supplied by P1 and P2. As discussed
above, once all of the inputs to the circuit have been supplied, each wire in the circuit carries some
particular value x ∈Zq. Suppose that instead of having either P1 or P2 store the value x, they
each store a “share” of x. Namely, P1 stores a value x1 ∈Zq, and P2 stores a value x2 ∈Zq, such
that x= x1 + x2, but neither x1 nor x2 alone leaks any information about x.
The protocol begins with the parties P1 and P2 sharing the values of their input wires. For
example, if x is one of P1’s inputs, then P1 sends a random x2 ∈Zq to P2 and keeps x1 = x−x2
to itself. This way, x1 + x2 = x where P1 holds the share x1 and P2 holds the share x2. Observe
that party P2’s share x2 reveals nothing to P2 about P1’s input x.
1005
Next, they process the gates in the circuit one by one. If the left input wire to a certain gate
carries the value x, and the right input wire carries the values y, then by construction, x= x1 + x2
and y = y1 + y2, where P1 holds the shares x1,y1 and P2 holds the shares x2,y2. Their goal is to
compute a sharing z= z1 + z2 of the output wire from the gate, so that P1 holds the share z1 and
P2 holds the share z2. We will show below that for addition, scalar multiplication, and constant
addition gates, this is straightforward. For a multiplication gate, creating a sharing z = z1 + z2
takes more work, and this is the only situation where the dealer is needed.
Eventually P1 and P2 obtain a sharing of all the output wires. If x = x1 + x2 is a sharing of
an output wire that P1 is supposed to learn, then P2 sends its share x2 to P1, and P1 computes
x1 + x2 to obtain the desired output x. They do the same for P2’s output wires (but with their
roles reversed).
23.2.2.1 The details
Now, let’s see the details. We start by describing how parties P1 and P2 process the input wires,
and then explain how they process each of the four types of gates (addition, scalar multiplication,
constant addition, and multiplication) and output wires. The dealer is only needed when processing
a multiplication gate.
Processing input wires. Suppose that the value x∈Zq is carried on an input wire for an input
provided by P1. In this case, P1 can choose x2 at random in Zq and set x1 ←x−x2. Now, P1
sends x2 to P2, and keeps x1 for itself. Clearly, x1 and x2 have the desired properties: x= x1 + x2,
but neither x1 nor x2 alone leak any information about x.
Processing addition gates. Now suppose we have an addition gate with inputsxand y. Assume
that P1 and P2 have already computed shares of xand yas described above: P1 holds x1,y1 and P2
holds x2,y2, where x= x1 + x2 and y= y1 + y2. They want to compute a sharing of z= x+ y. But
this is easy to do: P1 simply computes z1 ←x1 + y1 and P2 computes z2 ←x2 + y2. No interaction
is required. Clearly, z1 and z2 is a sharing of z, in the sense that z = z1 + z2. Moreover, if x1 and
y1 leaks no information to P1 about x or y, then z1 = x1 + y1 cannot leak any information to P1
about z, simply because P1 computed z1 locally. Similarly, z2 cannot leak any information to P2
about z.
Processing scalar multiplication gates. Now suppose we have a scalar multiplication gate
with input x. Assume that P1 and P2 have already computed shares of x as described above: P1
holds x1 and P2 holds x2, where x = x1 + x2. They want to compute a sharing of z = cx for a
constant cassociated with the gate. To do this, party P1 locally computes z1 ←cx1, and P2 locally
computes z2 ←cx2. Clearly, z1 and z2 is a sharing of z, in the sense that z= z1 + z2.
Processing constant addition gates. Now suppose we have a constant addition gate with
input x. Assume that P1 and P2 have already computed shares of x as described above: P1 holds
x1 and P2 holds x2, where x= x1 +x2. They want to compute a sharing of z= x+cfor a constant
c associated with the gate. To do this, party P1 just locally computes z1 ←x1 + c, and P2 locally
computes z2 ←x2. Clearly, z1 and z2 is a sharing of z, in the sense that z= z1 + z2.
1006
Processing multiplication gates. Handling multiplication gates is the tricky and interesting
part. To make the protocol work for multiplication gates, we will make use of a “dealer” D, which
(as discussed above) provides some information to P1 and P2 as part of a pre-processing phase.
Speciﬁcally, D will supply P1 and P2 with random sharings of triples a,b,c ∈Zq, such that a and
b are randomly chosen, and c= a·b. More precisely,
• D gives P1 a1,b1,c1 in Zq,
• D gives P2 a2,b2,c2 in Zq,
where a1,b1,c1,a2,b2,c2 are random, subject to
(a1 + a2)(b1 + b2) = (c1 + c2).
We call this a Beaver triple sharing.
The dealer Dmay generate a Beaver triple sharing, for example, by ﬁrst choosinga1,b1,a2,b2,c2
in Zq at random, and then computing c1 ←(a1 + a1)(b1 + b2) −c2. The dealer gives P1 the values
a1,b1,c1, and P2 the values a2,b2,c2.
The dealer genereates one such Beaver triple sharing for each multiplication gate in the circuit.
This can be done in a pre-processing phase, before any inputs are known to either party. The dealer
D plays no other role in the protocol. As we are still focused on the honest-but-curious security
model, we shall assume (for now) that the dealer is honest as well, and follows the above protocol
exactly.
So now consider a multiplication gate with inputs x and y. Assume that P1 and P2 have
already computed shares of xand y as described above: P1 holds x1,y1 and P2 holds x2,y2, where
x = x1 + x2 and y = y1 + y2. They want to compute a sharing of z = x·y. Moreover, assume
that P1 and P2 have a Beaver triple sharing, that is, they have shares of a,b,c , where c = ab, as
supplied by the dealer D. That is, P1 holds a1,b1,c1, and P2 holds a2,b2,c2, where a = a1 + a2,
b= b1 + b2, and c= a·b= c1 + c2.
To understand how the protocol works, observe that
z= xy= (x−a+ a)(y−b+ b) = ((x−a)  
=:u
+a)((y−b)  
=:v
+b)
= uv+ ub+ va+ c.
So the idea is that using the logic for addition and scalar multiplication gates, each party locally
computes its share of u= x−aand v= y−b. After this, the two parties exchange their shares of
uand v with each other so that they both can compute the values uand v. Now, since each party
knows that values uand v, and has a share of the values a, b, and c, they can each locally compute
a share of z from the equation
z= uv+ ub+ va+ c
and the logic for processing addition, scalar multiplication, and constant addition gates. The details
of the protocol are given in Fig. 23.3. Intuitively, the only thing each party learns by doing this
are the values u and v; moreover, u and v are essentially “one-time pad” encryptions of x and y,
so each party actually learns nothing at all about the values x and y.
1007
P1 : input x1,y1,a1,b1,c1 ∈Zq P2 : input x2,y2,a2,b2,c2 ∈Zq
u1 ←x1 −a1
v1 ←y1 −b1
u2 ←x2 −a2
v2 ←y2 −b2
u1,v1−−−−−−−−−−−−−−−−→
u2,v2←−−−−−−−−−−−−−−−−
u←u1 + u2, v←v1 + v2
z1 ←uv+ ub1 + va1 + c1
u←u1 + u2, v←v1 + v2
z2 ←ub2 + va2 + c2
Figure 23.3: Protocol for processing a multiplication gate
Processing output wires. Finally, suppose some output wire carries a value x. Suppose P1
and P2 have already computed a sharing of x, so P1 holds x1 and P2 holds x2, where x= x1 + x2.
Moreover, suppose this value xis designated as an output to be obtained by P1. Then in this case,
P2 sends its share x2 of x to P1, and P1 computes x←x1 + x2. Similarly, if x is designated as an
output to be obtained by P2, then P1 sends its share x1 of xto P2, and P2 computes x←x1 + x2.
Putting everything together. So the whole protocol runs as follows.
• Pre-processing phase: The dealer D ﬁrst distributes shares of Beaver triples to P1 and P2, as
described above, one Beaver triple per multiplication gate. The dealer is no longer needed
after this phase.
• Input phase: The parties P1 and P2 process each of the input wires to create a sharing of all
the inputs to the circuit.
• Evaluation phase: The parties P1 and P2 proceed gate by gate. For each gate, as soon as they
compute the shares of the values carried on the input wires for that gate, they can compute
the shares of the value carried on the output wire of that gate. Viewing the circuit as a
directed acyclic graph, the parties can use any topological ordering of the vertices in order to
schedule the order in which gates are processed.
• Output phase: Finally, the parties P1 and P2 process each of the output wires.
In the evaluation phase, every multiplication gate requires P1 to send two Zq elements to P2, and
vice versa. To reduce the “round complexity” of the protocol, the circuit can be processed level by
level, processing all the multiplication gates in a single level in parallel.
Performance. The amount of data exchanged between the three parties is proportional to the
number of multiplication gates in the circuit. The number of rounds of communication between
the parties P1 and P2 is proportional to the longest path of multiplication gates in the circuit, also
known as the multiplication depth of the circuit. The dealer is only needed in a pre-processing
phase before the party’s inputs are known, and need not participate in the actual joint evaluation
of the circuit.
1008
An interesting aspect of the protocol is that it does not rely on cryptographic assumptions.
Security when the parties honestly follow the protocol is unconditional.
23.2.3 Abstracting Beaver’s 2.5-party protocol
Before going further, we “repackage” Beaver’s 2.5-party protocol by introducing a simple layer of
abstraction. This will make it easier to generalize the protocol to handle more parties, and to
enhance the protocol so that it is secure against malicious adversaries, rather than just honest-but-
curious adversaries.
23.2.3.1 Sharings
For a value x∈Zq, we denote by [x] a sharing of xbetween P1 and P2, so that P1 holds x1 and P2
holds x2, where x= x1 + x2. We may write [ x] = (x1,x2), but it is also to be understood that P1
holds x1 and P2 holds x2.
We describe some low-level sub-protocols for working with such sharings.
Open a sharing [x] to Pi: if P1 and P2 hold a sharing [x] = (x1,x2), then P3−i sends x3−i to Pi,
and Pi computes the value x←x1 + x2.
Add two sharings: [z] ←[x] + [y]: if P1 and P2 hold a sharing [ x] = (x1,x2) and [ y] = (y1,y2),
then they both locally compute a sharing [ z] = (z1,z2) of z = x+ y, by simply adding their
shares: P1 computes z1 ←x1 + y1, and P2 computes z2 ←x2 + y2.
Multiply a sharing by a constant: [z] ←c[x]: if P1 and P2 hold a sharing [ x] = (x1,x2), and
c ∈Zq is a publicly known value, then they both locally compute a sharing [ z] = (z1,z2) of
z = cx, by simply multiplying their shares by c: P1 computes z1 ←cx1, and P2 computes
z2 ←cx2.
Add a constant to a sharing: [z] ←[x] + c: if P1 and P2 hold a sharing [ x] = ( x1,x2), and
c ∈Zq is a publicly known value, then they both locally compute a sharing [ z] = (z1,z2) of
z = x+ c by the following local computation: P1 computes z1 ←x1 + c, and P2 computes
z2 ←x2.
Note that among these sub-protocols, the only one that requires any interaction is the open
sub-protocol. The others involve only local computations.
23.2.3.2 Pre-processing phase
Recall that Beaver’s 2.5-party protocol starts with a pre-processing phase in which a dealer D
distributes sharings to P1 and P2 of various values.
Protocol 23.1 (The dealer’s protocol). To create a random sharing [x] = (x1,x2) of a value
x ∈Zq, the dealer D simply chooses x1 ∈Zq at random, and sets x2 ←x−x1 ∈Zq, and gives
x1 to P1 and x2 to P2. The dealer distributes shares of a number of random elements and Beaver
triples:
• A singleton sharing is a random sharing [ a] of a random element a∈Zq.
1009
• A Beaver triple sharing is a triple of random sharings ([ a],[b],[c]), where a,b ∈Zq are
chosen at random, and c= ab.
The dealer generates one singleton sharing [ a] for each input wire in the circuit, and one Beaver
triple sharing ([a],[b],[c]) for each multiplication gate in the circuit. 2
23.2.3.3 Beaver’s 2.5 party protocol
We can now state the main protocol:
Protocol 23.2 (Beaver’s 2.5-party protocol). After obtaining the required singleton and
Beaver triple sharings from the dealer D, parties P1 and P2 ﬁrst process input wires, then process
gates in a topological order, and then process output wires, as follows.
Input wire for Pi: to produce a sharing [ x] of one of Pi’s inputs x ∈Zq, a singleton sharing [ a]
from the dealer is used as follows:
1. execute open [ a] to Pi;
2. Pi sends δ←x−a to P3−i;
3. execute [ x] ←[a] + δ.
Addition gate: to add [x] and [y], obtaining [z] = [x+ y], the parties execute [ z] ←[x] + [y].
Scalar multiplication gate: to multiply [ x] by a constant c, obtaining [ z] = [ cx], the parties
execute [z] ←c[x].
Constant addition gate: to add a constant c to [x], obtaining [ z] = [x+ c], the parties execute
[z] ←[x] + c.
Multiplication gate: to multiply [ x] and [ y], obtaining [ z] = [ xy], a Beaver triple sharing
([a],[b],[c]) from the dealer is used as follows:
1. execute [ u] ←[x] −[a];
2. execute [ v] ←[y] −[b];
3. execute open [ u] and open [ v] to both P1 and P2;
4. execute [ z] ←uv+ u[b] + v[a] + [c].
Output wire for Pi: to give to Pi the value x of a sharing [ x], execute open [ x] to Pi. 2
This protocol is essentially the same as that described in Section 23.2.2. The only substantive
diﬀerence is the logic of processing the inputs. In this protocol, we make use of one singleton
sharing [a] per input, generated in the ﬁrst bullet of Protocol 23.1. This [ a] is used to share an
input x between the two parties, so that the two parties end up with a random sharing of x. The
formulation here is more suitable to further generalizations and enhancements, as we shall see.
Observe that the only interactive steps in the protocol (not including the pre-processing step), are
Steps 1 and 2 of the sub-protocol for processing input wires, and Step 3 of the sub-protocol for
processing multiplication gates, and the sub-protocol for processing output wires.
1010
23.2.4 A maliciously secure version of Beaver’s 2.5-party protocol
We now present a variant of Beaver’s 2.5-party protocol with pre-processing that is secure against
malicious adversaries. As before, this protocol involves two parties, P1 and P2, as well as a dealer
D who distributes information to P1 and P2 in a pre-processing phase. The protocol is secure even
if one of P1, P2, or Dis corrupt. Again, as discussed in Section 23.1.1, this means that the protocol
provides privacy, soundness, and input independence.
One can ask for more, and require that the protocol be secure even if P1 and D are corrupt, or
P2 and D are corrupt. While the protocol can be made secure against two corrupt players, here we
will only consider the single corruption case. This is called the honest majority setting, because we
are assuming that at least two of the three parties are honest.
23.2.4.1 Authenticated sharings
The idea is to use authenticated sharings. Intuitively, an authenticated sharing JxK of a value x∈Zq
has all the properties of an ordinary sharing ofx, but the operation of opening JxK to either P1 or P2
is authenticated, so that, say, if P1 opens JxK to P2, the value obtained by P2 is guaranteed to be x
(with overwhelming probability). We will build such authenticated sharings from ordinary sharings
using a very simple type of message authentication code. Speciﬁcally, as part of the pre-processing
phase, the dealer will distribute sharings [ K(1)] and [K(2)] to P1 and P2, where K(1) and K(2) are
random elements of Zq. Moreover, as part of the pre-processing phase, P1 and P2 will execute a
very simple interactive sub-protocol that allows each party Pi to reliably obtain the value K(i),
without learning anything about K(3−i).
An authenticated sharing JxK of a value x ∈ Zq consists of three ordinary sharings
([x],[x(1)],[x(2)]). Concretely, P1 has (x1,x(1)
1 ,x(2)
1 ) and P2 has (x2,x(1)
2 ,x(2)
2 ), such that x1 +x2 = x,
x(1)
1 + x(1)
2 = x(1), and x(2)
1 + x(2)
2 = x(2). Such an authenticated sharing JxK = ([x],[x(1)],[x(2)]) of
x∈Zq is called valid if
x(1) = K(1)x and x(2) = K(2)x.
We now show how the low-level subprotocols required for Beaver’s 2.5-party protocol can be adapted
from ordinary sharings to authenticated sharings. We describe the protocols in terms of the corre-
sponding sub-protocols for ordinary sharings.
Open a sharing JxK to Pi: if P1 and P2 hold an authenticated sharing JxK = ([x],[x(1)],[x(2)]),
then do the following:
1. execute open [ x] and [x(i)] to Pi;
2. Pi checks that K(i)x= x(i); if not, Pi aborts the protocol.
Add two sharings: JzK ←JxK + JyK: if P1 and P2 hold an authenticated sharing JxK =
([x],[x(1)],[x(2)]) and JyK = ([ y],[y(1)],[y(2)]), then they locally compute an authenticated
sharing JzK = ([z],[z(1)],[z(2)]) of z= x+ y by executing:
[z] ←[x] + [y], [z(1)] ←[x(1)] + [y(1)], [z(2)] ←[x(2)] + [y(2)].
Multiply a sharing by a constant: JzK ←cJxK: if P1 and P2 hold an authenticated sharing
JxK = ([ x],[x(1)],[x(2)]), and c ∈Zq is a publicly known value, then they locally compute
an authenticated sharing JzK = ([z],[z(1)],[z(2)]) of z= cx by executing:
[z] ←c[x], [z(1)] ←c[x(1)], [z(2)] ←c[x(2)].
1011
Add a constant to a sharing: JzK ←JxK + c: if P1 and P2 hold an authenticated sharing JxK =
([x],[x(1)],[x(2)]), and c∈Zq is a publicly known value, then they locally compute an authen-
ticated sharing JzK = ([z],[z(1)],[z(2)]) of z= x+ c by executing:
[z] ←[x] + c, [z(1)] ←[x(1)] + c[K(1)], [z(2)] ←[x(2)] + c[K(2)].
One can easily verify that for the addition, scalar multiplication, and constant addition sub-
protocols, if the two parties start with valid authenticated sharings of the inputs, and they both
follow the protocol, they end up with a valid authenticated sharing of the output.
Now consider the open sub-protocol. Suppose the two parties start with a valid authenticated
sharing JxK of a value x. If P3−i follows the protocol, then Pi will not abort and will compute the
value x. Now consider what happens if P3−i does not follow the protocol. In this case, after Step 1
of the sub-protocol, Pi could end up with incorrect values x+ δ and x(i) + δ(i), instead of x and
x(i), where δ ̸= 0 or δ(i) ̸= 0. We want to argue that Pi ends up with incorrect values with very
small probability, namely, 1/q. Indeed, if Pi does not abort, then the following equation holds:
K(i)(x+ δ) = x(i) + δ(i), (23.1)
and since x(i) = K(i)x, equation (23.1) is equivalent to
K(i)δ= δ(i). (23.2)
Moreover, we may assume that K(i) is independent of δ and δ(i). It is clear that (23.2) holds with
probability at most 1 /q: for any ﬁxed δ,δ(i), if δ ̸= 0, then it holds with probability 1 /q, while if
δ= 0 and δ(i) ̸= 0, then it holds with probability 0.
23.2.4.2 Pre-processing phase
We shall ﬁrst present a protocol that is secure assuming that the dealer D is not corrupt, but
remains secure if one of P1 or P2 is corrupt. Later, we will show how to protect against a dishonest
dealer (but assuming that neither P1 or P2 is corrupt).
Protocol 23.3 (The dealer’s protocol (authenticated sharings)). The dealer distributes a
number of random sharings and authenticated sharings:
• singleton sharings of [K(1)] and [K(2)], for random K(1),K(2) ∈Zq;
• several authenticated singleton sharings JaK, for random a ∈Zq — one for each input
wire, plus two additional authenticated singleton sharings (which will be used in the reliable
key opening sub-protocol described below);
• several authenticated Beaver triple sharings (JaK,JbK,JcK), where a,b ∈Zq are random,
and c= ab — one for each multipliplication gate. 2
After running Protocol 23.3, parties P1 and P2 need to run a special “reliable key opening”
sub-protocol that allows P1 to obtain K(1) and P2 to obtain K(2).
Protocol 23.4 (Reliable key opening sub-protocol). In this protocol, we reliably open [ K(i)]
to Pi using an authenticated singleton sharing JaK = ([a],[a(1)],[a(2)]) from the dealer, as follows:
1012
1. execute open [ a], [a(i)], and [K(i)] to Pi;
2. Pi checks that K(i)a= a(i); if not, Pi aborts the protocol. 2
Consider what happens if P3−i does not follow the protocol. We will show that in this case, Pi
aborts with probability ≥1 −1/q. Suppose that after opening the sharings for a, a(i), and K(i),
party Pi ends up with values a+ δ, a(i) + δ(i), and K(i) + ϵ, where at least one of δ, δ(i), or ϵis not
zero. If Pi does not abort, then the following equation holds:
(K(i) + ϵ)(a+ δ) = (a(i) + δ(i)), (23.3)
and since a(i) = K(i)a, equation (23.3) is equivalent to
K(i)δ+ aϵ+ ϵδ= δ(i). (23.4)
Moreover, we may assume that K(i) and aare independent of each other and of δ, δ(i), and ϵ. It is
clear that (23.4) holds with probability at most 1 /q: for any ﬁxed δ,δ(i),ϵ, if (δ,ϵ) ̸= (0,0), then it
holds with probability 1/q, while if ( δ,ϵ) = (0,0) and δ(i) ̸= 0, then it holds with probability 0.
23.2.4.3 Beaver’s maliciously secure 2.5-party protocol (with a trusted dealer)
Using the setup we built, it is now quite easy to describe the maliciously secure version of Beaver’s
protocol. The protocol is as follows:
• step 1: P1 and P2 run the reliable key opening sub-protocol for both K(1) and K(2),
• step 2: P1 and P2 run Protocol 23.2 exactly as is, except we replace all ordinary sharings [ ·]
with authenticated sharings J·K.
The net result is that by moving from ordinary sharings to authenticated sharings, the basic protocol
becomes maliciously secure. A ﬁne point:
A party Pi must not run any output wire subprotocol until
• it has received the value δ in Step 2 of the input wire subprotocol for every input
to P3−i, and
• it has actually received all of its own inputs.
23.2.4.4 Security
We will formally analyze the security of this protocol later, in Section 23.5.9, after we have a more
rigorous deﬁnition of security. For now, we give heuristic arguments as to why the protocol provides
privacy, soundness, and input independence.
Privacy. Privacy holds, at least heuristically, assuming that the dealer is not corrupt. Each party
Pi only sees random shares of values, rather than the values themselves, except at the end of the
protocol when the output values are revealed.
1013
Soundness. Suppose that P1 is corrupt and D and P2 are honest. By the above analysis (circa
(23.4)), after the dealing phase and the reliable key opening phase, with overwhelming probability
(assuming q is large enough), party P2 holds the correct authentication key K(2), which remains
unknown to P1.
During the execution of the circuit evaluation protocol, by the above analysis (circa (23.2)),
whenever P1 opens a share to P2, with overwhelming probability, party P1 must open the share
correctly. Thus, P1 is essentially constrained to follow the protocol.
Now consider P1’s inputs. It can choose its inputs however it likes. However, it must eﬀectively
commit to each of its individual inputs xat the point in time that the corresponding input wire is
processed. Recall the steps involved:
1. execute open JaK to P1;
2. P1 sends δ←x−a to P2;
3. execute JxK ←JaK + δ.
Here, a is a value generated by the dealer. A corrupt P1 can send any value δ it likes to P2, but
then its eﬀective input is x := a+ δ. It is at this time that P1 is fully committed to x. More
precisely, the function computed by the circuit will be a function of x (as well as all of the other
inputs provided by P1 and P2).
Input independence. One can also argue that the eﬀective inputs provided by P1 are inde-
pendent of P2’s inputs. Intuitively, this follows from the fact that at the time P1 commits to an
eﬀective input x as above, the value x is statistically independent of P2’s inputs.
23.2.4.5 Keeping the dealer honest
Now suppose that the dealer Dis corrupt, and does not follow the protocol. Since we are assuming
that at most one of P1, P2, or D is corrupt, then it must be the case that P1 and P2 are honest.
So in this case, the only thing we need to do is to ensure that the sharings distributed by D satisfy
the requisite relations: if they do not, the two honest parties P1 and P2 could compute incorrect
outputs. We do not, however, have to worry about whether or not D chooses values according to
any particular distribution.
The general version of the problem that we need to solve is as follows. The dealer sends to
party P1 values
a11,...,a m1, b 11,...,b m1, c 11,...,c m1 ∈Zq. (23.5)
and to P2 values
a12,...,a m2, b 12,...,b m2, c 12,...,c m2 ∈Zq. (23.6)
The goal is to have D send P1 and P2 additional data that will allow P1 and P2 to verify that the
following relation holds:
(ak1 + ak2)(bk1 + bk2) = (ck1 + ck2) for k= 1,...,m . (23.7)
The security requirements are:
(D1) If D is corrupt and both P1 and P2 are honest, and both successfully complete the protocol
without aborting, then with overwhelming probability, the relation (23.7) must hold.
1014
(D2) If Dis honest and one ofP1 or P2 is corrupt, then the corrupt party should not learn anything
about the honest party’s values.
Here is the protocol:
Protocol 23.5 (Proving product relations).
1. Given values aki, bki, and cki, for i = 1,2 and k = 1,...,m , which satisfy (23.7), the dealer
D performs the following local computations:
(a) The dealer chooses values a01,a02,b01,b02 ∈Zq at random, and chooses c01,c02 ∈Zq at
random, subject to ( a01 + a02)(b01 + b02) = (c01 + c02).
(b) The dealer runs a polynomial interpolation algorithm to obtain the unique polynomials
A1(X), A2(X), B1(X), B2(X), each of degree at most m, such that
Ai(k) = aki and Bi(k) = bki for i= 1,2 and k= 0,...,m .
(c) The dealer computes the polynomial C(X) ←(A1(X)+ A2(X))(B1(X)+ B2(X)), which
has degree at most 2 m.
(d) For i= 1,2 and k= m+ 1,..., 2m, the dealer chooses ck1,ck2 ∈Zq at random, subject
to ck1 + ck2 = C(k).
(e) Finally, for i= 1,2, the dealer sends to Pi the values
aki and bki for k= 0,...,m
and
cki for k= 0,..., 2m.
2. Each party Pi (for i= 1,2) runs a polynomial interpolation algorithm on the values it receives
to obtain polynomials Ai(X), Bi(X), and Ci(X), where Ai(X) and Bi(X) are of degree at
most m, and Ci(X) is of degree at most 2 m, such that
Ai(k) = aki and Bi(k) = bki for k= 0,...,m
and
Ci(k) = cki k= 0,..., 2m.
Note that by construction, we have
(A1(X) + A2(X))(B1(X) + B2(X)) = (C1(X) + C2(X)) (23.8)
if the dealer is honest.
3. Party P1 chooses r∈Zq \{0,...,m }at random, and sends it to P2.
4. Party P2 veriﬁes that r∈Zq \{0,...,m }; if not, P2 aborts.
5. Each party Pi (for i= 1,2) sends the other party
αi ←Ai(r), β i ←Bi(r), γ i ←Ci(r).
6. Each party Pi (for i= 1,2) locally checks that
(α1 + α2)(β1 + β2) = (γ1 + γ2).
If not, the party aborts the protocol. 2
1015
23.2.4.6 Analysis of property (D1)
The following lemma captures the security property (D1).
Lemma 23.1. Suppose D is corrupt and both P1 and P2 are honest. If the relation (23.7) does not
hold, then the probability that either P1 or P2 will ﬁnish the protocol without aborting is at most
2m
q−m−1.
Proof. On the one hand, if the polynomial identity (23.8) does not hold, then since the degrees
on both sides of the equation are at most 2 m, the probability that either P1 or P2 ﬁnish the
protocol without aborting is at most (2 m)/(q−m−1). This is because the polynomial ( A1(X) +
A2(X))(B1(X) +B2(X)) −(C1(X) +C2(X)) has degree at most 2 m, and hence at most 2 mroots,
and party P1 is choosing r from a set of size q−m−1. On the other hand, if the polynomial
identity does hold, then P1 and P2 have values that satisfy the relation (23.7). 2
23.2.4.7 Analysis of property (D2)
To analyze security property (D2), suppose P1 is corrupt and D and P2 are honest. The analysis
of the case where P2 is corrupt and D and P1 are honest is almost identical. Of course, during
the protocol, P1 obtains the values (23.5), and the goal is to show that P1 does not obtain any
information about P2’s values (23.6). Consider the other informationP1 obtains during the protocol:
(I1) The values a01, b01, and c01, as well as the values ck1 for k = m+ 1,..., 2m obtained from
D. These values were the result of the dealer creating random shares, and as such are just
random values in Zq.
(I2) The values α2,β2,γ2 obtained from P2. We have α2 = A2(r). By Lagrange interpolation, we
can write
A2(X) =
m∑
k=0
λk(X)A2(k),
where the λk(X)’s are the Lagrange basis polynomials for the evaluation points {0,...,m }.
Plugging in r for X, since r /∈{0,...,m }, the values λk(r) are non-zero; moreover, since
A2(0) was chosen at random by the dealer, from P1’s point of view, α2 is just a random value
in Zq. Similarly, β2 is just a random value in Zq. Finally, γ2 is uniquely determined by the
equation
(α1 + α2)(β1 + β2) = (γ1 + γ2), (23.9)
where α1 = A1(r), β1 = B1(r), and γ1 = C1(r).
(I3) P2’s abort decision. Party P1 sends arbitrary values ˆα1,ˆβ1,ˆγ1 ∈Zq to P2, who aborts if
(ˆα1 + α2)( ˆβ1 + β2) ̸= (ˆγ1 + γ2), (23.10)
where α2,β2,γ2 were already determined as above. Thus, P2’s decision to abort or not abort
(that is the question) does not tell P1 anything that it does not already know.
1016
So we see that the information P1 obtains could have been generated by P1 itself, just from
the data (23.5). We can formalize this result as follows. This will be convenient later when we
perform a more rigorous security analysis. We begin by deﬁning a machine, S(1)
D , which is called a
simulator. It is designed to interact with P1, playing the roles of both D and P2 in Protocol 23.5.
It is initialized just with values (23.5), and it generates all of the data that P1 receives (including
P2’s decision to abort) as above in (I1)–(I3):
• The values a01, b01, and c01, as well as the values ck1 for k= m+ 1,..., 2m, are generated at
random.
• The values α2 and β2 are generated at random, and γ2 is determined by (23.9).
• P2’s abort decision is determined by (23.10).
We consider two experiments involving an adversary Aand a challenger.
Experiment 0: Agenerates values (23.5) and (23.6) satisfying (23.7) and gives these to the chal-
lenger.
Athen interacts with the challenger: Aplays the role of P1, while the challenger plays the
roles of Dand P2, where Dis initialzed with (23.5) and (23.6). At the end of the experiment,
Aoutputs ˆb∈{0,1}.
Experiment 1: This is the same as Experiment 0, except that the challenger ignores the values
(23.6), and plays the roles of D and P2 using S(1)
D , which is initialized with the values (23.5).
From the above discussion, the following should be clear:
Fact 23.2. Every adversary A(even a computationally unbounded one) outputs 1 with the same
probability in both of the above experiments.
In the case where P2 is corrupt, and D and P1 are honest, there is a simulator S(2)
D that plays a
role analogous to that of S(1)
D . The only diﬀerence is that S(2)
D , playing the role now of P1, chooses
r itself.
23.2.4.8 Application to Protocol 23.3
We make a few remarks about how this general protocol is applied speciﬁcally to our authenticated
dealer’s protocol. The dealer distributes sharings [ K(1)] and [ K(2)]. Consider one authenticated
singleton sharing JaK. This is really three sharings ([a],[a(1)],[a(2)]), and the dealer has to prove two
product relations, namely, a(1) = K(1)a and a(2) = K(2)a. Now consider one authenticated Beaver
triple sharing ( JaK,JbK,JcK). For each of JaK, JbK, and JcK, the dealer has to prove two product
relations, as above, as well as the product relation c = ab. So each authenticated Beaver triple
sharing leads to seven product relations that need to be proved.
When these get translated into relations as presented in (23.7), one sees that there are in fact
additional relations among the aki’s, bki’s, and cki’s in (23.7). Speciﬁcally, many of these values
will be identical, and do not need to be explicitly transmitted from the dealer to the players. For
example, the shares of K(1) and K(2) appear many times.
1017
An optimization. Except for values that are supposed to be identical, all of the aki’s, bki’s, and
cki’s seen by either party are in fact just random values in Zq. As an optimization, the dealer could
just transmit to one party, say P1, the seed to a PRG, who can then generate these values by itself.
This will reduce the communication cost of the dealer’s protocol essentially in half.
23.2.4.9 Security of the entire circuit evaluation protocol
We have already analyzed the case where D is honest and either P1 or P2 is corrupt, but without
Protocol 23.5. Security property (D2) ensures that the protocol remains secure in this case.
Now consider the case whereDis corrupt and P1 and P2 are honest. Privacy is trivially attained,
simply because P1 and P2 are honest, and the fact that D learns nothing about either of P1’s or
P2’s inputs or outputs. Security property (D1) ensures that P1 and P2 compute correct outputs, so
the circuit evaluation protocol provides soundness. Input independence is also trivially attained,
simply because D contributes no inputs to the protocol.
The above is really only an intuitive argument of security. A formal proof will be presented
later, in Section 23.5.9.
23.3 Garbled circuits: another approach to MPC
In this section, we present another approach to MPC, which is based on a technique calledgarbled
circuits. This technique yields a simple 2-party protocol that is secure against honest-but-curious
adversaries. This technique can also be used to construct 2-party protocols that are secure against
malicious adversaries. However, we present a much simpler and more eﬃcient 3-party protocol that
is secure against malicious adversaries, assuming at most one of the three parties is corrupt. Just
like for Beaver’s 2.5-party maliciously secure protocol, for the protocol we present, only two parties
have inputs and outputs — the third party is only there to assist the other two parties, but does not
itself contribute any inputs or obtain any outputs. However, there are some key diﬀerences between
Beaver’s protocol and our 3-party garbled circuit protocol. Speciﬁcally, our 3-party garbled circuit
protocol
• only needs a constant number of rounds of communication between the parties (an important
advantage over Beaver’s protocol, where the number of rounds was essentially the multiplica-
tive depth of the arithmetic circuit);
• works on boolean circuits, rather than arithmetic circuits (a limitation compared to Beaver’s
protocol because the smallest boolean circuit for a function tends to be larger than the smallest
arithmetic circuit for the same function);
• uses the third party not just in a pre-processing phase, but in the execution of the actual
protocol.
23.3.1 Boolean circuit evaluation
A boolean circuit is a circuit with several types of logical gates, where each gate has several inputs
and one output. Typically, such a circuit may be comprised of AND, OR, and NOT gates, but
other logic gates may also be used. We will restrict ourselves to gates with exactly two inputs —
this is not a signiﬁcant restriction, as any boolean circuit can be eﬃciently converted to a circuit
1018
x 1
x 2
x 3
x 4
x 5
x 6
x 7
AND
XOR
NAND
OR
AND
x 8
Figure 23.4: A boolean circuit
with only such gates. Each input of a gate is either an input of the circuit or the output of some
other gate. There is no restriction on the “fan out” of a gate: the output of a gate may be fed as
an input to any number of gates. The output of some gates are designated as circuit outputs.
Fig. 23.4 shows a simple boolean circuit. This particular circuit has three inputs, x1, x2, and
x3, and two outputs x7 and x8. For each input to the circuit, and each gate output, we have drawn
a “wire” on which the value input to the circuit or computed by the gate is carried. Besides the
three input wires and two output wires, there are three internal wires, which carry the internal
values x4, x5, and x6. In general, such a circuit should contain no cycles. This guarantees that
once the values carried on the input wires are determined, we can calculate the values carried on
all of the remaining wires by evaluating the gates one at a time, in some ﬁxed order (known as a
“topological ordering”), so that by the time we evaluate a particular gate, the values carried on its
input wires have already been calculated. In this example, if we feed input values x1, x2, and x3
to the circuit, then the values on the remaining wires may be computed as follows:
x4 ←x1 ∧x2
x5 ←x2 ∧x3
x6 ←x4 ⊕x5
x7 ←x1 ∧x6
x8 ←x6 ∨x5
We want to design an algorithm that lets two parties, call them P1 and P2, securely evaluate
such a circuit. We assume the circuit itself is ﬁxed and publicly known. We assume the circuit takes
a number of inputs x1,...,x n and produces some number of outputs y1,...,y m, where each xi and
yj is a bit. Some inputs will be supplied by P1 and the others will be supplied by P2. For simplicity,
we assume that P1 and P2 obtain the same outputs (although it is not hard to generalize things
so that some outputs will be given to P1 and others will be given to P2). The goal is to design a
protocol that allows P1 and P2 to securely evaluate the circuit. As discussed in Section 23.1.1, this
means that the protocol provides privacy, soundness, and input independence.
privacy: neither party learns anything about the other party’s inputs (except for information that
is inherently revealed by the outputs);
1019
soundness: honest parties compute correct outputs (if they compute any output at all);
input independence: both parties must choose their inputs independently of the other’s.
23.3.2 Yao’s 2-party garbled circuit technique: basic ideas
At a very high level, the idea is this.
• P1 will generate a “garbled encoding” of the circuit and send this to P2.
• P1 and P2 then execute a special interactive subprotocol that lets P2 obtain “garbled encod-
ings” of both P1’s and P2’s inputs. These “garbled encodings” reveal to P2 nothing about
P1’s inputs, and the subprotocol itself reveals to P1 nothing about P2’s inputs.
• Once P2 has the “garbled encodings” of the circuit and all of the inputs, it locally runs a
special evaluation algorithm, which allows P2 to compute “garbled encodings” of the outputs.
• P2 then sends these “garbled encodings” of the outputs back toP1. These “garbled encodings”
of the outputs allows P1 to compute the actual outputs, but nothing more.
• Finally, P1 sends the actual outputs back to P2.
As a ﬁrst step towards ﬂeshing out the above idea, we give a formal deﬁnition of the syntax for
a garbling scheme.
Deﬁnition 23.1 (Garbling scheme). A garbling scheme consists of four eﬃcient algorithms:
1. A probabilistic circuit garbling algorithm Garble that is invoked as
(F ,e,d ) ←RGarble(f),
where the input f is a boolean circuit. The result F is called a garbled encoding of f, the
result e is called the input encoding data, and the result d is called the output decoding
data.
2. A deterministic input encoding algorithm Encode that is invoked as
X ←Encode(e,x),
where e is the input encoding data, and xis a vector of bits. The result X is called a garbled
encoding of x.
3. A deterministic garbled circuit evaluation algorithm Eval that is invoked as
Y ←Eval(F ,X ),
where F is a garbled encoding of a circuit and X is a garbled encoding of an input vector.
The result Y is called a garbled output.
4. A deterministic output decoding algorithm Decode that is invoked as
y←Decode(d ,Y ),
where d is the output decoding data and Y is a garbled output. The result y is either the
special symbol reject or a vector of bits.
1020
The basic correctness requirement for any general garbling scheme is as follows:
for every boolean circuit f, every possible output (F ,e,d ) of Garble (f), and every
boolean input vector xfor f, we have
Decode
(
d , Eval
(
F , Encode(e,x)
))
= f(x).
We next state an intuitive summary of the security properties we require from a garbling scheme.
Obliviousness: (F ,X ) reveals nothing about x;
Authenticity: given (F ,X ), it is hard to ﬁnd ˆY ̸= Eval(F ,X ) that decodes to something besides
reject;
Output simulatability: Y can be eﬃciently computed from f(x) and d .
Note that in our presentation here, our notion of obliviousness does not require that the garbled
encoding of a circuit hides the circuit itself. There are, in fact, stronger notions of obliviousness
that one can consider, in which the garbled encoding of the circuit should hide partial information
of the circuit. For simplicity, we do not discuss here these notions or a number of other security
notions that are useful in some applications.
We can recast our high-level idea presented above in the language of garbling schemes as follows:
• P1 executes (F ,e,d ) ←R Garble(f) and sends F to P2.
• P1 and P2 then execute a special interactive subprotocol that letsP2 obtain X := Encode(e,x),
where x is the vector comprising both P1’s and P2’s inputs. The subprotocol itself reveals
nothing to P1 and nothing to P2 besides X . The obliviousness property of the garbling scheme
ensures that F and X reveal to P2 nothing about P1’s inputs.
• P2 executes Y ←Eval(F ,X ) and sends Y to P1.
• P1 executes y ← Decode(d ,Y ), and sends y to P2. The output simulatability property
ensures that P1 learns nothing other than y.
In the honest-but-curious setting, while we need the obliviousness and output simulatability prop-
erties of the garbling scheme to ensure that each party’s inputs remain hidden from the other party,
we do not need the authenticity property. We will only need the authenticity property when we
discuss maliciously secure variants of this protocol.
23.3.3 Garbling schemes: an application to outsourcing computation
Garbling schemes have a number of applications. To get more of an idea of how one can use a
garbling scheme, we start with an application to outsourcing computation.
Alice is a spy on a mission, traveling with her mobile device, which has only limited compu-
tational power and communication bandwidth. She needs to perform a computation on sensitive
data she has just collected. The computation is too expensive to perform on her mobile device,
so she outsources it to a cloud-based service. Alice does not trust the cloud-based service at all:
neither to carry out the computation correctly nor to protect her sensitive data.
1021
Assume that the computation Alice wants to carry out can be represented by a boolean circuit
f. Suppose that before heading out on her mission, Alice computes a garbled encoding F of f,
along with corresponding encoding data e and decoding data d , as follows:
(F ,e,d ) ←R Garble(f).
She loads e and d on her mobile device and sends F to one or more cloud-based services. This
computation is done on a secure computing device, and the memory of that device is wiped clean
before she heads out.
So now, while Alice is out on her mission, e and d are stored on her mobile device, and nowhere
else. When Alice wants to evaluate f on a speciﬁc input vector xshe uses the input encoding data
e to compute the garbled encoding X of x, as follows:
X ←Encode(e,x).
She then sends X to the cloud-based service.
The cloud-based service now has the garbled encoding F of f, as well as the garbled encoding
X of x. It can apply the garbled circuit evaluation algorithm to compute a garbled encoding Y of
the output, as follows:
Y ←Eval(F ,X ).
The service then sends Y back to Alice.
Finally, Alice decodes the garbled encoding Y using the output decoding data d , as follows:
y←Decode(d ,Y ).
The obliviousness property of the garbling scheme ensures that the cloud-based service learns
nothing about Alice’s input x, while authenticity property ensures that Alice either computes the
correct output y = f(x), or outputs reject. The output simulatability property is not required
for this application. Of course, the basic correctness requirement ensures that Alice does get a
correct result as long as nothing goes wrong with the communication network, and as long as the
cloud-based service does its job correctly. If Alice does output reject, then Alice can retry the
computation, possibly with a diﬀerent cloud-based service (if she sent F to several services).
This technique is eﬀective when the size of the circuit is much bigger than the number of inputs
and outputs of the circuit. In this case, the computation and communication costs for Alice to
encode and transmit her input and to receive and decode the output will be small compared to the
cost of evaluating the circuit herself.
A severe limitation of this technique is that if Alice needs to compute f on several input
vectors while she is out on her mission, she will have to repeat the above procedure once for every
input. That is, she cannot use the same garbled encoding F on two diﬀerent input vectors without
compromising security.
23.3.4 Garble0: a simple but eﬃcient garbling scheme
We now present a simple but fairly eﬃcient garbling scheme, which we call Garble0. Actually,
Garble0 is a special case of a somewhat more generic scheme, which works as follows.
Consider a given boolean circuit f that we wish to garble. The circuit garbling algorithm will
generate a pair ( X(0),X(1)) of random “tokens” for each wire in the circuit: the token X(0) will
1022
represent the value 0 for that wire and the token X(1) will represent the value 1 for that wire. We
stress that each wire gets its own pair of random tokens, which are independent of the tokens for all
other wires. We also stress that there is no correlation between a token and the value it represents,
so learning the token representing a value yields no information about the value itself, nor does it
yield any information about the value of any other token.
Suppose the circuit f has n input wires. Then the input encoding data is deﬁned to be
e :=
(
(X(0)
1 ,X(1)
1 ),..., (X(0)
n ,X(1)
n )
)
, (23.11)
where, for the ith input wire, X(0)
i is the token representing the value 0 and X(1)
i is the token
representing the value 1 for that wire.
The garbled encoding of the input vector x = (x1,...,x n) ∈{0,1}n to the circuit consists of
the tuple
X = (X(x1)
1 ,...,X (xn)
n ).
That is, each input xi is encoded independently as the token that represents the value xi carried
on the ith input wire. We stress that the garbled encoding of the inputs includes only these tokens
— it does not include the tokens representing the values not carried on the input wires.
Suppose the circuit f has m output wires. Then the output decoding data is deﬁned to be
d =
(
(Y(0)
1 ,Y (1)
1 ),..., (Y(0)
m ,Y (1)
m )
)
, (23.12)
where, for the jth output wire, Y(0)
j is the token representing the value 0 and Y(1)
j is the token
representing the value 1 for that wire.
A garbled output Y is of the form ( Y1,...,Y m). The output decoding algorithm determines
the output vector y= (y1,...,y m) ∈{0,1}m as follows. For j = 1,...,m , tests if either Yj = Y(0)
j
or Yj = Y(1)
j : if the ﬁrst equality holds, then yj := 0; if the second holds, then yj := 1; if neither
holds, then a decoding error occurs, and the output of the decoding algorithm is reject.
The garbled encoding F of the circuit f consists of a gate-by-gate encoding of the circuit.
For each gate in the circuit, the circuit garbling algorithm will produce a corresponding garbled
encoding G of the gate. Intuitively, G will be designed to work in conjunction with an eﬃcient
garbled gate evaluation algorithm, GateEval, as follows. Suppose g : {0,1}×{0,1}→{ 0,1}
is the function computed by the gate. Further suppose that
• (X(0),X(1)) is the pair of tokens associated with the ﬁrst input wire of the gate,
• (Y(0),Y (1)) is the pair of tokens associated with the second input wire of the gate, and
• (Z(0),Z(1)) is the pair of tokens associated with the output wire of the gate.
Then for all values u,v ∈{0,1}, we must have
GateEval(G,X(u),Y (v)) = Z(g(u,v));
moreover, given G,X(u),Y (v), it should be hard to gain any information about the token Z(g(u′,v′))
for any (u′,v′) ̸= (u,v).
1023
For example, for an AND gate with two input wires we have
GateEval(G,X(0),Y (0)) = Z(0) (0 AND 0 = 0) , GateEval(G,X(0),Y (1)) = Z(0) (0 AND 1 = 0) ,
GateEval(G,X(1),Y (0)) = Z(0) (1 AND 0 = 0) , GateEval(G,X(1),Y (1)) = Z(1) (1 AND 1 = 1) .
Here is how the garbled circuit evaluation algorithm works. Recall that the garbled encoding of
the input gives us, for each input wire, the token representing the value carried on that wire. Now
we iterate the following procedure, gate by gate:
• pick a gate for which we have already computed the tokens X and Y representing the values
carried on its two input wires;
• if G is the garbled encoding of the gate, compute Z ←GateEval(G,X,Y ), which represents
the value carried on the output wire of the gate.
When this ﬁnishes, for each output wire, we will have the token representing the value on that
wire, and so can form the garbled output by simply concatenating those tokens.
23.3.4.1 An example
Consider again the application in Section 23.3.3. Suppose the circuit f Alice wants to evaluate is
the one in Fig. 23.4. There are 8 wires in this circuit, which we shall call wires 1–8. Wires 1–3
will carry the input values x1,x2,x3, wires 4–6 will carry the intermediate values x4,x5,x6, and
wires 7–8 will carry the output values x7,x8. When Alice generates the garbled encoding of f, she
generates tokens X(0)
i ,X(1)
i for i = 1,..., 8, along with garbled encodings of the gates, which we
call G4,..., G8, so that Gi is the garbled encoding of the gate whose output is carried on wire i.
The encoding data is
e = ((X(0)
1 ,X(1)
1 ),(X(0)
2 ,X(1)
2 ),(X(0)
3 ,X(1)
3 )),
the decoding data is
d = ((X(0)
7 ,X(1)
7 ),(X(0)
8 ,X(1)
8 )),
and the garbled encoding of the circuit is
F = (G5,G6,G7,G8).
All of this data is shown in Fig. 23.5.
Initially, Alice sends F to the cloud-based service and stores e and d on her mobile device.
While she is out on her mission, suppose that the wants to evaluate f on the input vector x =
(x1,x2,x3) = (1,0,1). Using the encoding data e, Alice computes the garbled encoding X of xas
X = (X(1)
1 ,X(0)
2 ,X(1)
3 ),
and sends X to the cloud-based service. The cloud-based service now knows the tokens for wires 1–3,
but is oblivious to the values they represent. Using F and X , the cloud-based service now proceeds
gate by gate, using the garbled gate evaluation algorithm, to compute the tokens representing the
values on wires 4–8 of the circuit, all the while oblivious to the values these tokens represent. Using
the tokens X(0)
1 and X(1)
2 , and the garbled gate encoding G4, it obtains the token X(1)
4 ; using the
tokens X(1)
2 and X(0)
3 , and the garbled gate encoding G(0)
5 , it obtains the token X(0)
5 ; using the
1024
AND
XOR
NAND
OR
AND
X
(0)
1 ,X
(1)
1
X
(0)
2 ,X
(1)
2
X
(0)
3 ,X
(1)
3
X
(0)
4 ,X
(1)
4
X
(0)
5 ,X
(1)
5
X
(0)
6 ,X
(1)
6
X
(0)
7 ,X
(1)
7
X
(0)
8 ,X
(1)
8
G 4
G 5
G 6
G 7
G 8
Figure 23.5: A garbled circuit
AND
XOR
NAND
OR
AND
X
(0)
1 ,X
(1)
1
X
(0)
2 ,X
(1)
2
X
(0)
3 ,X
(1)
3
X
(0)
4 ,X
(1)
4
X
(0)
5 ,X
(1)
5
X
(0)
6 ,X
(1)
6
X
(0)
7 ,X
(1)
7
X
(0)
8 ,X
(1)
8
G 4
G 5
G 6
G 7
G 8
Figure 23.6: Oblivious evaluation of a garbled circuit
tokens X(1)
4 and X(0)
5 , and the garbled gate encoding G6, it obtains the token X(1)
6 ; and so on. This
is illustrated in Fig. 23.6.
Finally, the cloud-based service sends the garbled output ( X(0)
7 ,X(1)
8 ) back to Alice, who uses
the decoding data d to determine the output vector y= (0,1).
During the process, for each wire in the circuit, the cloud-based service is either given or
computes the token that represents the value carried on that wire, but it cannot compute the other
token for that wire. This property ensures that the cloud-based service cannot make Alice accept
an incorrect result.
23.3.4.2 A simple implementation of garbled gate encodings
We will now discuss one possible implementation of garbled gate encoding that satisﬁes the re-
quirements informally introduced in Section 23.3.2. We stress, however, that there are a number
of other techniques that have been developed.
Our garbled gate encoding scheme is built from the following ingredients:
1025
• Let T := {0,1}ℓ be our set of tokens. We call tokens whose ﬁrst bit is 0 type-0 tokens and
tokens whose ﬁrst bit is 1 type-1 tokens. There is nothing really signiﬁcant about the way
we partition tokens into two types. The salient property is that it is easy to determine the
type of any token.
• Let Ibe a ﬁnite set of identiﬁers. Each gate in the circuit to be garbled will have a unique
identiﬁer gID ∈I.
• Let H : T ×T ×I→T be a hash function. To obtain a secure garbling scheme, we will
model H as a random oracle, and require that |T| is super-poly.
For each wire in the circuit, the garbling algorithm will choose a random type-0 token A(0) and
a random type-1 token A(1), along with a random bit r∈{0,1}. We call ( A(0),A(1),r) the private
encoding data for this wire — it is only known to the garbling algorithm itself. For u ∈{0,1},
deﬁne X(u) := A(u⊕r), which is the token that represents the value u for this wire.
Consider a gate in the circuit with associated identiﬁergID ∈I. Assume that the gate computes
the function g : {0,1}×{ 0,1}→{ 0,1}. Further assume that the private encoding data for the
gate’s ﬁrst input wire is (A(0),A(1),r), for the gate’s second input wire is (B(0),B(1),s), and for the
gate’s output wire is ( C(0),C(1),t). For a,b ∈{0,1}set
E(a,b) := H
(
A(a),B(b),gID
)
⊕C(g(a⊕r,b⊕s)⊕t) ∈T . (23.13)
We deﬁne the garbled encoding of this gate as the 4-tuple
G :=
(
gID,E(0,0),E(0,1),E(1,0),E(1,1))
∈I×T 4. (23.14)
The garbled gate evaluation algorithm is deﬁned as follows:
GateEval(G,X,Y ) := H(X,Y, gID) ⊕E(a,b), (23.15)
where G is as in (23.14), a is the type of X, and b is the type of Y.
To see why this works, let u,v ∈{0,1}, and suppose we are given two tokens:
• X(u) = A(u⊕r), which is the token representing u on the ﬁrst input wire, and which has type
u⊕r, and
• Y(v) = B(v⊕s), which is the token representing v on the second input wire, and which has
type v⊕s.
We want to show that
GateEval(G,X(u),Y (v)) = Z(g(u,v)),
where
• Z(g(u,v)) = C(g(u,v)⊕t), which is the token representing g(u,v) on the output wire.
We have
GateEval(G,X(u),Y (v))
= H(A(x⊕r),B(y⊕s),gID) ⊕E(x⊕r,y⊕s)
= C(g(u,v)⊕t)
= Z(g(u,v)).
1026
Intuitively, each E(a,b) is an encryption of the token C(g(a⊕r,b⊕s)⊕t), and one can decrypt E(a,b)
using GateEval as long as one knows bothA(a) and B(b). Moreover, in running the circuit evaluation
algorithm, one will only know one of A(0) and A(1) and one of B(0) and B(1), and so can decrypt
only one of the ciphertexts E(0,0),E(0,1),E(1,0),E(1,1).
23.3.5 Garbling schemes: formally deﬁning security properties
We can formulate the obliviousness property more precisely using an attack game, which is analo-
gous to the game used to deﬁne semantic security for encryption:
Attack Game 23.1 (Oblivious garbling). For a given garbling scheme
(Garble,Encode,Eval,Decode),
and for a given adversary, we deﬁne two experiments, Experiment 0 and Experiment 1. Forb= 0,1,
we deﬁne
Experiment b:
• The adversary submits a circuit f and two circuit input vectors x(0),x(1) to the challenger.
• The challenger computes
(F ,e,d ) ←R Garble(f), X ←Encode(e,x(b)),
and sends (F ,X ) to the adversary.
• The adversary outputs a bit ˆb∈{0,1}.
For b= 0,1, let Wb be the event that the adversary outputs 1 in Experiment b. We deﬁne the
adversary’s advantage in the game as |Pr[W0] −Pr[W1]|. 2
Deﬁnition 23.2 (Oblivious garbling). A garbling scheme is called oblivious if every eﬃcient
adversary has a negligible advantage in Attack Game 23.1.
We can formulate the authenticity property more precisely with an attack game, which is anal-
ogous to the game used to deﬁne ciphertext integrity for encryption:
Attack Game 23.2 (Authentic garbling). For a given garbling scheme
(Garble,Encode,Eval,Decode),
the attack game runs as follows:
• The adversary submits a circuit f and an input vector xto the challenger.
• The challenger computes
(F ,e,d ) ←R Garble(f), X ←Encode(e,x),
and sends (F ,X ) to the adversary.
1027
• The adversary outputs a garbled output ˆY .
We say the adversary wins the game if ˆY ̸= Eval(F ,X ) and Decode(d ,ˆY ) ̸= reject. We deﬁne
the adversary’s advantage in the game as the probability that it wins the game. 2
Deﬁnition 23.3 (Authentic garbling). A garbling scheme is called authentic if every eﬃcient
adversary has a negligible advantage in Attack Game 23.2.
Finally, we can deﬁne the output simulatability property as follows:
Deﬁnition 23.4 (Output simulatable garbling). A garblng scheme is called output simulat-
able if there is an eﬃcient deterministic algorithm Reverse with the following property: for every
circuit f, every possible output (F ,e,d ) of Garble (f), and every input vector xfor f, we have
Eval
(
F , Encode(e,x)
)
= Reverse
(
d , f(x)
)
.
On the security of Garble0. Recall our basic garbling scheme, Garble0. One can prove that
if the token space is of super-poly size and the hash function used in the garbled gate encoding
mechanism is modeled as a random oracle, then Garble0 is both oblivious and authentic. We leave
this proof to the reader. It is also easy to see that Garble0 is output simulatable unconditionally.
Indeed, the Reverse algorithm works in the same way as the Encode algorithm for Garble0.
Adaptive security. The careful reader may have noticed a potential weakness in our formal
deﬁnitions of obliviousness and authenticity, if we try to apply them directly to our computation
outsourcing example in Section 23.3.3. Suppose that the sensitive data x that our spy Alice has
collected has been tainted. Speciﬁcally, suppose that her adversaries, after bribing the cloud-based
service, learn the garbled encoding F of the circuit, and are able to plant some data that at least
partially inﬂuences the value of xin some clever way that depends on F . In general, in any such
application of this computation outsourcing technique, it may not be possible to ensure that the
circuit input vector xdoes not somehow depend on the garbled encoding F .
Thus, to use garbled circuits for general computation outsourcing, we need stronger notions
of obliviousness and authenticity. Speciﬁcally, we need to give the adversary more power in the
corresponding attack games. In Attack Game 23.1, the adversary should be allowed to choose the
input vectors x(0) and x(1) after seeing the garbled encoding F of the circuit f. Similarly, in Attack
Game 23.2, the adversary should be allowed to choose the input vector xafter seeing the garbled
encoding F of the circuit f. These modiﬁed attack games yield corresponding notions of adaptive
obliviousness and adaptive authenticity.
Again, we leave it to the reader to prove that if the token space is of super-poly size and the hash
function used in the garbled gate encoding mechanism is modeled as a random oracle, thenGarble0
is both adaptively oblivious and adaptively authentic. Note that for our eventual applications to
multi-party computation, we will not need these adaptive security notions. For the non-adaptive
security notions, it is possible to prove the security of Garble0 without modeling the hash function
as a random oracle, but rather, under a speciﬁc (and reasonable) indistinguishability assumption.
However, for the adaptive security notions, the only known proof of security of Garble0 is with a
random oracle.
1028
23.3.6 A 2-party garbling-based protocol secure against honest-but-curious ad-
versaries
We shall now present our ﬁrst protocol based on garbled circuits. It is a 2-party protocol, and is
only secure against honest-but-curious adversaries.
However, before presenting the protocol, we will need some more ingredients. One ingredient
consists of a syntactic property enjoyed by many practical garbling schemes, including our basic gar-
bling scheme, Garble0, called “projective input encoding”. The second ingredient is a subprotocol
for 1-out-of-2 oblivious transfer (OT). We presented several OT protocols in Section 11.6.
23.3.6.1 Projective input encoding
We shall require that our garbling scheme has the projective input encoding property, which
means that it encodes inputs in essentially the same way as Garble0. That is, the encoding data e
is of the form (23.11), and the encoding algorithm works as in (23.11). We do not, however, make
any particular requirements on the form of the elements X(b)
i appearing in (23.11), other than that
they come from some ﬁnite set T of bit strings of some speciﬁed length. We call T the set of
input encoding tokens.
23.3.6.2 1-out-of-2 oblivious transfer (OT)
Roughly speaking, in a 1-out-of-2 OT protocol, one party, called the sender, has two inputs,
X(0),X(1), which may be bit strings of some given length, and another party, called the receiver,
has a one-bit input u∈{0,1}, called a choice bit; at the end of the protocol, the receiver obtains
the value X(u). The essential security properties of such a protocol are:
Sender privacy: the receiver learns nothing about the sender’s other input X(1−u);
Receiver privacy: the sender learns nothing about the receiver’s choice bit u.
As we will only need security against honest-but-curious adversaries, the protocols presented in
Section 11.6 are adequate for our purposes here.
The 2-party protocol we describe will run multiple instances of the 1-out-of-2 OT protocol. We
can do so by running many parallel instances of the protocols presented in Section 11.6 without
sacriﬁcing security. However, an eﬃcient technique calledOT extension, discussed in Section 23.7,
lets one run n instances of the 1-out-of-2 OT protocol far more eﬃciently than running n parallel
instances of the protocols in Section 11.6.
23.3.6.3 The protocol
Without further ado, here is the protocol for a given circuit f, which really just ﬁlls in the details
of the high-level idea presented in Section 23.3.2.
1. Party P1 computes (F ,e,d ) ←R Garble(f) and sends F to P2.
Note: because we are assuming projective input encoding, the encoding data e is of the form
e = ((X(0)
1 ,X(1)
1 ),..., (X(0)
n ,X(1)
n )).
2. For each input xi provided by P1, party P1 sends X(xi)
i to P2.
1029
3. For each input xi provided by P2, party P1 obliviously transfers X(xi)
i to P2 using a 1-out-of-2
OT protocol, where ( X(0)
i ,X(1)
i ) are P1’s inputs (in the role of sender) to the OT protocol,
and xi is P2’s input (in the role of receiver) to the OT protocol.
4. Now party P2 has F as well as
X = (X(x1)
1 ,...,X (xn)
n ).
Party P2 computes Y ←Eval(F ,X ) and sends Y to P1.
5. After receiving Y from P2, party P1 computes y←Decode(d ,Y ).
Party P1 outputs yand also sends yto P2.
6. Party P2 receives yand then outputs y.
Note that Steps 1–3 require only a constant number of rounds of communication, assuming all
of the 1-out-of-2 OTs are constant round and run in parallel. Thus, the entire protocol requires
only a constant number of rounds of communication. Observe that we used the special projective
input encoding property of the garbling scheme directly in the design of the protocol.
23.3.6.4 Security of the protocol
We shall now give some intuition as to why the protocol is secure against honest-but-curious
adversaries. Here, we are assuming that both P1 and P2 follow the protocol, and the goal is to argue
that neither party learns any information about the other party’s input, other than information
revealed by the output y= f(x).
The assumptions we rely on are the following:
• sender and receiver privacy properties for the OT protocol, and
• the obliviousness and output simulatability properties of the garbling scheme.
Privacy for P1. We argue thatP2 learns no information about P1’s input, other than information
revealed by the output y= f(x). This argument relies on the following assumptions:
• sender privacy for the OT protocol, and
• the obliviousness property of the garbling scheme.
Because of sender privacy for the OT protocol, the only information P2 obtains during the protocol,
besides y = f(x), is the garbled circuit encoding F and the garbled input encoding X . By the
obliviousness property of the garbling scheme, P2 learns nothing more about the input vector x.
Privacy for P2. We argue thatP1 learns no information about P2’s input, other than information
revealed by the output y= f(x). This argument relies on the following assumptions:
• receiver privacy for the OT protocol, and
• the output simulatability property of the garbling scheme.
Because of receiver privacy for the OT protocol, the only informationP1 obtains during the protocol
is the garbled output Y . Because of the output simulatability property of the garbling scheme, P1
could have computed Y directly from y= f(x), and hence Y gives P1 no more information about
P2’s input than that implied by y.
1030
ANDAND
XOR
G
x 1
x 2
x 3
y
P 1 ’s inputs
{
P 2 ’s input { {
(a) circuit computing y = x 1   ( x 2 ^ x 3 )
XORG
0
X 1
X 2
X
(0)
3
,X
(1)
3
(b) garbled circuit with input tokens
Figure 23.7: What goes wrong without OT
23.3.6.5 Why OT is needed
It is instructive to see why OT is needed in this protocol. More precisely, we address the question:
why not just have P1 send to P2 both tokens X(0)
i ,X(1)
i for those inputs xi provided by P2, and
let P2 use the appropriate token X(xi)
i ? To see why this is a bad idea, let us assume the garbling
scheme is our basic garbling scheme, Garble0. Further, let us assume that the circuit computes
the function y = x1 ⊕(x2 ∧x3), where inputs x1 and x2 are contributed by P1 and the input x3
is contributed by P2. The reader may verify the following: for every x2,x3,y ∈{0,1}, there exists
x1 ∈{0,1}such that y = x1 ⊕(x2 ∧x3). For example, if x3 = 0, just set x1 ←y, and if x3 = 1,
just set x1 ←y⊕x2. This means that if P2 knows his own input x3, and learns the circuit output
y, he really has no information about x2: this information he has is consistent with both x2 = 0
and x2 = 1. The privacy property for the protocol should ensure that after the protocol runs, P2
still has no information about x2. But we will see that if P2 obtains both tokens corresponding to
the input x3, then P2 can determine x2.
Fig. 23.7(a) shows the circuit computing y= x1 ⊕(x2 ∧x3), which is composed of one AND gate
and one XOR gate. Fig. 23.7(b) shows the situation where P2 has received a token X1 representing
P1’s input x1, a token X2 representing P1’s input x2, and two tokens X(0)
3 ,X(1)
3 , so that P2 can use
X(x3)
3 to represent his input x3. Party P2 has also received the garbled encodings G of the AND
gate and G′ of the XOR gate. Even if P2 honestly runs the protocol, a curious P2 can determine
P1’s input x2 as follows. Party P2 simply runs the garbled gate evaluation algorithm twice: once
on inputs G, X2, and X(0)
3 , and again on inputs G, X2, and X(1)
3 . By the logic of an AND gate,
if x2 = 0, both executions of the algorithm will output the same token, and if x2 = 1, the two
executions will produce diﬀerent tokens. Thus, P2 can determine x2. We conclude that this protocol
does not provide privacy.
23.3.7 A 3-party garbling-based protocol secure against malicious adversaries
We shall now present a protocol based on garbled circuits that is secure against malicious adver-
saries. It is a 3-party protocol, and is secure against a malicious adversary that may corrupt at
most one of the three parties. While the protocol involves three parties, P1, P2, and P3, only P1
and P2 provide inputs or obtain outputs — party P3 is only there to assist P1 and P2.
The protocol requires a garbling scheme that provides projective input encoding (see Sec-
tion 23.3.6.1).
1031
The protocol will make use of a collision resistant hash function which is used as a non-interactive
commitment scheme, as discussed in Section 8.12 (We could, in fact, using any non-interactive
commitment scheme.) Speciﬁcally, the protocol makes use of a collision resistant hash function
H1 : T ×R→C,
where T is the set of input encoding tokens,Ris some suitably large space of randomizing elements,
and Cis some ﬁnite output space (the “commitment” space).
The randomizing elements will be used to hide the tokens. This hiding property means that
for adversarially chosen X(0),X(1) ∈T , and for randomly chosen r∈R,
H1(X(0),r) and H1(X(1),r)
are computationally indistinguishable. We could, in fact, use any secure (non-interactive) commit-
ment scheme.
Finally we need a secure pseudo-random generator G. Let S be the seed space of G. We
shall assume that the output space of G is large enough so that we can use its output in lieu of
the random bits needed to run the circuit garbling algorithm, as well as to generate several other
random objects.
23.3.7.1 The protocol
Here is the protocol for a given circuit f:
1. Upon receiving all of their input values, parties P1 and P2 each send each other a “ready”
message:
(a) party P1’s ready message consists of a randomly chosen seed s∈S;
(b) party P2’s ready message is empty.
2. Upon receiving their respective ready messages, each party P1 and P2 does the following,
using the output of G(s) in lieu of any random bits:
(a) Compute ( F ,e,d ) ←R Garble(f).
Note: because we are assuming projective input encoding, the encoding data e is of the
form
e = ((X(0)
1 ,X(1)
1 ),..., (X(0)
n ,X(1)
n )).
(b) Compute bi ←R {0,1}for i= 1,...,n .
(c) Compute r(b)
i ←R Rfor i= 1,...,n and b= 0,1.
(d) Compute C(b)
i ←H1(X(b⊕bi)
i ,r(b)
i ) for i= 1,...,n and b= 0,1.
(e) Send F along with the collection of all hashes
{
C(b)
i
}
i,b to P3.
3. Each party P1 and P2 also sends encodings of their input bits to P3 as follows: for each party,
for i= 1,...,n , if that party contributes the ith bit xi of the input vector, then it sends to
P3 the tuple (i,ai,Xi,ri), where ai := xi ⊕bi, Xi := X(xi)
i , and ri := r(ai)
i .
4. Upon receiving all of the above data from both P1 and P2, party P3 does the following:
1032
(a) Check that the values F and
{
C(b)
i
}
i,b received from P1 and P2 are identical — if not,
abort.
(b) For i= 1,...,n , check that C(ai)
i = H1(Xi,ri) — if not, abort.
(c) Compute Y ←Eval(F ,X ), where X = (X1,...,X n), and send Y to both P1 and P2.
5. Upon receiving Y from P3, each party P1 and P2 computes y ←Decode(d ,Y ), and then
either outputs y(if y̸= reject) or aborts (if y= reject).
At a high level, in Step 2, each of the parties P1 and P2 (using the same random seed) generate
the garbled encoding F of the circuit, along with commitments to each token associated with each
input wire, and send these to party P3. For pair of tokens (X(0)
i ,X(1)
i ) associated with the ith input
wire, a random bit bi is used to randomize the order of the committed pair. In Step 3, each of the
parties P1 and P2 open the commitments to the tokens corresponding to their own inputs. Because
of the random ordering of the commitment pairs, party P3 does not learn the input values xi of
parties P1 and P2. In Step 4, party P3 checks that the data it receives from P1 and P2 is consistent,
and then applies the garbled circuit evaluation algorithm to the garbled inputs it received, and
sends the garbled output back to both P1 and P2, which each party then decodes.
23.3.7.2 Security of the protocol
We shall now give some intuition as to why the protocol is secure against malicious adversaries,
assuming at most one party is corrupt.
The assumptions we rely on are the following:
• the collision resistance and hiding properties of the hash function H1,
• the security of the pseudo-random generator G, and
• the obliviousness, authenticity, and output simulatability properties of the garbling scheme.
We consider in turn the cases where P1, P2, and P3 are corrupt.
Party P1 corrupt. In this case, the assumptions we need are:
• the collision resistance of the hash function H1,
• the output simulatability of the garbling scheme.
First, observe that P1 must send to P3 the same values F and
{
C(b)
i
}
i,b that P2 sends to P3,
since otherwise, P3 aborts in Step 4(a), in which case P1 cannot aﬀect the security of the protocol.
Because of the collision resistance property of H1, if P3 does not abort in Step 4(b), we may assume
that for i= 1,...,n , if the ith input is contributed by P1, then the corresponding tuple (i,ai,Xi,ri)
received by P3 in Step 4(b) satisﬁes Xi = X(xi)
i , where xi = ai⊕bi, the value X(xi)
i was derived from
the seed sby P2 in Step 2(a), and the value bi was derived from the seed sby P2 in Step 2(b). This
means that at this step, each of P1’s inputs xi is eﬀectively determined by the information P1 sent
to both P2 and P3, and, moreover, P3 will use the corresponding input encoding tokens X(xi)
i when
it evaluates the garbled circuit. The soundness property for the protocol now follows immediately.
The input independence property is also clear, since at the time P1’s inputs are determined, it
1033
literally has no information related to P2’s inputs. The privacy property for the protocol follows
from the output simulatability property of the garbling scheme. Indeed, the only information P1
obtains during the protocol is the garbled output Y . This is information P1 could already compute
by itself from the circuit output vector y = f(x), along with the decoding data d as derived in
Step 2(a) from the value sthat P1 shared with P2 in Step 1. Thus, party P1 learns no information
about P2’s input, other than that implied by the circuit output vector y.
Party P2 corrupt. This is essentially the same argument as the case where P1 is corrupt, with
the roles of P1 and P2 reversed.
Party P3 corrupt. In this case, the assumptions we need are:
• the hiding properties of the hash function H1,
• the security of the pseudo-random generator G, and
• the obliviousness and authenticity properties of the garbling scheme.
First, by the security of the pseudo-random generator G, we can assume that the data generated
by P1 and P2 was generated using truly random bits, rather than bits output by G. By the hiding
property of H1, the only information obtained by P3 is the garbled encoding F of the circuit f,
and the garbled encoding X of the input vector x. The privacy property for the protocol follows
from the obliviousness property of the garbling scheme: from F and X , party P3 learns nothing
at all about x. The soundness property for the protocol follows from the authenticity property
of the garbling scheme: party P3 must compute Y = Eval(F ,X ), as otherwise, parties P1 and P2
will reject with overwhelming probability. Note that the exchange of “ready” messages in Step 1
of the protocol ensures that all of P1’s and P2’s inputs are locked in before P3 sees F ; otherwise,
we would actually need to assume adaptive security properties for the garbling scheme. The input
independence property for the protocol holds trivially, since P3 contributes no inputs.
23.3.7.3 An optimization
A simple optimization can reduce the amount of data transmitted over the network by a factor of
two. Recall that in Step 2(e), both parties P1 and P2 send gbData :=
(
F ,
{
C(b)
i
}
i,b
)
to P3, and
then in Step 4(a), party P3 veriﬁes that it received the same value of gbData from both P1 and P2.
The optimization is to have one of the parties, say P2, send to P3 a collision-resistant hash hof
gbData and to have P3 verify that the hash of the value gbData it receives from P1 is equal of the
value h it receives from P2. In the security analysis, the collision-resistance property of this hash
function is only needed in the analysis where P1 is corrupt.
23.4 Multi-party computation based on a secure distributed core
In practice, a typical way to build a multi-party protocol is to start with a secure 3-party protocol,
and to use that 3-party protocol as a service provided to a large set of parties. We show how this
can be done using the maliciously secure version of Beaver’s 2.5-party protocol (Section 23.2.4).
1034
Recall that Beaver’s 2.5-party protocol makes use of three parties, P1, P2, and D. Only P1 and
P2 supply inputs and obtain outputs, while the dealer D is only used to distribute information to
P1 and P2 as part of a pre-processing phase.
Now suppose we have N parties Q1,...,Q N who want to securely evaluate an arithmetic circuit
over Zq. Each party Qj supplies zero or more inputs and obtains zero or more outputs. Each input
or output is an element of Zq. The Qj’s will use the “secure core” consisting of P1, P2, and D to
evaluate the circuit. We shall present a protocol that is secure provided at most one of P1, P2, or
D are corrupt, while any number of the Qj’s may be corrupt. Moreover, the Qj’s participate in a
very simple way, and the overall structure of the protocol is as follows:
• The Qj’s begin by appropriately sharing each of their inputs with P1 and P2.
• After running the pre-processing phase with D, parties P1 and P2 run the secure circuit
evaluation protocol on an appropriately modiﬁed arithmetic circuit.
• Finally, P1 and P2 forward their outputs to the Qj’s.
23.4.1 Processing inputs
We ﬁrst describe how each Qj processes its inputs.
Processing inputs: a ﬁrst idea. Here is a ﬁrst idea that does not quite work. For each input
x ∈ Zq that it supplies, party Qj breaks x ∈ Zq into random shares x1,x2 ∈ Zq, subject to
x= x1 +x2, and gives x1 to P1 and x2 to P2. The arithmetic circuit evaluated by Beaver’s protocol
is then modiﬁed so that instead of x being provided as an input to the circuit, x1 is an input
supplied by P1, and x2 is an input supplied by P2, and an addition gate is added to the circuit that
computes x as x1 + x2.
With this approach, a corrupt partyPi learns nothing about Qj’s input x. Unfortunately, except
in the honest-but-curious setting, this approach does not work, as there is nothing preventing a
corrupt Pi from changing its input xi into something else, thereby causing the circuit to compute
an incorrect result. That incorrect result may leak unintended information about Qj’s secret input,
for example, if the circuit output is meant to become publicly available.
To give a concrete example, suppose that the circuit computes the function f(x) = x(x−1)
and makes the result public, where xis Qj’s input. If the output is zero, then everyone learns that
x∈Zq is binary (i.e., x∈{0,1}), but without learning the exact value of x. Now suppose Qj writes
x= x1 + x2, and sends x1 to P1 and x2 to P2. A corrupt P1 can add one to x1 and use that as its
input to the protocol. This eﬀectively changes Qj’s input to the protocol from x to x+ 1. Now,
when the incorrect result is published, if x= 0 then the output is zero, but if x= 1 the output is
non-zero. Hence, if x is binary, then the incorrect result leaks that exact value of x.
Processing inputs: a second idea. Roughly speaking, for i= 1,2, the idea is to use a simple
authentication code to force each party Pi to use the share xi it was given by Qj. This problem is
only relevant when Qj is honest.
1. The ﬁrst step is for Qj to choose A,B ∈Zq at random, set C ←AB, and generate random
sharings of A, B, and C, that is, random A1,A2,B1,B2,C1,C2 ∈Zq, subject to A= A1 +A2,
B = B1 + B2, and C = C1 + C2. Party Qj sends Ai,Bi,Ci, to party Pi for i= 1,2.
1035
2. For each input x∈Zq that it supplies, party Qj will do the following:
(a) compute E ←Ax∈Zq,
(b) generate random sharings of x and E, that is, random x1,x2,E1,E2 ∈Zq, subject to
x= x1 + x2 and E = E1 + E2;
(c) send xi,Ei, to party Pi, for i= 1,2.
3. Upon receiving Ai,Bi,Ci, party Pi chooses random Ri ∈Zq, and supplies inputs Ai,Bi,Ci,Ri
to a modiﬁed circuit, described below.
4. For each input x supplied by Qj, upon receiving xi,Ei from Qj, party Pi chooses random
Si ∈Zq, and supplies inputs xi,Ei,Si to the modiﬁed circuit.
We next describe the modiﬁed circuit:
1. For each Qj, the circuit computes a validity code
D= (R1 + R2)[(A1 + A2)(B1 + B2) −(C1 + C2)] ∈Zq,
based on the inputs Ai,Bi,Ci,Ri supplied by Pi for i= 1,2.
2. For Qj and each input x supplied by Qj, the circuit computes a validity code
F = (S1 + S2)[(A1 + A2)(x1 + x2) −(E1 + E2)],
based on the inputs xi,Ei,Si from Pi for i= 1,2.
3. The circuit computes the sum V of all validity codes (for all Qj’s and all of their inputs from
Steps 1 and 2 above).
4. The rest of the circuit is the same as the original circuit, using x1 + x2 in place of x.
5. Before obtaining any other outputs, parties P1 and P2 both obtain the value V.
6. Each party Pi checks that V = 0, and aborts if it is not. We discuss below how other circuit
outputs are processed (which may require additional modiﬁcations to the circuit).
Note that to implement the above strategy, we exploit the fact that in Beaver’s 2.5-party
protocol, party P1 needs to actively participate in order for P2 to obtain an output; likewise, party
P2 needs to actively participate in order for P1 to obtain an output. Thus, we can implement the
logic in Steps 5 and 6 that requires that both parties ﬁrst obtain V and verify that V = 0 before
allowing any other outputs to be obtained.
Analysis assuming P1 corrupt and P2 and Qj honest. Assume that P1 is corrupt and Qj
is honest. The case where P2 is corrupt and Qj is honest is symmetric. There are other cases to
consider, such as when both P1 and Qj are corrupt, or when D is corrupt and Qj is honest, but
these are less interesting.
The values A2,B2,C2 by themselves reveal no information to P1 about A or B. Now suppose
P1 supplies inputs to the circuit A2 + δA, B2 + δB, and C2 + δC, in place of A2,B2,C2. Then in
this case, if R= R1 + R2, the validity code D is computed as
D= R[(A+ δA)(B+ δB) −(C+ δC)] = R[AδB + BδA + δAδB −δC].
1036
On the one hand, if any of δA, δB, or δC are non-zero, then because A, B, and R are independent
of each other and of everything else in P1’s view, the computed validity code D will be a random
element of Zq. On the other hand, if δA = δB = δC = 0, the computed validity D code is zero.
Now consider what happens in processing one input x. The values x2,E2 by themselves reveal
no information to P1 about A or x. Now suppose P1 supplies inputs x2 + δx and E2 + δE to the
circuit, in place of x2 and E2. Also assume that δA, deﬁned above, is zero. Then in this case, if
S = S1 + S2, the validity code F is computed as
F = S[A(x+ δx) −(E+ δE)] = S[Aδx −δE].
On the one hand, if either of δx or δE are non-zero, then because Aand S are independent of each
other and of everything else in P1’s view, the computed validity code F will be a random element
of Zq. On the other hand, if δx = δE = 0, the computed validity code F is zero.
It follows that if P1 ever supplies the wrong inputs, the value V will be a random value in Zq,
and hence with probability 1 −1/q, party P2 aborts the protocol before allowing any outputs are
provided to any of the Qj’s.
23.4.2 Processing outputs
Processing outputs: a ﬁrst idea. Suppose the value x on some wire is to be obtained as an
output by Qj. If x is a public output that may also be obtained by anyone, then P1 and P2 can
both be allowed to obtain x and then just forward x to Qj. Then Qj checks that it received the
same value xfrom both P1 and P2. If not, Qj aborts; otherwise, the common value xis the output
obtained by Qj.
It should be clear that even if one of P1 and P2 is corrupt, party Qj is guaranteed to either
obtain the correct output or abort.
Processing outputs: a second idea. If the output x is a “private output” that should be
obtained only by Qj, then the protocol is easily modiﬁed to deal with that as follows. For each
such output x, party Qj generates a random “output mask” G ∈Zq, which is supplied as an
additional input, subject to the same processing as an ordinary input, as in Section 23.4.1. The
circuit itself is modiﬁed to compute the “masked output” x′←x+ G, rather than the “unmasked
output” x. Party Qj checks that it receives the same value x′ from both P1 and P2. If not, Qj
aborts; otherwise, Qj computes the unmasked output x←x′−G.
It should be clear that even if one of P1 and P2 is corrupt, party Qj is guaranteed to either
obtain the correct output or abort. Moreover, neither party P1 or P2 learns anything about the
unmasked output x.
23.5 Formal models for multi-party computation: the universal
composability framework
We now turn to presenting a precise security model for multi-party computation (MPC). Such a
model is needed in order to prove that a proposed MPC protocol has the claimed security properties.
Without a formal security model we cannot rigorously prove security; we can only give an intuitive
argument of security, and such arguments can miss clever attacks that the protocol designers were
not aware of.
1037
In Section 23.1.3, we brieﬂy introduced the basic ideas of our formal security deﬁnition. The
reader should review that section, if necessary, before continuing.
23.5.1 The real protocol and its execution
Let us describe in more detail anN-party protocol and its execution. We haveN parties P1,...,P N,
each executing some protocol Π. We shall assume that each party Pi knows its own index i. There
is, of course, an adversary A. For now, we will focus on completely malicious adversaries. Later,
in Section 23.5.6, we will consider honest-but-curious adversaries. If any of the parties P1,...,P N
are corrupt, we simply absorb the behavior of the corrupt parties into the logic of Aitself. This
models the fact that corrupt parties may collude with each other and with “outside” attackers, so
it is simpler to just roll this all together into a single entity modeled by A.
We assume that the parties communicate over an asynchronous communication network C. We
will generally assume that Cprovides secure point-to-point channels. Each point-to-point channel
provides both message privacy and integrity. Assuming keys have been pre-distributed, such secure
point-to-point channels can be implemented using the techniques discussed in Chapter 9. However,
in this chapter, we will abstract all of this away, and simply model Cas providing physically secure
point-to-point channels, which is essentially justiﬁed by the analysis in Section 9.3. Nevertheless,
the adversary learns some information about and exerts some control over messages sent over these
channels:
• when a message is sent, the adversary learns of this fact, as well as the length of the message,
the sender of the message, and its intended recipient — this models the fact that encryption
schemes leak some information about the length of the encrypted message, and the fact that
in any real implementation of the network, the adversary may see who sent the message and
who receives the message;
• the adversary determines if and when a message is delivered — this models the fact that we
are viewing Cas an asynchronous communication network, with no guarantees on if and when
a message is delivered, so we simply let the adversary make these scheduling decisions;
• the adversary learns no other information about a message; nor can he modify a message or
its apparent sender; nor can he deliver it to anyone other than its intended recipient.
There is one more element in the picture, which we call the environment. The environment is
responsible for supplying inputs to the honest parties, and obtaining outputs from honest parties.
It also determines which parties are corrupt and which are honest. It also may coordinate and
interact with Aduring the execution of the protocol. We denote the environment by Z.
Fig. 23.8 illustrates the basic topology of a real protocol execution.
• In this ﬁgure, there are 4 parties, P1,...,P 4; however, we are assuming here that parties P3
and P4 are corrupt, and absorbed into the adversary A.
• The communication network Chas 4 wires, which connect to each of the 4 parties; however,
since parties P3 and P4 are absorbed into A, these wires connect directly to A. In this way,
Acan send a message, say, to P1 as if it came from P3, simply by sending the message along
the wire labeled 3. Similarly, Amay receive a message from P1 intended for P3 along the
same wire. We call these C-I/O wires.
1038
P 1
P 2
Z
A
C
1 2 3 4
Z -I/O wires
C -I/O wires
A -control wire
C -control wire
Figure 23.8: Real protocol execution, including communication channel C, adversary A, and
environment Z, along with honest parties P1 and P2, and corrupt parties P3 and P4
• There is also a “control wire”, which we call the C-control wire, between Aand C. Along this
wire, Areceives “leakage messages” from C, which give Apartial information about protocol
messages sent from an honest party. Also along this control wire, Amay send to C“control
messages”, which instruct Cabout when to actually deliver protocol messages to an honest
party.
• There is also an environment Z, which has wires connecting it to the honest parties P1 and
P2, through which the environment can send protocol inputs and receive protocol outputs.
We call these the Z-I/O wires. There is also a “control wire”, which we call the A-control
wire, that allows communication directly between Zand A. The idea is that along this wire,
Zmay send “control messages” to A, which may give Aguidance on what it should do next,
and Zmay receive “leakage messages” from A, which represent information that leaks to the
environment.
Remark 23.1 (The role of the environment). As its name suggests, the environment models
the environment in which a protocol executes. Indeed, a protocol may be used as a subprotocol
in some larger protocol, and all of the elements of the larger protocol are abstracted away and
modeled by a single entity modeled by Z. Because the environment is allowed allowed interact
with the adversary during protocol execution, we can model possible interactions between the
subprotocol and the larger protocol. This will allow us to prove important composition theorems,
which (intuitively) say that if a protocol is secure, then it remains secure when the protocol is
used as a subprotocol in any larger protocol, and also when many instances of the protocol are run
concurrently. Because of these composition theorems, the security model we present here is called
the universal composability framework. 2
23.5.1.1 The real protocol execution
We now describe the detailed mechanics of a real protocol execution, which involves a number of
machines. There is a machine representing A, a machine representing Z, a machine representing
1039
C, and a separate machine representing each individual honest party Pi. So, for example, every box
in Fig. 23.8 is a machine. Communication among machines is done by message passing. Moreover,
our formal model of protocol execution only allows one machine to execute at a time; nevertheless,
this model still captures concurrent execution of multiple machines, with the adversary, in eﬀect,
controlling how the computations of various machines are interleaved.
After computing for a while, the currently executing machine passes a message to a second
machine; at that point, the ﬁrst machine suspends execution, and the second machine resumes exe-
cution. This continues for the entirety of the protocol. Initially, Zstarts executing and determines
which subset of the parties P1,...,P N is to be considered corrupt. This information is written to
a special tape that is visible to Aand C. Note, however, that this information is not visible to
the honest parties, reﬂecting the fact that honest parties do not know who the corrupt parties are.
Protocol execution halts when Zhalts. Before halting, Zwrites a single bit to a special output
tape.
Next, we describe the types of messages that may be passed between machines.
1. Zmay pass a protocol input to any honest party Pi along a Z-I/O wire.
2. Zmay pass a control message to Aalong the A-control wire.
3. An honest party Pi, or Aon behalf of a corrupt party Pi, may pass a send messages request
to Calong the C-I/O wire corresponding to Pi. Such a send messages request is of the form
(send,(j1,m1),(j2,m2),... ), where each jk is the intended recipient of the message, and mk
is the actual payload of the message.
4. Amay pass a deliver message request along the C-control wire. Such a deliver message request
is of the form ( deliver,k), where k is an index that determines the message that is to be
delivered — details of this are described below.
5. Amay pass a leakage message to Zalong the A-control wire.
6. An honest party Pi may pass a protocol output to Zalong its Z-I/O wire.
7. Upon receiving a send messages request (send,(j1,m1),(j2,m2),... ) along the C-I/O wire
of party Pi (either honest or corrupt), the communication network C appends the tuples
(i,j1,m1),(i,j2,m2),... to an internal list buf (initially empty), and passes the leakage mes-
sage (sent,i, (j1,len(m1)),(j2,len(m2)),... ) to Aalong the C-control wire.
8. Upon receiving a deliver message request (deliver,k) from Aalong the C-control wire, the
communication network Cdoes the following: if the kth tuple in buf is (i,j,m ), it passes the
message (receive,i,m ) along the C-I/O wire corresponding to Pj, and this goes to either Pj
(if Pj is honest) or A(if Pj is corrupt); if there is no kth tuple, it passes an error message to
Aalong the C-control wire.
23.5.1.2 Direct communication between the adversary and honest parties
As it stands, the adversary Acannot interact directly with the honest parties Pi: all such inter-
actions are performed indirectly through the secure network C. While not strictly necessary, we
may also allow direct interaction between the adversary and the honest parties. This is especially
useful when we consider variations on the model in which instead endowing the model with a
1040
built-in secure communication network C, we use the adversary to model an insecure communica-
tion network, and eﬀectively construct a secure communication channel using some other type of
built-in functionality (for example, a public-key infrastructure). We will see an example of this in
Section 23.6.1.2.
23.5.1.3 The trivial adversary
For a variety of reasons, it is convenient to introduce a special adversary, called the “trivial adver-
sary”, denoted as Atriv. The trivial adversary acts as a simple “router” between the environment
and other machines: control messages from Z are instructions that tell Atriv to send a speciﬁc
message to a speciﬁc machine along a speciﬁc wire; moreover, messages received by Atriv from
any machine other than Z are forwarded to Z (along with information indicating from which
machine/wire the message came from).
23.5.1.4 Running time considerations
When considering such a system, we need to restrict ourselves to protocols, adversaries, and envi-
ronments that are eﬃcient in some appropriate sense. It is a bit tricky to get this type of deﬁnition
correct. One workable approach is as follows. We deﬁne two notions.
• We say an environment Z is well behaved if its running time is strictly bounded by a
polynomial (which may depend on Z) in the security parameter.
• We say that Ais compatible with Π if for every well-behaved Z, the total running time
of the honest Pi’s and Ais bounded (with overwhelming probability) by some polynomial
(which depends on Π and Abut not on Z) in the security parameter and the sum of the
lengths of all messages sent by Zalong the Z-I/O wires and the A-control wire.
With these deﬁnitions, if Zis well behaved and Ais compatible with Π, then the overall running
time of all machines will be bounded (with overwhelming probability) by some polynomial (which
depends on Π, A, and Z) in the security parameter. We make one more deﬁnition:
• Protocol Π is eﬃcient if the trivial adversary is compatible with Π.
This notion of an eﬃcient protocol essentially says that the running time of all the machines
in the protocol will be bounded, with overwhelming probability, by a ﬁxed polynomial in the
amount of data that ﬂows into the protocol from the environment. This deﬁnition is ﬂexible
enough to accommodate protocols that process an a priori unbounded number of requests from
their environment.
23.5.2 The ideal protocol and its execution
We now describe the ideal protocol execution. The main changes from the real protocol execution
are as follows:
• There is a diﬀerent adversary S, which we shall call the simulator.
• Instead of a communication network, there is an ideal functionality F. The logic of the
ideal functionality depends on the goals of the protocol. Below, we will describe an ideal
1041
Z
1 2 3 4
S
F
F -I/O wires
F -control wire
S -control wire
Figure 23.9: Ideal protocol execution, including the ideal functionality C, simulator S, and
environment Z, along with honest parties P1 and P2, and corrupt parties P3 and P4
functionality suitable for the secure function evaluation problem, which basically receives
inputs from all parties (both honest and corrupt), evaluates the function, and then delivers
outputs to all parties (again, both honest and corrupt). However, other protocols for other
tasks may be best modeled by diﬀerent ideal functionalities.
• The honest parties themselves play no active role in the protocol: any input messages they
receive from Zare passed directly to F, and any output messages they receive from Fare
output directly to Z.
Fig. 23.9 illustrates the basic topology of an ideal protocol execution. Instead of C-I/O wires,
we have F-I/O wires: for honest parties, these connect directly to the environment, and for corrupt
parties, these connect to S. In place of the C-control wire, we have an F-control wire, connecting
Sand F. In place of the A-control wire, we have an S-control wire, connecting Zand S.
As will become apparent soon, an important feature of this system is that both the real and
ideal protocols present exactly the same interface to the environment Z.
23.5.2.1 The ideal protocol execution
We now describe the mechanics of an ideal protocol execution. As in the real protocol execution,
initially, Zstarts executing and determines which subset of P1,...,P N is to be considered corrupt.
This information is written to a special tape that is also visible toSand F. Also, protocol execution
halts when Zhalts. Before halting, Zwrites a single bit to a special output tape.
Next, we describe the types of messages that may be passed between machines.
1. Zmay pass a protocol input to any honest party Pi along an F-I/O wire, which is forwarded
directly to F.
2. Zmay pass a control message to Salong the S-control wire.
1042
3. Smay pass a protocol input to Falong one of the F-I/O wires corresponding to a corrupt
party, or a control message to Falong the F-control wire.
4. Smay pass a leakage message to Zalong the S-control wire.
5. Fmay do one of the following:
(a) pass a protocol output to Zalong one of the F-I/O wires corresponding to an honest
party;
(b) pass a protocol output to Salong one of the F-I/O wires corresponding to a corrupt
party;
(c) pass a leakage message to Salong the F-control wire.
23.5.2.2 Running time considerations
Similar to what we did above, we need to restrict ourselves to ideal functionalities, simulators, and
environments that are eﬃcient in some appropriate sense. We make the following deﬁnition:
• We say that Sis compatible with Fif for every well-behaved Z, the total running time of
Fand Sis bounded (with overwhelming probability) by some polynomial (which depends on
Fand Sbut not on Z) in the security parameter and the sum of the lengths of all messages
sent by Zalong the F-I/O wires and the S-control wire.
With this deﬁnition, if Zis well behaved and Sis compatible with F, then the overall running
time of all machines will be bounded (with overwhelming probability) by some polynomial (which
depends on F, S, and Z) in the security parameter.
23.5.3 Example: the ideal functionality for secure function evaluation
We now describe the ideal functionality, Fsfe, for secure function evaluation. At a high level, Fsfe
receives protocol inputs from all parties (both honest and corrupt) and delivers protocol outputs
to all parties (both honest and corrupt). Fsfe acts as a “trusted third party”, in the sense that,
by design, it computes all outputs correctly, and leaks no additional information to the adversary
(other than the outputs delivered to corrupt parties). However, Fsfe does allow the adversary to
control when and if honest parties actually receive their outputs. In particular, the design of Fsfe
provides no guarantee of output delivery: an honest party may fail to receive some or all of its
outputs (even if the corrupt parties receive all of their outputs).
In somewhat more detail, Fsfe works as follows. First of all, we assume that the function itself
is publicly known and ﬁxed in advance. Typically, such a function may be described by a boolean
or arithmetic circuit, but our presentation here is more general. Nevertheless, we shall still speak
of input wires and output wires of the function. We assume that each input wire is labeled with
• an inputID that identiﬁes the wire, and
• the index of the party that is designated to supply the value on that wire.
We also assume that each output wire is labeled with
• an outputID that identiﬁes the wire, and
1043
• a list of the indices of the parties that are designated to obtain that output.
Here is how the functionality Fsfe operates:
1. The functionality Fsfe may receive a protocol input along a Fsfe-I/O wire, either from an
honest party Pi or from the adversary Son behalf of a corrupt party Pi. A protocol input
is a tuple of the form ( input,inputID,x). As a pre-condition, party Pi must be the party
that is designated to supply this input value, and must not already have supplied a value for
this input wire. The functionality Fsfe records the tuple (input,inputID,x) and passes to S,
along the Fsfe-control wire, the leakage message ( input,inputID).
2. The functionality Fsfe may receive a control message from Salong the Fsfe-control wire. Such
a control message is a tuple of the form ( output,outputID,i). As a pre-condition, all inputs
for all parties (both honest and corrupt) must have already been supplied, and i must be in
the list of parties designated to obtain this output value. Functionality Fsfe computes the
appropriate output value yand passes the tuple (output,outputID,y) along the Fsfe-I/O wire
corresponding to party Pi, which goes to Pi (if Pi is honest) or to S(if Pi is corrupt).
23.5.3.1 Probabilistic functionalities
So far, we have only discussed Fsfe for functions. However, one can generalize Fsfe to cover prob-
abilistic functions as well. Essentially, this means that the function computed takes an auxiliary
random input, which is supplied by the functionality Fsfe itself.
23.5.4 Secure implementation: a strong security notion
We can now formally deﬁne protocol security.
• We denote by Exec[Π,A,Z] the random variable representing the output of an environment
Zinteracting with a protocol Π and an adversary Ain the real world execution.
• We denote by Exec[F,S,Z] the random variable representing the output of an environment
Zinteracting with an ideal functionality Fand a simulator Sin the ideal world execution.
To streamline notation in this section, for random variables X,Y , we will write X ≈Y to
mean X and Y are statistically indistinguiable (as in Section 3.11). When X and Y are 0/1-valued
random variables, this is equivalent to saying that |Pr[X = 1] −Pr[Y = 1]|is negligible.
We begin with a preliminary deﬁnition that deﬁnes security with respect to a speciﬁc adversary.
Deﬁnition 23.5. Let Abe an adversary that is compatible with protocolΠ. We say that Π securely
implements Fagainst Aif there exists a simulator S(which may depend on A) that is compatible
with F, such that for every well-behaved environment Z, we have
Exec[Π,A,Z] ≈Exec[F,S,Z].
Fig. 23.10 gives a schematic diagram for Deﬁnition 23.5. Fig. 23.10(a) shows the real world. The
cloud labeled Π represents the honest protocol machines P1,P2,... that are running protocol Π.
The protocol machines interact directly with the environment Z, with the communication network
C, and with the adversary A. The adversary Amay also interact directly with the environment and
1044
Z
A
Z
<latexit sha1_base64="dF1FgHoDpW3xfetyeMxpjZHp4Es=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEao8FLx4r2g9oQ9lsN+3SzSbsToQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMbud+54lrI2L1iNOE+xEdKREKRtFKD/2mGJQrbtVdgKwTLycVyNEclL/6w5ilEVfIJDWm57kJ+hnVKJjks1I/NTyhbEJHvGepohE3frY4dUYurDIkYaxtKSQL9fdERiNjplFgOyOKY7PqzcX/vF6KYd3PhEpS5IotF4WpJBiT+d9kKDRnKKeWUKaFvZWwMdWUoU2nZEPwVl9eJ+2rqleruvfXlUY9j6MIZ3AOl+DBDTTgDprQAgYjeIZXeHOk8+K8Ox/L1oKTz5zCHzifPx3Bjag=</latexit>
⇧
<latexit sha1_base64="1XoURWGEIR7YcCUC7jht5NjlzGU=">AAAB8nicbVDLSsNAFL2pr1pfVZdugkVwVRIR7bIgiMsK9gFtKJPppB06mQkzN0IJ/Qw3LhRx69e482+ctFlo9cDA4Zx7mXNPmAhu0PO+nNLa+sbmVnm7srO7t39QPTzqGJVqytpUCaV7ITFMcMnayFGwXqIZiUPBuuH0Jve7j0wbruQDzhIWxGQsecQpQSv1BzHBCSUiu50PqzWv7i3g/iV+QWpQoDWsfg5GiqYxk0gFMabvewkGGdHIqWDzyiA1LCF0Ssasb6kkMTNBtog8d8+sMnIjpe2T6C7UnxsZiY2ZxaGdzCOaVS8X//P6KUaNIOMySZFJuvwoSoWLys3vd0dcM4piZgmhmtusLp0QTSjaliq2BH/15L+kc1H3r+re/WWt2SjqKMMJnMI5+HANTbiDFrSBgoIneIFXB51n5815X46WnGLnGH7B+fgGdouRVw==</latexit>
F
<latexit sha1_base64="jGBr9UghEgYlTE3kbmr1C7r+gf0=">AAAB+3icbVC7TsMwFL0pr1JeoYwsFhVSWaqkA3SsxMJYpL6kNqocx2mtOk5kO0AV9VdYGECIlR9h429w2wzQciRLR+fco3t9/IQzpR3n2ypsbe/s7hX3SweHR8cn9mm5q+JUEtohMY9l38eKciZoRzPNaT+RFEc+pz1/ervwew9UKhaLtp4l1IvwWLCQEayNNLLLVXyF2hOKTIijx1jyYGRXnJqzBNokbk4qkKM1sr+GQUzSiApNOFZq4DqJ9jIsNSOczkvDVNEEkyke04GhAkdUedny9jm6NEqAwliaJzRaqr8TGY6UmkW+mYywnqh1byH+5w1SHTa8jIkk1VSQ1aIw5UjHaFEECpikRPOZIZhIZm5FZIIlJtrUVTIluOtf3iTdes29rjn39UqzkddRhHO4gCq4cANNuIMWdIDAEzzDK7xZc+vFerc+VqMFK8+cwR9Ynz9e35NX</latexit>
(a) The real world
<latexit sha1_base64="3UHvu/LXpBcY1rrhloOJB8n1u9s=">AAAB/HicbVBNS8NAEN3Ur1q/oj16WSxCvZSkB+2x4MVjhX5BG8pmM22XbjZhd6OEUP+KFw+KePWHePPfuG1z0NYHA4/3ZpiZ58ecKe0431Zha3tnd6+4Xzo4PDo+sU/PuipKJIUOjXgk+z5RwJmAjmaaQz+WQEKfQ8+f3S783gNIxSLR1mkMXkgmgo0ZJdpII7tc9a9wewqYBUA4fowkD0Z2xak5S+BN4uakgnK0RvbXMIhoEoLQlBOlBq4Tay8jUjPKYV4aJgpiQmdkAgNDBQlBedny+Dm+NEqAx5E0JTReqr8nMhIqlYa+6QyJnqp1byH+5w0SPW54GRNxokHQ1aJxwrGO8CIJHDAJVPPUEEIlM7diOiWSUG3yKpkQ3PWXN0m3XnOva859vdJs5HEU0Tm6QFXkohvURHeohTqIohQ9o1f0Zj1ZL9a79bFqLVj5TBn9gfX5AxVDk70=</latexit>
(b) The ideal world
<latexit sha1_base64="0wv+Rjb2To8Pu292t2Qiroxx268=">AAAB8nicbVDLSsNAFL2pr1pfVZdugkVwVRIR7bLgxmVF+4A2lMl00g6dzISZG6GEfoYbF4q49Wvc+TdO2iy0emDgcM69zLknTAQ36HlfTmltfWNzq7xd2dnd2z+oHh51jEo1ZW2qhNK9kBgmuGRt5ChYL9GMxKFg3XB6k/vdR6YNV/IBZwkLYjKWPOKUoJX6g5jghBKR3c+H1ZpX9xZw/xK/IDUo0BpWPwcjRdOYSaSCGNP3vQSDjGjkVLB5ZZAalhA6JWPWt1SSmJkgW0Seu2dWGbmR0vZJdBfqz42MxMbM4tBO5hHNqpeL/3n9FKNGkHGZpMgkXX4UpcJF5eb3uyOuGUUxs4RQzW1Wl06IJhRtSxVbgr968l/Suaj7V3Xv7rLWbBR1lOEETuEcfLiGJtxCC9pAQcETvMCrg86z8+a8L0dLTrFzDL/gfHwDikyRZA==</latexit>
S
<latexit sha1_base64="lnaxowBWkX0s68oQ5UUcNobJ0QE=">AAAB8nicbVBNS8NAFHypX7V+VT16CRbBU0lEtMdCLx4r2FpoQ9lsN+3SzW7YfRFK6M/w4kERr/4ab/4bN20O2jqwMMy8x86bMBHcoOd9O6WNza3tnfJuZW//4PCoenzSNSrVlHWoEkr3QmKY4JJ1kKNgvUQzEoeCPYbTVu4/PjFtuJIPOEtYEJOx5BGnBK3UH8QEJ5SIrDUfVmte3VvAXSd+QWpQoD2sfg1GiqYxk0gFMabvewkGGdHIqWDzyiA1LCF0Ssasb6kkMTNBtog8dy+sMnIjpe2T6C7U3xsZiY2ZxaGdzCOaVS8X//P6KUaNIOMySZFJuvwoSoWLys3vd0dcM4piZgmhmtusLp0QTSjaliq2BH/15HXSvar7N3Xv/rrWbBR1lOEMzuESfLiFJtxBGzpAQcEzvMKbg86L8+58LEdLTrFzCn/gfP4AcfyRVA==</latexit>
C
Figure 23.10: Schematic diagram for Deﬁnition 23.5
communication network. Fig. 23.10(b) shows the ideal world. Here, instead of interacting with the
honest protocol machines, the environment instead interacts with the ideal functionality F, and
instead of interacting with the adversaryA, the environment instead interacts with the simulatorS.
The simulator and ideal functionality may also interact directly with one another. Deﬁnition 23.5
essentially says that there is no environment that can eﬀectively distinguish between the real world
and ideal world.
The next deﬁnition deﬁnes security with respect to all adversaries. This deﬁnition captures
what we mean by a secure protocol:
Deﬁnition 23.6. We say that Π securely implements Fif Π securely implements Fagainst A
for every adversary Athat is compatible with Π.
Unpacking this deﬁnition, it says that Π securely implementsFif and only if for every adversary
A(compatible with Π) there exists a simulator S(compatible with F, and which may depend on
A), such that for every well-behaved environment Z, we have Exec[Π,A,Z] ≈Exec[F,S,Z].
Hopefully, it is clear that this formal deﬁnition captures the intuition described at the beginning
of this chapter in Section 23.1.3. Security means that the damage caused by any adversary A
attacking the real protocol Π is limited to the damage caused by another adversary S(which may
depend on A) attacking the ideal protocol. In the case of secure function evaluation, where the ideal
functionality is Fsfe, it should be clear that this implies privacy, soundness, and input independence.
A fact that is convenient in proving the security of any protocol is the following:
Theorem 23.3 (Completeness of the trivial adversary). Let Π be an eﬃcient protocol. If Π
securely implements Fagainst the trivial adversary, then Π securely implements F.
Proof sketch. Suppose Π securely implements Fagainst the trivial adversary. This means there
is a simulator Striv (that is compatible with F) such that for every well-behaved environment Z,
which interacts with the trivial adversary via the Atriv-control wire, we have Exec[Π ,Atriv,Z] ≈
Exec[F,Striv,Z].
1045
Now consider an adversary A(that is compatible with Π). Our goal is to show that Π securely
implements Fagainst A. Consider a well-behaved environment Zthat interacts with Avia the
A-control wire. We can construct a new environment A|Zthat embeds Ainto Z. The environment
A|Zinteracts with the trivial adversary via the Atriv-control wire, running the code for both A
and Z. (Some care must be taken to ensure that A|Z is well behaved.) One can show that
Exec[Π,A,Z] ≈Exec[Π,Atriv,A|Z]. See Fig. 23.11 for diagrams that illustrate this and other
steps in the proof.
Using the fact that Π securely implements F against the trivial adversary, we have
Exec[Π,Atriv,A|Z] ≈Exec[F,Striv,A|Z].
Finally, we can construct a new simulator Striv|Athat embeds Ainto Striv. The simulator
Striv|Ainteracts with the environment via the A-control wire, running the code for both Striv and
A. One can show that Exec[ F,Striv,A|Z] ≈Exec[F,Striv|A,Z] (and that Striv|Ais compatible
with F).
Putting this all together, we have
Exec[Π,A,Z] ≈Exec[Π,Atriv,A|Z] ≈Exec[F,Striv,A|Z] ≈Exec[F,Striv|A,Z],
which shows that Exec[Π ,A,Z] ≈Exec[F,Striv|A,Z]. From this, we conclude that Π securely
implements Fagainst A. 2
This theorem says that to prove that Π securely implements F, instead of showing how to build
a simulator corresponding to every adversary, it suﬃces to just build a simulator for the trivial
adversary.
Remark 23.2 (Black box reductions). The proof of Theorem 23.3 actually shows that in Def-
inition 23.6, the simulator Sthat we need to show exists for every adversary Acan actually be
constructed using Aas a black box. Speciﬁcally, we can set S := Striv|A(see proof of Theo-
rem 23.3). This simulator Sis essentially an “elementary wrapper around A” (although there are
some technical diﬀerences between this and Deﬁnition 2.12.) 2
Remark 23.3 (On a simpler deﬁnition of security). In light of Theorem 23.3, in principle,
we could have deﬁned security more simply, by deﬁning it strictly in terms of the trivial adversary.
However, there are situations where we need to work with more general adversaries. Examples of
this may be seen below in the proofs of Theorems 23.5 and 23.6. 2
23.5.5 Consequences of secure implementation
Secure implementation is a very strong security notion. In this section we explore some useful
consequences. We state some of these consequences as theorems, but only provide the proof ideas,
as formal proofs would be quite tedious.
23.5.5.1 Concurrent composition
One consequence of secure implementation is that it guarantees that a secure protocol remains
secure even when many instances of the protocol are allowed to execute concurrently with one
another. (In Exercise 11.20, we saw an example of how a seemingly secure protocol became insecure
when two instances ran concurrently.)
1046
Z
A
<latexit sha1_base64="dF1FgHoDpW3xfetyeMxpjZHp4Es=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEao8FLx4r2g9oQ9lsN+3SzSbsToQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMbud+54lrI2L1iNOE+xEdKREKRtFKD/2mGJQrbtVdgKwTLycVyNEclL/6w5ilEVfIJDWm57kJ+hnVKJjks1I/NTyhbEJHvGepohE3frY4dUYurDIkYaxtKSQL9fdERiNjplFgOyOKY7PqzcX/vF6KYd3PhEpS5IotF4WpJBiT+d9kKDRnKKeWUKaFvZWwMdWUoU2nZEPwVl9eJ+2rqleruvfXlUY9j6MIZ3AOl+DBDTTgDprQAgYjeIZXeHOk8+K8Ox/L1oKTz5zCHzifPx3Bjag=</latexit>
⇧
<latexit sha1_base64="lnaxowBWkX0s68oQ5UUcNobJ0QE=">AAAB8nicbVBNS8NAFHypX7V+VT16CRbBU0lEtMdCLx4r2FpoQ9lsN+3SzW7YfRFK6M/w4kERr/4ab/4bN20O2jqwMMy8x86bMBHcoOd9O6WNza3tnfJuZW//4PCoenzSNSrVlHWoEkr3QmKY4JJ1kKNgvUQzEoeCPYbTVu4/PjFtuJIPOEtYEJOx5BGnBK3UH8QEJ5SIrDUfVmte3VvAXSd+QWpQoD2sfg1GiqYxk0gFMabvewkGGdHIqWDzyiA1LCF0Ssasb6kkMTNBtog8dy+sMnIjpe2T6C7U3xsZiY2ZxaGdzCOaVS8X//P6KUaNIOMySZFJuvwoSoWLys3vd0dcM4piZgmhmtusLp0QTSjaliq2BH/15HXSvar7N3Xv/rrWbBR1lOEMzuESfLiFJtxBGzpAQcEzvMKbg86L8+58LEdLTrFzCn/gfP4AcfyRVA==</latexit>
C
Z
A
<latexit sha1_base64="dF1FgHoDpW3xfetyeMxpjZHp4Es=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEao8FLx4r2g9oQ9lsN+3SzSbsToQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMbud+54lrI2L1iNOE+xEdKREKRtFKD/2mGJQrbtVdgKwTLycVyNEclL/6w5ilEVfIJDWm57kJ+hnVKJjks1I/NTyhbEJHvGepohE3frY4dUYurDIkYaxtKSQL9fdERiNjplFgOyOKY7PqzcX/vF6KYd3PhEpS5IotF4WpJBiT+d9kKDRnKKeWUKaFvZWwMdWUoU2nZEPwVl9eJ+2rqleruvfXlUY9j6MIZ3AOl+DBDTTgDprQAgYjeIZXeHOk8+K8Ox/L1oKTz5zCHzifPx3Bjag=</latexit>
⇧
<latexit sha1_base64="lnaxowBWkX0s68oQ5UUcNobJ0QE=">AAAB8nicbVBNS8NAFHypX7V+VT16CRbBU0lEtMdCLx4r2FpoQ9lsN+3SzW7YfRFK6M/w4kERr/4ab/4bN20O2jqwMMy8x86bMBHcoOd9O6WNza3tnfJuZW//4PCoenzSNSrVlHWoEkr3QmKY4JJ1kKNgvUQzEoeCPYbTVu4/PjFtuJIPOEtYEJOx5BGnBK3UH8QEJ5SIrDUfVmte3VvAXSd+QWpQoD2sfg1GiqYxk0gFMabvewkGGdHIqWDzyiA1LCF0Ssasb6kkMTNBtog8dy+sMnIjpe2T6C7U3xsZiY2ZxaGdzCOaVS8X//P6KUaNIOMySZFJuvwoSoWLys3vd0dcM4piZgmhmtusLp0QTSjaliq2BH/15HXSvar7N3Xv/rrWbBR1lOEMzuESfLiFJtxBGzpAQcEzvMKbg86L8+58LEdLTrFzCn/gfP4AcfyRVA==</latexit>
C
<latexit sha1_base64="zJkGuDxsJOb5PI9T5qPQEwb+rM8=">AAACAnicbVBNS8NAEN3Ur1q/op7ES7AInkoioj1WvHisYD+gDWGz3bRLd5OwOymWELz4V7x4UMSrv8Kb/8ZNm4O2Phh4vDfDzDw/5kyBbX8bpZXVtfWN8mZla3tnd8/cP2irKJGEtkjEI9n1saKchbQFDDjtxpJi4XPa8cc3ud+ZUKlYFN7DNKauwMOQBYxg0JJnHvUFhhHBPL3OvD7QB5AiBckmmWdW7Zo9g7VMnIJUUYGmZ371BxFJBA2BcKxUz7FjcFMsgRFOs0o/UTTGZIyHtKdpiAVVbjp7IbNOtTKwgkjqCsGaqb8nUiyUmgpfd+YHq0UvF//zegkEdTdlYZwADcl8UZBwCyIrz8MaMEkJ8KkmmEimb7XICEtMQKdW0SE4iy8vk/Z5zbms2XcX1Ua9iKOMjtEJOkMOukINdIuaqIUIekTP6BW9GU/Gi/FufMxbS0Yxc4j+wPj8AZXDmCs=</latexit>
A triv
<latexit sha1_base64="z3P778goNnkC3iN4Z5d0R/0U25s=">AAACAHicbZDLSsNAFIZP6q3WW9SFCzeDRXBVEhHtsuLGZQV7wTaUyXTSDp1MwsxEKDEbX8WNC0Xc+hjufBsnbRFt/WHg4z/nMOf8fsyZ0o7zZRWWlldW14rrpY3Nre0de3evqaJEEtogEY9k28eKciZoQzPNaTuWFIc+py1/dJXXW/dUKhaJWz2OqRfigWABI1gbq2cfdEOshwTz9DJ7+OG7rGeXnYozEVoEdwZlmKnesz+7/YgkIRWacKxUx3Vi7aVYakY4zUrdRNEYkxEe0I5BgUOqvHRyQIaOjdNHQSTNExpN3N8TKQ6VGoe+6cxXVPO13Pyv1kl0UPVSJuJEU0GmHwUJRzpCeRqozyQlmo8NYCKZ2RWRIZaYaJNZyYTgzp+8CM3TintecW7OyrXqLI4iHMIRnIALF1CDa6hDAwhk8AQv8Go9Ws/Wm/U+bS1Ys5l9+CPr4xuPCZb/</latexit>
A|Z
Z
A
<latexit sha1_base64="z3P778goNnkC3iN4Z5d0R/0U25s=">AAACAHicbZDLSsNAFIZP6q3WW9SFCzeDRXBVEhHtsuLGZQV7wTaUyXTSDp1MwsxEKDEbX8WNC0Xc+hjufBsnbRFt/WHg4z/nMOf8fsyZ0o7zZRWWlldW14rrpY3Nre0de3evqaJEEtogEY9k28eKciZoQzPNaTuWFIc+py1/dJXXW/dUKhaJWz2OqRfigWABI1gbq2cfdEOshwTz9DJ7+OG7rGeXnYozEVoEdwZlmKnesz+7/YgkIRWacKxUx3Vi7aVYakY4zUrdRNEYkxEe0I5BgUOqvHRyQIaOjdNHQSTNExpN3N8TKQ6VGoe+6cxXVPO13Pyv1kl0UPVSJuJEU0GmHwUJRzpCeRqozyQlmo8NYCKZ2RWRIZaYaJNZyYTgzp+8CM3TintecW7OyrXqLI4iHMIRnIALF1CDa6hDAwhk8AQv8Go9Ws/Wm/U+bS1Ys5l9+CPr4xuPCZb/</latexit>
A|Z
<latexit sha1_base64="1XoURWGEIR7YcCUC7jht5NjlzGU=">AAAB8nicbVDLSsNAFL2pr1pfVZdugkVwVRIR7bIgiMsK9gFtKJPppB06mQkzN0IJ/Qw3LhRx69e482+ctFlo9cDA4Zx7mXNPmAhu0PO+nNLa+sbmVnm7srO7t39QPTzqGJVqytpUCaV7ITFMcMnayFGwXqIZiUPBuuH0Jve7j0wbruQDzhIWxGQsecQpQSv1BzHBCSUiu50PqzWv7i3g/iV+QWpQoDWsfg5GiqYxk0gFMabvewkGGdHIqWDzyiA1LCF0Ssasb6kkMTNBtog8d8+sMnIjpe2T6C7UnxsZiY2ZxaGdzCOaVS8X//P6KUaNIOMySZFJuvwoSoWLys3vd0dcM4piZgmhmtusLp0QTSjaliq2BH/15L+kc1H3r+re/WWt2SjqKMMJnMI5+HANTbiDFrSBgoIneIFXB51n5815X46WnGLnGH7B+fgGdouRVw==</latexit>
F
Z
A
<latexit sha1_base64="1XoURWGEIR7YcCUC7jht5NjlzGU=">AAAB8nicbVDLSsNAFL2pr1pfVZdugkVwVRIR7bIgiMsK9gFtKJPppB06mQkzN0IJ/Qw3LhRx69e482+ctFlo9cDA4Zx7mXNPmAhu0PO+nNLa+sbmVnm7srO7t39QPTzqGJVqytpUCaV7ITFMcMnayFGwXqIZiUPBuuH0Jve7j0wbruQDzhIWxGQsecQpQSv1BzHBCSUiu50PqzWv7i3g/iV+QWpQoDWsfg5GiqYxk0gFMabvewkGGdHIqWDzyiA1LCF0Ssasb6kkMTNBtog8d8+sMnIjpe2T6C7UnxsZiY2ZxaGdzCOaVS8X//P6KUaNIOMySZFJuvwoSoWLys3vd0dcM4piZgmhmtusLp0QTSjaliq2BH/15L+kc1H3r+re/WWt2SjqKMMJnMI5+HANTbiDFrSBgoIneIFXB51n5815X46WnGLnGH7B+fgGdouRVw==</latexit>
F
S triv |A
<latexit sha1_base64="aozw0NvpZ7ltNSI0I0iRMjZwpUY=">AAACGXicbZDLSsNAFIYn9VbrLerSzWArVJCSdKFdVkRwWcFeMAllMp22QycXZiZiCXkNN76KGxeKuNSVb+MkDaitPwx8/Occ5pzfDRkV0jC+tMLS8srqWnG9tLG5tb2j7+51RBBxTNo4YAHvuUgQRn3SllQy0gs5QZ7LSNedXKT17h3hggb+jZyGxPHQyKdDipFUVl83qugYVmwPyTH34st7ghPLbtGTzMGIxefJD98mTqWvl42akQkugplDGeRq9fUPexDgyCO+xAwJYZlGKJ0YcUkxI0nJjgQJEZ6gEbEU+sgjwomzyxJ4pJwBHAZcPV/CzP09ESNPiKnnqs50STFfS83/alYkhw0npn4YSeLj2UfDiEEZwDQmOKCcYMmmChDmVO0K8RhxhKUKs6RCMOdPXoROvWae1ozrernZyOMoggNwCKrABGegCa5AC7QBBg/gCbyAV+1Re9betPdZa0HLZ/bBH2mf37M/oBw=</latexit>
(a) Exec[ ⇧ , A , Z ]
<latexit sha1_base64="t+NTASiV065DWGcbvibfE1tWzp0=">AAACM3icbZDLSsNAFIYn9VbrrerSzWArVJCSdKFdVkQQVxXsBZMQJtNJO3RyYWZSLDHv5MYXcSGIC0Xc+g5OL0qtHhj4+P9zmHN+N2JUSF1/1jILi0vLK9nV3Nr6xuZWfnunKcKYY9LAIQt520WCMBqQhqSSkXbECfJdRlpu/2zktwaECxoG13IYEdtH3YB6FCOpJCd/WXIPYdHykexxPzm/JTg1rTo9GisYseQ0db5dyekgnXXufvgmtYtOvqCX9XHBv2BMoQCmVXfyj1YnxLFPAokZEsI09EjaCeKSYkbSnBULEiHcR11iKgyQT4SdjG9O4YFSOtALuXqBhGN1diJBvhBD31WdoyXFvDcS//PMWHpVO6FBFEsS4MlHXsygDOEoQNihnGDJhgoQ5lTtCnEPcYSlijmnQjDmT/4LzUrZOC7rV5VCrTqNIwv2wD4oAQOcgBq4AHXQABjcgyfwCt60B+1Fe9c+Jq0ZbTqzC36V9vkFcn6sDQ==</latexit>
(b) Exec[ ⇧ , A triv , A|Z ]
<latexit sha1_base64="uCciaLX7k3BHqT1uoMHnBT0ZZOU=">AAACAnicbVBNS8NAEN3Ur1q/op7ES7AInkoioj0WvHisaD+gDWGz3bRLd5OwOymWELz4V7x4UMSrv8Kb/8ZNm4O2Phh4vDfDzDw/5kyBbX8bpZXVtfWN8mZla3tnd8/cP2irKJGEtkjEI9n1saKchbQFDDjtxpJi4XPa8cfXud+ZUKlYFN7DNKauwMOQBYxg0JJnHvUFhhHBPL3LvD7QB5AiBckmmWdW7Zo9g7VMnIJUUYGmZ371BxFJBA2BcKxUz7FjcFMsgRFOs0o/UTTGZIyHtKdpiAVVbjp7IbNOtTKwgkjqCsGaqb8nUiyUmgpfd+YHq0UvF//zegkEdTdlYZwADcl8UZBwCyIrz8MaMEkJ8KkmmEimb7XICEtMQKdW0SE4iy8vk/Z5zbms2bcX1Ua9iKOMjtEJOkMOukINdIOaqIUIekTP6BW9GU/Gi/FufMxbS0Yxc4j+wPj8AbIZmD0=</latexit>
S triv
<latexit sha1_base64="uCciaLX7k3BHqT1uoMHnBT0ZZOU=">AAACAnicbVBNS8NAEN3Ur1q/op7ES7AInkoioj0WvHisaD+gDWGz3bRLd5OwOymWELz4V7x4UMSrv8Kb/8ZNm4O2Phh4vDfDzDw/5kyBbX8bpZXVtfWN8mZla3tnd8/cP2irKJGEtkjEI9n1saKchbQFDDjtxpJi4XPa8cfXud+ZUKlYFN7DNKauwMOQBYxg0JJnHvUFhhHBPL3LvD7QB5AiBckmmWdW7Zo9g7VMnIJUUYGmZ371BxFJBA2BcKxUz7FjcFMsgRFOs0o/UTTGZIyHtKdpiAVVbjp7IbNOtTKwgkjqCsGaqb8nUiyUmgpfd+YHq0UvF//zegkEdTdlYZwADcl8UZBwCyIrz8MaMEkJ8KkmmEimb7XICEtMQKdW0SE4iy8vk/Z5zbms2bcX1Ua9iKOMjtEJOkMOukINdIOaqIUIekTP6BW9GU/Gi/FufMxbS0Yxc4j+wPj8AbIZmD0=</latexit>
S triv
<latexit sha1_base64="/S0ZFkXzRRbMiPo+UhzKtSGLb9w=">AAACO3icbZBLSwMxFIUz9V1fVZdugq2gIGXGhXZZEcWlr1axHUomvW2DmQfJndIyzv9y459w58aNC0XcujetRXxdCHw5515yc7xICo22/WBlxsYnJqemZ7Kzc/MLi7ml5aoOY8WhwkMZqkuPaZAigAoKlHAZKWC+J+HCu94f+BddUFqEwTn2I3B91g5ES3CGRmrkTjf4Ji3UfYYd5ScHPeBpbXjjTCaH6dYXn6WNOkIPTRcq0f3m7KU3X3yVuoVGLm8X7WHRv+CMIE9GddzI3debIY99CJBLpnXNsSN0E6ZQcAlpth5riBi/Zm2oGQyYD9pNhn9P6bpRmrQVKnMCpEP1+0TCfK37vmc6B0vq395A/M+rxdgquYkIohgh4J8PtWJJMaSDIGlTKOAo+wYYV8LsSnmHKcbRxJ01ITi/v/wXqttFZ6don2zny6VRHNNklayRDeKQXVImR+SYVAgnt+SRPJMX6856sl6tt8/WjDWaWSE/ynr/AAJbr+o=</latexit>
(c) Exec[ F , S triv , A|Z ]
<latexit sha1_base64="a4X9bV5z2mYBzGdgaL8hzhSmYwM=">AAACO3icbVDJSgNBEO2Je9yiHr00JoKChBkPmmNEFI9uiWIyhJ5OJWnsWeiuCQnj/JcXf8KbFy8eFPHq3U4M4lbQ8Pq9V1TV8yIpNNr2g5UZG5+YnJqeyc7OzS8s5paWqzqMFYcKD2WoLj2mQYoAKihQwmWkgPmehAvven+gX3RBaREG59iPwPVZOxAtwRkaqpE73Whu0kLdZ9hRfnLQA57Whj/OZHKYbn3hs7RRR+ihcaES3fTmS9n75rpK3UIjl7eL9rDoX+CMQJ6M6riRu683Qx77ECCXTOuaY0foJkyh4BLSbD3WEDF+zdpQMzBgPmg3Gd6e0nXDNGkrVOYFSIfs946E+Vr3fc84B0vq39qA/E+rxdgquYkIohgh4J+DWrGkGNJBkLQpFHCUfQMYV8LsSnmHKcbRxJ01ITi/T/4LqttFZ6don2zny6VRHNNklayRDeKQXVImR+SYVAgnt+SRPJMX6856sl6tt09rxhr1rJAfZb1/AAfvr+s=</latexit>
(d) Exec[ F , S triv |A , Z ]
Figure 23.11: Diagram for proof of Theorem 23.3
1047
P 1 P 2
Z
A
C
1 2 3 4
P 1 P 2
C
1 2 3 4
Figure 23.12: Real protocol execution with two concurrent instances
Z
1 2 3 4 1 2 3 4
S
FF
Figure 23.13: Ideal protocol execution with two concurrent instances
It takes some work to properly formulate this result. To start with, we have to generalize the
notions of real and ideal protocol execution to multiple protocol instances. The essential features
are as follows:
• for each instance of a real protocol, there is a separate communication network;
• for each instance of an ideal protocol, there is a separate ideal functionality.
See Figures 23.12 and 23.13, which generalize Figures 23.8 and 23.9 from one protocol instance to
two instances. In these ﬁgures, we have four parties P1,...,P 4 that are running two independent
instances of the same protocol Π. Parties P1 and P2 are honest and parties P3 and P4 are corrupt
(in both instances of the protocol, in both the real and ideal executions).
Note that because the two communication networks appearing in Fig. 23.12 are separate, if P1
in the ﬁrst instance sends a message to P2 in the ﬁrst instance, it is impossible for the adversary to
forward that message to P2 in the second instance. In practice, it is easy to eﬀectively implement
such separate communication networks. For example, if each protocol instance has a unique “in-
stance ID”, then this instance ID can be included as associated data in an authenticated encryption
scheme that supports associated data (see the notion of an AEAD cipher in Section 9.5). Similarly,
because the two ideal functionalities appearing in Fig. 23.13 are separate, it is impossible for the
adversary Sto create any surprising interactions between them.
1048
In fact, in any situation where we want to model the execution of several protocols, or several
instances of the same protocol, we need to add to our formal model the notion of a protocol “instance
ID”, which can be used by all parties to identify which protocol instance a message belongs to.
Every protocol machine Pi in the real world execution will be initialized not only with its index i
but also with its instance ID. Similarly, an ideal functionality in the ideal world execution will be
initialized with its instance ID.
We state the following theorem:
Theorem 23.4 (concurrent composition). If Π is an eﬃcient protocol in the single-instance
setting, it remains an eﬃcient protocol in the multi-instance setting. Moreover, if Π securely im-
plements Fin the single-instance setting, then it does so as well in the multi-instance setting.
Proof sketch. We do not address the running time claim here, as it is proved by a fairly simple
(though somewhat tedious) argument. We focus on the security claim.
We are assuming single-instance security, which means that there is a simulator Striv (that is
compatible with F) such that for every well-behaved single-instance environmentZ, which interacts
with the trivial adversary via the Atriv-control wire, we have Exec[Π,Atriv,Z] ≈Exec[F,Striv,Z].
The proof is essentially a hybrid argument. For simplicity, we illustrate the proof idea with
just two protocol instances. By a generalization of Theorem 23.3, we may assume the adversary
in the multi-instance real world is the trivial adversary that simply routes messages between the
environment and other protocol machines. This is illustrated in Fig. 23.14(a). Here, the unlabeled
boxes represent simple routing machines — the top unlabeled box with which the environment
interacts directly routes messages to the lower unlabeled boxes based on instance IDs that are
assumed to be embedded in messages.
By virtue of single-instance security, we can replace the ﬁrst real-world protocol instance by
a corresponding ideal world protocol instance. This is illustrated in Fig. 23.14(b). To formally
justify this step, we deﬁne a single-instance environment Z1 that absorbs the second protocol
instance (as illustrated by the dashed box in the ﬁgure). Single-instance security implies that
Exec[Π,Atriv,Z1] ≈Exec[F,Striv,Z1], which implies that the multi-instance environmentZcannot
distinguish between the real world in Fig. 23.14(a) and the hybrid world in Fig. 23.14(b).
Again by virtue of single-instance security, we can replace the second real-world protocol in-
stance by a corresponding ideal world protocol instance. This is illustrated in Fig. 23.14(c). To
formally justify this step, we deﬁne a single-instance environment Z2 that absorbs the ﬁrst proto-
col instance (as illustrated by the dashed box in the ﬁgure). Single-instance security implies that
Exec[Π,Atriv,Z2] ≈Exec[F,Striv,Z2], which implies that the multi-instance environmentZcannot
distinguish between the hybrid world in Fig. 23.14(b) and the ideal world in Fig. 23.14(c). The
multi-instance simulator in the ideal world consists of a simple routing machine plus two copies of
the single-instance simulator Striv. 2
23.5.5.2 Subprotocol composition
Another consequence of secure implementation is that we can use a secure protocol as a subprotocol
in a larger protocol, and the larger protocol will remain secure.
To describe what this means takes some work, and we will just sketch the idea.
Suppose we design a protocol Π and we want to prove that Π securely implements some ideal
functionality F. Moreover, suppose that Π uses another protocol Π ∗as a subprotocol, and suppose
that Π∗securely implements some ideal functionality F∗.
1049
<latexit sha1_base64="dF1FgHoDpW3xfetyeMxpjZHp4Es=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEao8FLx4r2g9oQ9lsN+3SzSbsToQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMbud+54lrI2L1iNOE+xEdKREKRtFKD/2mGJQrbtVdgKwTLycVyNEclL/6w5ilEVfIJDWm57kJ+hnVKJjks1I/NTyhbEJHvGepohE3frY4dUYurDIkYaxtKSQL9fdERiNjplFgOyOKY7PqzcX/vF6KYd3PhEpS5IotF4WpJBiT+d9kKDRnKKeWUKaFvZWwMdWUoU2nZEPwVl9eJ+2rqleruvfXlUY9j6MIZ3AOl+DBDTTgDprQAgYjeIZXeHOk8+K8Ox/L1oKTz5zCHzifPx3Bjag=</latexit>
⇧
<latexit sha1_base64="jGBr9UghEgYlTE3kbmr1C7r+gf0=">AAAB+3icbVC7TsMwFL0pr1JeoYwsFhVSWaqkA3SsxMJYpL6kNqocx2mtOk5kO0AV9VdYGECIlR9h429w2wzQciRLR+fco3t9/IQzpR3n2ypsbe/s7hX3SweHR8cn9mm5q+JUEtohMY9l38eKciZoRzPNaT+RFEc+pz1/ervwew9UKhaLtp4l1IvwWLCQEayNNLLLVXyF2hOKTIijx1jyYGRXnJqzBNokbk4qkKM1sr+GQUzSiApNOFZq4DqJ9jIsNSOczkvDVNEEkyke04GhAkdUedny9jm6NEqAwliaJzRaqr8TGY6UmkW+mYywnqh1byH+5w1SHTa8jIkk1VSQ1aIw5UjHaFEECpikRPOZIZhIZm5FZIIlJtrUVTIluOtf3iTdes29rjn39UqzkddRhHO4gCq4cANNuIMWdIDAEzzDK7xZc+vFerc+VqMFK8+cwR9Ynz9e35NX</latexit>
(a) The real world
(c) The ideal world
<latexit sha1_base64="lnaxowBWkX0s68oQ5UUcNobJ0QE=">AAAB8nicbVBNS8NAFHypX7V+VT16CRbBU0lEtMdCLx4r2FpoQ9lsN+3SzW7YfRFK6M/w4kERr/4ab/4bN20O2jqwMMy8x86bMBHcoOd9O6WNza3tnfJuZW//4PCoenzSNSrVlHWoEkr3QmKY4JJ1kKNgvUQzEoeCPYbTVu4/PjFtuJIPOEtYEJOx5BGnBK3UH8QEJ5SIrDUfVmte3VvAXSd+QWpQoD2sfg1GiqYxk0gFMabvewkGGdHIqWDzyiA1LCF0Ssasb6kkMTNBtog8dy+sMnIjpe2T6C7U3xsZiY2ZxaGdzCOaVS8X//P6KUaNIOMySZFJuvwoSoWLys3vd0dcM4piZgmhmtusLp0QTSjaliq2BH/15HXSvar7N3Xv/rrWbBR1lOEMzuESfLiFJtxBGzpAQcEzvMKbg86L8+58LEdLTrFzCn/gfP4AcfyRVA==</latexit>
C
<latexit sha1_base64="dF1FgHoDpW3xfetyeMxpjZHp4Es=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEao8FLx4r2g9oQ9lsN+3SzSbsToQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMbud+54lrI2L1iNOE+xEdKREKRtFKD/2mGJQrbtVdgKwTLycVyNEclL/6w5ilEVfIJDWm57kJ+hnVKJjks1I/NTyhbEJHvGepohE3frY4dUYurDIkYaxtKSQL9fdERiNjplFgOyOKY7PqzcX/vF6KYd3PhEpS5IotF4WpJBiT+d9kKDRnKKeWUKaFvZWwMdWUoU2nZEPwVl9eJ+2rqleruvfXlUY9j6MIZ3AOl+DBDTTgDprQAgYjeIZXeHOk8+K8Ox/L1oKTz5zCHzifPx3Bjag=</latexit>
⇧
<latexit sha1_base64="lnaxowBWkX0s68oQ5UUcNobJ0QE=">AAAB8nicbVBNS8NAFHypX7V+VT16CRbBU0lEtMdCLx4r2FpoQ9lsN+3SzW7YfRFK6M/w4kERr/4ab/4bN20O2jqwMMy8x86bMBHcoOd9O6WNza3tnfJuZW//4PCoenzSNSrVlHWoEkr3QmKY4JJ1kKNgvUQzEoeCPYbTVu4/PjFtuJIPOEtYEJOx5BGnBK3UH8QEJ5SIrDUfVmte3VvAXSd+QWpQoD2sfg1GiqYxk0gFMabvewkGGdHIqWDzyiA1LCF0Ssasb6kkMTNBtog8dy+sMnIjpe2T6C7U3xsZiY2ZxaGdzCOaVS8X//P6KUaNIOMySZFJuvwoSoWLys3vd0dcM4piZgmhmtusLp0QTSjaliq2BH/15HXSvar7N3Xv/rrWbBR1lOEMzuESfLiFJtxBGzpAQcEzvMKbg86L8+58LEdLTrFzCn/gfP4AcfyRVA==</latexit>
C
Z Z
<latexit sha1_base64="1XoURWGEIR7YcCUC7jht5NjlzGU=">AAAB8nicbVDLSsNAFL2pr1pfVZdugkVwVRIR7bIgiMsK9gFtKJPppB06mQkzN0IJ/Qw3LhRx69e482+ctFlo9cDA4Zx7mXNPmAhu0PO+nNLa+sbmVnm7srO7t39QPTzqGJVqytpUCaV7ITFMcMnayFGwXqIZiUPBuuH0Jve7j0wbruQDzhIWxGQsecQpQSv1BzHBCSUiu50PqzWv7i3g/iV+QWpQoDWsfg5GiqYxk0gFMabvewkGGdHIqWDzyiA1LCF0Ssasb6kkMTNBtog8d8+sMnIjpe2T6C7UnxsZiY2ZxaGdzCOaVS8X//P6KUaNIOMySZFJuvwoSoWLys3vd0dcM4piZgmhmtusLp0QTSjaliq2BH/15L+kc1H3r+re/WWt2SjqKMMJnMI5+HANTbiDFrSBgoIneIFXB51n5815X46WnGLnGH7B+fgGdouRVw==</latexit>
F
Z
<latexit sha1_base64="1XoURWGEIR7YcCUC7jht5NjlzGU=">AAAB8nicbVDLSsNAFL2pr1pfVZdugkVwVRIR7bIgiMsK9gFtKJPppB06mQkzN0IJ/Qw3LhRx69e482+ctFlo9cDA4Zx7mXNPmAhu0PO+nNLa+sbmVnm7srO7t39QPTzqGJVqytpUCaV7ITFMcMnayFGwXqIZiUPBuuH0Jve7j0wbruQDzhIWxGQsecQpQSv1BzHBCSUiu50PqzWv7i3g/iV+QWpQoDWsfg5GiqYxk0gFMabvewkGGdHIqWDzyiA1LCF0Ssasb6kkMTNBtog8d8+sMnIjpe2T6C7UnxsZiY2ZxaGdzCOaVS8X//P6KUaNIOMySZFJuvwoSoWLys3vd0dcM4piZgmhmtusLp0QTSjaliq2BH/15L+kc1H3r+re/WWt2SjqKMMJnMI5+HANTbiDFrSBgoIneIFXB51n5815X46WnGLnGH7B+fgGdouRVw==</latexit>
F
<latexit sha1_base64="1XoURWGEIR7YcCUC7jht5NjlzGU=">AAAB8nicbVDLSsNAFL2pr1pfVZdugkVwVRIR7bIgiMsK9gFtKJPppB06mQkzN0IJ/Qw3LhRx69e482+ctFlo9cDA4Zx7mXNPmAhu0PO+nNLa+sbmVnm7srO7t39QPTzqGJVqytpUCaV7ITFMcMnayFGwXqIZiUPBuuH0Jve7j0wbruQDzhIWxGQsecQpQSv1BzHBCSUiu50PqzWv7i3g/iV+QWpQoDWsfg5GiqYxk0gFMabvewkGGdHIqWDzyiA1LCF0Ssasb6kkMTNBtog8d8+sMnIjpe2T6C7UnxsZiY2ZxaGdzCOaVS8X//P6KUaNIOMySZFJuvwoSoWLys3vd0dcM4piZgmhmtusLp0QTSjaliq2BH/15L+kc1H3r+re/WWt2SjqKMMJnMI5+HANTbiDFrSBgoIneIFXB51n5815X46WnGLnGH7B+fgGdouRVw==</latexit>
F
<latexit sha1_base64="dF1FgHoDpW3xfetyeMxpjZHp4Es=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEao8FLx4r2g9oQ9lsN+3SzSbsToQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMbud+54lrI2L1iNOE+xEdKREKRtFKD/2mGJQrbtVdgKwTLycVyNEclL/6w5ilEVfIJDWm57kJ+hnVKJjks1I/NTyhbEJHvGepohE3frY4dUYurDIkYaxtKSQL9fdERiNjplFgOyOKY7PqzcX/vF6KYd3PhEpS5IotF4WpJBiT+d9kKDRnKKeWUKaFvZWwMdWUoU2nZEPwVl9eJ+2rqleruvfXlUY9j6MIZ3AOl+DBDTTgDprQAgYjeIZXeHOk8+K8Ox/L1oKTz5zCHzifPx3Bjag=</latexit>
⇧
<latexit sha1_base64="lnaxowBWkX0s68oQ5UUcNobJ0QE=">AAAB8nicbVBNS8NAFHypX7V+VT16CRbBU0lEtMdCLx4r2FpoQ9lsN+3SzW7YfRFK6M/w4kERr/4ab/4bN20O2jqwMMy8x86bMBHcoOd9O6WNza3tnfJuZW//4PCoenzSNSrVlHWoEkr3QmKY4JJ1kKNgvUQzEoeCPYbTVu4/PjFtuJIPOEtYEJOx5BGnBK3UH8QEJ5SIrDUfVmte3VvAXSd+QWpQoD2sfg1GiqYxk0gFMabvewkGGdHIqWDzyiA1LCF0Ssasb6kkMTNBtog8dy+sMnIjpe2T6C7U3xsZiY2ZxaGdzCOaVS8X//P6KUaNIOMySZFJuvwoSoWLys3vd0dcM4piZgmhmtusLp0QTSjaliq2BH/15HXSvar7N3Xv/rrWbBR1lOEMzuESfLiFJtxBGzpAQcEzvMKbg86L8+58LEdLTrFzCn/gfP4AcfyRVA==</latexit>
C
<latexit sha1_base64="4jOkOJhK3VgTqmE3vHGIHegEbRk=">AAAB+3icbVBNT8JAEJ3iF+JXxaOXjcQEL6TloBwxXjxiIh8JNGS73cKGbbfZ3aoN4a948aAxXv0j3vw3LtCDgi+Z5OW9mczM8xPOlHacb6uwsbm1vVPcLe3tHxwe2cfljhKpJLRNBBey52NFOYtpWzPNaS+RFEc+p11/cjP3uw9UKibie50l1IvwKGYhI1gbaWiXq/4FukbjzJcsQI9C8mBoV5yaswBaJ25OKpCjNbS/BoEgaURjTThWqu86ifamWGpGOJ2VBqmiCSYTPKJ9Q2McUeVNF7fP0LlRAhQKaSrWaKH+npjiSKks8k1nhPVYrXpz8T+vn+qw4U1ZnKSaxmS5KEw50gLNg0ABk5RonhmCiWTmVkTGWGKiTVwlE4K7+vI66dRr7mXNuatXmo08jiKcwhlUwYUraMIttKANBJ7gGV7hzZpZL9a79bFsLVj5zAn8gfX5A1yPk1Y=</latexit>
(b) A hybrid world
<latexit sha1_base64="IX/2tGcO95kFTqSvAO7hpXxvJ8s=">AAAB9HicbVBNTwIxFHyLX4hfqEcvjcTEE9k1RjmSePGIiYARNuRtKdDQ7a5tl4Rs+B1ePGiMV3+MN/+NXdiDgpM0mcy8lzedIBZcG9f9dgpr6xubW8Xt0s7u3v5B+fCopaNEUdakkYjUQ4CaCS5Z03Aj2EOsGIaBYO1gfJP57QlTmkfy3kxj5oc4lHzAKRor+d0QzYiiSB9nPa9XrrhVdw6ySrycVCBHo1f+6vYjmoRMGipQ647nxsZPURlOBZuVuolmMdIxDlnHUokh0346Dz0jZ1bpk0Gk7JOGzNXfGymGWk/DwE5mIfWyl4n/eZ3EDGp+ymWcGCbp4tAgEcREJGuA9Lli1IipJUgVt1kJHaFCamxPJVuCt/zlVdK6qHpXVffuslKv5XUU4QRO4Rw8uIY63EIDmkDhCZ7hFd6cifPivDsfi9GCk+8cwx84nz/Cn5IP</latexit>
Z 1
<latexit sha1_base64="pEr6Ac/XVRcILKWZkdewbQiyWdg=">AAAB9HicbVDLSgMxFL2pr1pfVZdugkVwVWaKaJcFNy4r2Ae2Q8mkmTY0kxmTTKEM/Q43LhRx68e482/MtLPQ1gOBwzn3ck+OHwuujeN8o8LG5tb2TnG3tLd/cHhUPj5p6yhRlLVoJCLV9YlmgkvWMtwI1o0VI6EvWMef3GZ+Z8qU5pF8MLOYeSEZSR5wSoyVvH5IzJgSkT7OB7VBueJUnQXwOnFzUoEczUH5qz+MaBIyaaggWvdcJzZeSpThVLB5qZ9oFhM6ISPWs1SSkGkvXYSe4wurDHEQKfukwQv190ZKQq1noW8ns5B61cvE/7xeYoK6l3IZJ4ZJujwUJAKbCGcN4CFXjBoxs4RQxW1WTMdEEWpsTyVbgrv65XXSrlXd66pzf1Vp1PM6inAG53AJLtxAA+6gCS2g8ATP8ApvaIpe0Dv6WI4WUL5zCn+APn8AxCOSEA==</latexit>
Z 2
<latexit sha1_base64="uCciaLX7k3BHqT1uoMHnBT0ZZOU=">AAACAnicbVBNS8NAEN3Ur1q/op7ES7AInkoioj0WvHisaD+gDWGz3bRLd5OwOymWELz4V7x4UMSrv8Kb/8ZNm4O2Phh4vDfDzDw/5kyBbX8bpZXVtfWN8mZla3tnd8/cP2irKJGEtkjEI9n1saKchbQFDDjtxpJi4XPa8cfXud+ZUKlYFN7DNKauwMOQBYxg0JJnHvUFhhHBPL3LvD7QB5AiBckmmWdW7Zo9g7VMnIJUUYGmZ371BxFJBA2BcKxUz7FjcFMsgRFOs0o/UTTGZIyHtKdpiAVVbjp7IbNOtTKwgkjqCsGaqb8nUiyUmgpfd+YHq0UvF//zegkEdTdlYZwADcl8UZBwCyIrz8MaMEkJ8KkmmEimb7XICEtMQKdW0SE4iy8vk/Z5zbms2bcX1Ua9iKOMjtEJOkMOukINdIOaqIUIekTP6BW9GU/Gi/FufMxbS0Yxc4j+wPj8AbIZmD0=</latexit>
S triv
<latexit sha1_base64="uCciaLX7k3BHqT1uoMHnBT0ZZOU=">AAACAnicbVBNS8NAEN3Ur1q/op7ES7AInkoioj0WvHisaD+gDWGz3bRLd5OwOymWELz4V7x4UMSrv8Kb/8ZNm4O2Phh4vDfDzDw/5kyBbX8bpZXVtfWN8mZla3tnd8/cP2irKJGEtkjEI9n1saKchbQFDDjtxpJi4XPa8cfXud+ZUKlYFN7DNKauwMOQBYxg0JJnHvUFhhHBPL3LvD7QB5AiBckmmWdW7Zo9g7VMnIJUUYGmZ371BxFJBA2BcKxUz7FjcFMsgRFOs0o/UTTGZIyHtKdpiAVVbjp7IbNOtTKwgkjqCsGaqb8nUiyUmgpfd+YHq0UvF//zegkEdTdlYZwADcl8UZBwCyIrz8MaMEkJ8KkmmEimb7XICEtMQKdW0SE4iy8vk/Z5zbms2bcX1Ua9iKOMjtEJOkMOukINdIOaqIUIekTP6BW9GU/Gi/FufMxbS0Yxc4j+wPj8AbIZmD0=</latexit>
S triv
<latexit sha1_base64="uCciaLX7k3BHqT1uoMHnBT0ZZOU=">AAACAnicbVBNS8NAEN3Ur1q/op7ES7AInkoioj0WvHisaD+gDWGz3bRLd5OwOymWELz4V7x4UMSrv8Kb/8ZNm4O2Phh4vDfDzDw/5kyBbX8bpZXVtfWN8mZla3tnd8/cP2irKJGEtkjEI9n1saKchbQFDDjtxpJi4XPa8cfXud+ZUKlYFN7DNKauwMOQBYxg0JJnHvUFhhHBPL3LvD7QB5AiBckmmWdW7Zo9g7VMnIJUUYGmZ371BxFJBA2BcKxUz7FjcFMsgRFOs0o/UTTGZIyHtKdpiAVVbjp7IbNOtTKwgkjqCsGaqb8nUiyUmgpfd+YHq0UvF//zegkEdTdlYZwADcl8UZBwCyIrz8MaMEkJ8KkmmEimb7XICEtMQKdW0SE4iy8vk/Z5zbms2bcX1Ua9iKOMjtEJOkMOukINdIOaqIUIekTP6BW9GU/Gi/FufMxbS0Yxc4j+wPj8AbIZmD0=</latexit>
S triv
Figure 23.14: Diagram for proof of Theorem 23.4
1050
P 1
P 2
CP 1 P 2
C
(a) Protocol ⇧
P 1 P 2C
F
(b) Protocol ⇧ F ⇤
⇤⇤⇤⇤
Figure 23.15: A protocol Π and a hybrid protocol Π F∗
To carry out the proof, we can proceed in a modular fashion. We ﬁrst consider the hybrid
protocol ΠF∗ that uses the ideal functionalityF∗as a subprotocol in place of Π∗. Typically, ΠF∗ is
much simpler and easier to analyze than Π itself, as we have stripped out all of the implementation
details of Π ∗. Now suppose we prove that the hybrid protocol Π F∗ securely implements F. Then
it turns out that this implies that Π securely implements F.
See Fig. 23.15 for an illustration. Fig. 23.15(a) shows a protocol Π that uses a protocol Π ∗ as
a subprotocol. Here, we are assuming there are just two parties, and this ﬁgure does not show the
adversary or the environment. The machines labeled P1 and P2, which implement the “top part”
of Π (i.e., the part that does not include the subprotocol Π ∗), are peers communicating through a
secure communication network C. The machines labeled P∗
1 and P∗
2 , which implement Π∗, are peers
communicating through a separate communication network C∗. Both machines P1 and P∗
1 belong
to the ﬁrst party. It is best to think of them as two diﬀerent programs executing on the same
computer, so P1 may supply inputs directly to P∗
1 and obtain outputs directly from P∗
1 . Similarly,
both machines P2 and P∗
2 belong to the second party.
Fig. 23.15(b) shows the hybrid protocol ΠF∗. It is called a hybrid protocol as it has elements of
both the real world ( P1, P2, and C) and an ideal world (the ideal functionality F∗). In this hybrid
protocol, instead of P1 supplying an input to P∗
1 , that input is supplied directly to F∗. Similarly,
instead of P1 obtaining an output from P∗
1 , that output comes directly from F∗.
The formal model for protocol execution of such a hybrid protocol is essentially the same as
that of a real-world protocol, except that now there is both a communication network Cand an
ideal functionality F∗. It should be straightforward for the reader to ﬁll in these details.
We state the following theorem:
Theorem 23.5 (subprotocol composition). Suppose the following:
• Π∗ is an eﬃcient protocol that securely implements an ideal functionality F∗,
• Π is an eﬃcient protocol that uses Π∗ as a subprotocol, and
• the hybrid protocol ΠF∗ (in which the ideal functionality F∗ replaces the subprotocol Π∗)
securely implements F.
Then Π securely implements F.
Proof sketch. We are assuming that Π ∗ securely implements F∗, which means that there is a
simulator S∗
triv (that is compatible with F∗) such that for every well-behaved environment Z, we
1051
have Exec[Π∗,A∗
triv,Z] ≈Exec[F∗,S∗
triv,Z]. Here, A∗
triv is the trivial adversary (which interacts
with Π∗).
We are also assuming that Π F∗ securely implements F, which means that for every adversary
A(that is compatible with Π F∗), there is a simulator S(that is compatible with F), such that for
every well-behaved environment Z, we have Exec[ΠF∗,A,Z] ≈Exec[F,S,Z].
The proof is essentially a hybrid argument. By a generalization of Theorem 23.3, we may assume
the adversary in the real world execution of protocol Π is the trivial adversary that simply routes
messages between the environment and other protocol machines. This is illustrated in Fig. 23.16(a).
Here, the cloud labeled Π ∗ represents the honest protocol machines running the subprotocol Π ∗,
while the cloud labeled Π top represents the honest protocol machines running the “top part” of
Π (i.e., the part that does not include the subprotocol Π ∗). The unlabeled box represents a
simple routing machine, and the box labeled A∗
triv is a the trivial adversary that interacts with
the subprotocol Π∗— the unlabeled box routes messages from Zto either Πtop or A∗
triv, based on
instance IDs that are assumed to be embedded in messages.
By virtue of the fact that Π ∗ securely implements F∗, we can replace Π ∗ by F∗, as illustrated
in Fig. 23.16(b). To formally justify this step, we deﬁne an environment Ztop that absorbs Π top
(as illustrated by the dashed box labeled Ztop in the ﬁgure). The fact that Π ∗securely implements
F∗implies that Exec[Π∗,A∗
triv,Ztop] ≈Exec[F∗,S∗
triv,Ztop], which implies that the environment Z
cannot distinguish between the real world in Fig. 23.16(a) and the hybrid world in Fig. 23.16(b).
By virtue of the fact that ΠF∗ securely implements F, we can replace ΠF∗ by F, as illustrated in
Fig. 23.16(c). To formally justify this step, we deﬁne an adversaryAthat comprises both the router
and S∗
triv (as illustrated by the dashed box labeled Ain the ﬁgure). The fact that Π F∗ securely
implements F implies that Exec[Π F∗,A,Z] ≈Exec[F,S,Z], which implies that environment Z
cannot distinguish between the hybrid world in Fig. 23.16(b) and the ideal world in Fig. 23.16(c).
2
We shall illustrate the use of Theorem 23.5 below in Section 23.5.10.
The subprotocol composition theorem holds in a more general sense. Speciﬁcally, a single
instance of protocol Π may invoke many instances of subprotocol Π ∗, which may themselves run
concurrently with another. Similarly, we can replace each of those instances of subprotocol Π∗by an
instance of the ideal functionality F∗to obtain a corresponding hybrid protocol ΠF∗. Theorem 23.5
holds in this more general sense as well.
Remark 23.4 (On the utility of a simulator that works in all environments). The
techniques used in the proofs of Theorems 23.4 and 23.5 highlight the usefulness of separating the
adversary and the environment in deﬁning protocol security to say that there must be a simulator
that works for all environments. In the proofs of these composition theorems, this allows us to
construct environments that represent some elements of the system, while replacing other elements
of the system by idealized elements. 2
23.5.5.3 Hybrid protocols and transitivity of secure implementation
We introduced in Section 23.5.5.2 the notion of ahybrid protocol, which consists of a combination of
ordinary protocol machines (which execute the code of the honest parties) and ideal functionalities.
A real protocol is a special kind of hybrid protocol, which only makes use of the ideal functionality
Crepresenting a secure communication network. Similarly, an ideal protocol is also a special kind
1052
Z
<latexit sha1_base64="lnaxowBWkX0s68oQ5UUcNobJ0QE=">AAAB8nicbVBNS8NAFHypX7V+VT16CRbBU0lEtMdCLx4r2FpoQ9lsN+3SzW7YfRFK6M/w4kERr/4ab/4bN20O2jqwMMy8x86bMBHcoOd9O6WNza3tnfJuZW//4PCoenzSNSrVlHWoEkr3QmKY4JJ1kKNgvUQzEoeCPYbTVu4/PjFtuJIPOEtYEJOx5BGnBK3UH8QEJ5SIrDUfVmte3VvAXSd+QWpQoD2sfg1GiqYxk0gFMabvewkGGdHIqWDzyiA1LCF0Ssasb6kkMTNBtog8dy+sMnIjpe2T6C7U3xsZiY2ZxaGdzCOaVS8X//P6KUaNIOMySZFJuvwoSoWLys3vd0dcM4piZgmhmtusLp0QTSjaliq2BH/15HXSvar7N3Xv/rrWbBR1lOEMzuESfLiFJtxBGzpAQcEzvMKbg86L8+58LEdLTrFzCn/gfP4AcfyRVA==</latexit>
C
<latexit sha1_base64="1XoURWGEIR7YcCUC7jht5NjlzGU=">AAAB8nicbVDLSsNAFL2pr1pfVZdugkVwVRIR7bIgiMsK9gFtKJPppB06mQkzN0IJ/Qw3LhRx69e482+ctFlo9cDA4Zx7mXNPmAhu0PO+nNLa+sbmVnm7srO7t39QPTzqGJVqytpUCaV7ITFMcMnayFGwXqIZiUPBuuH0Jve7j0wbruQDzhIWxGQsecQpQSv1BzHBCSUiu50PqzWv7i3g/iV+QWpQoDWsfg5GiqYxk0gFMabvewkGGdHIqWDzyiA1LCF0Ssasb6kkMTNBtog8d8+sMnIjpe2T6C7UnxsZiY2ZxaGdzCOaVS8X//P6KUaNIOMySZFJuvwoSoWLys3vd0dcM4piZgmhmtusLp0QTSjaliq2BH/15L+kc1H3r+re/WWt2SjqKMMJnMI5+HANTbiDFrSBgoIneIFXB51n5815X46WnGLnGH7B+fgGdouRVw==</latexit>
F
<latexit sha1_base64="kDA8vZU0W/wziCXptn+rti1oLZI=">AAAB+XicbVBNS8NAEN3Ur1q/oh69LBbBU0lEtMeCF48V7Ac0IWy223bpbjbsTool9J948aCIV/+JN/+N2zYHbX0w8Hhvhpl5cSq4Ac/7dkobm1vbO+Xdyt7+weGRe3zSNirTlLWoEkp3Y2KY4AlrAQfBuqlmRMaCdeLx3dzvTJg2XCWPME1ZKMkw4QNOCVgpct2gyaMA2BNomYNKZ5Fb9WreAnid+AWpogLNyP0K+opmkiVABTGm53sphDnRwKlgs0qQGZYSOiZD1rM0IZKZMF9cPsMXVunjgdK2EsAL9fdETqQxUxnbTklgZFa9ufif18tgUA9znqQZsIQuFw0ygUHheQy4zzWjIKaWEKq5vRXTEdGEgg2rYkPwV19eJ+2rmn9T8x6uq416EUcZnaFzdIl8dIsa6B41UQtRNEHP6BW9Obnz4rw7H8vWklPMnKI/cD5/ADsjlAU=</latexit>
⇧ top
<latexit sha1_base64="ntoTd/NpUmLERg4DxQMTsGp35KM=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LBZBPJRERHssePFYwbSFNpbNdtMu3WzC7kQopb/BiwdFvPqDvPlv3LY5aOuDgcd7M8zMC1MpDLrut1NYW9/Y3Cpul3Z29/YPyodHTZNkmnGfJTLR7ZAaLoXiPgqUvJ1qTuNQ8lY4up35rSeujUjUA45THsR0oEQkGEUr+d2GeLzolStu1Z2DrBIvJxXI0eiVv7r9hGUxV8gkNabjuSkGE6pRMMmnpW5meErZiA54x1JFY26CyfzYKTmzSp9EibalkMzV3xMTGhszjkPbGVMcmmVvJv7ndTKMasFEqDRDrthiUZRJggmZfU76QnOGcmwJZVrYWwkbUk0Z2nxKNgRv+eVV0rysetdV9/6qUq/lcRThBE7hHDy4gTrcQQN8YCDgGV7hzVHOi/PufCxaC04+cwx/4Hz+ADhKjkQ=</latexit>
⇧
⇤
<latexit sha1_base64="YqFfy5zBGDp/OHHEQK4U4LKDTNs=">AAAB9HicbVDLSgMxFL3xWeur6tJNsAjiosyIaJeFblxWsA9ox5JJM21oJjMmmUIZ+h1uXCji1o9x59+YaWehrQcCh3Pu5Z4cPxZcG8f5RmvrG5tb24Wd4u7e/sFh6ei4paNEUdakkYhUxyeaCS5Z03AjWCdWjIS+YG1/XM/89oQpzSP5YKYx80IylDzglBgreb2QmBElIq3PHi/7pbJTcebAq8TNSRlyNPqlr94goknIpKGCaN11ndh4KVGGU8FmxV6iWUzomAxZ11JJQqa9dB56hs+tMsBBpOyTBs/V3xspCbWehr6dzELqZS8T//O6iQmqXsplnBgm6eJQkAhsIpw1gAdcMWrE1BJCFbdZMR0RRaixPRVtCe7yl1dJ66ri3lSc++tyrZrXUYBTOIMLcOEWanAHDWgChSd4hld4QxP0gt7Rx2J0DeU7J/AH6PMHk12R8A==</latexit>
C
⇤
<latexit sha1_base64="brE2uj+C2LPGFgBJyL/OSsdw5Wo=">AAACBHicbVDLSsNAFJ3UV62vqMtugkUQFyUR0S4rblxWsA9oYphMJ+3QyYOZm2IJWbjxV9y4UMStH+HOv3HSZqGtBy4czrmXe+/xYs4kmOa3VlpZXVvfKG9WtrZ3dvf0/YOOjBJBaJtEPBI9D0vKWUjbwIDTXiwoDjxOu974Ove7Eyoki8I7mMbUCfAwZD4jGJTk6lU7wDAimKdX2f2pawN9ABGkINgkc/WaWTdnMJaJVZAaKtBy9S97EJEkoCEQjqXsW2YMTooFMMJpVrETSWNMxnhI+4qGOKDSSWdPZMaxUgaGHwlVIRgz9fdEigMpp4GnOvOT5aKXi/95/QT8hpOyME6AhmS+yE+4AZGRJ2IMmKAE+FQRTARTtxpkhAUmoHKrqBCsxZeXSeesbl3UzdvzWrNRxFFGVXSETpCFLlET3aAWaiOCHtEzekVv2pP2or1rH/PWklbMHKI/0D5/AL8GmMc=</latexit>
A
⇤
triv
Z
<latexit sha1_base64="lnaxowBWkX0s68oQ5UUcNobJ0QE=">AAAB8nicbVBNS8NAFHypX7V+VT16CRbBU0lEtMdCLx4r2FpoQ9lsN+3SzW7YfRFK6M/w4kERr/4ab/4bN20O2jqwMMy8x86bMBHcoOd9O6WNza3tnfJuZW//4PCoenzSNSrVlHWoEkr3QmKY4JJ1kKNgvUQzEoeCPYbTVu4/PjFtuJIPOEtYEJOx5BGnBK3UH8QEJ5SIrDUfVmte3VvAXSd+QWpQoD2sfg1GiqYxk0gFMabvewkGGdHIqWDzyiA1LCF0Ssasb6kkMTNBtog8dy+sMnIjpe2T6C7U3xsZiY2ZxaGdzCOaVS8X//P6KUaNIOMySZFJuvwoSoWLys3vd0dcM4piZgmhmtusLp0QTSjaliq2BH/15HXSvar7N3Xv/rrWbBR1lOEMzuESfLiFJtxBGzpAQcEzvMKbg86L8+58LEdLTrFzCn/gfP4AcfyRVA==</latexit>
C
<latexit sha1_base64="kDA8vZU0W/wziCXptn+rti1oLZI=">AAAB+XicbVBNS8NAEN3Ur1q/oh69LBbBU0lEtMeCF48V7Ac0IWy223bpbjbsTool9J948aCIV/+JN/+N2zYHbX0w8Hhvhpl5cSq4Ac/7dkobm1vbO+Xdyt7+weGRe3zSNirTlLWoEkp3Y2KY4AlrAQfBuqlmRMaCdeLx3dzvTJg2XCWPME1ZKMkw4QNOCVgpct2gyaMA2BNomYNKZ5Fb9WreAnid+AWpogLNyP0K+opmkiVABTGm53sphDnRwKlgs0qQGZYSOiZD1rM0IZKZMF9cPsMXVunjgdK2EsAL9fdETqQxUxnbTklgZFa9ufif18tgUA9znqQZsIQuFw0ygUHheQy4zzWjIKaWEKq5vRXTEdGEgg2rYkPwV19eJ+2rmn9T8x6uq416EUcZnaFzdIl8dIsa6B41UQtRNEHP6BW9Obnz4rw7H8vWklPMnKI/cD5/ADsjlAU=</latexit>
⇧ top
<latexit sha1_base64="Vw7lM/2tqRx8/HgjstoH2xkN9vY=">AAAB9HicbVDLSgMxFL3js9ZX1aWbYBHERZkR0S4LgrisYB/QjiWTZtrQTDImmUIZ+h1uXCji1o9x59+YaWehrQcCh3Pu5Z6cIOZMG9f9dlZW19Y3Ngtbxe2d3b390sFhU8tEEdogkkvVDrCmnAnaMMxw2o4VxVHAaSsY3WR+a0yVZlI8mElM/QgPBAsZwcZKfjfCZkgwT2+nj+e9UtmtuDOgZeLlpAw56r3SV7cvSRJRYQjHWnc8NzZ+ipVhhNNpsZtoGmMywgPasVTgiGo/nYWeolOr9FEolX3CoJn6eyPFkdaTKLCTWUi96GXif14nMWHVT5mIE0MFmR8KE46MRFkDqM8UJYZPLMFEMZsVkSFWmBjbU9GW4C1+eZk0LyreVcW9vyzXqnkdBTiGEzgDD66hBndQhwYQeIJneIU3Z+y8OO/Ox3x0xcl3juAPnM8fl/KR8w==</latexit>
F
⇤
<latexit sha1_base64="jGBr9UghEgYlTE3kbmr1C7r+gf0=">AAAB+3icbVC7TsMwFL0pr1JeoYwsFhVSWaqkA3SsxMJYpL6kNqocx2mtOk5kO0AV9VdYGECIlR9h429w2wzQciRLR+fco3t9/IQzpR3n2ypsbe/s7hX3SweHR8cn9mm5q+JUEtohMY9l38eKciZoRzPNaT+RFEc+pz1/ervwew9UKhaLtp4l1IvwWLCQEayNNLLLVXyF2hOKTIijx1jyYGRXnJqzBNokbk4qkKM1sr+GQUzSiApNOFZq4DqJ9jIsNSOczkvDVNEEkyke04GhAkdUedny9jm6NEqAwliaJzRaqr8TGY6UmkW+mYywnqh1byH+5w1SHTa8jIkk1VSQ1aIw5UjHaFEECpikRPOZIZhIZm5FZIIlJtrUVTIluOtf3iTdes29rjn39UqzkddRhHO4gCq4cANNuIMWdIDAEzzDK7xZc+vFerc+VqMFK8+cwR9Ynz9e35NX</latexit>
(a) The real world
<latexit sha1_base64="4jOkOJhK3VgTqmE3vHGIHegEbRk=">AAAB+3icbVBNT8JAEJ3iF+JXxaOXjcQEL6TloBwxXjxiIh8JNGS73cKGbbfZ3aoN4a948aAxXv0j3vw3LtCDgi+Z5OW9mczM8xPOlHacb6uwsbm1vVPcLe3tHxwe2cfljhKpJLRNBBey52NFOYtpWzPNaS+RFEc+p11/cjP3uw9UKibie50l1IvwKGYhI1gbaWiXq/4FukbjzJcsQI9C8mBoV5yaswBaJ25OKpCjNbS/BoEgaURjTThWqu86ifamWGpGOJ2VBqmiCSYTPKJ9Q2McUeVNF7fP0LlRAhQKaSrWaKH+npjiSKks8k1nhPVYrXpz8T+vn+qw4U1ZnKSaxmS5KEw50gLNg0ABk5RonhmCiWTmVkTGWGKiTVwlE4K7+vI66dRr7mXNuatXmo08jiKcwhlUwYUraMIttKANBJ7gGV7hzZpZL9a79bFsLVj5zAn8gfX5A1yPk1Y=</latexit>
(b) A hybrid world
(c) The ideal world
<latexit sha1_base64="SToktXskuYQtu57u20ckmuAydkQ=">AAACAXicbVBNS8NAEN34WetX1IvgJVgETyUR0R4LXjxWsB/YhLDZbtqlu0nYnYglxIt/xYsHRbz6L7z5b9y0OWjrg4HHezPMzAsSzhTY9rextLyyurZe2ahubm3v7Jp7+x0Vp5LQNol5LHsBVpSziLaBAae9RFIsAk67wfiq8Lv3VCoWR7cwSagn8DBiISMYtOSbh67AMCKYZ3e57wJ9ACkyiJPcN2t23Z7CWiROSWqoRMs3v9xBTFJBIyAcK9V37AS8DEtghNO86qaKJpiM8ZD2NY2woMrLph/k1olWBlYYS10RWFP190SGhVITEejO4l417xXif14/hbDhZSxKUqARmS0KU25BbBVxWAMmKQE+0QQTyfStFhlhiQno0Ko6BGf+5UXSOas7F3X75rzWbJRxVNAROkanyEGXqImuUQu1EUGP6Bm9ojfjyXgx3o2PWeuSUc4coD8wPn8A4PCXyA==</latexit>
Z top
Z
<latexit sha1_base64="asoQ/HogIfRDT4/Wl4KTQVcQPeQ=">AAAB8nicbVDLSsNAFL2pr1pfVZdugkVwVRIR7bLixmUF+4A2lMl00g6dzISZG6GEfoYbF4q49Wvc+TdO2iy0emDgcM69zLknTAQ36HlfTmltfWNzq7xd2dnd2z+oHh51jEo1ZW2qhNK9kBgmuGRt5ChYL9GMxKFg3XB6m/vdR6YNV/IBZwkLYjKWPOKUoJX6g5jghBKR3cyH1ZpX9xZw/xK/IDUo0BpWPwcjRdOYSaSCGNP3vQSDjGjkVLB5ZZAalhA6JWPWt1SSmJkgW0Seu2dWGbmR0vZJdBfqz42MxMbM4tBO5hHNqpeL/3n9FKNGkHGZpMgkXX4UpcJF5eb3uyOuGUUxs4RQzW1Wl06IJhRtSxVbgr968l/Suaj7V3Xv/rLWbBR1lOEETuEcfLiGJtxBC9pAQcETvMCrg86z8+a8L0dLTrFzDL/gfHwDbvKRUg==</latexit>
A
<latexit sha1_base64="0wv+Rjb2To8Pu292t2Qiroxx268=">AAAB8nicbVDLSsNAFL2pr1pfVZdugkVwVRIR7bLgxmVF+4A2lMl00g6dzISZG6GEfoYbF4q49Wvc+TdO2iy0emDgcM69zLknTAQ36HlfTmltfWNzq7xd2dnd2z+oHh51jEo1ZW2qhNK9kBgmuGRt5ChYL9GMxKFg3XB6k/vdR6YNV/IBZwkLYjKWPOKUoJX6g5jghBKR3c+H1ZpX9xZw/xK/IDUo0BpWPwcjRdOYSaSCGNP3vQSDjGjkVLB5ZZAalhA6JWPWt1SSmJkgW0Seu2dWGbmR0vZJdBfqz42MxMbM4tBO5hHNqpeL/3n9FKNGkHGZpMgkXX4UpcJF5eb3uyOuGUUxs4RQzW1Wl06IJhRtSxVbgr968l/Suaj7V3Xv7rLWbBR1lOEETuEcfLiGJtxCC9pAQcETvMCrg86z8+a8L0dLTrFzDL/gfHwDikyRZA==</latexit>
S
<latexit sha1_base64="KsAdvZie79w+nivdAjK+i4y/X/Y=">AAACBHicbVDLSsNAFJ3UV62vqMtugkUQFyUR0S4LblxWtA9oYphMJ+3QyYOZm2IJWbjxV9y4UMStH+HOv3HSZqGtBy4czrmXe+/xYs4kmOa3VlpZXVvfKG9WtrZ3dvf0/YOOjBJBaJtEPBI9D0vKWUjbwIDTXiwoDjxOu974Kve7Eyoki8I7mMbUCfAwZD4jGJTk6lU7wDAimKe32f2pawN9ABGkINgkc/WaWTdnMJaJVZAaKtBy9S97EJEkoCEQjqXsW2YMTooFMMJpVrETSWNMxnhI+4qGOKDSSWdPZMaxUgaGHwlVIRgz9fdEigMpp4GnOvOT5aKXi/95/QT8hpOyME6AhmS+yE+4AZGRJ2IMmKAE+FQRTARTtxpkhAUmoHKrqBCsxZeXSeesbl3UzZvzWrNRxFFGVXSETpCFLlETXaMWaiOCHtEzekVv2pP2or1rH/PWklbMHKI/0D5/ANuAmNk=</latexit>
S
⇤
triv
Figure 23.16: Diagram for proof of Theorem 23.5
1053
of hybrid protocol, in which the protocol machines simply pass their inputs and outputs directly
between the environment and the ideal functionality. Thus, all protocols, including those in the
real and ideal worlds, can be viewed as hybrid protocols.
The notion of securely implements can be extended to arbitrary hybrid protocols. For hybrid
protocols Π and Π ′, we say Π securely implements Π′ if for every adversary A(compatible
with Π) there exists an adversary A′ (compatible with Π ′, and which may depend on A), such
that for every well-behaved environment Z, we have Exec[Π,A,Z] ≈Exec[Π′,A′,Z]. Note that in
this context, there is no advantage in making a distinction in terminology and notation between
adversaries and simulators.
We note that Theorem 23.3 on the completeness of the trivial adversary, Theorem 23.4 on
concurrent composition, as well as Theorem 23.5 on subprotocol composition easily generalize to
arbitrary hybrid protocols. We can also prove a convenient transitivity property:
Theorem 23.6 (Transitivity of secure implementation). Let Π,Π′,Π′′ be eﬃcient hybrid
protocols. If Π securely implements Π′and Π′securely implements Π′′, then Π securely implements
Π′′.
Proof sketch. The assumption that Π securely implements Π ′ means that for every adversary A
(compatible with Π) there exists an adversary A′(compatible with Π′), such that Exec[Π,A,Z] ≈
Exec[Π′,A′,Z]. The assumption that Π′securely implements Π′′means that for every adversary A′
(compatible with Π′) there exists an adversaryA′′(compatible with Π′′), such that Exec[Π,A′,Z] ≈
Exec[Π′,A′′,Z]. It immediately follows that for every adversary A(compatible with Π) there exists
an adversary A′′(compatible with Π′′), such that Exec[Π,A,Z] ≈Exec[Π′′,A′′,Z]. 2
23.5.6 Deﬁning honest-but-curious security
The most natural way to deﬁne honest-but-curious security requires us to make some modiﬁcations
to our framework for protocol execution. We call this modiﬁed framework the restricted frame-
work. Unlike our original unrestricted framework, in this restricted framework, adversaries in the
real world (and simulators in the ideal world) are inherently limited in what they are allowed to do.
Instead of using this restricted framework, it is possible to equivalently deﬁne a restricted notion
of adversary (or simulator) in the original, unrestricted framework, but we shall not do this here.
23.5.6.1 Deﬁning the restricted framework
Restricted framework: the real world. We begin by describing the restricted framework for
the real world. Here, the corrupt parties are not absorbed into the adversary as in the unrestricted
framework. Rather, they are represented by machines separate and apart from the adversary.
Moreover, just like an honest party, a corrupt party receives its input directly from the environment
and delivers its outputs directly to the environment. The corrupt parties run an “honest but leaky”
version of the protocol, which means that they follow the protocol exactly, except that they report
to the adversary
• all messages that they receive from the communication network, and
• all random bit strings that they generate (we assume that all random objects are ultimately
derived from these random bit strings).
1054
These reports are special messages that have some special rules associated with them. Specif-
ically, upon receiving a report, a valid adversary must either
• return control immediately to the reporting machine, or
• send a sequence of one or more reports to the environment, and thereafter return control to
the reporting machine.
Moreover, a valid environment must obey the following rule: upon receiving a report from the
adversary, it must either
• return control immediately to the adversary, or
• halt.
We will always restrict ourselves to valid adversaries and environments that adhere to the above
rules.
The above rules essentially mean that while the corrupt parties follow the protocol, the adversary
(and, indirectly, the environment) is allowed to continuously observe the evolution of their internal
states. However, the rules pertaining to reports ensure that the act of observing does not interrupt
the normal ﬂow of control of the protocol.
Note that the behavior of the communication networkCis exactly the same as in our unrestricted
framework, except that now, the adversary can only communicate with Cover the C-control wire,
and not over the C-I/O wires: only the honest and corrupt parties may communicate with Cover
the C-I/O wires. In particular, the adversary can still decide if and when messages are delivered
to any party (honest or corrupt); however, it cannot learn or alter the content of a message sent
between two honest parties, nor can it alter the content of any message sent from a corrupt party
to any other party. Of course, because of the “leakiness” of the corrupt parties, the adversary will
learn the content of messages sent to corrupt parties, and will (in principle) also know the content
of messages sent by corrupt parties.
Just as in the unrestricted framework, one can deﬁne the trivial adversary Atriv, which essen-
tially just does the bidding of the environment. A diﬀerence to keep in mind, however, is that in
the restricted framework, in addition to the trivial adversary Atriv, we also have protocol machines
that execute the protocol on behalf of the corrupt parties. These protocol machines will report
information to Atriv, which will relay these reports to the environment.
Restricted framework: the ideal world. In the restricted framework for the ideal world, all
of the F-I/O wires connect the environment Zdirectly to the ideal functionality F. This means
that the simulator Scannot alter the inputs to or outputs from the corrupt parties. We also allow
Sto deliver reports to Z.
With these restrictions, not only is it impossible for S to alter inputs to or outputs from
corrupt parties, Scannot even see these values. This will unfortnately make it quite diﬃcult, if
not impossible, to prove the security of just about any protocol. To rectify this situation, we have
to provide Swith some mechanism for seeing the inputs to or outputs from the corrupt parties.
One could build such a mechanism into the framework, but it is easier to just assume that Fitself
provides such a mechanism. For example, when working with the Fsfe functionality for secure
function evaluation (as in Section 23.5.3), we can simply augment Fsfe with an interface that allows
Sto query the inputs to or outputs from corrupt parties (provided these inputs or outputs are
available).
1055
23.5.6.2 Security deﬁnitions and consequences
Deﬁnitions 23.5 and 23.5 carry over directly to the restricted framework. For the analog of of
Deﬁnition 23.5, we say that Π semi-securely implements Fagainst A, while for the analog of
Deﬁnition 23.6, we say that or Π semi-securely implements F.
We note that Theorem 23.3 on the completeness of the trivial adversary, Theorem 23.4 on
concurrent composition, Theorem 23.5 on subprotocol composition, as well as Theorem 23.6 on the
transitivity property, also carry over directly to the restricted framework, essentially just replacing
“securely implements” by “semi-securely implements” throughout.
23.5.7 A warmup honest-but-curious security proof: a simple OT protocol
Recall that in a 1-out-of-2 oblivious transfer (OT) protocol, one party, called the sender and denoted
Ps, has two inputs m0,m1, while the other party, called the receiver and denoted Pr, has an input
σ ∈{0,1}. Roughly speaking, at the end of the protocol, Pr should obtain mσ, while Ps obtains
nothing.
The ideal functionality for OT is just a special case of Fsfe, where
• the function computed is f(m0,m1,σ) := mσ,
• the inputs m0,m1 are supplied by Ps,
• the input σ is supplied by Pr, and
• the output mσ is only obtained by Pr.
We shall denote by Fot this speciﬁc ideal functionality. As discussed above in Section 23.5.6, in the
honest-but-curious setting, we assume Fot is augmented with an interface that allows the simulator
in the ideal world to retrieve inputs to and outputs from corrupt parties.
We shall present a simple protocol for 1-out-of-2 OT, and give a fairly complete proof that
it is secure in the honest-but-curious security model. The protocol is a variant of the protocol in
Section 11.6. Instead of being based on the hashed ElGamal scheme, it is based on the multiplicative
ElGamal scheme discussed in Exercise 11.5, and its security is based on the DDH assumption.
Let G be a group of prime order q generated by g∈G. We will require that Ps’s inputs m0,m1
are (or can be encoded as) elements of G.
The OT protocol Π is presented in Fig. 23.17, and in more detail, it runs as follows:
• Inputs:
– Ps takes as input m0,m1 ∈G;
– Pr takes as input σ∈{0,1}.
• Ps’s ﬁrst move:
– Ps computes d←R G and sends d to Pr.
• Pr’s move:
1056
Ps : input (m0,m1) ∈G ×G Pr : input σ∈{0,1}
d←R G d−−−−−−−−−−−−−−−−→α←R Zq
uσ ←gα, u1−σ ←d/gα
β ←R Zq, v←gβ
c0 ←uβ
0 m0, c1 ←uβ
1 m1
(u0,u1)←−−−−−−−−−−−−−−−−
(v,c0,c1)−−−−−−−−−−−−−−−−→output cσ/vα
Figure 23.17: A simple DDH-based OT protocol
– After receiving d from Ps, party Pr computes
α←R Zq, u σ ←gα, u 1−σ ←d/gα,
and sends (u0,u1) to Ps.
• Ps’s second move:
– After receiving (u0,u1) from Pr, party Ps computes
β ←R Zq, v ←gβ, c 0 ←uβ
0 m0, c 1 ←uβ
1 m1
and sends (v,c0,c1) to Pr.
• Pr’s output stage:
– After receiving (v,c0,c1) from Ps, party Pr outputs cσ/vα.
Assuming both parties follow the protocol, we see that
cσ = vα ·mσ and c1−σ = dβ/vα ·m1−σ.
So it is clear that Pr outputs mσ.
23.5.7.1 Security: intuition
We ﬁrst give some intuition as to why the protocol is secure in the honest-but-curious model,
assuming that the encryption scheme is semantically secure.
We ﬁrst want to argue intuitively that Ps learns nothing about Pr’s input σ. Observe that
(u0,u1) is a pair of random group elements that multiply out to d. The distribution of ( u0,u1) is
the same regardless of whether σ = 0 or σ = 1. This implies that Ps learns nothing about Pr’s
input σ.
We next want to argue intuitively that whilePr learns Ps’s input mσ, it does not learn anything
about Ps’s other input m1−σ. For this, we will need the DDH assumption for G. Now, Pr knows
1057
its own input σ, the random value α (which it generated itself), as well as its output mσ (which it
is supposed to learn). In addition, Pr receives the values d,v,c 0,c1 from Ps. The values d and v
are just random group elements, so by themselves, they reveal nothing about m1−σ. We also know
that cσ = vαmσ, so the value of cσ is completely determined by values Pr already knows, and so it
also does not reveal anything about m1−σ. Finally, we know that
c1−σ = dβ/vα ·m1−σ.
Under the DDH assumption, the triple ( d,gβ,dβ) is computationally indistinguishable from the
triple ( d,gβ,e), where e ∈G is a random group element. Thus, c1−σ is itself computationally
indistinguishable from a random group element, and hence it also does not reveal anything about
m1−σ.
23.5.7.2 Security: a formal proof
Now we want to give a more formal proof of security in the honest-but-curious setting. Speciﬁcally,
we want to prove the following:
Theorem 23.7. The above OT protocol Π semi-securely implements the ideal functionality Fot,
under the DDH assumption for G.
By the completeness of the trivial adversary, it suﬃces to build a simulator Ssuch that in the
restricted framework of Section 23.5.6, for every well-behaved environment Z, we have
Exec[Π,Atriv,Z] ≈Exec[Fot,S,Z]. (23.16)
Here, Atriv is the trivial adversary.
Review of the real world. Let us review the most important details of the real-world execu-
tion. The environment Zsupplies inputs to and receives outputs from all parties, after initially
deciding which parties are corrupt. (In this particular protocol, only Pr generates an output.) The
communication network is completely controlled by Z (indirectly, via instructions it sends over
the C-control wire via Atriv), and so it decides if and when protocol messages get delivered. As
the protocol executes, any corrupt party immediately reports to Z(again, indirectly via Atriv) the
details of any message it receives over the communication network or any random string that it
generates. Other than generating these reports, a corrupt party faithfully executes the protocol.
Recall that after receiving such a report, Zimmediately returns control to the reporting machine
(again, indirectly via Atriv).
Review of the ideal world. Now let us review the details of the ideal-world execution. There is
a simulator S. As in the real world, Zsupplies inputs to and receives outputs from all parties, after
initially deciding which parties are corrupt. Each party’s input is forwarded directly to the ideal
functionality. Once all inputs are provided to the ideal functionality, Scan choose if and when to
tell the ideal functionality to give any party’s output to Z. Moreover, at any point in time, Smay
ask the ideal functionality for the input to or output from a corrupt party, even though Scannot
modify these values.
1058
High-level proof strategy. So to prove security of protocol Π, we need to designSin such a way
that (23.16) holds for all well-behaved Z. Intuitively, this means that S, running in the ideal world,
must interact with Zin such a way that essentially makes Z“think” it is still in the real world.
In particular, whenever a real-world corrupt party would normally receive a protocol message from
a real-world honest party, it would normally be reported to Z, and so Smust somehow cook up a
simulated protocol message, and report this simulated protocol message to the environment instead.
Similarly, whenever a real-world corrupt party would normally generate a random string, Smust
also cook up a corresponding simulated random string. Smust do all of this knowing only the
corrupt parties’ inputs and outputs, but without knowing the honest parties’ inputs or outputs.
Moreover, all of this must be done so that Z, who knows all parties’ inputs and outputs, cannot
eﬀectively distinguish the ideal world execution from the real world execution.
Further assumptions about randomness. To formally prove security in the honest-but-
curious setting, the protocol itself must be very explicitly speciﬁed in terms of how random objects
are generated by all parties. This is because a corrupt party must follow the protocol exactly, in-
cluding when and how it generates random objects, and so this must actually be speciﬁed precisely.
In fact, we require that the only random objects generated are random bit strings.
For this protocol, we assume that coinss is the random string that real-world Ps uses in the
derivation of d ∈G and β ∈Zq. We assume that the distribution of ( d,β) is statistically indis-
tinguishable from the uniform distribution on G ×Zq (see Deﬁnition 3.6). We also assume that
real-world Ps generates coinss as soon as it receives its input.
Suppose that coinsr is the random string that real-world Pr uses in the derivation of α ∈Zq.
We assume that the distribution of αis statistically indistinguishable from the uniform distribution
on Zq. We also assume that real-world Pr generates coinsr as soon as it receives its input.
While we should specify the exact algorithms that are used for deriving ( d,β) from coinss and
for deriving α from coinsr, this is not necessary for the analysis of this particular protocol.
Note that for some protocols (such as the one we are analyzing here), Smay be able to just
generate the simulated random strings just as random strings; for other protocols, it may need to
be more clever (for example, see Exercise 23.3).
Remark 23.5 (Generating random numbers mod q). For deriving a nearly random element
of Zq from a random bit string, a protocol may use the following simple technique. Let k be a
parameter chosen so that the value q/2k is negligible. Then given a random bit string ρ∈{0,1}k,
we can naturally view ρ as an integer uniformly distributed over {0,..., 2k −1}, and then reduce
it mod q to get an element of Zq whose statistical distance from the uniform distribution on Zq is
less than q/2k (see Example 3.2). 2
The simulator S. Our simulator Swill work diﬀerently depending on which of the two parties
are corrupt. If neither party is corrupt or both parties are corrupt, there is not much for Sto do.
So we focus on the case where one of the two parties is corrupt.
Simulation when Ps is corrupt. Here is how Swill work assuming that Ps is corrupt.
• When Zsends an input ( m0,m1) to Ps, this is forwarded directly to Fot, which notiﬁes S
that this input has been received. At this point, Squeries Fot to obtain this input. Sthen
1059
generates the string coinss at random, and uses coinss to derive d∈G and β ∈Zq, just like
real-world Ps would do. Sthen reports coinss to Z(just like real-world Ps would do).
• At some later time,Zwill generate a control message that would cause real-worldPs to receive
the protocol message ( u0,u1) from real-world Pr. Of course, in the ideal world, Sreceives no
such protocol message, but has to cook up a simulated protocol message, as discussed above.
So it simply computes
u0 ←R G, u 1 ←d/u0,
and reports the message ( u0,u1) to Z(just like real-world Ps would do).
That completes our description of the simulator S. There are few insigniﬁcant details we have left
out, mostly related to how S processes control messages from Z related to delivery of protocol
messages to real-world Pr. The simulation is essentially perfect; in particular, (23.16) holds. This
holds without any computational assumption. The main thing to argue (as we already did above
in Section 23.5.7.1), is that regardless of the value of σ, the distribution of (u0,u1) is just a random
pair of group elements that multiply out to d.
Simulation when Pr is corrupt. Here is how Swill work assuming that Pr is corrupt.
• When Z sends an input σ to Pr, this input is forwarded directly to Fot, which notiﬁes S
that this input has been received. At this point, Squeries Fot to obtain this input. Sthen
generates the string coinsr at random, and uses coinsr to derive α∈Zq, just like real-world
Pr would do. Sthen reports coinsr to Z(just like real-world Pr would do).
• At some later time, Z will generate a control message that would cause real-world Pr to
receive the protocol message d from real-world Ps. Of course, in the ideal world, Sreceives
no such protocol message, but has to cook up a simulated message, as discussed above. So it
simply computes d←R G and reports the message d to Z(just like real-world Pr would do).
• At some later time, Z will generate a control message that would cause real-world Pr to
receive the protocol message ( v,c0,c1) from real-world Ps. Of course, in the ideal world, S
receives no such protocol message, and again, it has to cook up a simulated version of this
protocol message. Here is how it does so. At this point in time, the environment must have
already sent an input Ps, which was forwarded to Fot. Note that the values ( m0,m1) are not
directly available to S. However, Smay ask for Pr’s output mσ at this point in time, and
then compute
v←R G, c σ ←vαmσ, c 1−σ ←R G.
Sthen reports the simulated protocol message ( v,c0,c1) to Z(as real-world Pr would do).
When control returns to S, who then instructs Fot to deliver Pr’s output to Z.
That completes the description of the simulator S. There are a few insigniﬁcant details we have
left out, mostly related to how Sprocesses control messages from Zrelated to delivery of protocol
messages to real-world Ps. The simulation is not perfect, but one can show (using the argument
sketched above in Section 23.5.7.1) that (23.16) holds under the DDH assumption.
1060
23.5.7.3 Further remarks on this protocol
Remark 23.6. While this protocol is secure in the honest-but-curious setting, it is completely
insecure in the malicious setting. Indeed, suppose Pr is maliciously corrupt. Then Pr could simply
generate u0 and u1 so that it knows the discrete logs of both, thereby allowing it to compute both
m0 and m1. One could mitigate this attack by modifying the protocol so that Ps checks that
u0 ·u1 = d, and aborts if this is not the case. While this modiﬁcation does mitigate against this
attack, it does not yield a protocol that we can prove secure in the malicious setting. 2
Remark 23.7. In the honest-but-curious setting, we require that corrupt parties follow the protocol
exactly, and in particular, that they generate random bit strings as speciﬁed by the protocol. It
is possible to study a slightly stronger notion of security whereby the corrupt parties may use
adversarially chosen bit strings in place of random bit strings, but otherwise follow the protocol.
This protocol is secure in this stronger sense. 2
23.5.8 A warmup malicious security proof: a simple OT protocol
In Section 23.5.7, we presented a two-party protocol for 1-out-of-2 oblivious transfer (OT), and
proved that is was secure in the honest-but-curious setting. In this section, we present a 3-party
OT protocol, and prove that it is secure in the malicious setting, assuming that at most one out of
the three parties is corrupt.
In this protocol, we have a sender, Ps, which has two inputs m0,m1, a receiver, Pr, which has
an input σ∈{0,1}, and a “helper” Ph, which has no input. At the end of the protocol, Pr should
obtain mσ, while Ps and Ph obtain nothing.
We shall also assume that the message space M, from which m0,m1 are drawn, is the set of all
bit strings of some ﬁxed length.
The ideal functionality for such a protocol is essentially the same as Fot presented in Sec-
tion 23.5.7, except for the additional “helper” party Ph, which contributes no input and obtains no
output.
As in Section 23.3.7, we make use of a collision-resistant hash function
H : M×R→C ,
which is used as a non-interactive commitment scheme, as discussed in Section 8.12. Here,Ris some
suitably large space of randomizing elements, and Cis some ﬁnite output space (the “commitment”
space). Again, the randomizing elements will be used to hide input messages, meaning that for
adversarially chosen m0,m1 ∈M, and for randomly chosen r∈R,
H(m0,r) and H(m1,r)
are computationally indistinguishable.
The OT protocol Π is presented in Fig. 23.18, and in more detail, it runs as follows:
• Inputs:
– Ps takes as input m0,m1 ∈M;
– Pr takes as input σ∈{0,1}.
• Ph’s move:
1061
Ph Ps : input (m0,m1) ∈M×M Pr : input σ∈{0,1}
p0,p1 ←R M
r0,r1 ←R R
β ←R {0,1}
cβ⊕1 ←H(pβ⊕1,rβ⊕1) (β,pβ,rβ,cβ⊕1)−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−→τ ←σ⊕β
cβ ←H(pβ,rβ)
(p0,r0,p1,r1)−−−−−−−−−−→check c0 = H(p0,r0)
check c1 = H(p1,r1)
e0 ←m0 ⊕pτ
e1 ←m1 ⊕pτ⊕1
(τ,c0,c1)←−−−−−−−−−−
(e0,e1)−−−−−−−−−−→output eσ ⊕pβ
Figure 23.18: A 3-party OT protocol
– Ph computes
p0,p1 ←R M r0,r1 ←R R, β ←R {0,1}, c β⊕1 ←H(pβ⊕1,rβ⊕1),
and sends
∗ (β,pβ,rβ,cβ⊕1) to Pr, and
∗ (p0,r0,p1,r1) to Ps.
• Pr’s move:
– After receiving (β,pβ,rβ,cβ⊕1) from Ph, party Pr computes
τ ←σ⊕β, c β ←H(pβ,rβ)
and sends (τ,c0,c1) to Ps.
• Ps’s move:
– After receiving (p0,r0,p1,r1) from Ph and (τ,c0,c1) from Pr, party Ps ﬁrst checks that
c0 = H(p0,r0) and c1 = H(p1,r1).
– If this check fails, Ps aborts (and may also notify Pr that it should abort as well).
– Otherwise, Ps computes
e0 ←m0 ⊕pτ, e 1 ←m1 ⊕pτ⊕1,
and sends (e0,e1) to Pr.
1062
• Pr’s output stage:
– After receiving (e0,e1) from Ps, party Pr outputs eσ ⊕pβ.
Assuming that all parties follow the protocol, one can easily verify that
eσ = mσ ⊕pβ and eσ⊕1 = mσ⊕1 ⊕pβ⊕1. (23.17)
An intuitive description. Roughly speaking, the protocol is doing the following:
• The helper generates commitments to two random pads, p0, p1, and gives the sender openings
to both, while giving the receiver an opening to pβ, where β is a random bit, along with β
and just the commitment to the other pad.
• Next, the receiver uses β as a one-time pad to encrypt its selection bit σ, which yields τ, and
gives τ along with commitments to both p0 and p1 to the sender.
• The sender ﬁrst checks that the openings it obtained from the helper correspond to the
commitments it obtained from the sender. This is designed to catch a cheating helper who
attempts to give inconsistent information to the sender and receiver. If this check does not
pass, the sender aborts the protocol. Otherwise, it encrypts m0 using the pad pτ, which yields
e0, and m1 using the other pad, pτ⊕1, which yields e1, and gives both encryptions, e0 and e1,
to the receiver.
• Finally, the receiver decrypts eσ using the pad pβ, which, by (23.17), yields mσ.
23.5.8.1 Security: intuition
Before diving into a more formal security proof, we ﬁrst present some intuition.
The security analysis is done assuming at most one of the three parties is corrupt. We want
to argue, intuitively, that the protocol provides privacy, soundness, and input independence (see
Section 23.1.1).
Helper corrupt: By the collision resistance property ofH, if the corrupt helper sends inconsistent
information to the sender and receiver, the protocol will abort. Otherwise, if the corrupt
helper sends consistent information, the receiver will output the correct value, mσ, and the
corrupt helper will not learn anything at all about m0, m1, or σ. This follows from the
assumption that the communication channels between the sender and the receiver are secure,
and therefore the corrupt helper cannot modify or learn anything about the protocol messages
exchanged between the sender and receiver. Thus, the privacy and soundness properties are
satisﬁed. Since the corrupt helper has no input, the input independence property is trivially
satisﬁed.
Receiver corrupt: When the corrupt receiver gives the value τ to the honest sender, its selection
bit is eﬀectively determined asσ:= τ⊕β, where βis the bit chosen by the honest dealer. Since
at this point in time, the corrupt receiver has received nothing that depends in any way on
the honest sender’s inputs m0,m1, the input independence property is clearly satisﬁed. Now
consider what happens when the corrupt receiver obtains ( e0,e1) from the honest sender. By
(23.17), we see that eσ reveals mσ to the corrupt receiver. Also, by the hiding property of the
1063
hash function, the commitment cβ⊕1, which the corrupt receiver obtained from the honest
helper, reveals no information about pβ⊕1 to the corrupt receiver. Thus, by (23.17), we see
that eσ⊕1 reveals nothing about mσ⊕1 to the corrupt receiver. Thus, the privacy property is
satisﬁed. Finally, since the honest helper and sender have no outputs, the soundness property
is trivially satisﬁed.
Sender corrupt: When the corrupt sender gives ( e0,e1) to the honest receiver, its inputs m0,m1
are eﬀectively determined as m0 := e0 ⊕pτ and m1 := e1 ⊕pτ⊕1. At this point in time, the
corrupt sender has no information about the honest sender’s selection bit σ, which is perfectly
masked by β, and so the input independence property is satisﬁed. Indeed, the corrupt sender
never learns anything more about σ, and so the privacy property is also satisﬁed. By (23.17),
we see that the honest receiver decrypts eσ as mσ, and hence the soundness property is
satisﬁed.
23.5.8.2 Security: a formal proof
Now we want to give a more formal proof of security in the malicious setting. Speciﬁcally, we want
to prove the following:
Theorem 23.8. The above OT protocolΠ securely implements the ideal functionality Fot, assuming
H is collision resistant and satisﬁes the hiding property described above, and that at most one party
is corrupt.
By the completeness of the trivial adversary, it suﬃes to build a simulator Ssuch that for every
well-behaved environment Z, we have
Exec[Π,Atriv,Z] ≈Exec[Fot,S,Z]. (23.18)
Here, Atriv is the trivial adversary.
Review of the real world. Let us review the most important details of the real-world execution.
The environment Zsupplies inputs to and receives outputs from the honest parties, after initially
deciding which parties are corrupt. (In this particular protocol, only Ps and Pr receive an input
and only Pr generates an output.) The communication network is completely controlled by Z
(indirectly, via instructions it sends over the C-control wire via Atriv), and so it decides if and when
protocol messages get delivered to any honest parties. Recall that corrupt parties really do not
exist; rather, any message that a corrupt party would normally send to an honest party is actually
sent by Z(again, indirectly via Atriv), and any message that an honest party sends to a corrupt
party is actually sent to Z(again, indirectly via Atriv). Nevertheless, for simplicity, we shall talk
about “messages sent to an honest party from the corrupt party”, and “messages sent from an
honest party to the corrupt party”.
Review of the ideal world. Now let us review the details of the ideal-world execution. There
is a simulator S. As in the real world, Zsupplies inputs to and receives outputs from the honest
parties, after initially deciding which parties are corrupt. Each honest party’s input is forwarded
directly to the ideal functionality. The inputs for any corrupt parties are provided to the ideal
functionality by S. Once all inputs are provided to the ideal functionality, Scan choose if and
when to tell the ideal functionality generate outputs: an honest party’s output is forwarded directly
to Z, while a corrupt party’s output is given to S.
1064
High-level proof strategy. So to prove security of protocol Π, we need to design Sin such a
way that (23.18) holds for all well-behaved Z. Intuitively, this means that S, running in the ideal
world, should interact with Zin such a way that essentially makes Z“think” it is still in the real
world. In particular, Sshould exchange messages with Z(over the S-control wire) that look like
messages that would normally be used to control the communication network in the real world
(over the C-control wire). Thus, even in the ideal world, we can talk about “messages sent to an
honest party from a corrupt party”, whose contents are given to Sfrom Z, and “messages sent
from an honest party to a corrupt party”, whose contents must be generated by Sand given to Z.
There may also be “messages sent between honest parties”, whose contents are never given to or
generated by S; rather, Sjust exchanges control messages with Zthat correspond to the points in
time when these messages would have been sent or received in the real world.
Now, whenever a real-world honest party would normally send a protocol message to a real-
world corrupt party, S must somehow cook up a simulated protocol message to be sent to that
corrupt party (which is given to Z). In addition, based on the messages sent from the real-world
corrupt parties to the real-world honest parties, Smust somehow extract from those messages an
eﬀective input for the corrupt parties, and submit this to the ideal functionality. Smust do all this
without knowing any of the honest parties’ inputs or outputs, although it will (eventually) learn
the corrupt parties’ output. All of this must be done so that Z, who also knows all honest parties’
inputs and outputs, cannot eﬀectively distinguish the ideal world execution from the real world.
Note that one technical distinction (among several) between the malicious setting and the
honest-but-curious setting is that in the malicious setting, the simulator must extract eﬀective
inputs for corrupt parties, whereas in the honest-but-curious setting, those inputs are simply given
to the simulator.
The simulator S. Our simulator Swill work diﬀerently depending on which of the three parties
is corrupt. If no party is corrupt, there is not much for Sto do. So we focus on the case where
exactly one of the three parties is corrupt.
Simulation when Ph is corrupt. Here is how Sworks when the helper, Ph, is corrupt.
• At some point in time, Zwill generate a control message that corresponds to the instant at
which real-world Ps would have received both a message from real-world Ph and a message
from real-world Pr.
At this point in time, Shas obtained the message ( β,pβ,rβ,cβ⊕1) that real-world Ph sent to
Pr, as well as the message ( p0,r0,p1,r1) that real-world Ph sent to Ps: recall that Sobtains
the contents of all messages sent from the corrupt party to the honest parties. S checks
that these two messages are consistent, i.e., that that the values pβ,rβ sent to Pr match the
corresponding values sent to Ps, and that the value cβ⊕1 sent to Pr is indeed the hash of the
values pβ⊕1,rβ⊕1 sent to Ps.
If this check fails, Swill continue as if Ps aborted. In particular, it will never instruct Fot to
generate an output for Pr.
• If the above check passes, Swill continue as if Ps did not abort. At a later point in time, Z
will generate a control message that corresponds to the instant at which real-worldPr receives
its message from real-world Ps. At this point in time, Sinstructs Fot to generate the output
for Pr.
1065
That completes the description of the simulator S.
We now show that (23.18) holds under the collision-resistance assumption for the hash function.
To this end, let Game 0 be the real-world execution. Now deﬁne Game 1 to be the same as
Game 0, except that we make Ps abort if Ph sends inconsistent information to Ps and Ph. (Of
course, Ps could not do this in the real world, as it does not have access to all of this information;
however, that is not a problem, as this is just a thought experiment.) Observe that Games 0 and
1 proceed identically unless the values ( pβ,rβ) received by Ps and Ph diﬀer, yet hash to the same
value. This means that the probability that the environment can distinguish Game 0 from Game 1
is bounded by the probability of ﬁnding a collision for the hash function, which is negligible, by
the collision-resistance assumption.
Finally, one can verify that Game 1 and the ideal-world execution are completely equivalent
from the point of view of the environment.
Simulation when Pr is corrupt. Here is how Sworks when the receiver, Pr, is corrupt.
• At some point in time, Zwill generate a control message that corresponds to the instant at
which real-world Pr receives the message ( β,pβ,rβ,cβ⊕1) from real-world Ph. Smust “cook
up” a simulated message of this form. It does this as follows. It computes
β ←R {0,1}, p 0,p1 ←R M, r 0,r1 ←R R,
just as in the real world, as well as
c0 ←H(p0,r0), c 1 ←H(p1,r1). (23.19)
• At some later point in time, Zwill generate a control message that corresponds to the instant
at which real-world Ps would have received both a message from real-world Ph and a message
from real-world Pr.
At this point in time, Shas obtained the message ( τ,ˆc0,ˆc1) that real-world Pr sent to real-
world Ps. First, Schecks that ˆc0 = c0 and ˆc1 = c1, where c0,c1 are computed as above in
(23.19), just like Ps would have done.
If this check does not pass, Swill continue as if Ps aborted.
Otherwise, S“extracts” Pr’s eﬀective input σ:= τ ⊕β and submits this on Pr’s to Fot.
• Assuming the protocol did not abort, at some later point in time, Zwill generate a control
message that corresponds to the instant at which real-world Pr receives the message ( e0,e1)
from real-word Ps.
Smust “cook up” a simulated message of this form. It does so as follows. Note that at this
point in time, Fot must have received Ps’s inputs m0,m1, but Sis not directly privy to these
inputs. However, Scan instruct Fot to generate Pr’s output mσ, which Fot gives directly to
S. Having obtained mσ, Scomputes
eσ ←mσ ⊕pβ, e σ⊕1 ←R M.
So now Suses these two values e0,e1 for the simulated message.
1066
That completes the description of the simulator S.
We now show that (23.18) holds under the hiding assumption for the hash function.
To this end, let Game 0 be the real-world execution. Let us assume that the values c0,c1 are
computed by Ps and Ph as in (23.19), so that so that
• Ph sends the value cβ⊕1 to Pr, and
• when Ps receives a message ( τ,ˆc0,ˆc1) from Pr, it checks that ˆc0 = c0 and ˆc1 = c1.
Now deﬁne Game 1 to be the same as Game 0, except that instead of computing the values
c0,c1 as in (23.19), Ps and Ph instead compute these values as
cβ ←H(pβ,rβ), c β⊕1 ←H(p,rβ⊕1), (23.20)
where p∈M is chosen at random. Note that in this game, Ps still uses the values p0,p1 as usual
in the computation of e0,e1. (Of course, Ps could not do this in the real world, as it does not have
access to the value β; however, that is not a problem, as this is just a thought experiment.) It
is easily to see that the probability that the environment can distinguish Game 0 from Game 1 is
bounded by the probability of distinguishing H(pβ⊕1,rβ⊕1) from H(p,rβ⊕1), which is negligible,
by the hiding assumption for the hash function.
Now observe that in Game 1, the only place where pβ⊕1 is used at all is in the one-time pad
encryption of mσ⊕1, where σ:= τ ⊕β. So in Game 2, we further modify Ps so that it computes
eσ⊕1 ←R M.
One can verify that Game 1 and Game 2 are completely equivalent from the point of view of
the environment. Finally, one can also see that Game 2 is equivalent to the ideal world with our
simulator.
Simulation when Ps is corrupt. Here is how Sworks when the sender, Ps, is corrupt.
• At diﬀerent points in time, Zwill generate control messages that correspond to the instants
at which real-world Ps receives the message (p0,r0,p1,r1) from real-world Ph, and the message
(τ,c0,c1) from real-world Pr.
Smust “cook up” simulated messages of this form. It does so by computing
p0,p1 ←R M, r 0,r1 ←R R, c 0 ←H(p0,r0), c 1 ←H(p1,r1), τ ←R {0,1}.
That is, Sfollows the protocol exactly to compute the values p0,r0,c0 and p1,r1,c1, but it
just computes τ as a random bit.
• At some later point in time, Z will generate a control message that corresponds to the
instant at which real-world real-world Pr receives the message (e0,e1) from real-word Ps, and
Sobtains this message.
S“extracts” Ps’s eﬀective inputs m0,m1 by computing
m0 ←e0 ⊕pτ, m 1 ←e1 ⊕pτ⊕1.
Sthen submits these inputs on behalf of Ps to Fot. At this point in time, Fot has inputs
from both Ps and Pr, and so Sinstructs Fot to generate Ps’s output at this time, which is
forwarded directly to Z.
That completes the description of the simulator S. One can easily verify that (23.18) holds (per-
fectly).
1067
23.5.9 An example malicious security proof: Beaver’s 2.5-party protocol
We now prove that Beaver’s 2.5-party protocol in Section 23.2.4 is secure in the malicious setting.
Speciﬁcally, we prove:
Theorem 23.9. Beaver’s 2.5-party protocol in Section 23.2.4 securely implements the ideal func-
tionality Fsfe for arithmetic circuits over Zq, assuming q is super-poly, and that at most one party
is corrupt.
Proof. Let Π denote Beaver’s protocol. By the completeness of the trivial adversary, it suﬃces to
build a simulator Ssuch that for every well-behaved environment Z, we have
Exec[Π,Atriv,Z] ≈[Exec[Fsfe,S,Z].
The reader should review the general proof strategy in the proof of Theorem 23.8, which applies
equally well here (in particular, see the text labeled “review of the real world”, “review of the ideal
world”, and “high-level proof strategy”).
We now describe S. So there are four cases to consider:
1. no party corrupt;
2. P1 corrupt;
3. P2 corrupt;
4. D corrupt.
We will focus on case 2, where P1 is corrupt. Case 3, where P2 is corrupt, is essentially the same.
We leave Cases 1 and 4 to the reader.
The basic ideas of the operation of our simulator Sare as follows:
• Swill not see P2’s inputs. These inputs are chosen by the environment Zand forwarded
directly to the ideal functionality Fsfe. However, Sdoes receive a notiﬁcation from Fsfe that
an input was given.
• Smust somehow extract P1’s eﬀective inputs, and feed these to Fsfe.
• Sdoes not get to see P2’s outputs. Rather, Stells Fsfe when to forward these outputs directly
to Z.
• Smay ask Fsfe for P1’s outputs.
• Smust somehow play the roles of both P2 and D, cooking up simulated protocol messages
to send to P1 as if they were coming from P2 and D, in a way that fools Zinto thinking it
is still operating in the real world, even though Shas no idea what P2’s inputs or outputs
actually are.
Now the details.
1068
The dealer. Recall that the dealer distributes singleton sharings [ K(1)] and [ K(2)], several au-
thenticated singleton sharings JaK, and several authenticated Beaver triple sharings ( JaK,JbK,JcK),
where c= ab. Also recall that an authenticated sharing JxK is a tuple ([x],[x(1)],[x(2)]) of ordinary
sharings such that x(1) = K(1)x and x(2) = K(2)x.
For each sharing that the dealer would normally distribute, the simulator S will generate a
random share for P1, and then use the simulator S(1)
D , as described in Section 23.2.4.7, to simulate
the rest of the dealing protocol. Looking forward, an invariant that we will maintain throughout
the ideal world execution is this: S will know the values of all shares that P1 is supposed
to be holding (if it were actually following the protocol). However, Swill not know the
shares that P2 should be holding.
Simulated opening logic. At various points in the protocol, a sharing [ x] is opened to P1. As
noted above, the simulator Swill know the value of the share x1 of x that P1 is supposed to be
holding. If Salso knows the valuexto be opened, then Scan compute P2’s share x2 as x2 ←x−x1.
Therefore, if Sknows the value being opened, it can cook up the message thatP2 would be expected
to send to P1 in the real execution of the protocol. We call this simulated opening logic . As
described below, Swill make use of this logic at various points in its execution.
Reliable key opening. Swill simulate the reliable key opening subprotocol (Protocol 23.4) as
follows. First, Sgenerates the value K(1) at random. (Note that Sknows K(1), but does not
know K(2).)
• Consider reliably opening K(1) to P1. Recall that this subprotocol makes use of an authen-
ticated singleton sharing JaK = ([a],[a(1)],[a(2)]), and opens [ a], [a(1)], and [ K(1)] to P1. The
simulator Swill generate a at random, from which it can compute a(1) ←K(1)a. Since S
knows a, a(1) and K(1), it may use the simulated opening logic to open [ a], [a(1)], and [K(1)]
to P1.
• Consider reliably opening K(2) to P2. Recall that this subprotocol makes use of an authen-
ticated singleton sharing JaK = ([a],[a(1)],[a(2)]), and opens [ a], [a(2)], and [K(2)] to P2. Also
recall that Sknows the values of the shares of a, a(2), and K(2) that P1 is supposed to be
holding. The logic of Sis simply this: if the shares supplied by P1 do not match these known
shares, Smakes P2 abort (i.e., Sproceeds as if P2 aborted).
Working with authenticated sharings. Let us next discuss the low-level subprotocols for
working with authenticated sharings: open, add, multiply constant, and add constant.
Again, recall that a valid authenticated sharing JxK is a tuple ([ x],[x(1)],[x(2)]) of ordinary
sharings such that x(1) = K(1)x and x(2) = K(2)x. We are assuming that Sknows K(1), as well as
• P1’s share x1 of x,
• P1’s share x(1)
1 of x(1), and
• P1’s share x(2)
1 of x(2).
Consider the operation opening such an authenticated sharingJxK to P1. Here, P2 needs to open
[x] and [x(1)] to P1. If Salso knows the value x, then Scan also compute the value x(1) = K(1)x,
1069
and therefore the simulated opening logic can be extended to allow Sto simulate the opening
of the authenticated sharing JxK to P1.
Consider the operation opening such an authenticated sharing JxK to P2. Here, P1 opens [x] and
[x(2)] to P2. Our simulator Swill always use guarded opening logic, in which Ssimply makes P2
abort if P1 sends any wrong shares to P2 (i.e., shares of xand x(2) that do not match those already
known to S). By using this logic, Swill essentially force P1 to correctly follow the protocol.
As for the other low-level subprotocols, add, multiply constant, and add constant, these are all
just local computations. Since Sknows all of P1’s shares going into these subprotocols, and these
are just local computations, Salso knows all of P1’s shares coming out of these subprotocols.
The main subprotocols. Now let’s walk through the main subprotocols of Beaver’s protocol.
For each of these, we start be stating again the logic of the actual subprotocol, and then we describe
the logic of the simulator S.
Input wire for P1: to produce an authenticated sharing JxK of one of P1’s inputs x ∈Zq, an
authenticated singleton sharing JaK from the dealer is used as follows:
1. execute open JaK to P1;
2. P1 sends δ←x−a to P2;
3. execute JxK ←JaK + δ.
In Step 1, Schooses a at random, and uses the simulated opening logic with this value to open
JaK to P1. Now, in Step 2, P1 can send any value δ that it likes. Our simulator Swill extract
P1’s eﬀective input x:= a+ δ, and feed this input to the ideal functionality Fsfe. Step 3 is a local
computation (so Scomputes the share of x that P1 should be holding).
Input wire for P2: to produce an authenticated sharing JxK of one of P2’s inputs x ∈Zq, an
authenticated singleton sharing JaK from the dealer is used as follows:
1. execute open JaK to P2;
2. P2 sends δ←x−a to P1;
3. execute JxK ←JaK + δ.
The open in Step 1 will be done using the guarded opening logic. Now, in Step 2, Sdoes not have
access to the value of P2’s input x (this was passed directly to Fsfe). Instead, Sjust chooses the δ
at random and sends that to P1. Again, Step 3 is a local computation (so Scomputes the share of
x that P1 should be holding).
Multiplication gate: to multiply JxK and JyK, obtaining JzK = JxyK, an authenticated Beaver
triple sharing (JaK,JbK,JcK) from the dealer is used as follows:
1. execute JuK ←JxK −JaK;
2. execute JvK ←JyK −JbK;
3. execute open JuK and open JvK to both P1 and P2;
4. execute JzK ←uv+ uJbK + vJaK + JcK.
1070
Steps 1 and 2 are local computations (so Scomputes the share of both u and v that P1 should be
holding). In Step 3, Schooses the values u and v at random and uses the simulated opening logic
with these values to open JuK and JvK to P1. In addition, Suses the guarded opening logic when
P1 opens JuK and JvK to P2. Step 4 is a local computation (so Scomputes the share of z that P1
should be holding).
Output wire for P1: to give to P1 the value x of an authenticated sharing JxK, execute open JxK
to P1.
This subprotocol will only be run after all inputs have been extracted from P1 and fed to Fsfe, and
all inputs from P2 have been forwarded toFsfe. So at this point, Srequests the corresponding output
value xfrom Fsfe (by sending it an appropriate control message of the form (output,outputID,1)).
Using this output value x, Sruns the simulated opening logic.
Output wire for P2: to give to P2 the value x of an authenticated sharing JxK, execute open JxK
to P2.
Again, this subprotocol will only be run after all inputs have been extracted from P1 and fed to
Fsfe, and all inputs from P2 have been forwarded to Fsfe. The simulator Sruns the guarded opening
logic, and then sends an appropriate control message toFsfe that results in the corresponding output
being forwarded directly to the environment as an output from P2.
Proving that S works. That completes the description of S. However, we still have to prove
that Sdoes the job. One can prove that for every environment Z, we have
⏐⏐Pr[Exec[Π,Atriv,Z] = 1] −Pr[Exec[Fsfe,S,Z] = 1]
⏐⏐≤Q+ 1
q ,
where Q is the number of authenticated sharings that are opened to P2 during the protocol. Note
that Qis equal to the number of inputs for P2, plus twice the number of multiplication gates, plus
the number of outputs for P2.
We sketch the proof outline, which can be organized as a sequence of games. In Gamej, we
denote by Wj the event that the environment outputs 1.
Game0: This is the real world.
Game1: In this game, instead of running Protocol 23.5, we run the simulator S(1)
D , as described in
Section 23.2.4.7. By Fact 23.2, we have Pr[ W0] = Pr[W1].
Game2: In this game, we keep track of the shares that P1 should be holding, and we introduce
aborts if P1 sends any bad shares in the reliable key opening subprotocol, or whenever P1
opens an authenticated sharing to P2. Based on the fact that we are using S(1)
D in place of
Protocol 23.5, and on the soundness analysis in Section 23.2.4.4, we have |Pr[W2]−Pr[W1]|≤
(Q+ 1)/q.
Game3: This is the ideal world, with the simulator S interacting with the ideal functionality
Fsfe. It is not hard to see that the distribution of the environment’s view in Game 2 is
identical to that in Game 3. Indeed, we are essentially replacing one collection of random,
1071
independent values by another collection of random, independent values (the value K(1),
the value a in the subprotocol for reliably opening K(1) to P1, the value a in the P1-input-
wire subprotocol, the value δ in the in P2-input-wire subprotocol, and the values u,v in
the multiplication-gate subprotocol), and using the fact that the simulated opening logic is
identical to the real opening logic, when the simulator uses the correct value to be opened.
Therefore, Pr[W2] = Pr[W3].
That completes the proof. 2
23.5.10 An example malicious security proof: multi-party computation based
on a secure distributed core
We sketch the idea of a proof based on the composition theorem (Theorem 23.5) for the protocol
in Section 23.4.
Let Π denote this protocol. It involves parties Q1,...,Q N, as well as P1, P2, and D. The
assumption is that at most one of P1, P2, or D is corrupt, but any number of the Qj’s may be
corrupt. Ultimately, we want to show that Π securely emulates Fsfe. Protocol Π makes use of a
protocol Π∗for secure arithmetic circuit evaluation. Also recall that in implementing this protocol,
we relied on a feature of Beaver’s protocol in which the parties could enforce a rule that some
outputs must be delivered ﬁrst and other outputs may not be delivered at all. Let us call this ideal
functionality F∗
sfe. Although we do not do so here, it is not too diﬃcult to show that Π ∗ securely
implements F∗
sfe by adapting the proof of Theorem 23.9. Note also that the circuits evaluated by
F∗
sfe and Fsfe are not the same — as described in Section 23.4, the circuit ofF∗
sfe is derived from that
of Fsfe, with some extra operations used to process inputs and outputs for the parties Q1,...,Q N.
We sketch a partial proof of the following:
Theorem 23.10. Protocol Π securely implements Fsfe, assuming that the subprotocol Π∗ securely
implements F∗
sfe, that q is super-poly, and that at most one of P1, P2, or D is corrupt.
To make things more transparent, let us denote by P1 and P2 the machines that implement the
high-level logic described in Section 23.4, and let us denote by P∗
1 and P∗
2 the machines that (along
with D) securely implement F∗
sfe. That is, for i= 1,2, each party Pi:
• receives messages from their peers Q1,...,Q N,
• passes inputs to P∗
i ,
• receives outputs from P∗
i , and
• passes messages to their peers Q1,...,Q N.
Now consider the hybrid protocol Π F∗
sfe, in which the subprotocol Π ∗ is replaced by the ideal
functionality F∗
sfe. Since Π ∗ securely implements F∗
sfe, the composition theorem tells us that is
suﬃces to show that the hybrid protocol Π F∗
sfe securely implements Fsfe. The beauty of this is that
now, we do not have to worry about any of the details of the protocol Π ∗.
We shall just sketch some of the ideas for the simulator. Let us assume that P1 is corrupt, so
that P2 and D are honest. This means that P∗
1 is also viewed as corrupt, and that P∗
2 is honest.
(Recall that for i = 1,2, we view Pi and P∗
i as two programs running on the same machine, and
that this machine is either corrupt or honest.)
The basic ideas of the operation our simulator Sare as follows:
1072
P 1
P 2
Q j
F
⇤
sce
P 1
P 2
Q j
F
⇤
sce
(a) Honest Q j (b) Corrupt Q j
Figure 23.19: Proof of Theorem 23.10
• For an honest Qj, the simulator Swill not see Qj’s inputs. These inputs are chosen by the
environment Zand forwarded directly to the ideal functionalityFsfe. However, Sdoes receive
a notiﬁcation from Fsfe that an input was given.
• For a corrupt Qj, the simulator Smust somehow extract Qj’s eﬀective inputs, and feed these
to Fsfe.
• For an honest Qj, the simulator Swill not see Qj’s outputs. Rather, Stells Fsfe when to
forward these outputs directly to Z.
• For a corrupt Qj, the simulator may ask Fsfe for Qj’s outputs.
• The simulator will play the roles of F∗
sfe, P2, and the honest Qj’s, interacting with P1 and
the corrupt Qj’s in a way that fools Zinto thinking it is still operating in the hybrid world,
even though Shas no idea what the honest Qj’s inputs or outputs actually are.
As stated above,Swill be playing the role ofF∗
sfe. This means that Swill receive inputs normally
intended for F∗
sfe from P1, and must provide outputs to P1 that would normally be coming from
F∗
sfe. However, in the ideal world in which Sis running, there is no F∗
sfe — Smust do all of the
work that F∗
sfe would normally do (with the help of Fsfe, of course).
Fig. 23.19 may be helpful in keeping track of things. The shaded boxes represent machines
under adversarial control. In the hybrid world, the unshaded machines are running the hybrid
protocol ΠF∗
sfe. In the ideal world, the unshaded machines (including F∗
sfe) are simulated by S—
more precisely, communication between shaded and unshaded machines (shown in solid lines) is
simulated.
We begin with discussing the input processing logic, which is the heart of the analysis. The
reader should refer back to Section 23.4.1.
Honest Qj.
• In Step 1 of that logic, in the hybrid world, P1 would receive random shares A1,B1,C1 from
Qj. So instead, Sjust gives P1 random values A1,B1,C1.
• In Step 2, in the hybrid world, for each input x from Qj, P1 would receive random shares
x1,E1 from Qj. So instead, Sjust gives P1 random values x1,E1.
1073
• In Step 3, in the hybrid world,P1 inputs some values A′
1,B′
1,C′
1,R′
1 to F∗
sfe. In the ideal world,
since Sis playing the role of F∗
sfe, these values are actually submitted to S, and Schecks that
(A1,B1,C1) = (A′
1,B′
1,C′
1); if not, Snotes that an input inconsistency has occurred .
• In Step 4, in the hybrid world, P1 inputs some values x′
1,E′
1,S′
1 to F∗
sfe. In the ideal world,
since Sis playing the role of F∗
sfe, these values are actually submitted to S, and Schecks that
(x1,E1) = (x′
1,E′
1); if not, Snotes that an input inconsistency has occurred .
Corrupt Qj.
• In the ideal world, Qj will send some values A2,B2,C2 to party P2, and Ssees these values.
Also, Swill see the corresponding values A1,B1,C1,R1 that P1 inputs to F∗
sfe.
Ssets A:= A1 + A2, B := B1 + B2, and C := C1 + C2, and checks that AB = C; if not, it
notes that an input inconsistency has occurred .
• In the ideal world, for each input wire belonging to Qj, party Qj will send some values x2,E2
to party P2, and Ssees these values. Also, Swill see corresponding values x1,E1,S1 that P1
inputs to F∗
sfe.
Ssets x:= x1 + x2 and E := E1 + E2, and checks that Ax= E; if not, it notes that an input
inconsistency has occurred. The value x is the “extracted input” that Sfeeds to Fsfe, unless
this input wire corresponds to an “output mask” Gdiscussed in Section 23.4.2, in which case
Sjust records this value for later use.
After all inputs are processed as above, if S noted at any step that an input inconsistency
occurred, it sends P1 a random, non-zero value V (as if it were an output from F∗
sfe), and behaves
as if P2 has aborted, which brings the entire protocol to a halt without producing any outputs for
any of the Qj’s. Otherwise, if no error has occurred, Sdeals with the output stage as follows.
• For each masked output (see Section 23.4.2) belonging to an honest Qj, the simulator S
delivers to P1 a random value x′ (as if it were an output from F∗
sfe); if P1 delivers this same
value x′unchanged to Qj, then Sinstructs Fsfe to deliver the corresponding unmasked output
to the environment.
• For each masked output belonging to a corrupt Qj, the simulator Sobtains from Fsfe the
unmasked output x, and sends to P1 the value x′:= x+ G(as if it were an output from F∗
sfe),
where G was an “output mask” that was previously input by Qj and extracted as described
above.
We leave it to the reader to verify that the above simulator works. The simulator for the case
where P2 is corrupt is essentially the same. We also leave it to the reader to provide details for
the simulator in the case where D is corrupt. After ﬁlling in all of these details, one can prove
Theorem 23.10.
23.5.11 An example malicious security proof: the 3-party garbled circuit pro-
tocol
We now prove that the 3-party garbled circuit protocol in Section 23.3.7 is secure in the malicious
setting. Speciﬁcally, we prove:
1074
Theorem 23.11. The 3-party garbled circuit protocol in Section 23.3.7 securely implements the
ideal functionality Fsfe, under the assumptions listed in Section 23.3.7.2, and assuming that at most
one party is corrupt.
Proof. Let Π denote the 3-party garbled circuit protocol. By the completeness of the trivial adver-
sary (Theorem 23.3), it suﬃces to build a simulator Ssuch that for every well-behaved environment
Z, we have
Exec[Π,Atriv,Z] ≈Exec[Fsfe,S,Z].
We describe simulators for the cases where P1 is corrupt and where P3 is corrupt. The case
where P2 is nearly identical to the case where P1 is corrupt.
P1 corrupt. We start by giving a description of a simulator S. The basic ideas of the operations
of Sare as follows:
• Swill not see P2’s inputs. These inputs are chosen by the environment Zand forwarded
directly to the ideal functionality Fsfe. However, Sdoes receive a notiﬁcation from Fsfe that
an input was given.
• Smust somehow extract P1’s eﬀective inputs, and feed these to Fsfe.
• Smay ask Fsfe for the common output vector (which is the same for both P1 and P2), and
tells Fsfe when to forward this output vector directly to Zas an output for P2.
• Smust somehow play the roles of P2 and P3, interacting with P1 in a way that fools Zinto
thinking it is still operating in the real world, even though Shas no idea what P2’s actual
inputs are.
Here is how our simulator works. As we said above, it will play the role of bothP2 and P3. When
P2 receives the seed s from P1, the simulator generates all of the data computed in Step 2 of the
protocol. When P3 receives the data from P1, the simulator performs the following computations,
in lieu of those that P3 would normally compute in Step (4) of the protocol:
(a) Check that the values F and
{
C(b)
i
}
i,b received from P1 match those generated from the seed
— if not, abort.
(b) For i = 1,...,n , if the ith input is contributed by P1, check that C(ai)
i = H1(Xi,ri); if not,
abort; otherwise, extract the input xi := ai ⊕bi from P1 (where C(ai)
i and bi are generated
from the seed), and feed this input xi to Fsfe.
(c) Obtain the circuit output vector y from Fsfe, and use the output simulatability property to
compute a corresponding garbled output Y , and send Y to P1.
We leave it to the reader to verify that this simulator works, under the following assumptions:
• the collision resistance of the hash function H1,
• the output simulatability of the garbling scheme.
1075
P3 corrupt. We start by giving a description of a simulator S. The basic ideas of the operations
of Sare as follows:
• Swill not see any of P1 or P2’s inputs. These inputs are chosen by the environment Zand
forwarded directly to the ideal functionality Fsfe. However, Sdoes receive a notiﬁcation from
Fsfe that an input was given.
• S does does not see the common output vector (which is the same for both P1 and P2).
Rather, it tells Fsfe when to forward this output vector directly to Zas an output for either
P1 or P2.
• Smust somehow play the roles of P1 and P2, interacting with P3 in a way that fools Zinto
thinking it is still operating in the real world, even though Shas no idea what either P1’s or
P2’s inputs or outputs actually are.
Here is how our simulator works. As we said above, it will play the role of both P1 and P2.
In fact, the simulator will simply run the actual protocol for P1 and P2, with only the following
changes:
• In Step 3 of the protocol, use 0 in place of xi for i= 1,...,n .
• In Step 5 of the protocol, instead of using the decoding data d to decode Y , simply abort if
Y ̸= Eval(F ,X ), where X = (X(0)
1 ,...,X (0)
n ).
Of course, the actual inputs for P1 and P2 are forwarded directly from the environment to Fsfe,
and the outputs for P1 and P2 are forwarded directly from Fsfe to the environment (assuming no
abort occurs).
One can show that this simulator works under the following assumptions:
• the hiding properties of the hash function H1,
• the security of the pseudo-random generator G, and
• the obliviousness and authenticity properties of the garbling scheme.
The proof involves a sequence of games, which goes roughly as follows, starting from Game 0, the
real world execution:
Game1: Using PRG security, replace the output of the PRG by truly random bits.
Game2: Modify the computation in Steps 2(b)–(d) of the protocol as follows:
(b) Compute ai ←R {0,1}for i= 1,...,n .
(c) Compute r(b)
i ←R Rfor i= 1,...,n and b= 0,1.
(d) Compute C(ai⊕b)
i ←H1(X(xi⊕b)
i ,r(ai⊕b)
i ) for i= 1,...,n and b= 0,1.
Also modify Step 3 of the protocol so that instead of computing ai as xi⊕bi, we just use the
random value ai as chosen above. This modiﬁcation does not really change anything at all
(instead of choosing bi at random and setting ai := xi ⊕bi, we are choosing ai at random,
and eﬀectively setting bi := xi ⊕ai).
1076
Game3: Using hiding, for i= 1,...,n , replace C(ai⊕1)
i with H1(X,r(ai⊕1)
i ), where X is some ﬁxed
input token.
Game4: Using authenticity, do not use d to decode Y in Step 5 of the protocol. Rather, reject
any garbled output Y ̸= Eval(F ,X ) received from P3, and compute the output for P1 and P2
by simply evaluating the circuit.
Game5: Using obliviousness, and the fact that d is not used at all, use 0 in place of xi in Steps 2
and 3 of the protocol. Speciﬁcally, for i= 1,...,n , replace C(ai)
i with H1(X(0)
i ,r(ai)
i ) and Xi
with X(0)
i .
Game6: Undo changes made in Game 3.
Game7: Undo changes made in Game 2.
Game8: Undo changes made in Game 1. This game is equivalent to the ideal world with our
simulator.
We leave the remaining details to the reader. Note that we could have stopped at Game 5, but
then the description of the simulator would have been a bit more complicated. 2
23.6 Distributed key generation: ideal functionalities and exten-
sion to threshold MPC
In this section, we revisit the distributed key generation (DKG) problem that we studied back in
Section 22.4. Our goals are twofold:
• First, we discuss how to adapt the deﬁnitions presented in Section 22.4.1.2 to the framework
of this chapter.
• Second, to show how the protocol in Section 22.4.2 can be extended to obtain a threshold MPC
protocol for evaluating an arbitrary arithmetic circuit. This protocol works with an arbitrary
number N of parties, and is secure and provides guaranteed output delivery, provided at most
L<N/ 3 of these parties are corrupt.
23.6.1 Formal models for secure DKG
We already gave a deﬁnition of security for a DKG protocol in Section 22.4.1.2. That deﬁnition was
already in the same spirit of the ideal-world/real-world framework we have deﬁned in this chapter
in Section 23.5, but there are some diﬀerences.
23.6.1.1 Ideal worlds
The diﬀerences between the ideal world in Section 22.4.1.2 and the ideal world in Section 23.5
are essentially syntactic. The ideal world described in Section 22.4.1.2 describes an “adversary” Z
interacting with a challenger and a simulator S, where the challenger runs the dealer algorithm.
This corresponds to an ideal world as in Section 23.5, where the role of the challenger is played by
an ideal functionality Fdkg, and the role of Zis played by the environment. Note that the ideal
functionality Fdkg in this application does not take any inputs — it only produces outputs.
1077
23.6.1.2 Real worlds
The real world execution in Section 22.4.1.2 corresponds to the execution of a hybrid protocol as in
Section 23.5.5.2.
The real world described in Section 22.4.1.2 describes an “adversary” Z interacting with a
challenger. The interactions here correspond to:
• the steps of decentralized key provisioning,
• messages sent to/from the honest parties from/to other parties, and
• queries made to any random oracles or other types of idealized object.
In the formal model of Section 23.5, the decentralized key provisioning steps and the queries
to idealized objects would be modeled by an ideal functionality F∗, and the DKG protocol would
be modeled as a hybrid protocol Π F∗, as in Section 23.5.5.2. Unlike in Section 23.5.5.2, we do
not really need a communication network C, as the decentralized key provisioning functionality is
suﬃcient to implement such a network.
The “adversary” Z in the ideal world of Section 22.4.1.2 corresponds to the environment of
Section 23.5. In Section 22.4.1.2, Z interacts with directly with the challenger, which, in the
framework of Section 23.5, corresponds to the environment Zinteracting with the trivial adversary
Atriv.
For example, in Section 22.4.2.3, we presented a hybrid DKG protocol that relies on the following
idealized components:
• decentralized key provisioning,
• a secret bulletin board, and
• a random oracle used in the VESS scheme (the zero-knowledge property for the VESS scheme
relies explicitly on modeling a certain hash function as a random oracle).
The ideal functionality F∗would model these idealized components.
We also saw in Section 22.4.2.4 how we could implement a secret bulletin board in terms of
a public bulletin board and a random oracle, and we noted that a public bulletin board can be
implemented using a Byzantine Agreement protocol (as well as decentralized key provisioning, and
sometimes also a random oracle). This gives rise to a hybrid DKG protocol that only relies on the
following idealized components:
• decentralized key provisioning, and
• some random oracles.
As we noted in Section 22.4.2.4, Byzantine Agreement can be realized assuming L < N/3 (in
the asynchronous network setting we are considering here). This is as close to a “real world”
DKG protocol as we can get in our formal model: in an actual deployment, the decentralized key
provisioning would have to be realized by some other mechanism (for example, using a certiﬁcate
authority, or using some ad hoc manual process), and the random oracles would be (heuristically)
instantiated with concrete hash functions.
1078
23.6.1.3 Security proofs
Theorem 22.12 analyzes a speciﬁc DKG protocol that relies on decentralized key provisioning, a
private bulletin board, and a secure VESS scheme. As discussed above, we can view this as a hybrid
protocol Π F∗, where F∗ is an ideal functionality that models the decentralized key provisioning,
the secret bulletin board, and the random oracle used in the VESS scheme. Theorem 22.12 implies
that protocol ΠF∗ securely implements Fdkg.
As noted above, a secret bulletin board can be implemented as a hybrid protocol that relies
on decentralized key provisioning and a random oracle. Applying the Composition Theorem 23.5,
this ultimately implies a hybrid protocol that securely implements Fdkg, and which relies only on
decentralized key provisioning and some random oracles.
23.6.1.4 Guaranteed output delivery
Our DKG protocol actual achieves guaranteed output delivery. Just as for the liveness property of
Byzantine Agreement, this means that even though an adversary may arbitrarily delay the delivery
of protocol messages, if and when all protocol messages sent from honest parties to honest parties
have been delivered, all honest parties will obtain an output.
23.6.2 A threshold MPC protocol
In this section, we sketch how the techniques of Section 22.4.2 can be fairly easily extended to obtain
a threshold MPC protocol for evaluating an arbitrary arithmetic circuit. This protocol works with
an arbitrary number N of parties, and is secure and provides guaranteed output delivery, provided
at most L < N/3 of these parties are corrupt. This protocol works in an asynchronous network
setting, is secure against malicious adversaries, and relies only on decentralized key provisioning
and a public bulletin board. We shall also assume secure channels, but this can easily be built on
top of decentralized key provisioning, so this is not an extra assumption.
Just as in Section 22.4, we use the following notation:
• q is a prime;
• G is a group of order q generated by g∈G;
• N is the number of parties,
• L is a bound on the number of corrupt parties.
We assume L < N/3: not only is this assumption needed to implement a public bulletin board,
but the MPC protocol itself relies explicitly on this assumption.
We will make use of Shamir’s t-out-of-N secret sharing scheme (see Section 22.1), where t :=
L+ 1. We use the following notation, which slightly generalizes that in Section 22.4:
• If u ∈G is a group elements and ω= κ0 + κ1x+ ··· + κt−1xt−1 ∈Zq[x] is a polynomial of
degree less than t with coeﬃcients in Zq, then
uω := (uκ0,...,u κt−1) ∈Gt.
1079
• If U = (u0,...,u t−1) ∈Gt is a vector of group elements, and β ∈Zq, we deﬁne the group
element
U(β) :=
t−1∏
j=0
uβj
j .
With this notation, we have the relation
(uω)(β) = uω(β).
We introduce some additional notation. For U,U ∈Gt, we deﬁne U ◦U ∈Gt to be the vector
of group elements obtained by component-wise multiplication. With this notation, for u,¯u ∈G
and ω,¯ω∈Zq[x] of degree less than t, and β ∈Zq, we have
(uω◦¯u¯ω)(β) = uω(β) ◦¯u¯ω(β).
For U ∈Gt and β ∈Zq, we will write for Uβ to be the vector of group elements obtained by
component-wise exponentiation.
We assume that in addition to the generator g∈G, we have a second, random generator ¯g∈G
We will assume that the discrete logarithm problem in G is hard, and, in particular, that it is
infeasible to compute log g ¯g. (The generator ¯ g could, for example, or be the output of a hash
function modeled as a random oracle, or be generated using a secure DKG protocol; in fact, we
could actually just use the DKG protocol built using a public bulletin board as discussed at the
end of Section 22.4.2.4, since the only requirement on ¯g is that it is hard to compute log g ¯g.)
23.6.2.1 Masked VESS
An essential tool in our threshold MPC protocol will be a generalization of the notion of a VESS
scheme introduced in Section 22.4.2.1. We call this generalization a masked VESS scheme .
Such a scheme works as follows. Given two polynomials ω,¯ω∈Zq[x] of degree less than t, we can
generate a transcript (C,U,π), where
• C = (c1,...,c N) is a vector of ciphertexts, where each ci is an encryption of
(αi,¯αi) := (ω(i),¯ω(i)) ∈Zq ×Zq
under Pi’s public key;
• U := gω◦¯g¯ω ∈Gt;
• π is a non-interactive zero-knowledge proof that ci is indeed an encryption of ( αi,¯αi) ∈Zq
under pki’s public key satisfying
gαi¯g¯αi = U(i)
for i= 1,...,N .
We call U the polynomial commitment of the transcript.
1080
As in Section 22.4.2.1, there may be one public key per sender/receiver pair, in which case
semantically secure encryption is suﬃcient. Alternatively, as in Remark 22.12, we can use just one
public key per receiver, in which case we need an AD-only CCA-secure encryption scheme, where
the associated data identiﬁes the sender. Not only does this reduce the number of public keys
dramatically, it also makes it easier to allow parties external to P1,...,P N to generate transcripts.
This may be useful in some modes of deployment.
We describe here the intuition behind the security properties of such a transcript. Suppose
we have a secret α ∈Zq, and as in Shamir’s secret sharing scheme, we set κ0 := α and choose
κ1,...,κ t−1 ∈Zq at random, forming the polynomial
ω:= κ0 + κ1x··· + κt−1xt−1.
Thus, ωis a random polynomial of degree less than t, subject to ω(0) = α. Suppose we also choose
a random polynomial
¯ω:= ¯κ0 + ¯κ1x··· + ¯κt−1xt−1 ∈Zq[x]
of degree less than t, and then construct a transcript ( C,U,π) as above from ω,¯ω. We call this a
random transcript for the secret α.
We argue that the adversary does not learn any information about α. We are assuming the
adversary controls a set Lof corrupt parties, where |L|≤ L. For simplicity, let us assume that
|L|= L. So the adversary can decrypt the ciphertexts in C directed towards the corrupt parties
to obtain ( αℓ,¯αℓ) for ℓ∈L. As we already know (see Theorem 22.3), these shares are all random
and independent of ( κ0,¯κ0) = ( α,¯κ0). In addition, the ﬁrst component gα¯g¯κ0 of the polynomial
commitment U is a random group element that is independent ofα, and all of the other components
of U can be computed from this ﬁrst component and the values ( αℓ,¯αℓ), via interpolation in the
exponent; therefore, the polynomial commitment U leaks no information about α. Moreover, by
semantic security, the ciphertexts inC directed towards honest parties do not leak any information,
and because of the zero knowledge property, neither does the proof π. This is why we called such
a scheme a masked VESS scheme — it reveals no information about the secret α, not even gα, as
in an ordinary, “unmasked” VESS scheme.
That describes (at an intuitive level) the security property for a random transcript for a secret
generated by an honest party. We now consider the security properties (again, at an intuitive level)
for a transcript created by a corrupt party. We assume that such a transcript is valid, in the sense
that the proof π is valid. Let Ibe the set of honest parties. By the soundness of the proof system,
this means that the decryptions ( αi,¯αi) for i∈I are all correct, in the sense that
gαi¯g¯αi = U(i) (for i∈I).
By our assumption that N > 3L, we have |I| = N −L ≥t = L+ 1 (in fact, N > 2L would
have suﬃced). This means that we (or a simulator) can choose a subset I′ of t honest parties
and interpolate their shares to obtain polynomials ω,¯ω ∈Zq[x] of degree less than t such that
gω ◦¯g¯ω = U. It must be the case that ( αi,¯αi) = ( ω(i),¯ω(i)) for all i ∈I, assuming (as we are)
that it is hard to compute logg ¯g. Indeed, if we had ( αi,¯αi) ̸= (ω(i),¯ω(i)), then these would be two
distinct representations of U(i) with respect to g and ¯g (as in Section 10.6.1), which would allow
us to compute log g ¯g. Similarly, if a corrupt party Pℓ later reveals (αℓ,¯αℓ) such that
gαℓ¯g¯αℓ = U(ℓ)
1081
it must be the case that
(αℓ,¯αℓ) = (ω(ℓ),¯ω(ℓ)),
assuming it is hard to compute log g ¯g. Let us call ω,¯ω the underlying polynomials of the
transcript.
23.6.2.2 Sharings
Like Beaver’s MPC protocol, our threshold MPC protocol evaluates an arithmetic circuit deﬁned
over Zq, and is based on the notion of a sharing. However, when a value is shared, the shares are
derived as in Shamir’s secret sharing scheme, but with a twist. If a value α∈Zq is shared among
P1,...,P N, there are polynomials ω,¯ω∈Zq[x] of degree less than t such that ω(0) = α, and the
share held by each honest party Pi is the pair
(αi,¯αi) := (ω(i),¯ω(i)) ∈Zq ×Zq.
Associated with such a share is a polynomial commitment U ∈Gt, which is public and is agreed
upon by all honest parties, and which satisﬁes
U = gω◦¯g¯ω.
By design, the polynomials ω,¯ω are guaranteed to exist and are eﬃciently computable from
the information available (in aggregate) to the honest parties, assuming the soundness of the VESS
proof system and that it is hard to compute log g ¯g. Let us call ω,¯ωthe underlying polynomials
of the sharing.
Observe that for each honest party Pi, we have
gαi¯g¯αi = U(i). (23.21)
For a any party Pi, honest or corrupt, we say that a (purported) share ( αi,¯αi) from Pi is valid
if (23.21) holds. Assuming it is hard to compute log g ¯g, a valid share from Pi must be equal to
(ω(i),¯ω(i)).
Just as we did in Beaver’s protocol, we will denote by [ α] such a sharing. For each sharing [ α],
an honest party Pi stores the following local data:
• a polynomial commitment U ∈Gt, and
• a share (αi,¯αi) ∈Zq ×Zq.
23.6.2.3 Reliable broadcast
In addition to a public bulletin board another tool from distributed computing we will need is a
Reliable Broadcast protocol. Basically, such a protocol ensures that when a party broadcasts a
message, if any honest party receives a message, then every honest party will eventually receive the
same message. This prevents the situation where a corrupt party sends one message to one honest
party, but a diﬀerent message, or no message at all, to another honest party.
We will deﬁne more precisely the properties such a protocol must satisfy. Designing a Reliable
Broadcast protocol is actually fairly easy — much easier than designing a public bulletin board.
As such, we present a very simple protocol that satisﬁes these properties.
1082
In a Reliable Broadcast protocol, we have partiesP1,...,P N, of which L<N/ 3 may be corrupt,
and we assume an asynchronous communication network. In one instance of the protcol, each party
Pi is allowed to broadcast a single message to all other parties, and each partyPj may reliably receive
a single message from each party Pi.
The key properties of such a protocol are the following, which should hold for each instance of
of the protocol’s execution:
Totality: if one honest party reliably receives a message from Pi, then eventually each honest
party does so.
Consistency: if two honest parties reliably receive messages mand m′, respectively, from a party
Pi, then m= m′.
Integrity: if an honest party reliably receives a message m from an honest party Pi, then Pi
previously reliably broadcast m.
Validity: if an honest partyPi reliably broadcasts a message m, then every honest party eventually
reliably receives m from Pi.
For the totality and validity properties, “eventually” means when all relevant messages generated
by honest parties have been delivered.
Here is a very simple protocol for Reliable Broadcast. It assumes that each party has a signing
key that is set up via decentralized key provisioning. A single instance of the protocol, with
associated “instance ID” iid, runs as follows:
(1) Propose. When party Pi wishes to broadcast a message m, it sends the signed message
(iid,propose,i,m ) to all parties.
(2) Vote. When a party receives the ﬁrst signed message of the form ( iid,propose,i,m ) from Pi,
it sends the signed message ( iid,vote,i,m ) to all parties.
(3) Commit. If for a given i,m, a party receives signed messages ( iid,vote,i,m ) from N −L
distinct parties, the party willreliably receivemfrom Pi, and forward all of these vote messages
to all other parties.
It is easy to show that this protocol satisﬁes all of the properties required for Reliable Broadcast,
assuming secure signatures. Totality follows from the fact that if an honest party reliably receives a
message from Pi, then it will forward the required vote messages to all other honest parties to make
them do the same. Consistency follows from a counting argument. Suppose there are exactly L
corrupt parties and so N−Lhonest parties. Suppose one honest party reliably receives a message
m from Pi. This means it must have received N −L signed votes for m, and so at least N −2L
honest parties must have voted for m. Similarly, if another honest party reliably receives a message
m′ ̸= m from Pi, at least N −2L honest parties must have voted for m′. Since an honest party
only votes for one message from any given party, the set of honest parties who voted for m must
be disjoint from the set of honest parties that voted for m′. Therefore,
(N −2L) + (N −2L) ≤N −L,
which implies N ≤3L, which contradicts the assumption that N >3L. We leave the argument for
integrity and validity to the reader.
1083
Note also that to reduce the communication complexity, one can use an ( N −L)-out-of-N
threshold signature on the vote messages — in the commit stage, only this threshold signature
needs to be forwarded. However, for this to work, we need a scheme that satisﬁes the notion of
threshold signature gap security introduced in Section 22.6.1. This will guarantee that any threshold
signature on a particular vote message implies that at least N−2Lhonest parties threshold signed
the message.
23.6.2.4 A threshold MPC protocol
We now describe our threshold MPC protocol for securely evaluating a given arithmetic circuit
over Zq. As in Section 23.2, we have to present subprotocols for processing input wires, addition
gates, scalar multiplication gates , constant addition gates , multiplication gates, and output wires,
Just as Beaver’s protocol in Section 23.2.2, this protocol ﬁrst processes input wires. For each input
wire, the protocol constructs a sharing [ α] of the value α carried in that wire. Then the protocol
processes each gate, computing a sharing of the value output by that gate, using the sharings of
the input values to that gate. Finally, for output wires, the protocol reveals the value carried on
the wire to the appropriate parties.
Input wires. Each honest party Pi does the following:
1. For each of its inputs α, construct a random transcript for α.
2. Construct a message containing all of these transcripts, and broadcast that message using a
Reliable Broadcast protocol as in Section 23.6.2.3.
3. Using the Reliable Broadcast protocol, reliably receive a message from each party.
4. For each input αto the circuit, locate the transcript (C,U,π) associated with that input, and
record the local data for the sharing [ α], which consists of (1) the polynomial commitment
U, and (2) the share (αi,¯αi) obtained by decrypting the ciphertext in C directed towards Pi.
(Note that the underlying polynomials of the sharing [ α] are the same as the underlying
polynomials of the transcript ( C,U,π).)
Addition gates. These are very easily processed locally, without any communication. Suppose
we have sharings [ α] and [ α†], and we want to compute the sharing [ α+ α†]. We assume that
each party Pi has corresponding shares ( αi,¯αi) and (α†
i,¯α†
i), along with corresponding polynomial
commitments U and U†. Its share of α+ α†can be locally computed as ( αi+ α†
i,¯αi+ ¯α†
i), and the
corresponding polynomial commitment can be locally computed as U ◦U†.
One can see that if the underlying polynomials of [ α] are ω,¯ω, and the underlying polynomials
of [α†] are ω†,¯ω†, then the underlying polynomials of the sharing [ α+ α†] are ω+ ω†and ¯ω+ ¯ω†.
Scalar multiplication gates. These are also very easily processed locally, without any com-
munication. Suppose we have a sharing [ α] and a constant γ ∈Zq, and we want to compute the
sharing [ γα]. We assume that each party Pi has a corresponding share ( αi,¯αi) and polynomial
commitment U. Its share of γα can be locally computed as ( γαi,γ¯αi), and the corresponding
polynomial commitment can be locally computed as Uγ.
1084
One can see that if the underlying polynomials of [α] are ω,¯ω, then the underlying polynomials
of the sharing [ γα] are γωand γ¯ω.
Constant addition gates. These are also very easily processed locally, without any commu-
nication. Suppose we have a sharing [ α] and a constant γ ∈Zq, and we want to compute the
sharing [γ+ γ]. We assume that each party Pi has a corresponding share ( αi,¯αi) and polynomial
commitment U. Its share of γα can be locally computed as ( αi + γ,¯αi), and the corresponding
polynomial commitment can be locally computed by multiplying each component of U by gγ.
One can see that if the underlying polynomials of [α] are ω,¯ω, then the underlying polynomials
of the sharing [ α+ γ] are ω+ γ and ¯ω.
Multiplication gates. This is the interesting case. Suppose we have sharings [ α] and [ α†] and
we want to compute the sharing [ α·α†]. We assume that each party Pi has corresponding shares
(αi,¯αi) and (α†
i,¯α†
i), along with corresponding polynomial commitments U and U†.
For this, we will need to use an instance of our public bulletin board subprotocol (we will need a
separate instance for each multiplication gate). To understand the design of the multiplication sub-
protocol, suppose that the underlying polynomials of [ α] are ω,¯ω, and the underlying polynomials
of [α†] are ω†,¯ω†. Each party Pi will
• generate a random transcript ( Ci,Ui,πi) for the secret ω(i) ·ω†(i) ∈Zq, along with a non-
interactive zero-knowledge proof π∗
i that this secret is what it is supposed to be (details of
this below), and
• submit this augmented transcript ( Ci,Ui,πi,π∗
i) to the public bulletin board.
The public bulletin board will then produce a collection
{(Ck,Uk,πk,π∗
k)}k∈K
of |K|= 2L+ 1 valid augmented transcripts. By “valid”, we mean that each proof πk and π∗
k is
valid.
The reason we need 2 L+ 1 transcripts is that ω and ω† are each of degree at most t−1 = L,
and so the product polynomial ω·ω† has degree at most 2 L, and we want 2 L+ 1 transcripts to
be able to compute ω(0) ·ω†(0) from the values {ω(k) ·ω†(k)}k∈K via polynomial interpolation.
Indeed, suppose {λk}k∈Kare the Lagrange interpolation coeﬃcients (as in Lemma 22.1) such that
ω(0) ·ω†(0) =
∑
k∈K
λkω(k)ω†(k),
which depend only on, and can be eﬃciently computed from, the set K.
The polynomial commitment U‡for the sharing [ α·α†] will be computed as
U‡←
∏
k∈K
Uλk
k .
If Ck = (ck1,...,c kN) for k∈K, party Pi will decrypt cki to obtain (αki,¯αki) for each k∈K, and
then compute is share ( α‡
i,¯α‡
i) of [α·α†] as
α‡
i ←
∑
k∈K
λkαki, ¯α‡
i ←
∑
k∈K
λk¯αki.
1085
To see why this works, each augmented transcript ( Ck,Uk,πk,π∗
k) will have underlying poly-
nomials ωk,¯ωk such that
ωk(0) = ω(k) ·ω†(k),
and
ωk(i) = αki, ¯ωk(i) = ¯αki for each honest party Pi.
It follows that the underlying polynomials of [ α·α†] are
ω‡=
∑
k∈K
λkωk and ¯ω‡=
∑
k∈K
λk¯ωk.
Moreover, we have
ω‡(0) =
∑
k∈K
λkωk(0) =
∑
k∈K
λkω(k)ω†(k) = ω(0)ω†(0),
as required.
Since we need to collect 2L+ 1 transcripts in the bulletin board, we must have 2L+ 1 ≤N−L,
which is equivalent top saying L < N/3. This is the same bound needed to implement Byzantine
Agreement (and Reliable Broadcast).
The last remaining detail is the design of the non-interactive zero-knowledge proof π∗
i that each
Pi must construct to show that its transcript ( Ci,Ui,πi) is a transcript for the secret ω(i) ·ω†(i).
This is fairly straightforward. First, we start with Sigma protocol for an appropriate relation (see
Section 19.4). A statement for this relation is a triple ( u,v,w ) ∈G3. A corresponding witness is a
triple (β, ¯β,γ) ∈Z3
q such that
gβ¯g
¯β = v and uβ¯gγ = w.
We can build such a Sigma protocol as in Section 19.5.3 and convert it to a zero-knowledge non-
interactive proof system via the Fiat-Shamir transform as in Section 20.3. Now, Pi will construct
a proof for the statement
(u,v,w ) = (U(i),(U†)(i),U(0)
i )
using the witness
(β, ¯β,γ) = ( ω†(i), ¯ω†(i), ¯ωi(0) −¯ω(i)ω†(i) ),
where ωi,¯ωi are the polynomials underlying the transcript ( Ci,Ui,πi). Of course, Pi generated
this transcript, and so it knows the polynomials ωi,¯ωi. Moreover, we have ωi(0) = ω(i) ·ω†(i), by
construction. Let us verify that this is indeed a witness. We have
gβ¯g
¯β = gω†(i)¯g¯ω†(i) = (U†)(i) = v
as required. We also have
uβ¯gγ = (U(i))ω†(i) ·¯g¯ωi(0)−¯ω(i)ω†(i)
= (gω(i)¯g¯ω(i))ω†(i) ·¯g¯ωi(0)−¯ω(i)ω†(i)
= gω(i)ω†(i)¯g¯ωi(0)
= gωi(0)¯g¯ωi(0)
= U(0)
i = w
as required.
1086
Output wires. Consider a output wire which carries the value α, and assume the protocol has
already computed a corresponding sharing [ α] with polynomial commitment U.
1. To reveal α to a designated receiver Pj, each honest party Pi sends its share ( αi,¯αi) to Pj.
This is sent over a secure channel (but note that if the output wire is designated to reveal its
value to all parties, party Pi can simply send this share to all parties in the clear).
2. To obtain the value α, a designated receiver Pj will wait until it obtains valid shares ( αi,¯αi)
from t= L+ 1 distinct parties Pi, and then interpolate to obtain α as the constant term of
the polynomial ωthat satisﬁes ω(i) = αi for each such Pi.
Security. That completes the description of our threshold MPC protocol. While we do not do so
here, one can show that this protocol securely emulates the ideal functionality Fsfe. The protocol
is a hybrid protocol that relies on
• decentralized key provisioning,
• a public bulletin board, and
• random oracles (used in the VESS and multiplication proof systems).
The proof of security relies on the hardness of the discrete logarithm problem, as well as the security
properties of VESS scheme. The analysis of the proof system used in the multiplication subprotocol
requires a kind of rewinding argument (such as we did for Schnorr signatures in Section 19.2) to
actually extract a witness. Note that the simulator itself does not need to do any rewinding — the
rewinding is only needed to show that the simulator works correctly.
Fairness. If we implement the public bulletin board using a Byzantine Agreement protocol, then
the fairness property follows from (i) the fairness of the threshold MPC protocol, (ii) the liveness
property of the Byzantine Agreement protocol, (iii) the totality property of the Reliable Broadcast
subprotocol, and the (iv) fact that the protocol only delivers outputs after all inputs have been
obtained via the Reliable Broadcast subprotocol. Once that happens, the protocol is guaranteed
to deliver all outputs to all honest parties. Indeed, one of two things will happen: either all inputs
are obtained and all outputs are delivered, or some inputs are not obtained and no outputs are
delivered.
23.7 OT extension
Oblivious transfer (OT) is an important tool in cryptography: it is used in the garbled circuit MPC
protocol (Section 23.3.6), Exercise 23.5 shows how it can be used to construct Beaver triple sharings
for the Beaver MPC protocol (Section 23.2.2), and more applications are discussed in Section 11.6.
All these applications require that the parties run many instances of the OT protocol.
Naively, to run N instances of an OT protocol, we can simply choose our favorite OT protocol
(say, from Section 11.6) and run it N times. However, a beautiful technique called OT extension
shows that after running only a small number of OT instances, it is possible to obtain many OT
instances using only fast symmetric primitives such as a hash function and a PRF. This gives a
tremendous speed-up to OT applications that require many instances of OT.
1087
We will describe OT extension in the honest-but-curious settings. An enhancement to the basic
protocol makes it secure against malicious adversaries.
Let us ﬁrst recall a simple variation of OT previously discussed in Exercise 11.17. A 1-out-of- t
Random OT, or ROT, is a 2-party protocol where
• input: the sender has no input, and the receiver has a choice value s∈{1,...,t },
• output: the sender has a random tuple ( y[1],...,y [t]) ←R Yt, and the receiver has y[s] ∈Y.
The diﬀerence from standard 1-out-of- t OT is that the sender does not specify an input tuple,
but is instead given a random one at the end. Exercise 11.17 shows how to eﬃciently construct
a standard 1-out-of- t OT from 1-out-of- t ROT. Therefore, it suﬃces to focus on building many
instances of 1-out-of-t ROT.
Recall also from Exercise 11.17 that a 1-out-of-2 OT correlation is a state where the sender
has a random pair ( k1,k2) ∈K2 and the recipient has ( r,kr) ∈{0,1}×K , where r is random in
{0,1}. Neither party knows anything else about the other party’s data.
The OT extension protocol has two phases. In a setup phase the two parties establish npairs of
1-out-of-2 OT correlations. Then in the online phase, the two parties use the setup data to quickly
run N instances of 1-out-of-t ROT, for some N that is much bigger than n. The setup phase may
require a complex protocol, but once that’s done, the parties can quickly generate as many ROTs
as needed. For the applications of OT in this chapter it suﬃces to set t= 2, but the construction
works for larger t. We will need the following tools:
• Parameters: given n 1-out-of-2 OT correlations we build N instances of 1-out-of- t ROT,
where N is much larger than n. The goal is to minimize nso as to minimize the cost of setup.
• We will do arithmetic in Zℓ, for some prime ℓ. One often simply sets ℓ= 2.
• Let F be a secure PRF deﬁned over ( K, {1,...,N }, Zℓ).
• Let H1,...,H N : Zn
ℓ →Y be N hash functions that will be modeled as independent random
oracles. One can derive all the Hi from a single hash function H as Hi(x) := H(i,x).
In addition, we will need a list of t codewords, C= (C[1],..., C[t]), where each codeword C[i] is a
vector in Zn
ℓ. These codewords must be far apart: for all i̸= j the Hamming distance between C[i]
and C[j] must be at least n′ for some n′≤n (i.e., C[i] −C[j] has at least n′ non-zero entries). As
we will see, to maintain security, we will need n′ to be suﬃciently large so that 1 /2n′
is negligible
(e.g., set n′ := 128). The parameters ℓ,t,n,n ′ need to be chosen so that there is an explicit list
of codewords Csatisfying this property. For example, if t= 2, then we can take C:= (0n,1n) and
n′:= n.
The setup phase. Two parties, a sender and a receiver, prepare n OT correlations:
• The receiver has n random pairs (kj,0, kj,1) ∈K2 for j = 1,...,n .
• The sender has n pairs (rj,ˆkj), where rj ∈{0,1}functions as a choice bit so that ˆkj = kj,rj
for j = 1,...,n . All the choice bits in r := (r1,...,r n) are sampled uniformly from {0,1}.
We will treat ras a binary vector in Zn
ℓ.
1088
Neither party knows anything else about the other party’s data. Notice that we reversed the roles of
the sender and the receiver: the receiver has ( kj,0,kj,1) and the sender has the choice bit rj ∈{0,1}
and kj,rj. The two parties can generate these n OT correlations using any of the OT protocols
from Section 11.6.
The end goal. OT extension uses the setup data to implement N instances of 1-out-of- t ROT.
Speciﬁcally, in addition to the setup data, the receiver also takes as input N choice values
s1,...,s N ∈{1,...,t }. At the end of the OT extension protocol:
• the sender has N tuples y1,..., yN ∈Yt, that are indistinguishable from N random tuples;
• the receiver has ˆy1,... ˆyN ∈Y, where ˆyi = yi[si] for i= 1,...,N .
Here the receiver obtained one entry from each sender tuple, namely entry number si from tuple
number i. Neither party should learn anything else about the other’s party’s data.
The OT extension protocol. The protocol, between the sender and receiver, is remarkably
simple, using only a single message.
• Receiver: for i= 1,...,N the receiver computes:
ui ←
(
F(k1,0,i),...,F (kn,0,i)
)
∈Zn
ℓ,
vi ←
(
F(k1,1,i),...,F (kn,1,i)
)
∈Zn
ℓ,
pi ←vi −ui + C[si] ∈Zn
ℓ.
The receiver’s output for ROT number i is ˆyi ←Hi(ui) ∈Y.
• Receiver →Sender: send (p1,..., pN) ∈Zn×N
ℓ .
• Sender: for i= 1,...,N the sender computes:
qi ←
(
F(k1,r1,i), ..., F (kn,rn,i)
)
−r◦pi ∈Zn
ℓ,
yi ←
(
Hi
(
qi + (r◦C[1])
)
, ..., Hi
(
qi + (r◦C[t])
))
∈Yt,
where ◦denotes the entry-wise product of two vectors, so that r◦c:= (r1c1,...,r ncn) ∈Zn
ℓ.
The sender’s output for ROT number i is yi ∈Yt.
That’s it. As promised, the parties only use simple symmetric primitives, namely a PRF and a
hash function.
Let’s ﬁrst verify that the protocol is correct, namely that yi[si] = ˆyi = Hi(ui) for all i =
1,...,N . We can write the vector qi ∈Zn
ℓ computed by the sender as
qi = ui + r◦(vi −ui) −r◦pi = ui −r◦C[si].
where the last equality follows by plugging in pi = vi −ui + C[si] and canceling terms. Then, by
deﬁnition of yi, for a= 1,...,t we have
yi[a] = Hi
(
qi + r◦C[a]
)
= Hi
(
ui + r◦(C[a] −C[si])
)
. (23.22)
Setting a= si shows that yi[si] = Hi(ui), as required.
1089
Remark 23.8. Notice that in the OT extension protocol, in computing the quantities ui, vi, and
qi, for i = 1,...,N , the same PRF keys are used for all i. If ℓ = 2, so the output of the PRF is
just a single bit, then we can can implement the protocol using, say, AES, which outputs 128 bits,
so that the total number of applications of AES for the receiver (resp., sender) is just 2 ⌈Nn/128⌉
(resp., ⌈Nn/128⌉). If we take n= 128 (as discussed below), the number of AES applications is 2 N
(resp., N). 2
Security for the recipient. The sender clearly learns nothing about the recipient’s choice
value si. This si is only used in pi where it is blinded by vi −ui. The vector vi −ui is in-
distinguishable from random to the sender because the sender only knows one of the two keys used
in each component of the vector.
Security for the sender. We need to argue that the receiver learns nothing about the sender’s
tuples y1,..., yN ∈Yt, other than the entries selected by the receiver’s choice values s1,...,s N.
Let’s ﬁrst consider the simple case when t = 2, C= (0n,1n), and n′ = n. In addition, let’s
assume ℓ= 2, so that addition in Zℓ is simply an XOR. For these parameters, yi = (yi[1],yi[2]) is
a vector in Y2.
• One entry in yi is yi[si] = Hi(ui), where ui ∈Zn
2 is chosen by the honest recipient.
• By (23.22), the other entry in yi is yi[3 −si] = Hi(ui + r◦1n) = Hi(ui + r). This quantity
should be hidden from the recipient, for all i= 1,...,N .
The requirement in the second bullet is equivalent to saying that no eﬃcient adversary that is given
u1,..., uN ∈Zn
2 , can distinguish the distribution
(
H1(u1 + r),...,H N(uN + r)
)
, where r←R Zn
2 ,
from the uniform distribution on YN. This is really the only security property we require from the
hash functions H1,...,H N, and it is easy to verify that this property holds when H1,...,H N are
modeled as independent random oracles, assuming 1/2n is negligible. In practice, taking n:= 128 is
suﬃcient. Recall that ndetermines the number of OT correlations that the parties need to generate
in the setup phase. Also recall that, as we mentioned earlier, one can instantiate H1,...,H N by
setting Hi(x) := H(i,x) for some ﬁxed hash function H such as SHA256. This completes the
analysis of sender security when C= (0n,1n).
The analysis for general parameters t,ℓ,n,n ′is similar. For r∈{0,1}n, deﬁne the set of t(t−1)
related keys
K+(r) :=
{
ki,j := r◦(C[i] −C[j]) ∈Zn
ℓ for 1 ≤i̸= j ≤t
}
By (23.22) we know that yi[a] = Hi(ka,si + ui) for a= 1,...,t and i= 1,...,N . For security, we
need that no eﬃcient adversary that is given u1,..., uN ∈Zn
2 , can distinguish the distribution
(
Hi(ka,si + ui) for i= 1,...,N, a = 1,...,t, a ̸= si
)
where r←R Zn
2 ,
from the uniform distribution on YN(t−1). When the functions H1,...,H N are modeled as inde-
pendent random oracles, this holds assuming (t−1)/2n′
is negligible. Recall that n′is the minimum
Hamming distance between the codewords in C, so that C[i] −C[j] contains at least n′ non-zero
entries for all i̸= j. In practice, taking n′:= 128 + ⌈log2 t⌉is suﬃcient. One then has to ﬁnd the
smallest nfor which there is a set of codewords C⊆ Zn
ℓ of size twith minimal Hamming distance n′.
1090
We can eliminate the additive term ⌈log2 t⌉from n′, and simply set n′ := 128, by adding the
coordinate number as an input to the hash functions. Speciﬁcally, let Hi,a(x) := H(i,a, x), for
some ﬁxed hash function H. Now, deﬁne the sender’s output as
yi ←
(
Hi,1
(
qi + (r◦C[1])
)
, ..., Hi,t
(
qi + (r◦C[t])
))
∈Yt.
Notice that every entry now uses a diﬀerent hash function. Similarly, the receiver’s output is
ˆyi ←Hi,si(ui) ∈Y . If we model all these hash functions as independent random oracles, then
security holds assuming 1/2n′
is negligible. With this modiﬁcation, taking n′:= 128 is suﬃcient.
An attack by a corrupt receiver. The OT extension scheme is insecure if the receiver is corrupt
and does not honestly follow the protocol. Let’s see an attack. Let ej ∈Zn
ℓ be a vector that is zero
everywhere, except for coordinate j where it is one. Suppose a corrupt receiver computes
p∗
1 ←v1 −u1 + (C[1] −e1) ∈Zn
ℓ (23.23)
and sends this p∗
1 to the sender in place of a valid p1. Then by (23.22) the sender obtains
y1[1] = H1
(
u1 + r◦
(
C[1] −(C[1] −e1)
))
= H1
(
u1 + r◦e1
)
.
Suppose that the receiver learns this y1[1] due to a later step in a larger protocol. Now the receiver
has y1[1] = H1(u1 + r◦e1) and it knows u1. This reveals the ﬁrst bit of r∈{0,1}n. Repeating
this by constructing p∗
2 using e2 as in (23.23) reveals the second bit of r. Continuing this way using
the ﬁrst n ROTs reveals all of r to the recipient. Once r is known, the remaining ( N −n) ROTs
are no longer secure because the recipient can compute all the sender’s data. There are eﬃcient
enhancements to OT extension that make it secure against a malicious recipient [137].
23.8 A fun application: another stab at private set intersection
To be written.
23.9 Notes
Citations to the literature to be added.
23.10 Exercises
23.1 (Majority as an arithmetic circuit). For odd n, the majority function takes as input n
binary values x1,...,x n ∈{0,1}. It outputs 1 if the majority of the inputs is 1, and outputs zero
otherwise. Show how to implement the majority function as an arithmetic circuit over Zq, for some
prime q >n. Your arithmetic circuit should have just n−1 multiplication gates and O(n) addition
gates.
Hint: use polynomial interpolation.
1091
23.2 (Table lookup as an arithmetic circuit). Consider the function f : Zn+1
q → {0,1},
deﬁned as follows:
f(x1,...,x n, x) :=
{
1 if x∈{x1,...,x n},
0 otherwise.
Give an arithmetic circuit for f that uses O(n + log q) multiplication gates and O(n) addi-
tion/subtraction gates.
Hint: use Fermat’s little theorem.
23.3 (Generating Beaver triple sharings I). In Section 23.2.2 we described Beaver’s 2-party
protocol for evaluating an arithmetic circuit. Our description relied on a dealer D to provide the
two parties P1 and P2 with Beaver triple sharings. If P1 and P2 could generate these on their own,
without the dealer, the protocol would become a true 2-party protocol. Let’s see how to do that in
the honest-but-curious setting.
Let (G,E,D ) be a public key encryption scheme with message space Zq. Moreover, let’s assume
that the scheme is additively homomorphic, so that for all x,y ∈Zq and (pk,sk) ←R G():
• there is an eﬃcient probabilistic algorithm Sum that takes pk and two ciphertexts E(pk,x)
and E(pk,y) as input, and outputs a fresh ciphertext distributed as E(pk,z), where z :=
x+ y∈Zq;
• there is an eﬃcient probabilistic algorithm ScalarMul that takes pk, a ciphertext E(pk,x),
and scalar s ∈Zq as input, and outputs a fresh ciphertext distributed as E(pk,z), where
z:= s·x∈Zq.
We saw an additively homomorphic scheme in Exercise 11.13.
Now the protocol:
P1 : P2 :
(pk,sk) ←R G()
a1,b1 ←R Zq
εa ←R E(pk,a1), εb ←R E(pk,b1) (pk,εa,εb)−−−−−−−−−−−−−→a2,b2,c2 ←R Zq
compute a fresh encryption ε of
a2 ·b1 + b2 ·a1 + (a2b2 −c2) ∈Zq
output (a2,b2,c2)
c1 ←D(sk,ε) + a1b1 ∈Zq
output (a1,b1,c1)
ε←−−−−−−−−−−−−−
Here, P2 only knows encryptions of a1 and b1, but the homomorphic property allows it to eﬃciently
to construct a fresh encryption ε of a2 ·b1 + b2 ·a1 + (a2b2 −c2), as required.
In the above protocol, we assume that random elements of Zq are generated as in Remark 23.5.
(a) Show that at the end of the protocol we have c1 + c2 = (a1 + a2)(b1 + b2).
(b) Show exactly how P2 should compute ε, using the given algorithms Sum and ScalarMul.
1092
(c) You are now to show that if (G,E,D ) is semantically secure, then the above protocol is secure
in the honest-but-curious setting. Speciﬁcally, you are to show that under this assumption,
the protocol semi-securely implements (see Section 23.5.6.2) the corresponding probabilistic
instance of Fsfe (see Section 23.5.3.1), where
• There are no inputs.
• Fsfe generates a1,b1,a2,b2,c2 in Zq at random, and then computes c1 ←(a1 + a1)(b1 +
b2) −c2.
• P1 obtains the output ( a1,b1,c1) from Fsfe,
• P2 obtains the output ( a2,b2,c2) from Fsfe.
To do this, you need to design a simulator S.
(i) First suppose that P1 is corrupt. Your simulator Sobtains P1’s outputs a1,b1,c1 from
Fsfe. Let coins1 denote the random bits that are used byP1 to generate a1,b1 ∈Zq (as in
Remark 23.5). In the real world, P1 generates coins1 at random. Using the values a1,b2,
your simulatorSshould cook up a simulated value ofcoins1 (hint: see Exercise 3.11). Let
coins′
1 denote the random bits that are used by P1 to generate the objects pk,sk,εa,εb.
In the real-world, P1 generates coins′
1 at random. Your simulator Sshould do the same.
In the real-word, P1 would receive the protocol message ε from P2. Using the values
a1,b1,c1, your simulator Sshould cook up a simulated protocol message ε. Show how
S can do all this. Also argue that the environment (which knows all parties’ inputs
and outputs) cannot distinguish the simulation from the real world with non-negligible
probability. For this, you do not need to make any security assumptions.
(ii) Second suppose that P2 is corrupt. Your simulator Sobtains P2’s outputs a2,b2,c2 from
Fsfe. Let coins2 denote the random bits that are used by P2 to generate a2,b2,c2 ∈Zq
(as in Remark 23.5). In the real world, P2 generates coins2 at random. Using the values
a1,b2,c2, you simulator Sshould cook up a simulated value of coins2 (hint: again see
Exercise 3.11). Let coins′
2 be the random bits that are used by P2 to then compute ε.
In the real-world, P2 generates coins′
2 at random. Your simulator Sshould do the same.
When the real-world P2 would receive the protocol message ( pk,εa,εb) from P1, your
simulator must cook up a simulation of the protocol message ( pk,εa,εb). Show how S
can do this, and ﬁll in the rest of the details of S. Also argue that the environment
(which knows all parties’ inputs and outputs) cannot distinguish the simulation from
the real world with non-negligible probability. For this, you will need to assume that
the encryption scheme is semantically secure.
23.4 (A multiplication protocol from OT). Let’s design a 2-party protocol in the honest-but-
curious setting for the following problem: P1 has an input a∈Zq and P2 has an input b∈Zq. At
the end of the protocol, the parties should have a sharing ( c1,c2) of a·b. That is, P1 should have
c1 and P2 should have c2 so that a·b= c1 + c2. Moreover, P1 should learn nothing about b, and P2
should learn nothing about a. Here is a simple protocol using 1-out-of-2 oblivious transfer (OT):
• Init: Let α0,...,α n ∈{0,1}be the binary representation of a, so that a = ∑n
i=0 αi2i. P2
chooses random r0,...,r n ←R Zq.
• OT step: P1 and P2 engage in n+ 1 parallel 1-out-of-2 OTs, where in OT instance number i,
– P2 plays the role of sender with input Ti = (ri, ri + b) ∈Z2
q, and
1093
– P1 plays the role of receiver with input αi ∈{0,1}.
When the OT protocols ﬁnish, P1 has t0,...,t n ∈Zq where ti = Ti[αi] for i= 0,...,n .
• Finalize: P1 computes c1 = ∑n
i=0 2iti and P2 computes c2 = −∑n
i=0 2iri.
(a) Show that if P1 and P2 honestly follow the protocol, then c1 + c2 = a·b.
Hint: Use the fact that a·b= ∑n
i=0 αi ·2ib.
(b) Design a simulator to show that if the OT protocol is secure, then P1 learns nothing about
b, and P2 learns nothing about a.
(c) In Section 23.7 we presented a protocol, called OT extension, to cheaply execute many 1-out-
of-2 OTs. Show how to optimize this OT extension protocol to the setting above where the
shift b∈Zq is the same in all sender pairs.
23.5 (Generating Beaver triple sharings II). Continuing with Exercise 23.3, let’s see a more
eﬃcient protocol for P1 and P2 to jointly generate a Beaver triple sharing, this time using 1-out-of-2
oblivious transfer (OT). Again, we restrict to the honest-but-curious setting.
• Init: P1 chooses random a1,b1 ←R Zq, and P2 chooses random a2,b2 ←R Zq.
Their goal is to generate a random sharing ( c1,c2) ∈Z2
q, where P1 has c1 and P2 has c2, such
that c1 + c2 = (a1 + a2)(b1 + b2).
• Multiply: P1 and P2 run the protocol from Exercise 23.4 twice:
– once to compute a sharing ( d1,d2) where a1 ·b2 = d1 + d2, and
– once to compute a sharing ( e1,e2) where a2 ·b1 = e1 + e2.
• Finalize: P1 computes c1 = a1b1 + d1 + e1 ∈Zq and P2 computes c2 = a2b2 + d2 + e2 ∈Zq.
(a) Show that if P1 and P2 honestly follow the protocol, then c1 + c2 = (a1 + a2)(b1 + b2).
(b) Design a simulator to show that if the multiplication protocol from Exercise 23.4 is secure,
then P1 learns nothing about a2,b2, and P2 learns nothing about a1,b1.
Discussion: The protocol in this exercise is far more eﬃcient than the protocol in Exercise 23.3
thanks to the OT extension technique discussed in Section 23.7.
1094
Part IV
Appendices
1095
Appendix A
Basic number theory
A.1 Cyclic groups
Notation: for a ﬁnite cyclic group G we let G∗denote the set of generators of G.
A.2 Arithmetic modulo primes
A.2.1 Basic concepts
We use the letters p and q to denote prime numbers. We will be using large primes, e.g. on the
order of 300 digits (1024 bits).
1. For a prime p> 2 let Zp = {0,1,2,...,p −1}.
Elements of Zp can be added modulo pand multiplied modulo p. For x,y ∈Zp we write x+y
and x·y to denote the sum and product of x and y modulo p.
2. Fermat’s theorem: gp−1 = 1 for all 0 ̸= g∈Zp
Example: 3 4 = 81 ≡1 (mod 5).
3. The inverse of x∈Zp is an element a∈Zp satisfying a·x= 1 in Zp.
The inverse of x in Zp is denoted by x−1.
Example: 1 . 3−1 in Z5 is 2 since 2 ·3 ≡1 (mod 5).
2. 2−1 in Zp is p+1
2 .
4. All elements x∈Zp except for x= 0 are invertible.
Simple (but ineﬃcient) inversion algorithm: x−1 = xp−2 in Zp.
Indeed, xp−2 ·x= xp−1 = 1 in Zp.
5. We denote by Z∗
p the set of invertible elements in Zp. Then Z∗
p = {1,2,...,p −1}.
6. We now have an algorithm for solving linear equations in Zp: a·x= b.
Solution: x= b·a−1 = b·ap−2.
What about an algorithm for solving quadratic equations?
1096
A.2.2 Structure of Z∗
p
1. Z∗
p is a cyclic group.
In other words, there exists g∈Z∗
p such that Z∗
p = {1,g,g 2,g3,...,g p−2}.
Such a g is called a generator of Z∗
p.
Example: in Z∗
7: ⟨3⟩= {1,3,32,33,34,35}≡{ 1,3,2,6,4,5}(mod 7) = Z∗
7.
2. Not every element of Z∗
p is a generator.
Example: in Z∗
7 we have ⟨2⟩= {1,2,4}̸= Z∗
7.
3. The order of g∈Z∗
p is the smallest positive integer a such that ga = 1.
The order of g∈Z∗
p is denoted orderp(g).
Example: order 7(3) = 6 and order 7(2) = 3.
4. Lagrange’s theorem: for all g ∈Z∗
p we have that order p(g) divides p−1. Observe that
Fermat’s theorem is a simple corollary:
for g∈Z∗
p we have gp−1 = (gorder(g))(p−1)/order(g) = (1)(p−1)/order(g) = 1.
5. If the factorization of p−1 is known then there is a simple and eﬃcient algorithm to
determine orderp(g) for any g∈Z∗
p.
A.2.3 Quadratic residues
1. The square root of x∈Zp is a number y∈Zp such that y2 = xmod p.
Example: 1.
√
2 in Z7 is 3 since 3 2 = 2 mod 7.
2.
√
3 in Z7 does not exist.
2. An element x∈Z∗
p is called a Quadratic Residue (QR for short) if it has a square root in Zp.
3. How many square roots does x∈Zp have?
If x2 = y2 in Zp then 0 = x2 −y2 = (x−y)(x+ y).
Zp is an “integral domain” which implies that x−y= 0 or x+ y= 0, namely x= ±y.
Hence, elements in Zp have either zero square roots or two square roots.
If a is the square root of x then −a is also a square root of x in Zp.
4. Euler’s theorem: x∈Zp is a QR if and only if x(p−1)/2 = 1.
Example: 2 (7−1)/2 = 1 in Z7 but 3 (7−1)/2 = −1 in Z7.
5. Let g∈Z∗
p. Then a= g(p−1)/2 is a square root of 1. Indeed, a2 = gp−1 = 1 in Zp.
Square roots of 1 in Zp are 1 and −1.
Hence, for g∈Z∗
p we know that g(p−1)/2 is 1 or −1.
6. Legendre symbol: for x∈Zp deﬁne
(
x
p
)
:=



1 if x is a QR in Zp
−1 if x is not a QR in Zp
0 if x= 0 mod p
.
7. By Euler’s theorem we know that
(
x
p
)
= x(p−1)/2 in Zp.
=⇒ the Legendre symbol can be eﬃciently computed.
1097
8. Easy fact: let g be a generator of Z∗
p. Let x= gr for some integer r.
Then x is a QR in Zp if and only if r is even.
=⇒ the Legendre symbol reveals the parity of r.
9. Since x = gr is a QR if and only if r is even it follows that exactly half the elements of Zp
are QR’s.
10. When p= 3 mod 4 computing square roots of x∈Zp is easy.
Simply compute a= x(p+1)/4 in Zp.
a= √x since a2 = x(p+1)/2 = x·x(p−1)/2 = x·1 = x in Zp.
11. When p= 1 mod 4 computing square roots in Zp is possible but somewhat more complicated;
it requires a randomized algorithm.
12. We now have an algorithm for solving quadratic equations in Zp.
We know that if a solution to ax2 + bx+ c= 0 mod p exists then it is given by:
x1,2 = −b±
√
b2 −4ac
2a
in Zp. Hence, the equation has a solution in Zp if and only if ∆ = b2 −4ac is a QR in Zp.
Using our algorithm for taking square roots in Zp we can ﬁnd
√
∆ mod pand recover x1 and
x2.
13. What about cubic equations in Zp? There exists an eﬃcient randomized algorithm that solves
any equation of degree d in time polynomial in d.
A.2.4 Computing in Zp
1. Since p is a huge prime (e.g. 1024 bits long) it cannot be stored in a single register.
2. Elements of Zp are stored in buckets where each bucket is 32 or 64 bits long depending on
the processor’s chip size.
3. Adding two elements x,y ∈Zp can be done in linear time in the length of p.
4. Multiplying two elements x,y ∈Zp can be done in quadratic time in the length of p. If p is
n bits long, better algorithms work in time O(n1.7) (rather than O(n2)).
5. Inverting an element x∈Zp can be done in quadratic time in the length of p.
6. Using the repeated squaring algorithm, xr ∈Zp can be computed using at most (2 log 2 r)
multiplications in Zp.
7. When the base of the exponentiation x is ﬁxed, one can pre-compute a table of powers of x
containing (2w/w) ·log2 r group elements, and reduce the time to compute xr ∈Zp to only
(log2 r)/w multiplications in Zp. Here w is a small constant, such as w= 5.
1098
A.2.5 Summary: arithmetic modulo primes
Let p be a 1024 bit prime. Easy problems in Zp:
1. Generating a random element. Adding and multiplying elements.
2. Computing gr mod p is easy even if r is very large.
3. Inverting an element. Solving linear systems.
4. Testing if an element is a QR and computing its square root if it is a QR.
5. Solving polynomial equations of degree d can be done in polynomial time in d.
Problems that are believed to be hard in Zp:
1. Let g be a generator of Z∗
p. Given x∈Z∗
p ﬁnd an r such that x= gr mod p. This is known
as the discrete log problem.
2. Let g be a generator of Z∗
p. Given x,y ∈Z∗
p where x= gr1 and y= gr2. Find z= gr1r2. This
is known as the Diﬃe-Hellman problem.
A.3 Arithmetic modulo composites
We are dealing with integers non the order of 300 digits long, (1024 bits). Unless otherwise stated,
we assume that nis the product of two equal size primes, e.g. on the order of 150 digits each (512
bits).
1. For a composite n let Zn = {0,1,2,...,n −1}.
Elements of Zn can be added and multiplied modulo n.
2. The inverse of x∈Zn is an element y∈Zn such that x·y= 1 mod n.
An element x∈Zn has an inverse if and only if xand nare relatively prime. In other words,
gcd(x,n) = 1.
3. Elements of Zn can be eﬃciently inverted using Euclid’s algorithm. If gcd( x,n) = 1 then
using Euclid’s algorithm it is possible to eﬃciently construct two integers a,b ∈Z such that
ax+ bn= 1. Reducing this relation modulo nleads to ax= 1 mod n. Hence a= x−1 mod n.
note: this inversion algorithm also works in Zp for a prime p and is more eﬃcient than
inverting x by computing xp−2 mod p.
4. We let Z∗
n denote the set of invertible elements in Zn.
5. We now have an algorithm for solving linear equations: a·x= bmod n.
Solution: x= b·a−1 where a−1 is computed using Euclid’s algorithm.
6. How many elements are in Z∗
n? We denote by ϕ(n) the number of elements in Z∗
n. We already
know that ϕ(p) = p−1 for a prime p.
7. One can show that if n= pe1
1 ···pem
m then ϕ(n) = n·∏m
i=1
(
1 −1
pi
)
.
In particular, when n= pq we have that ϕ(n) = (p−1)(q−1) = n−p−q+ 1.
Example: ϕ(15) =
⏐⏐{1,2,4,7,8,11,13,14}
⏐⏐= 8 = 2 ∗4.
1099
8. Euler’s theorem: all a∈Z∗
n satisfy aϕ(n) = 1 in Zn.
note: For primes p Euler’s theorem implies that aϕ(p) = ap−1 = 1 for all a ∈Z∗
p. Hence,
Euler’s theorem is a generalization of Fermat’s theorem.
Structure of Zn
Theorem A.1 (Chinese Remainder Theorem (CRT)). state theorem
Summary
Let n be a 1024 bit integer which is a product of two 512 bit primes. Easy problems in Zn:
1. Generating a random element. Adding and multiplying elements.
2. Computing gr mod n is easy even if r is very large.
3. Inverting an element. Solving linear systems.
Problems that are believed to be hard if the factorization of n is unknown, but become easy if the
factorization of n is known:
1. Finding the prime factors of n.
2. Testing if an element is a QR in Zn.
3. Computing the square root of a QR in Zn. This is provably as hard as factoring n. When the
factorization of n= pq is known one computes the square root of x∈Z∗
n by ﬁrst computing
the square root in Zp of xmod p and the square root in Zq of xmod q and then using the
CRT to obtain the square root of x in Zn.
4. Computing e’th roots modulo n when gcd(e,ϕ(n)) = 1.
5. More generally, solving polynomial equations of degree d >1. This problem is easy if the
factorization of n is known: one ﬁrst ﬁnds the roots of the polynomial equation modulo the
prime factors of n and then uses the CRT to obtain the roots in Zn.
Problems that are believed to be hard in Zn:
1. Let g be a generator of Z∗
n. Given x∈Z∗
n ﬁnd an r such that x= gr mod n. This is known
as the discrete log problem.
2. Let g be a generator of Z∗
n. Given x,y ∈Z∗
n where x= gr1 and y= gr2. Find z= gr1r2. This
is known as the Diﬃe-Hellman problem.
1100
Appendix B
Basic probability theory
Includes a description of statistical distance.
B.1 The birthday Paradox
Theorem B.1. Let Mbe a set of size n and let X1,...,X k be k independent random variables
uniform in M. Let C be the event that for some distinct i,j ∈{1,...,k }we have that Xi = Xj.
Then
(i) Pr[C] ≥1 −e−k(k−1)/2n ≥min
{k(k−1)
4n ,0.63
}
, and
(ii) Pr[C] ≤1 −e−k(k−1)/n when k<n/ 2.
Proof. These all follow easily from the inequality
1 −x≤e−x ≤1 −x/2,
which holds for all x∈[0,1]. 2
Most frequently we will use the lower bound to say that a collision happens with at least a
certain probability. But occasionally we will use the upper bound to argue that collisions do not
happen.
It is well documented that birthdays are not really uniform throughout the year. For example,
in the U.S. the percentage of births in September is higher than in any other month. We show next
that this non-uniformity only increases the probability of collision.
We present a stronger version of the birthday paradox that applies to independent random
variables that are not necessarily uniform in M. We do, however, require that all random variables
are identically distributed. Such random variables are called i.i.d (independent and identically
distributed). This version of the birthday paradox is due to Blom [Blom, D. (1973), ”A birthday
problem”, American Mathematical Monthly, vol. 80, pp. 1141-1142].
Corollary B.2. Let Mbe a set of size n and let X1,...,X k be k i.i.d random variables over M
where k ≥2. Let C be the event that for some distinct i,j ∈{1,...,k }we have that Xi = Xj.
Then
Pr[C] ≥1 −e−k(k−1)/2n ≥min
{k(k−1)
4n ,0.63
}
.
1101
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
 0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000
Collision probabilityNumber of samples (k)
The graph shows that collision probability for n= 106 elements and k ranging from one sample to
5000 samples. It illustrates the threshold phenomenon around the square root. At the square root,√n= 1000, the collision probability is about 0 .4. Already at 4 √n= 4000 the collision probability
is almost 1. At 0 .5√n= 500 the collision probability is small.
Figure B.1: Birthday Paradox
1102
Proof. Let X be a random variable distributed as X1. Let M= {a1,...,a n}and let pi = Pr[X =
ai]. Let I be the set of all k-tuples over Mcontaining distinct elements. Then I contains
(n
k
)
k!
tuples. Since the variables are independent we have that:
Pr[¬C] =
∑
(b1,...,bk)∈I
Pr[X1 = b1 ∧... ∧Xk = bk] =
∑
(b1,...,bk)∈I
k∏
j=1
pbj (B.1)
We show that this sum is maximized when p1 = p2 = ... = pn = 1/n. This will mean that the
probability of collision is minimized when all the variables are uniform. The Corollary will then
follow from Theorem B.1.
Suppose some pi is not 1 /n, say pi < 1/n. Since ∑n
j=1 pi = 1 there must be another pj such
that pj > 1/n. Let ϵ = min((1 /n) −pi, pj −1/n) and note that pj −pi > ϵ. We show that
replacing pi by pi+ ϵand pj by pj−ϵincreases the value of the sum in (B.1). Clearly, the resulting
p1,...,p n still sum to 1. Hence, the resulting p1,...,p n form a distribution over Min which there
is one less value that is not 1 /n. Furthermore, the probability of no collision in this distribution is
greater than in the unmodiﬁed distribution. Repeating this replacement process at most n times
will show that the sum is maximized when all the pi’s are equal to 1/n. Again, this means that the
probability of not getting a collision is maximized when the variables are uniform.
Now, consider the sum in (B.1). There are four types of terms. First, there are terms that
do not contain either pi or pj. These terms are unaﬀected by the change to pi and pj. Second,
there are terms that contain exactly one of pi or pj. These terms pair up. For every k-tuple that
contains i but not j there is a corresponding tuple that contains j but not i. Then the sum of the
corresponding two terms in (B.1) looks like A(pi + ϵ) + A(pj −ϵ) for some A ∈[0,1]. Since this
equals Api + Apj the sum of these two terms is not aﬀected by the change to pi and pj. Finally,
there are terms in (B.1) that contain both pi and pj. These terms change by
B(pi + ϵ)(pj −ϵ) −Bpipj = B[ϵ(pj −pi) −ϵ2]
for some B ∈[0,1]. By deﬁnition of ϵ we know that pj −pi >ϵ and therefore ϵ(pj −pi) −ϵ2 >0.
Hence, the sum with modiﬁed pi and pj is larger than the sum with the unmodiﬁed values.
Overall, we proved that the modiﬁcation to pi and pj increases the value of the sum in (B.1),
as required. This completes the proof of the Corollary. 2
B.1.1 More collision bounds
Consider the sequence xi ←f(xi−1) for a random function f : X→X . Analyze the cycle time of
this walk (needed for Pollard). Now, consider the same sequence for a permutation π : X →X.
Analyze the cycle time (needed for analysis of SecurID identiﬁcation).
B.1.2 A simple distinguisher
We describe a simple algorithm that distinguishes two distributions on strings in {0,1}n. Let
X1,...,X n and Y1,...,Y n be independent random variables taking values in {0,1}. Then
X := (X1,...,X n) and Y := (Y1,...,Y n)
are elements of {0,1}n. Suppose, that for i= 1,...,n we have
Pr[Xi = 1] = p and Pr[ Yi = 1] = (1 + 2ϵ) ·p
1103
for some p∈[0,1] and some small ϵ> 0 so that (1 + 2ϵ) ·p≤1. Then X and Y induce two distinct
distributions on {0,1}n.
We are given an n-bit string T and are told that it is either sampled according to the distribu-
tion X or the distribution Y, so that both p and ϵ are known to us. Our goal is to decide which
distribution T was sampled from. Consider the following simple algorithm A:
input: T = (t1,...,t n) ∈{0,1}n
output: 1 if T is sampled from X and 0 otherwise
s←(1/n) ·∑n
i=1 ti
if s>p ·(1 + ϵ) output 0 else output 1
We are primarily interested in the quantity
∆ :=
⏐⏐Pr[A(Tx) = 1] −Pr[A(Ty) = 1]
⏐⏐ ∈[0,1]
where Tx ←R X and Ty ←R Y. This quantity captures how well Adistinguishes the distributions X
and Y. For a good distinguisher ∆ will be close to 1. For a weak distinguisher ∆ will be close to 0.
The following theorem shows that when n is about 1/(pϵ2) then ∆ is about 1 /2.
Theorem B.3. For all p∈[0,1] and ϵ< 0.3, if n= 4⌈1/(pϵ2)⌉then ∆ >0.5
Proof. The proof follows directly from the Chernoﬀ bound. When T is sampled from Xthe Chernoﬀ
bound implies that
Pr[A(Tx) = 1] = Pr[s>p (1 + ϵ)] ≤e−n·(pϵ2/2) ≤e−2 ≤0.135
When T is sampled from Y then the Chernoﬀ bound implies that
Pr[A(Ty) = 0] = Pr[s<p (1 + ϵ)] ≤e−n·(pϵ2/4) ≤e−1 ≤0.368
Hence, ∆ >|(1 −0.368) −0.135|= 0.503 and the bound follows. 2
1104
Appendix C
Basic complexity theory
To be written.
1105
Appendix D
Probabilistic algorithms
To be written.
1106
Bibliography
[1] M. Abboud and T. Prest. Cryptographic divergences: New techniques and new applications.
Cryptology ePrint Archive, Report 2020/815, 2020. https://eprint.iacr.org/2020/815.
[2] D. Adrian, K. Bhargavan, Z. Durumeric, P. Gaudry, M. Green, J. A. Halderman, N. Heninger,
D. Springall, E. Thom´ e, L. Valenta, B. VanderSloot, E. Wustrow, S. Z. B´ eguelin, and P. Zim-
mermann. Imperfect forward secrecy: how diﬃe-hellman fails in practice. Commun. ACM,
62(1):106–114, 2019.
[3] M. R. Albrecht, K. G. Paterson, and G. J. Watson. Plaintext recovery attacks against SSH.
In 30th IEEE Symposium on Security and Privacy , pages 16–26, 2009.
[4] N. AlFardan, D. Bernstein, K. Paterson, B. Poettering, and J. Schuldt. On the security of
RC4 in TLS. In Proceedings of the 22th USENIX Security Symposium , pages 305–320, 2013.
[5] N. J. AlFardan and K. G. Paterson. Lucky thirteen: Breaking the TLS and DTLS record
protocols. In 2013 IEEE Symposium on Security and Privacy , pages 526–540, 2013.
[6] J. Alwen, B. Chen, K. Pietrzak, L. Reyzin, and S. Tessaro. Scrypt is maximally memory-
hard. In Annual International Conference on the Theory and Applications of Cryptographic
Techniques, pages 33–62. Springer, 2017.
[7] P. Ananth and V. Vaikuntanathan. Optimal bounded-collusion secure functional encryption.
IACR Cryptology ePrint Archive , 2019:314, 2019.
[8] A. Ash, R. Gross, and R. Gross. Elliptic tales: curves, counting, and number theory . Princeton
University Press, 2012.
[9] N. Aviram, S. Schinzel, J. Somorovsky, N. Heninger, M. Dankel, J. Steube, L. Valenta,
D. Adrian, J. A. Halderman, V. Dukhovni, E. K¨ asper, S. Cohney, S. Engels, C. Paar, and
Y. Shavitt. DROWN: Breaking TLS with SSLv2. In 25th USENIX Security Symposium ,
Aug. 2016.
[10] P. S. L. M. Barreto, B. David, R. Dowsley, K. Morozov, and A. C. A. Nascimento. A
framework for eﬃcient adaptively secure composable oblivious transfer in the rom. Cryptology
ePrint Archive, Report 2017/993, 2017. https://eprint.iacr.org/2017/993.
[11] I. Bashmakova. Diophantus and Diophantine equations. Number 20 in Dolciani Mathematical
Expositions. The Mathematical Association of America, 1997.
1107
[12] M. Bellare and T. Kohno. A theoretical treatment of related-key attacks: RKA-PRPs, RKA-
PRFs, and applications. In Proceedings of Eurocrypt ’03, volume 2656 of LNCS. Springer-
Verlag, 2003.
[13] M. Bellare, T. Ristenpart, and S. Tessaro. Multi-instance security and its application to
password-based cryptography. In CRYPTO 2012, pages 312–329. Springer, 2012.
[14] M. Bellare and P. Rogaway. Collision-resistant hashing: Towards making UOWHFs practical.
In Proceedings of Crypto ’97, volume 1294 of LNCS. Springer-Verlag, 1997.
[15] S. M. Bellovin. Frank miller: Inventor of the one-time pad. Cryptologia, 35(3):203–222, 2011.
[16] K. Bentahar, P. Farshim, J. Malone-Lee, and N. P. Smart. Generic constructions of identity-
based and certiﬁcateless kems. J. Cryptology, 21(2):178–199, 2008.
[17] D. Bernstein and T. Lange. Montgomery curves and the montgomery ladder. In J. Bos and
A. Lenstra, editors, Topics in Computational Number Theory Inspired by Peter L. Mont-
gomery. Cambridge University Press, 2017. Available at eprint.iacr.org/2017/293.
[18] D. J. Bernstein. Curve25519: new Diﬃe-Hellman speed records. In International Workshop
on Public Key Cryptography , pages 207–228. Springer, 2006.
[19] K. Bhargavan and G. Leurent. On the practical (in-)security of 64-bit block ciphers: Collision
attacks on HTTP over TLS and openvpn. In Proceedings of the 2016 ACM SIGSAC Con-
ference on Computer and Communications Security, Vienna, Austria, October 24-28, 2016 ,
pages 456–467, 2016.
[20] E. Biham and R. Anderson. Tiger: a fast new hash function. In Proceedings of Fast Software
Encryption (FSE) ’96 , volume 1039 of LNCS. Springer-Verlag, 1996.
[21] A. Biryukov and D. Khovratovich. Related-key cryptanalysis of the full AES-192 and AES-
256. In ASIACRYPT 2009, pages 1–18. 2009.
[22] A. Bishop, A. Jain, and L. Kowalczyk. Function-hiding inner product encryption. In ASI-
ACRYPT 2015, pages 470–491, 2015.
[23] J. Black and P. Rogaway. A block-cipher mode of operation for parallelizable message au-
thentication. In Proceedings of Eurocrypt ’02, volume 2332 of LNCS. Springer-Verlag, 2002.
[24] J. Black, P. Rogaway, and T. Shrimpton. Black-box analysis of the block-cipher-based
hash-function constructions from pgv. In Proceedings of Crypto ’02 , volume 2442 of LNCS.
Springer-Verlag, 2002.
[25] K. K. Bodo M¨ oller, Thai Duong. This poodle bites: Exploiting the ssl 3.0 fallback. Google
security advisory, 2014.
[26] A. Bogdanov, D. Khovratovich, and C. Rechberger. Biclique cryptanalysis of the full AES.
In ASIACRYPT 2011, pages 344–371. 2011.
[27] A. Boldyreva, J. P. Degabriele, K. G. Paterson, and M. Stam. Security of symmetric en-
cryption in the presence of ciphertext fragmentation. In Proceedings of Eurocrypt’12, pages
682–699, 2012.
1108
[28] D. Boneh. Simpliﬁed OAEP for the RSA and rabin functions. In CRYPTO 2001, pages
275–291, 2001.
[29] D. Boneh and X. Boyen. Short signatures without random oracles and the SDH assumption
in bilinear groups. J. Cryptology, 21(2):149–177, 2008. Conference version in Eurocrypt 2004.
[30] D. Boneh, X. Boyen, and S. Halevi. Chosen ciphertext secure public key threshold encryption
without random oracles. In Topics in Cryptology - CT-RSA 2006, The Cryptographers’ Track
at the RSA Conference 2006, San Jose, CA, USA, February 13-17, 2006, Proceedings , pages
226–243, 2006.
[31] D. Boneh, B. B¨ unz, and B. Fisch. Batching techniques for accumulators with applications to
iops and stateless blockchains. In CRYPTO 2019, pages 561–586, 2019.
[32] D. Boneh, R. Canetti, S. Halevi, and J. Katz. Chosen-ciphertext security from identity-based
encryption. SIAM J. Comput. , 36(5):1301–1328, 2007.
[33] D. Boneh, M. Drijvers, and G. Neven. Compact multi-signatures for smaller blockchains.
In ASIACRYPT 2018, volume 11273 of Lecture Notes in Computer Science , pages 435–464.
Springer, 2018.
[34] D. Boneh, C. Gentry, S. Gorbunov, S. Halevi, V. Nikolaenko, G. Segev, V. Vaikuntanathan,
and D. Vinayagamurthy. Fully key-homomorphic encryption, arithmetic circuit ABE and
compact garbled circuits. In EUROCRYPT 2014, pages 533–556, 2014.
[35] D. Boneh and M. Zhandry. Multiparty key exchange, eﬃcient traitor tracing, and more from
indistinguishability obfuscation. Algorithmica, 79(4):1233–1285, 2017. Conference version in
Crypto 2014.
[36] J. Bonneau and I. Mironov. Cache-collision timing attacks against AES. In in Proc. Crypto-
graphic Hardware and Embedded Systems (CHES) 2006. Lecture Notes in Computer Science ,
pages 201–215. Springer, 2006.
[37] W. Bosma and P. Stevenhagen. On the computation of quadratic 2-class groups. Journal de
Theorie des Nombres, 1996.
[38] Z. Brakerski and G. Segev. Function-private functional encryption in the private-key setting.
J. Cryptology, 31(1):202–225, 2018.
[39] J. A. Buchmann, E. Dahmen, and M. Schneider. Merkle tree traversal revisited. PQCrypto,
8:63–78, 2008.
[40] R. Canetti, O. Goldreich, and S. Halevi. The random oracle methodology, revisited. J. ACM,
51(4):557–594, July 2004.
[41] S. Chen, R. Wang, X. Wang, and K. Zhang. Side-channel leaks in web applications: A reality
today, a challenge tomorrow. In 31st IEEE Symposium on Security and Privacy, S&P 2010,
16-19 May 2010, Berleley/Oakland, California, USA , pages 191–206, 2010.
[42] J. H. Cheon. Security analysis of the strong diﬃe-hellman problem. In EUROCRYPT 2006,
pages 1–11, 2006.
1109
[43] H. Cohen. A course in computational algebraic number theory . Graduate texts in mathemat-
ics. Springer, 2010.
[44] D. Coppersmith. The data encryption standard and its strength against attack. IBM Journal
of Research and Development , 38(3), 1994.
[45] D. Coppersmith. Finding a small root of a bivariate integer equation; factoring with high bits
known. In EUROCRYPT 1996, volume 1070 of Lecture Notes in Computer Science , pages
178–189. Springer, 1996.
[46] J. Coron. Security proof for partial-domain hash signature schemes. In CRYPTO 2002, pages
613–626, 2002.
[47] V. Costan and S. Devadas. Intel sgx explained. IACR Cryptology ePrint Archive , 2016:086,
2016.
[48] N. T. Courtois, P. Emirdag, and F. Valsorda. Private key recovery combination attacks:
On extreme fragility of popular bitcoin key management, wallet and cold storage solutions
in presence of poor rng events. Cryptology ePrint Archive, Report 2014/848, 2014. http:
//eprint.iacr.org/2014/848.
[49] J. Daemen and V. Rijmen. The Design of Rijndael: AES - The Advanced Encryption Stan-
dard. Springer, 2002.
[50] S. Data. Announcing our worst passwords of 2016, 2017.
[51] Y. Desmedt and A. Oldyzko. A chosen text attack on the rsa cryptosystem and some dis-
crete logarithm schemes. In Proceedings of Crypto ’85, volume 218 of LNCS, pages 516–521.
Springer-Verlag, 1985.
[52] T. Dierks and C. Allen. The TLS protocol version 1.0. Internet RFC 2246, 1999.
[53] I. Dinur, O. Dunkelman, N. Keller, and A. Shamir. Eﬃcient dissection of composite prob-
lems, with applications to cryptanalysis, knapsacks, and combinatorial search problems. In
CRYPTO 2012, pages 719–740. 2012.
[54] Y. Dodis, S. Guo, and J. Katz. Fixing cracks in the concrete: Random oracles with auxiliary
input, revisited. In EUROCRYPT 2017, pages 473–495, 2017.
[55] Y. Dodis, E. Kiltz, K. Pietrzak, and D. Wichs. Message authentication, revisited. In EURO-
CRYPT 2012, volume 7237, pages 355–374. Springer, 2012.
[56] D. Dolev, C. Dwork, and M. Naor. Nonmalleable cryptography. SIAM J. Comput., 30(2):391–
437, 2000.
[57] O. Dunkelman and N. Keller. The eﬀects of the omission of last round’s mixcolumns on AES.
Inf. Process. Lett., 110(8-9):304–308, 2010.
[58] T. Duong and J. Rizzo. Here come the ⊕ninjas, 2011. See also en.wikipedia.org/wiki/
Transport_Layer_Security#BEAST_attack.
1110
[59] F. B. Durak, T. M. DuBuisson, and D. Cash. What else is revealed by order-revealing
encryption? In Proceedings of ACM CCS, pages 1155–1166, 2016.
[60] Ethereum 2.0 speciﬁcations, 2019. https://github.com/ethereum/eth2.0-specs.
[61] A. Fiat and M. Naor. Rigorous time/space tradeoﬀs for inverting functions. In STOC’91,
pages 534–541. ACM, 1991.
[62] J. Fischer and J. Stern. An eﬃcient pseudo-random generator provably as secure as syndrome
decoding. In EUROCRYPT ’96, pages 245–255, 1996.
[63] S. Fluhrer, I. Mantin, and A. Shamir. Weaknesses in the key scheduling algorithm of RC4.
In proceedings of selected areas of cryptography (SAC), pages 1–24, 2001.
[64] S. Fluhrer and D. McGrew. Statistical analysis of the alleged RC4 keystream generator. In
Proceedings of FSE 2000, volume 1978 of LNCS. Springer-Verlag, 2000.
[65] A. M. Frieze, R. Kannan, and J. C. Lagarias. Linear congruential generators do not produce
random sequences. In FOCS, pages 480–484, 1984.
[66] E. Fujisaki, T. Okamoto, D. Pointcheval, and J. Stern. RSA-OAEP is secure under the RSA
assumption. J. Cryptology, 17(2):81–104, 2004.
[67] S. Galbraith. Mathematics of Public Key Cryptography . Cambridge University Press, 2012.
[68] S. Garﬁnkel and A. Shelat. Remembrance of data passed: A study of disk sanitization
practices. IEEE Security and Privacy , January 2003.
[69] C. Garman, M. Green, G. Kaptchuk, I. Miers, and M. Rushanan. Dancing on the lip of the
volcano: Chosen ciphertext attacks against apple imessage. In USENIX Security Symposium,
2017.
[70] P. Gaudry. Index calculus for abelian varieties of small dimension and the elliptic curve
discrete logarithm problem. J. Symb. Comput. , 44(12):1690–1702, 2009.
[71] D. Genkin, A. Shamir, and E. Tromer. RSA key extraction via low-bandwidth acoustic
cryptanalysis. In CRYPTO 2014, pages 444–461, 2014.
[72] M. Georgiev, S. Iyengar, S. Jana, R. Anubhai, D. Boneh, and V. Shmatikov. The most
dangerous code in the world: validating SSL certiﬁcates in non-browser software. In the
ACM Conference on Computer and Communications Security, CCS’12 , pages 38–49, 2012.
[73] V. Gligor and P. Donescu. Fast encryption and authentication: XCBC encryption and XECB
authentication modes. In Proceedings of fast software encryption (FSE) ’01 , volume 2355 of
LNCS, pages 92–108. Springer-Verlag, 2001.
[74] S. Goldwasser, S. D. Gordon, V. Goyal, A. Jain, J. Katz, F. Liu, A. Sahai, E. Shi, and
H. Zhou. Multi-input functional encryption. In EUROCRYPT 2014, pages 578–602, 2014.
[75] A. Golynski. Cell probe lower bounds for succinct data structures. In Proceedings of the
twentieth Annual ACM-SIAM Symposium on Discrete Algorithms , pages 625–634. Society
for Industrial and Applied Mathematics, 2009.
1111
[76] M. T. Goodrich, C. Papamanthou, and R. Tamassia. On the cost of persistence and authen-
tication in skip lists. In Experimental Algorithms, 6th International Workshop, WEA 2007,
Rome, Italy, June 6-8, 2007, Proceedings , pages 94–107, 2007.
[77] M. T. Goodrich, J. Z. Sun, and R. Tamassia. Eﬃcient tree-based revocation in groups of
low-state devices. In CRYPTO 2004, pages 511–527, 2004.
[78] V. Goyal, O. Pandey, A. Sahai, and B. Waters. Attribute-based encryption for ﬁne-grained
access control of encrypted data. In Proceedings of the 13th ACM Conference on Computer
and Communications Security, CCS 2006, Alexandria, VA, USA, Ioctober 30 - November 3,
2006, pages 89–98, 2006.
[79] M. Green and S. Hohenberger. Universally composable adaptive oblivious transfer. In ASI-
ACRYPT 2008, volume 5350 of LNCS, pages 179–197. Springer, 2008.
[80] L. K. Grover. A fast quantum mechanical algorithm for database search. In Proceedings of
the twenty-eighth annual ACM symposium on Theory of computing , pages 212–219. ACM,
1996.
[81] T. Guneysu, T. Kasper, M. Novotny, C. Paar, and A. Rupp. Cryptanalysis with copacobana.
Computers, IEEE Transactions on , 57(11):1498–1513, 2008.
[82] S. Halevi and H. Krawczyk. Strengthening digital signatures via randomized hashing. In
CRYPTO 2006, pages 41–59, 2006.
[83] D. Halevy and A. Shamir. The LSD broadcast encryption scheme. In CRYPTO 2002, pages
47–60, 2002.
[84] G. Hardy and E. Wright. An Introduction to the Theory of Numbers . Clarendon Press, 1979.
[85] J. Haynes and H. Klehr. Venona: Decoding Soviet Espionage in America . Yale University
Press, 1999.
[86] N. Heninger, Z. Durumeric, E. Wustrow, and J. A. Halderman. Mining your Ps and Qs:
Detection of widespread weak keys in network devices. In T. Kohno, editor,USENIX Security,
pages 205–220. USENIX Association, 2012.
[87] A. Herzberg, S. Jarecki, H. Krawczyk, and M. Yung. Proactive secret sharing or: How to
cope with perpetual leakage. In CRYPTO 1995, pages 339–352, 1995.
[88] J. Hoﬀstein, J. Pipher, and J. Silverman. NTRU: A ring-based public key cryptosystem. In
Algorithmic Number Theory (ANTS) , pages 267–288, 1998.
[89] S. Hohenberger, A. B. Lewko, and B. Waters. Detecting dangerous queries: A new approach
for chosen ciphertext security. In Eurocrypt 2012, pages 663–681, 2012.
[90] N. Howgrave-Graham. Finding small roots of univariate modular equations revisited. In
Cryptography and Coding , volume 1355 of Lecture Notes in Computer Science , pages 131–
142. Springer, 1997.
[91] R. Impagliazzo and M. Naor. Eﬃcient cryptographic schemes provably as secure as subset
sum. In 30th Annual Symposium on Foundations of Computer Science , pages 236–241, 1989.
1112
[92] R. Impagliazzo and S. Rudich. Limits on the provable consequences of one-way permutations.
In Proceedings of the Symposium on Theory of Computing (STOC) , pages 44–61, 1989.
[93] T. Iwata and K. Kurosawa. OMAC: One-key CBC MAC. In Proceedings of fast software
encryption (FSE) ’03 , volume 2887 of LNCS, pages 129–153. Springer-Verlag, 2003.
[94] J. Jaeger and S. Tessaro. Expected-time cryptography: Generic techniques and applications
to concrete soundness. In TCC 2020 , volume 12552 of Lecture Notes in Computer Science ,
pages 414–443. Springer, 2020.
[95] J. Jonsson and B. Kaliski. On the security of RSA encryption in TLS. In CRYPTO 2002,
pages 127–142, 2002.
[96] M. Joye and M. Tunstall, editors. Fault Analysis in Cryptography. Information Security and
Cryptography. Springer, 2012.
[97] P. Junod. On the complexity of matsui’s attack. In Selected Areas in Cryptography (SAC) ,
pages 199–211, 2001.
[98] J. Katz, A. Sahai, and B. Waters. Predicate encryption supporting disjunctions, polynomial
equations, and inner products. J. Cryptology, 26(2):191–224, 2013.
[99] P. C. Kocher, J. Jaﬀe, B. Jun, and P. Rohatgi. Introduction to diﬀerential power analysis.
J. Cryptographic Engineering, 1(1):5–27, 2011.
[100] D. Kogan, N. Manohar, and D. Boneh. T/key: Second-factor authentication from secure
hash chains. In the ACM Conference on Computer and Communications Security, CCS’17 ,
2017.
[101] A. Langley. Maintaining digital certiﬁcate security, 2014. security.googleblog.com/2014/
07/maintaining-digital-certificate-security.html.
[102] J. Len, P. Grubbs, and T. Ristenpart. Partitioning oracle attacks. IACR Cryptol. ePrint
Arch., 2020:1491, 2020.
[103] A. Lenstra. Key lengths. Wiley, 2005.
[104] A. K. Lenstra, J. P. Hughes, M. Augier, J. W. Bos, T. Kleinjung, and C. Wachter. Public
keys. In CRYPTO 2012, volume 7417, pages 626–642. Springer, 2012.
[105] S. Lucks. Attacking triple encryption. In Proceedings of Fast Software Encryption 1998 ,
LNCS, pages 239–253, 1998.
[106] J. Manger. A chosen ciphertext attack on RSA optimal asymmetric encryption padding
(OAEP) as standardized in PKCS #1 v2.0. In CRYPTO 2001, pages 230–238, 2001.
[107] I. Mantin and A. Shamir. A practical attack on broadcast RC4. In Proceedings of FSE 2001.
Springer-Verlag, 2001.
[108] M. Matsui. The ﬁrst experimental cryptanalysis of the data encryption standard. In Pro-
ceedings of Crypto’94, pages 1–11, 1994.
1113
[109] M. Matsui. Linear cryptanalysis method for des cipher. In Proceedings of Eurocrypt’93, pages
386–397, 1994.
[110] U. Maurer and S. Wolf. The relationship between breaking the diﬃe-hellman protocol and
computing discrete logarithms. SIAM J. Comput. , 28(5):1689–1721, 1999.
[111] R. McEliece. A public-key cryptosystem based on algebraic coding theory. DSN Progress
Report, 44:114–116, 1978.
[112] R. Merkle and M. Hellman. On the security of multiple encryption. Communications of the
ACM, 24:465–467, 1981.
[113] Erroneous verisign-issued digital certiﬁcates pose spooﬁng hazard. Microsoft Security Bulletin
MS01-017, 2001.
[114] I. Mironov. Hash functions: From merkle-damgard to shoup. In Proceedings of Eurocrypt
’01, volume 2045 of LNCS, pages 166–181. Springer-Verlag, 2001.
[115] F. Monrose, M. K. Reiter, and S. Wetzel. Password hardening based on keystroke dynamics.
International Journal of Information Security , 1(2):69–83, 2002.
[116] T. Moran, M. Naor, and G. Segev. An optimally fair coin toss. In TCC, pages 1–18, 2009.
[117] S. Murdoch. Hot or not: Revealing hidden services by their clock skew. In Proceedings of the
13th ACM Conference on Computer and Communications Security , pages 27–36, 2006.
[118] S. Myers and A. Shelat. Bit encryption is complete. In FOCS 2009, pages 607–616, 2009.
[119] C. Namprempre, P. Rogaway, and T. Shrimpton. Reconsidering generic composition. In
EUROCRYPT 2014, pages 257–274, 2014.
[120] M. Nandi and T. Pandit. Generic conversions from CPA to CCA secure functional encryption.
IACR Cryptology ePrint Archive , 2015:457, 2015.
[121] D. Naor, M. Naor, and J. Lotspiech. Revocation and tracing schemes for stateless receivers.
In CRYPTO 2001, pages 41–62, 2001.
[122] M. Naor. Bit commitment using pseudo-randomness. In CRYPTO 1989 , pages 128–136,
1989.
[123] R. Napier. RNCryptor HMAC Vulnerability, 2013. http://robnapier.net/
rncryptor-hmac-vulnerability.
[124] M. Nemec, M. S´ ys, P. Svenda, D. Klinec, and V. Matyas. The return of coppersmith’s attack:
Practical factorization of widely used RSA moduli. In B. M. Thuraisingham, D. Evans,
T. Malkin, and D. Xu, editors, ACM CCS’17, pages 1631–1648. ACM, 2017.
[125] M. Nielsen and I. Chuang. Quantum Computation and Quantum Information . Cambridge
University Press, 2011.
[126] K. Nissim and M. Naor. Certiﬁcate revocation and certiﬁcate update. In Proceedings of the
7th USENIX Security Symposium, San Antonio, TX, USA, January 26-29, 1998 , 1998.
1114
[127] Nist recommendation for key management part 1: General, 2005.
[128] Recommendation for block cipher modes of operation: The ccm mode for authentication and
conﬁdentiality, 2004.
[129] P. Patton. The bucklands boys and other tales of the atm. Wired magazine, 1993. www.
wired.com/1993/05/atm-2.
[130] H. Poincar´ e. Sur les propri´ et´ es arithm´ etiques des courbes alg´ ebriques. Journal de
math´ ematiques pures et appliqu´ ees, 7:161–234, 1901.
[131] A. Prado, N. Harris, and Y. Gluck. SSL, gone in 30 seconds: a BREACH beyond CRIME,
2013.
[132] B. Preneel, R. Govaerts, and J. Vandewalle. Hash functions based on block ciphers: a
synthetic approach. In Proceedings of Crypto ’93 , volume 773 of LNCS. Springer-Verlag,
1993.
[133] T. Prest. Sharper bounds in lattice-based cryptography using the r´ enyi divergence. In ASI-
ACRYPT 2017, volume 10624 of LNCS, pages 347–374, 2017.
[134] R. Rivest. The MD4 message digest algorithm. In Proceedings of Crypto ’90, volume 537 of
LNCS. Springer-Verlag, 1990.
[135] R. Rivest. The MD5 message digest algorithm. Internet RFC 1321, 1992.
[136] J. Rizzo and T. Duong. The CRIME attack. Presentation at Ekoparty 2012, 2012.
[137] L. Roy. SoftSpokenOT: Quieter OT extension from small-ﬁeld silent VOLE in the minicrypt
model. In CRYPTO ’22, volume 13507 of Lecture Notes in Computer Science, pages 657–687.
Springer, 2022.
[138] M. Sabt and J. Traor´ e. Breaking into the keystore: A practical forgery attack against android
keystore. Cryptology ePrint Archive, Report 2016/677, 2016. http://eprint.iacr.org/
2016/677.
[139] A. Sahai and B. Waters. How to use indistinguishability obfuscation: deniable encryption,
and more. In Symposium on Theory of Computing, STOC 2014, New York, NY, USA, May
31 - June 03, 2014 , pages 475–484, 2014.
[140] B. Schneier and Mudge. Cryptanalysis of microsoft’s point-to-point tunneling protocol
(PPTP). In Proceedings of the 5th ACM Conference on Communications and Computer
Security, 1998.
[141] R. Schoof. Elliptic curves over ﬁnite ﬁelds and the computation of square roots mod p.
Mathematics of computation , 44(170):483–494, 1985.
[142] P. Schoppmann, A. Gasc´ on, L. Reichert, and M. Raykova. Distributed vector-ole: Improved
constructions and implementation. In ACM CCS, pages 1055–1072. ACM, 2019.
[143] M. Scott. On the eﬃcient implementation of pairing-based protocols. In IMA International
Conference on Cryptography and Coding , pages 296–308. Springer, 2011.
1115
[144] M. Scott. Pairing implementation revisited. Cryptology ePrint Archive, Report 2019/077,
2019. https://eprint.iacr.org/2019/077.
[145] V. Shoup. Lower bounds for discrete logarithms and related problems. In EUROCRYPT
1997, pages 256–266, 1997.
[146] V. Shoup. A composition theorem for universal one-way hash functions. In Proceedings of
Eurocrypt ’00, volume 1807 of LNCS, pages 445–452. Springer-Verlag, 2000.
[147] V. Shoup. OAEP reconsidered. J. Cryptology, 15(4):223–249, 2002.
[148] V. Shoup. A computational introduction to number theory and algebra. Cambridge University
Press, 2008. 2nd edition.
[149] D. Song, D. Wagner, and X. Tian. Timing analysis of keystrokes and timing attacks on SSH.
In 10th USENIX Security Symposium , 2001.
[150] F.-X. Standaert, G. Piret, and J.-J. Quisquater. Cryptanalysis of block ciphers: A survey.
UCL Crypto Group , 2003.
[151] A. Stubbleﬁeld, J. Ioannidis, and A. Rubin. A key recovery attack on the 802.11b wired
equivalent privacy protocol (WEP). ACM Transactions on Information. Systems Security ,
7(2):319–332, 2004.
[152] M. Szydlo. Merkle tree traversal in log space and time. In Eurocrypt, volume 3027, pages
541–554. Springer, 2004.
[153] B. Vall´ ee. Gauss’ algorithm revisited.J. Algorithms, 12(4):556–572, 1991.
[154] P. C. van Oorschot and M. J. Wiener. Parallel collision search with application to hash
functions and discrete logarithms. In Proceedings of the 2nd ACM Conference on Computer
and Communications Security, pages 210–218, 1994.
[155] X. Wang, A. Yao, and F. Yao. New collision search for SHA-1. Rump Session Crypto’05,
2005.
[156] A.-C. Yao. Coherent functions and program checkers. In Proceedings of the twenty-second
annual ACM symposium on Theory of computing , pages 84–94. ACM, 1990.
1116
